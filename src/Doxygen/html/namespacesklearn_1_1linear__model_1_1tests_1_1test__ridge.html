<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.linear_model.tests.test_ridge Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model.html">linear_model</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests.html">tests</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html">test_ridge</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">sklearn.linear_model.tests.test_ridge Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ae7e934ffb91f7ec7239615ad1a93b533" id="r_ae7e934ffb91f7ec7239615ad1a93b533"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ae7e934ffb91f7ec7239615ad1a93b533">DENSE_FILTER</a> (X)</td></tr>
<tr class="separator:ae7e934ffb91f7ec7239615ad1a93b533"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8c2b7a860ad327adb8eea53a9f5f6863" id="r_a8c2b7a860ad327adb8eea53a9f5f6863"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a8c2b7a860ad327adb8eea53a9f5f6863">SPARSE_FILTER</a> (X)</td></tr>
<tr class="separator:a8c2b7a860ad327adb8eea53a9f5f6863"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a729df6979d25432f8b396ffef3ba6e4c" id="r_a729df6979d25432f8b396ffef3ba6e4c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a729df6979d25432f8b396ffef3ba6e4c">_accuracy_callable</a> (y_test, y_pred)</td></tr>
<tr class="separator:a729df6979d25432f8b396ffef3ba6e4c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aca5473f7fc1009ff71c1b2dbb018ad67" id="r_aca5473f7fc1009ff71c1b2dbb018ad67"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aca5473f7fc1009ff71c1b2dbb018ad67">_mean_squared_error_callable</a> (y_test, y_pred)</td></tr>
<tr class="separator:aca5473f7fc1009ff71c1b2dbb018ad67"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a90d507dbc605d30d323679761a6d808c" id="r_a90d507dbc605d30d323679761a6d808c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a90d507dbc605d30d323679761a6d808c">ols_ridge_dataset</a> (global_random_seed, request)</td></tr>
<tr class="separator:a90d507dbc605d30d323679761a6d808c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4bf6b96f2996b73688507fce991f2622" id="r_a4bf6b96f2996b73688507fce991f2622"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a4bf6b96f2996b73688507fce991f2622">test_ridge_regression</a> (solver, fit_intercept, <a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a90d507dbc605d30d323679761a6d808c">ols_ridge_dataset</a>, global_random_seed)</td></tr>
<tr class="separator:a4bf6b96f2996b73688507fce991f2622"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a42b3764169d6c2c4d40a8abedc81a472" id="r_a42b3764169d6c2c4d40a8abedc81a472"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a42b3764169d6c2c4d40a8abedc81a472">test_ridge_regression_hstacked_X</a> (solver, fit_intercept, <a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a90d507dbc605d30d323679761a6d808c">ols_ridge_dataset</a>, global_random_seed)</td></tr>
<tr class="separator:a42b3764169d6c2c4d40a8abedc81a472"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae2eb503ce5ebda6d5d75221cbc22d49e" id="r_ae2eb503ce5ebda6d5d75221cbc22d49e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ae2eb503ce5ebda6d5d75221cbc22d49e">test_ridge_regression_vstacked_X</a> (solver, fit_intercept, <a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a90d507dbc605d30d323679761a6d808c">ols_ridge_dataset</a>, global_random_seed)</td></tr>
<tr class="separator:ae2eb503ce5ebda6d5d75221cbc22d49e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa04f02536a05b012ab63d026d16272af" id="r_aa04f02536a05b012ab63d026d16272af"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aa04f02536a05b012ab63d026d16272af">test_ridge_regression_unpenalized</a> (solver, fit_intercept, <a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a90d507dbc605d30d323679761a6d808c">ols_ridge_dataset</a>, global_random_seed)</td></tr>
<tr class="separator:aa04f02536a05b012ab63d026d16272af"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2fd19bb2eb4d843fadedaa48c626d0f6" id="r_a2fd19bb2eb4d843fadedaa48c626d0f6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a2fd19bb2eb4d843fadedaa48c626d0f6">test_ridge_regression_unpenalized_hstacked_X</a> (solver, fit_intercept, <a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a90d507dbc605d30d323679761a6d808c">ols_ridge_dataset</a>, global_random_seed)</td></tr>
<tr class="separator:a2fd19bb2eb4d843fadedaa48c626d0f6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0480dae63456e7f5917979d4bc43a4e8" id="r_a0480dae63456e7f5917979d4bc43a4e8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a0480dae63456e7f5917979d4bc43a4e8">test_ridge_regression_unpenalized_vstacked_X</a> (solver, fit_intercept, <a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a90d507dbc605d30d323679761a6d808c">ols_ridge_dataset</a>, global_random_seed)</td></tr>
<tr class="separator:a0480dae63456e7f5917979d4bc43a4e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf42c34730eabac1a5067679900ffb28" id="r_abf42c34730eabac1a5067679900ffb28"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#abf42c34730eabac1a5067679900ffb28">test_ridge_regression_sample_weights</a> (solver, fit_intercept, sparseX, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>, <a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a90d507dbc605d30d323679761a6d808c">ols_ridge_dataset</a>, global_random_seed)</td></tr>
<tr class="separator:abf42c34730eabac1a5067679900ffb28"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aef45afbbf6c1360d7566105b7861222a" id="r_aef45afbbf6c1360d7566105b7861222a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aef45afbbf6c1360d7566105b7861222a">test_primal_dual_relationship</a> ()</td></tr>
<tr class="separator:aef45afbbf6c1360d7566105b7861222a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af022883948abb70a94d06b8c03d55168" id="r_af022883948abb70a94d06b8c03d55168"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#af022883948abb70a94d06b8c03d55168">test_ridge_regression_convergence_fail</a> ()</td></tr>
<tr class="separator:af022883948abb70a94d06b8c03d55168"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aee6699e551c0586d545874d9421b6fa0" id="r_aee6699e551c0586d545874d9421b6fa0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aee6699e551c0586d545874d9421b6fa0">test_ridge_shapes_type</a> ()</td></tr>
<tr class="separator:aee6699e551c0586d545874d9421b6fa0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae9662805b3391dc2e31a4baac38862ac" id="r_ae9662805b3391dc2e31a4baac38862ac"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ae9662805b3391dc2e31a4baac38862ac">test_ridge_intercept</a> ()</td></tr>
<tr class="separator:ae9662805b3391dc2e31a4baac38862ac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6e3a54e9990bd428910212a5d98e3289" id="r_a6e3a54e9990bd428910212a5d98e3289"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a6e3a54e9990bd428910212a5d98e3289">test_ridge_vs_lstsq</a> ()</td></tr>
<tr class="separator:a6e3a54e9990bd428910212a5d98e3289"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a532d5f8767c3d9002edf7cbed411aec5" id="r_a532d5f8767c3d9002edf7cbed411aec5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a532d5f8767c3d9002edf7cbed411aec5">test_ridge_individual_penalties</a> ()</td></tr>
<tr class="separator:a532d5f8767c3d9002edf7cbed411aec5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad8cf170c1f38434250f72ee9b0162371" id="r_ad8cf170c1f38434250f72ee9b0162371"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ad8cf170c1f38434250f72ee9b0162371">test_X_CenterStackOp</a> (n_col)</td></tr>
<tr class="separator:ad8cf170c1f38434250f72ee9b0162371"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae719e559f5d94e0e61fac6245586f0a8" id="r_ae719e559f5d94e0e61fac6245586f0a8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ae719e559f5d94e0e61fac6245586f0a8">test_compute_gram</a> (shape, uniform_weights)</td></tr>
<tr class="separator:ae719e559f5d94e0e61fac6245586f0a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf17982f3ee6889ad5a43773e3887659" id="r_aaf17982f3ee6889ad5a43773e3887659"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aaf17982f3ee6889ad5a43773e3887659">test_compute_covariance</a> (shape, uniform_weights)</td></tr>
<tr class="separator:aaf17982f3ee6889ad5a43773e3887659"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a6ac7663e93c8a9fba592f5802ea8f0" id="r_a9a6ac7663e93c8a9fba592f5802ea8f0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a9a6ac7663e93c8a9fba592f5802ea8f0">_make_sparse_offset_regression</a> (n_samples=100, n_features=100, proportion_nonzero=0.5, n_informative=10, n_targets=1, bias=13.0, X_offset=30.0, noise=30.0, shuffle=True, coef=False, positive=False, random_state=None)</td></tr>
<tr class="separator:a9a6ac7663e93c8a9fba592f5802ea8f0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3e1d30548804d649c9d64a2cb4175933" id="r_a3e1d30548804d649c9d64a2cb4175933"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a3e1d30548804d649c9d64a2cb4175933">test_solver_consistency</a> (solver, proportion_nonzero, n_samples, dtype, sparse_X, seed)</td></tr>
<tr class="separator:a3e1d30548804d649c9d64a2cb4175933"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afb2365a5ae4d88b70d6f4db53961afdd" id="r_afb2365a5ae4d88b70d6f4db53961afdd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#afb2365a5ae4d88b70d6f4db53961afdd">test_ridge_gcv_vs_ridge_loo_cv</a> (gcv_mode, X_constructor, X_shape, y_shape, fit_intercept, noise)</td></tr>
<tr class="separator:afb2365a5ae4d88b70d6f4db53961afdd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1270a62a81a6a4f2dec065e90967720b" id="r_a1270a62a81a6a4f2dec065e90967720b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a1270a62a81a6a4f2dec065e90967720b">test_ridge_loo_cv_asym_scoring</a> ()</td></tr>
<tr class="separator:a1270a62a81a6a4f2dec065e90967720b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aba9cdead74873310486511879657c5cc" id="r_aba9cdead74873310486511879657c5cc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aba9cdead74873310486511879657c5cc">test_ridge_gcv_sample_weights</a> (gcv_mode, X_constructor, fit_intercept, n_features, y_shape, noise)</td></tr>
<tr class="separator:aba9cdead74873310486511879657c5cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4c67bbde5440f0f64d6439c0bf09618c" id="r_a4c67bbde5440f0f64d6439c0bf09618c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a4c67bbde5440f0f64d6439c0bf09618c">test_check_gcv_mode_choice</a> (sparse, mode, mode_n_greater_than_p, mode_p_greater_than_n)</td></tr>
<tr class="separator:a4c67bbde5440f0f64d6439c0bf09618c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9b812a4460e51c5864e9dd20b4017c07" id="r_a9b812a4460e51c5864e9dd20b4017c07"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a9b812a4460e51c5864e9dd20b4017c07">_test_ridge_loo</a> (filter_)</td></tr>
<tr class="separator:a9b812a4460e51c5864e9dd20b4017c07"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a76cfee83d153dac2dd1085462a6976a4" id="r_a76cfee83d153dac2dd1085462a6976a4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a76cfee83d153dac2dd1085462a6976a4">_test_ridge_cv</a> (filter_)</td></tr>
<tr class="separator:a76cfee83d153dac2dd1085462a6976a4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6edb0e4dc0ca2cd2555e1fb0445d0b3b" id="r_a6edb0e4dc0ca2cd2555e1fb0445d0b3b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a6edb0e4dc0ca2cd2555e1fb0445d0b3b">test_ridge_gcv_cv_values_not_stored</a> (ridge, make_dataset)</td></tr>
<tr class="separator:a6edb0e4dc0ca2cd2555e1fb0445d0b3b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2dc851e27e61bd1c849d28e7617ad565" id="r_a2dc851e27e61bd1c849d28e7617ad565"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a2dc851e27e61bd1c849d28e7617ad565">test_ridge_best_score</a> (ridge, make_dataset, cv)</td></tr>
<tr class="separator:a2dc851e27e61bd1c849d28e7617ad565"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acb04ee82f1e72e8879915690eb95f493" id="r_acb04ee82f1e72e8879915690eb95f493"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#acb04ee82f1e72e8879915690eb95f493">test_ridge_cv_individual_penalties</a> ()</td></tr>
<tr class="separator:acb04ee82f1e72e8879915690eb95f493"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3907320e5b88b668af3ab2f46357fef" id="r_ab3907320e5b88b668af3ab2f46357fef"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ab3907320e5b88b668af3ab2f46357fef">_test_ridge_diabetes</a> (filter_)</td></tr>
<tr class="separator:ab3907320e5b88b668af3ab2f46357fef"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18376195498960af0f1044bbf6e0c3aa" id="r_a18376195498960af0f1044bbf6e0c3aa"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a18376195498960af0f1044bbf6e0c3aa">_test_multi_ridge_diabetes</a> (filter_)</td></tr>
<tr class="separator:a18376195498960af0f1044bbf6e0c3aa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6714ed66bc94f5d5ab4239db4c2cd3b6" id="r_a6714ed66bc94f5d5ab4239db4c2cd3b6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a6714ed66bc94f5d5ab4239db4c2cd3b6">_test_ridge_classifiers</a> (filter_)</td></tr>
<tr class="separator:a6714ed66bc94f5d5ab4239db4c2cd3b6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5119ea42f2e00c2cd42eb1baf41141b" id="r_ab5119ea42f2e00c2cd42eb1baf41141b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ab5119ea42f2e00c2cd42eb1baf41141b">test_ridge_classifier_with_scoring</a> (filter_, scoring, cv)</td></tr>
<tr class="separator:ab5119ea42f2e00c2cd42eb1baf41141b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad070c287bef26ae83ed26eda493251ef" id="r_ad070c287bef26ae83ed26eda493251ef"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ad070c287bef26ae83ed26eda493251ef">test_ridge_regression_custom_scoring</a> (filter_, cv)</td></tr>
<tr class="separator:ad070c287bef26ae83ed26eda493251ef"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a29e70c22da55461406fb1007327cf6cd" id="r_a29e70c22da55461406fb1007327cf6cd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a29e70c22da55461406fb1007327cf6cd">_test_tolerance</a> (filter_)</td></tr>
<tr class="separator:a29e70c22da55461406fb1007327cf6cd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afdc9d49a250c5466a873f93d8fc637c4" id="r_afdc9d49a250c5466a873f93d8fc637c4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#afdc9d49a250c5466a873f93d8fc637c4">check_dense_sparse</a> (test_func)</td></tr>
<tr class="separator:afdc9d49a250c5466a873f93d8fc637c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acd40e96f852f930a6ddbe797ad8aee88" id="r_acd40e96f852f930a6ddbe797ad8aee88"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#acd40e96f852f930a6ddbe797ad8aee88">test_dense_sparse</a> (test_func)</td></tr>
<tr class="separator:acd40e96f852f930a6ddbe797ad8aee88"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac54f07149e26ed6af2bf8ce9727c5b4e" id="r_ac54f07149e26ed6af2bf8ce9727c5b4e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ac54f07149e26ed6af2bf8ce9727c5b4e">test_class_weights</a> ()</td></tr>
<tr class="separator:ac54f07149e26ed6af2bf8ce9727c5b4e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a92161e8defa89312e4d69c9d429f20cd" id="r_a92161e8defa89312e4d69c9d429f20cd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a92161e8defa89312e4d69c9d429f20cd">test_class_weight_vs_sample_weight</a> (reg)</td></tr>
<tr class="separator:a92161e8defa89312e4d69c9d429f20cd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeab3b250dd148ec587a138ab9fface36" id="r_aeab3b250dd148ec587a138ab9fface36"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aeab3b250dd148ec587a138ab9fface36">test_class_weights_cv</a> ()</td></tr>
<tr class="separator:aeab3b250dd148ec587a138ab9fface36"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af98b5b52efe75b5d16a4bf1f482254df" id="r_af98b5b52efe75b5d16a4bf1f482254df"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#af98b5b52efe75b5d16a4bf1f482254df">test_ridgecv_store_cv_values</a> (scoring)</td></tr>
<tr class="separator:af98b5b52efe75b5d16a4bf1f482254df"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa14d15118da8c34739f39001b8131431" id="r_aa14d15118da8c34739f39001b8131431"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aa14d15118da8c34739f39001b8131431">test_ridge_classifier_cv_store_cv_values</a> (scoring)</td></tr>
<tr class="separator:aa14d15118da8c34739f39001b8131431"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a33c1d21f50a8f9aa7aa6509ef01beca3" id="r_a33c1d21f50a8f9aa7aa6509ef01beca3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a33c1d21f50a8f9aa7aa6509ef01beca3">test_ridgecv_alphas_conversion</a> (Estimator)</td></tr>
<tr class="separator:a33c1d21f50a8f9aa7aa6509ef01beca3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a99f7635bcd52d19cd46189f85e802e8a" id="r_a99f7635bcd52d19cd46189f85e802e8a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a99f7635bcd52d19cd46189f85e802e8a">test_ridgecv_sample_weight</a> ()</td></tr>
<tr class="separator:a99f7635bcd52d19cd46189f85e802e8a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4ca9462e9f3a9da110d4c1614adecd64" id="r_a4ca9462e9f3a9da110d4c1614adecd64"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a4ca9462e9f3a9da110d4c1614adecd64">test_raises_value_error_if_sample_weights_greater_than_1d</a> ()</td></tr>
<tr class="separator:a4ca9462e9f3a9da110d4c1614adecd64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aff5883e4de00594746f76fb1336d46a2" id="r_aff5883e4de00594746f76fb1336d46a2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aff5883e4de00594746f76fb1336d46a2">test_sparse_design_with_sample_weights</a> ()</td></tr>
<tr class="separator:aff5883e4de00594746f76fb1336d46a2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a41e496e501ccba233f11b697979063d6" id="r_a41e496e501ccba233f11b697979063d6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a41e496e501ccba233f11b697979063d6">test_ridgecv_int_alphas</a> ()</td></tr>
<tr class="separator:a41e496e501ccba233f11b697979063d6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa57728b84b22f95943c421330ac59721" id="r_aa57728b84b22f95943c421330ac59721"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aa57728b84b22f95943c421330ac59721">test_ridgecv_alphas_validation</a> (Estimator, params, err_type, err_msg)</td></tr>
<tr class="separator:aa57728b84b22f95943c421330ac59721"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0679a6715391a428a6d9d5809f0cb7e9" id="r_a0679a6715391a428a6d9d5809f0cb7e9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a0679a6715391a428a6d9d5809f0cb7e9">test_ridgecv_alphas_scalar</a> (Estimator)</td></tr>
<tr class="separator:a0679a6715391a428a6d9d5809f0cb7e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac2e3f4588951ecdbd7985115f47ae87a" id="r_ac2e3f4588951ecdbd7985115f47ae87a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ac2e3f4588951ecdbd7985115f47ae87a">test_raises_value_error_if_solver_not_supported</a> ()</td></tr>
<tr class="separator:ac2e3f4588951ecdbd7985115f47ae87a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4cb628654242257a120821b583200e1" id="r_ab4cb628654242257a120821b583200e1"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ab4cb628654242257a120821b583200e1">test_sparse_cg_max_iter</a> ()</td></tr>
<tr class="separator:ab4cb628654242257a120821b583200e1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a25ad1ec8243274949f216189dc2073f0" id="r_a25ad1ec8243274949f216189dc2073f0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a25ad1ec8243274949f216189dc2073f0">test_n_iter</a> ()</td></tr>
<tr class="separator:a25ad1ec8243274949f216189dc2073f0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf6e654ea81704532a2f54aa7cb7630a" id="r_abf6e654ea81704532a2f54aa7cb7630a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#abf6e654ea81704532a2f54aa7cb7630a">test_ridge_fit_intercept_sparse</a> (solver, with_sample_weight, global_random_seed)</td></tr>
<tr class="separator:abf6e654ea81704532a2f54aa7cb7630a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a203b521ba6a9af7d01cc7b36322d54c9" id="r_a203b521ba6a9af7d01cc7b36322d54c9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a203b521ba6a9af7d01cc7b36322d54c9">test_ridge_fit_intercept_sparse_error</a> (solver)</td></tr>
<tr class="separator:a203b521ba6a9af7d01cc7b36322d54c9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0fddc87a5ad9b0d6e712ac69be0382a" id="r_af0fddc87a5ad9b0d6e712ac69be0382a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#af0fddc87a5ad9b0d6e712ac69be0382a">test_ridge_fit_intercept_sparse_sag</a> (with_sample_weight, global_random_seed)</td></tr>
<tr class="separator:af0fddc87a5ad9b0d6e712ac69be0382a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac53ef9ec2530193f33c80ededa0c56fc" id="r_ac53ef9ec2530193f33c80ededa0c56fc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ac53ef9ec2530193f33c80ededa0c56fc">test_ridge_regression_check_arguments_validity</a> (return_intercept, sample_weight, arr_type, solver)</td></tr>
<tr class="separator:ac53ef9ec2530193f33c80ededa0c56fc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab16e405feab6e57a76a3818674cb0879" id="r_ab16e405feab6e57a76a3818674cb0879"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ab16e405feab6e57a76a3818674cb0879">test_dtype_match</a> (solver)</td></tr>
<tr class="separator:ab16e405feab6e57a76a3818674cb0879"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f489b4afdb7a7d55b41194c3f882f49" id="r_a4f489b4afdb7a7d55b41194c3f882f49"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a4f489b4afdb7a7d55b41194c3f882f49">test_dtype_match_cholesky</a> ()</td></tr>
<tr class="separator:a4f489b4afdb7a7d55b41194c3f882f49"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ada3f213d5cfb5d6c212f1db5bfaa7dd2" id="r_ada3f213d5cfb5d6c212f1db5bfaa7dd2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ada3f213d5cfb5d6c212f1db5bfaa7dd2">test_ridge_regression_dtype_stability</a> (solver, seed)</td></tr>
<tr class="separator:ada3f213d5cfb5d6c212f1db5bfaa7dd2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18e49a9b79d318e4eda8b85962fc14be" id="r_a18e49a9b79d318e4eda8b85962fc14be"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a18e49a9b79d318e4eda8b85962fc14be">test_ridge_sag_with_X_fortran</a> ()</td></tr>
<tr class="separator:a18e49a9b79d318e4eda8b85962fc14be"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab1572701f799d60456b1b005ff4e677f" id="r_ab1572701f799d60456b1b005ff4e677f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ab1572701f799d60456b1b005ff4e677f">test_ridgeclassifier_multilabel</a> (Classifier, params)</td></tr>
<tr class="separator:ab1572701f799d60456b1b005ff4e677f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3d0d19f55663ab3e53f45047b8997a5f" id="r_a3d0d19f55663ab3e53f45047b8997a5f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a3d0d19f55663ab3e53f45047b8997a5f">test_ridge_positive_regression_test</a> (solver, fit_intercept, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>)</td></tr>
<tr class="separator:a3d0d19f55663ab3e53f45047b8997a5f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a058873709e9c3e09aeef1652b848333e" id="r_a058873709e9c3e09aeef1652b848333e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a058873709e9c3e09aeef1652b848333e">test_ridge_ground_truth_positive_test</a> (fit_intercept, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>)</td></tr>
<tr class="separator:a058873709e9c3e09aeef1652b848333e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae9757b138d0636d34eec2fd9d9b2cc4a" id="r_ae9757b138d0636d34eec2fd9d9b2cc4a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ae9757b138d0636d34eec2fd9d9b2cc4a">test_ridge_positive_error_test</a> (solver)</td></tr>
<tr class="separator:ae9757b138d0636d34eec2fd9d9b2cc4a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af482000e723069200afe81b80b64fc16" id="r_af482000e723069200afe81b80b64fc16"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#af482000e723069200afe81b80b64fc16">test_positive_ridge_loss</a> (<a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>)</td></tr>
<tr class="separator:af482000e723069200afe81b80b64fc16"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3fe9dd7400ed5201b6ae67b8bc6f3289" id="r_a3fe9dd7400ed5201b6ae67b8bc6f3289"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a3fe9dd7400ed5201b6ae67b8bc6f3289">test_lbfgs_solver_consistency</a> (<a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>)</td></tr>
<tr class="separator:a3fe9dd7400ed5201b6ae67b8bc6f3289"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a57e9e49ba271912fadf5547aaef71a59" id="r_a57e9e49ba271912fadf5547aaef71a59"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a57e9e49ba271912fadf5547aaef71a59">test_lbfgs_solver_error</a> ()</td></tr>
<tr class="separator:a57e9e49ba271912fadf5547aaef71a59"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aec975be623ef3c2979e9dbd3fb6fa085" id="r_aec975be623ef3c2979e9dbd3fb6fa085"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aec975be623ef3c2979e9dbd3fb6fa085">test_ridge_sample_weight_invariance</a> (solver)</td></tr>
<tr class="separator:aec975be623ef3c2979e9dbd3fb6fa085"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:ad782224cbe16bce2c9ede7efac4e6c64" id="r_ad782224cbe16bce2c9ede7efac4e6c64"><td class="memItemLeft" align="right" valign="top">tuple&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ad782224cbe16bce2c9ede7efac4e6c64">SOLVERS</a> = (&quot;svd&quot;, &quot;sparse_cg&quot;, &quot;cholesky&quot;, &quot;lsqr&quot;, &quot;sag&quot;, &quot;saga&quot;)</td></tr>
<tr class="separator:ad782224cbe16bce2c9ede7efac4e6c64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a41c17009ec8619bc9350ad8222099b32" id="r_a41c17009ec8619bc9350ad8222099b32"><td class="memItemLeft" align="right" valign="top">tuple&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a41c17009ec8619bc9350ad8222099b32">SPARSE_SOLVERS_WITH_INTERCEPT</a> = (&quot;sparse_cg&quot;, &quot;sag&quot;)</td></tr>
<tr class="separator:a41c17009ec8619bc9350ad8222099b32"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6feb81e045a57ca37e2ba3a8f003634b" id="r_a6feb81e045a57ca37e2ba3a8f003634b"><td class="memItemLeft" align="right" valign="top">tuple&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a6feb81e045a57ca37e2ba3a8f003634b">SPARSE_SOLVERS_WITHOUT_INTERCEPT</a> = (&quot;sparse_cg&quot;, &quot;cholesky&quot;, &quot;lsqr&quot;, &quot;sag&quot;, &quot;saga&quot;)</td></tr>
<tr class="separator:a6feb81e045a57ca37e2ba3a8f003634b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a919b2df8f10f777148a95d3f302b7c59" id="r_a919b2df8f10f777148a95d3f302b7c59"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a919b2df8f10f777148a95d3f302b7c59">diabetes</a> = datasets.load_diabetes()</td></tr>
<tr class="separator:a919b2df8f10f777148a95d3f302b7c59"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4582a107b030dad4bcba71aff09fcb04" id="r_a4582a107b030dad4bcba71aff09fcb04"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a4582a107b030dad4bcba71aff09fcb04">X_diabetes</a></td></tr>
<tr class="separator:a4582a107b030dad4bcba71aff09fcb04"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a033fd3c4bf70e42b1469403dece2654d" id="r_a033fd3c4bf70e42b1469403dece2654d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a033fd3c4bf70e42b1469403dece2654d">y_diabetes</a></td></tr>
<tr class="separator:a033fd3c4bf70e42b1469403dece2654d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab04905d865109ec38039a9a8020d9838" id="r_ab04905d865109ec38039a9a8020d9838"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#ab04905d865109ec38039a9a8020d9838">ind</a> = np.arange(X_diabetes.shape[0])</td></tr>
<tr class="separator:ab04905d865109ec38039a9a8020d9838"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acd5259fc50fe976391436d21eda84abb" id="r_acd5259fc50fe976391436d21eda84abb"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#acd5259fc50fe976391436d21eda84abb">rng</a> = np.random.RandomState(0)</td></tr>
<tr class="separator:acd5259fc50fe976391436d21eda84abb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aacddf4d95eb4c1544fd70e1b80afd45f" id="r_aacddf4d95eb4c1544fd70e1b80afd45f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#aacddf4d95eb4c1544fd70e1b80afd45f">iris</a> = datasets.load_iris()</td></tr>
<tr class="separator:aacddf4d95eb4c1544fd70e1b80afd45f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5a061a6f140fc3d42d96efa0f07edf6f" id="r_a5a061a6f140fc3d42d96efa0f07edf6f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a5a061a6f140fc3d42d96efa0f07edf6f">X_iris</a> = sp.csr_matrix(iris.data)</td></tr>
<tr class="separator:a5a061a6f140fc3d42d96efa0f07edf6f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5d065c6ae19f578db504b7f27e3974bf" id="r_a5d065c6ae19f578db504b7f27e3974bf"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__ridge.html#a5d065c6ae19f578db504b7f27e3974bf">y_iris</a> = iris.target</td></tr>
<tr class="separator:a5d065c6ae19f578db504b7f27e3974bf"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Function Documentation</h2>
<a id="a729df6979d25432f8b396ffef3ba6e4c" name="a729df6979d25432f8b396ffef3ba6e4c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a729df6979d25432f8b396ffef3ba6e4c">&#9670;&#160;</a></span>_accuracy_callable()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge._accuracy_callable </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_test</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_pred</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   78</span><span class="keyword">def </span>_accuracy_callable(y_test, y_pred):</div>
<div class="line"><span class="lineno">   79</span>    <span class="keywordflow">return</span> np.mean(y_test == y_pred)</div>
<div class="line"><span class="lineno">   80</span> </div>
<div class="line"><span class="lineno">   81</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a9a6ac7663e93c8a9fba592f5802ea8f0" name="a9a6ac7663e93c8a9fba592f5802ea8f0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9a6ac7663e93c8a9fba592f5802ea8f0">&#9670;&#160;</a></span>_make_sparse_offset_regression()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge._make_sparse_offset_regression </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em> = <code>100</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_features</em> = <code>100</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>proportion_nonzero</em> = <code>0.5</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_informative</em> = <code>10</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_targets</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>bias</em> = <code>13.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X_offset</em> = <code>30.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>noise</em> = <code>30.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>shuffle</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coef</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>positive</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  685</span>):</div>
<div class="line"><span class="lineno">  686</span>    X, y, c = make_regression(</div>
<div class="line"><span class="lineno">  687</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  688</span>        n_features=n_features,</div>
<div class="line"><span class="lineno">  689</span>        n_informative=n_informative,</div>
<div class="line"><span class="lineno">  690</span>        n_targets=n_targets,</div>
<div class="line"><span class="lineno">  691</span>        bias=bias,</div>
<div class="line"><span class="lineno">  692</span>        noise=noise,</div>
<div class="line"><span class="lineno">  693</span>        shuffle=shuffle,</div>
<div class="line"><span class="lineno">  694</span>        coef=<span class="keyword">True</span>,</div>
<div class="line"><span class="lineno">  695</span>        random_state=random_state,</div>
<div class="line"><span class="lineno">  696</span>    )</div>
<div class="line"><span class="lineno">  697</span>    <span class="keywordflow">if</span> n_features == 1:</div>
<div class="line"><span class="lineno">  698</span>        c = np.asarray([c])</div>
<div class="line"><span class="lineno">  699</span>    X += X_offset</div>
<div class="line"><span class="lineno">  700</span>    mask = (</div>
<div class="line"><span class="lineno">  701</span>        np.random.RandomState(random_state).binomial(1, proportion_nonzero, X.shape) &gt; 0</div>
<div class="line"><span class="lineno">  702</span>    )</div>
<div class="line"><span class="lineno">  703</span>    removed_X = X.copy()</div>
<div class="line"><span class="lineno">  704</span>    X[~mask] = 0.0</div>
<div class="line"><span class="lineno">  705</span>    removed_X[mask] = 0.0</div>
<div class="line"><span class="lineno">  706</span>    y -= removed_X.dot(c)</div>
<div class="line"><span class="lineno">  707</span>    <span class="keywordflow">if</span> positive:</div>
<div class="line"><span class="lineno">  708</span>        y += X.dot(np.abs(c) + 1 - c)</div>
<div class="line"><span class="lineno">  709</span>        c = np.abs(c) + 1</div>
<div class="line"><span class="lineno">  710</span>    <span class="keywordflow">if</span> n_features == 1:</div>
<div class="line"><span class="lineno">  711</span>        c = c[0]</div>
<div class="line"><span class="lineno">  712</span>    <span class="keywordflow">if</span> coef:</div>
<div class="line"><span class="lineno">  713</span>        <span class="keywordflow">return</span> X, y, c</div>
<div class="line"><span class="lineno">  714</span>    <span class="keywordflow">return</span> X, y</div>
<div class="line"><span class="lineno">  715</span> </div>
<div class="line"><span class="lineno">  716</span> </div>
<div class="line"><span class="lineno">  717</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  718</span>    <span class="stringliteral">&quot;solver, sparse_X&quot;</span>,</div>
<div class="line"><span class="lineno">  719</span>    (</div>
<div class="line"><span class="lineno">  720</span>        (solver, sparse_X)</div>
<div class="line"><span class="lineno">  721</span>        <span class="keywordflow">for</span> (solver, sparse_X) <span class="keywordflow">in</span> product(</div>
<div class="line"><span class="lineno">  722</span>            [<span class="stringliteral">&quot;cholesky&quot;</span>, <span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;sparse_cg&quot;</span>, <span class="stringliteral">&quot;lsqr&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>, <span class="stringliteral">&quot;ridgecv&quot;</span>],</div>
<div class="line"><span class="lineno">  723</span>            [<span class="keyword">False</span>, <span class="keyword">True</span>],</div>
<div class="line"><span class="lineno">  724</span>        )</div>
<div class="line"><span class="lineno">  725</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> (sparse_X <span class="keywordflow">and</span> solver <span class="keywordflow">not</span> <span class="keywordflow">in</span> [<span class="stringliteral">&quot;sparse_cg&quot;</span>, <span class="stringliteral">&quot;ridgecv&quot;</span>])</div>
<div class="line"><span class="lineno">  726</span>    ),</div>
<div class="line"><span class="lineno">  727</span>)</div>
<div class="line"><span class="lineno">  728</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  729</span>    <span class="stringliteral">&quot;n_samples,dtype,proportion_nonzero&quot;</span>,</div>
<div class="line"><span class="lineno">  730</span>    [(20, <span class="stringliteral">&quot;float32&quot;</span>, 0.1), (40, <span class="stringliteral">&quot;float32&quot;</span>, 1.0), (20, <span class="stringliteral">&quot;float64&quot;</span>, 0.2)],</div>
<div class="line"><span class="lineno">  731</span>)</div>
<div class="line"><span class="lineno">  732</span><span class="preprocessor">@pytest.mark.parametrize(&quot;seed&quot;, np.arange(3)</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="aca5473f7fc1009ff71c1b2dbb018ad67" name="aca5473f7fc1009ff71c1b2dbb018ad67"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aca5473f7fc1009ff71c1b2dbb018ad67">&#9670;&#160;</a></span>_mean_squared_error_callable()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge._mean_squared_error_callable </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_test</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_pred</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   82</span><span class="keyword">def </span>_mean_squared_error_callable(y_test, y_pred):</div>
<div class="line"><span class="lineno">   83</span>    <span class="keywordflow">return</span> ((y_test - y_pred) ** 2).mean()</div>
<div class="line"><span class="lineno">   84</span> </div>
<div class="line"><span class="lineno">   85</span> </div>
<div class="line"><span class="lineno">   86</span><span class="preprocessor">@pytest.fixture(params=[&quot;long&quot;, &quot;wide&quot;])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a18376195498960af0f1044bbf6e0c3aa" name="a18376195498960af0f1044bbf6e0c3aa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a18376195498960af0f1044bbf6e0c3aa">&#9670;&#160;</a></span>_test_multi_ridge_diabetes()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge._test_multi_ridge_diabetes </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>filter_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1115</span><span class="keyword">def </span>_test_multi_ridge_diabetes(filter_):</div>
<div class="line"><span class="lineno"> 1116</span>    <span class="comment"># simulate several responses</span></div>
<div class="line"><span class="lineno"> 1117</span>    Y = np.vstack((y_diabetes, y_diabetes)).T</div>
<div class="line"><span class="lineno"> 1118</span>    n_features = X_diabetes.shape[1]</div>
<div class="line"><span class="lineno"> 1119</span> </div>
<div class="line"><span class="lineno"> 1120</span>    ridge = Ridge(fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1121</span>    ridge.fit(filter_(X_diabetes), Y)</div>
<div class="line"><span class="lineno"> 1122</span>    <span class="keyword">assert</span> ridge.coef_.shape == (2, n_features)</div>
<div class="line"><span class="lineno"> 1123</span>    Y_pred = ridge.predict(filter_(X_diabetes))</div>
<div class="line"><span class="lineno"> 1124</span>    ridge.fit(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno"> 1125</span>    y_pred = ridge.predict(filter_(X_diabetes))</div>
<div class="line"><span class="lineno"> 1126</span>    assert_array_almost_equal(np.vstack((y_pred, y_pred)).T, Y_pred, decimal=3)</div>
<div class="line"><span class="lineno"> 1127</span> </div>
<div class="line"><span class="lineno"> 1128</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a6714ed66bc94f5d5ab4239db4c2cd3b6" name="a6714ed66bc94f5d5ab4239db4c2cd3b6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6714ed66bc94f5d5ab4239db4c2cd3b6">&#9670;&#160;</a></span>_test_ridge_classifiers()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge._test_ridge_classifiers </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>filter_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1129</span><span class="keyword">def </span>_test_ridge_classifiers(filter_):</div>
<div class="line"><span class="lineno"> 1130</span>    n_classes = np.unique(y_iris).shape[0]</div>
<div class="line"><span class="lineno"> 1131</span>    n_features = X_iris.shape[1]</div>
<div class="line"><span class="lineno"> 1132</span>    <span class="keywordflow">for</span> reg <span class="keywordflow">in</span> (RidgeClassifier(), RidgeClassifierCV()):</div>
<div class="line"><span class="lineno"> 1133</span>        reg.fit(filter_(X_iris), y_iris)</div>
<div class="line"><span class="lineno"> 1134</span>        <span class="keyword">assert</span> reg.coef_.shape == (n_classes, n_features)</div>
<div class="line"><span class="lineno"> 1135</span>        y_pred = reg.predict(filter_(X_iris))</div>
<div class="line"><span class="lineno"> 1136</span>        <span class="keyword">assert</span> np.mean(y_iris == y_pred) &gt; 0.79</div>
<div class="line"><span class="lineno"> 1137</span> </div>
<div class="line"><span class="lineno"> 1138</span>    cv = KFold(5)</div>
<div class="line"><span class="lineno"> 1139</span>    reg = RidgeClassifierCV(cv=cv)</div>
<div class="line"><span class="lineno"> 1140</span>    reg.fit(filter_(X_iris), y_iris)</div>
<div class="line"><span class="lineno"> 1141</span>    y_pred = reg.predict(filter_(X_iris))</div>
<div class="line"><span class="lineno"> 1142</span>    <span class="keyword">assert</span> np.mean(y_iris == y_pred) &gt;= 0.8</div>
<div class="line"><span class="lineno"> 1143</span> </div>
<div class="line"><span class="lineno"> 1144</span> </div>
<div class="line"><span class="lineno"> 1145</span><span class="preprocessor">@pytest.mark.parametrize(&quot;scoring&quot;, [None, &quot;accuracy&quot;, _accuracy_callable])</span></div>
<div class="line"><span class="lineno"> 1146</span><span class="preprocessor">@pytest.mark.parametrize(&quot;cv&quot;, [None, KFold(5)</span>])</div>
<div class="line"><span class="lineno"> 1147</span><span class="preprocessor">@pytest.mark.parametrize(&quot;filter_&quot;, [DENSE_FILTER, SPARSE_FILTER])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a76cfee83d153dac2dd1085462a6976a4" name="a76cfee83d153dac2dd1085462a6976a4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a76cfee83d153dac2dd1085462a6976a4">&#9670;&#160;</a></span>_test_ridge_cv()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge._test_ridge_cv </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>filter_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  993</span><span class="keyword">def </span>_test_ridge_cv(filter_):</div>
<div class="line"><span class="lineno">  994</span>    ridge_cv = RidgeCV()</div>
<div class="line"><span class="lineno">  995</span>    ridge_cv.fit(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno">  996</span>    ridge_cv.predict(filter_(X_diabetes))</div>
<div class="line"><span class="lineno">  997</span> </div>
<div class="line"><span class="lineno">  998</span>    <span class="keyword">assert</span> len(ridge_cv.coef_.shape) == 1</div>
<div class="line"><span class="lineno">  999</span>    <span class="keyword">assert</span> type(ridge_cv.intercept_) == np.float64</div>
<div class="line"><span class="lineno"> 1000</span> </div>
<div class="line"><span class="lineno"> 1001</span>    cv = KFold(5)</div>
<div class="line"><span class="lineno"> 1002</span>    ridge_cv.set_params(cv=cv)</div>
<div class="line"><span class="lineno"> 1003</span>    ridge_cv.fit(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno"> 1004</span>    ridge_cv.predict(filter_(X_diabetes))</div>
<div class="line"><span class="lineno"> 1005</span> </div>
<div class="line"><span class="lineno"> 1006</span>    <span class="keyword">assert</span> len(ridge_cv.coef_.shape) == 1</div>
<div class="line"><span class="lineno"> 1007</span>    <span class="keyword">assert</span> type(ridge_cv.intercept_) == np.float64</div>
<div class="line"><span class="lineno"> 1008</span> </div>
<div class="line"><span class="lineno"> 1009</span> </div>
<div class="line"><span class="lineno"> 1010</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1011</span>    <span class="stringliteral">&quot;ridge, make_dataset&quot;</span>,</div>
<div class="line"><span class="lineno"> 1012</span>    [</div>
<div class="line"><span class="lineno"> 1013</span>        (RidgeCV(store_cv_values=<span class="keyword">False</span>), make_regression),</div>
<div class="line"><span class="lineno"> 1014</span>        (RidgeClassifierCV(store_cv_values=<span class="keyword">False</span>), make_classification),</div>
<div class="line"><span class="lineno"> 1015</span>    ],</div>
<div class="line"><span class="lineno"> 1016</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ab3907320e5b88b668af3ab2f46357fef" name="ab3907320e5b88b668af3ab2f46357fef"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab3907320e5b88b668af3ab2f46357fef">&#9670;&#160;</a></span>_test_ridge_diabetes()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge._test_ridge_diabetes </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>filter_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1109</span><span class="keyword">def </span>_test_ridge_diabetes(filter_):</div>
<div class="line"><span class="lineno"> 1110</span>    ridge = Ridge(fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1111</span>    ridge.fit(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno"> 1112</span>    <span class="keywordflow">return</span> np.round(ridge.score(filter_(X_diabetes), y_diabetes), 5)</div>
<div class="line"><span class="lineno"> 1113</span> </div>
<div class="line"><span class="lineno"> 1114</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a9b812a4460e51c5864e9dd20b4017c07" name="a9b812a4460e51c5864e9dd20b4017c07"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9b812a4460e51c5864e9dd20b4017c07">&#9670;&#160;</a></span>_test_ridge_loo()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge._test_ridge_loo </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>filter_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  939</span><span class="keyword">def </span>_test_ridge_loo(filter_):</div>
<div class="line"><span class="lineno">  940</span>    <span class="comment"># test that can work with both dense or sparse matrices</span></div>
<div class="line"><span class="lineno">  941</span>    n_samples = X_diabetes.shape[0]</div>
<div class="line"><span class="lineno">  942</span> </div>
<div class="line"><span class="lineno">  943</span>    ret = []</div>
<div class="line"><span class="lineno">  944</span> </div>
<div class="line"><span class="lineno">  945</span>    fit_intercept = filter_ == DENSE_FILTER</div>
<div class="line"><span class="lineno">  946</span>    ridge_gcv = _RidgeGCV(fit_intercept=fit_intercept)</div>
<div class="line"><span class="lineno">  947</span> </div>
<div class="line"><span class="lineno">  948</span>    <span class="comment"># check best alpha</span></div>
<div class="line"><span class="lineno">  949</span>    ridge_gcv.fit(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno">  950</span>    alpha_ = ridge_gcv.alpha_</div>
<div class="line"><span class="lineno">  951</span>    ret.append(alpha_)</div>
<div class="line"><span class="lineno">  952</span> </div>
<div class="line"><span class="lineno">  953</span>    <span class="comment"># check that we get same best alpha with custom loss_func</span></div>
<div class="line"><span class="lineno">  954</span>    f = ignore_warnings</div>
<div class="line"><span class="lineno">  955</span>    scoring = make_scorer(mean_squared_error, greater_is_better=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  956</span>    ridge_gcv2 = RidgeCV(fit_intercept=<span class="keyword">False</span>, scoring=scoring)</div>
<div class="line"><span class="lineno">  957</span>    <a class="code hl_variable" href="__lapack__subroutines_8h.html#af01a903df7bdb7a494f5827e45bf3a2a">f</a>(ridge_gcv2.fit)(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno">  958</span>    <span class="keyword">assert</span> ridge_gcv2.alpha_ == pytest.approx(alpha_)</div>
<div class="line"><span class="lineno">  959</span> </div>
<div class="line"><span class="lineno">  960</span>    <span class="comment"># check that we get same best alpha with custom score_func</span></div>
<div class="line"><span class="lineno">  961</span>    <span class="keyword">def </span><a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>(x, y):</div>
<div class="line"><span class="lineno">  962</span>        <span class="keywordflow">return</span> -mean_squared_error(x, y)</div>
<div class="line"><span class="lineno">  963</span> </div>
<div class="line"><span class="lineno">  964</span>    scoring = make_scorer(func)</div>
<div class="line"><span class="lineno">  965</span>    ridge_gcv3 = RidgeCV(fit_intercept=<span class="keyword">False</span>, scoring=scoring)</div>
<div class="line"><span class="lineno">  966</span>    <a class="code hl_variable" href="__lapack__subroutines_8h.html#af01a903df7bdb7a494f5827e45bf3a2a">f</a>(ridge_gcv3.fit)(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno">  967</span>    <span class="keyword">assert</span> ridge_gcv3.alpha_ == pytest.approx(alpha_)</div>
<div class="line"><span class="lineno">  968</span> </div>
<div class="line"><span class="lineno">  969</span>    <span class="comment"># check that we get same best alpha with a scorer</span></div>
<div class="line"><span class="lineno">  970</span>    scorer = get_scorer(<span class="stringliteral">&quot;neg_mean_squared_error&quot;</span>)</div>
<div class="line"><span class="lineno">  971</span>    ridge_gcv4 = RidgeCV(fit_intercept=<span class="keyword">False</span>, scoring=scorer)</div>
<div class="line"><span class="lineno">  972</span>    ridge_gcv4.fit(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno">  973</span>    <span class="keyword">assert</span> ridge_gcv4.alpha_ == pytest.approx(alpha_)</div>
<div class="line"><span class="lineno">  974</span> </div>
<div class="line"><span class="lineno">  975</span>    <span class="comment"># check that we get same best alpha with sample weights</span></div>
<div class="line"><span class="lineno">  976</span>    <span class="keywordflow">if</span> filter_ == DENSE_FILTER:</div>
<div class="line"><span class="lineno">  977</span>        ridge_gcv.fit(filter_(X_diabetes), y_diabetes, sample_weight=np.ones(n_samples))</div>
<div class="line"><span class="lineno">  978</span>        <span class="keyword">assert</span> ridge_gcv.alpha_ == pytest.approx(alpha_)</div>
<div class="line"><span class="lineno">  979</span> </div>
<div class="line"><span class="lineno">  980</span>    <span class="comment"># simulate several responses</span></div>
<div class="line"><span class="lineno">  981</span>    Y = np.vstack((y_diabetes, y_diabetes)).T</div>
<div class="line"><span class="lineno">  982</span> </div>
<div class="line"><span class="lineno">  983</span>    ridge_gcv.fit(filter_(X_diabetes), Y)</div>
<div class="line"><span class="lineno">  984</span>    Y_pred = ridge_gcv.predict(filter_(X_diabetes))</div>
<div class="line"><span class="lineno">  985</span>    ridge_gcv.fit(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno">  986</span>    y_pred = ridge_gcv.predict(filter_(X_diabetes))</div>
<div class="line"><span class="lineno">  987</span> </div>
<div class="line"><span class="lineno">  988</span>    assert_allclose(np.vstack((y_pred, y_pred)).T, Y_pred, rtol=1e-5)</div>
<div class="line"><span class="lineno">  989</span> </div>
<div class="line"><span class="lineno">  990</span>    <span class="keywordflow">return</span> ret</div>
<div class="line"><span class="lineno">  991</span> </div>
<div class="line"><span class="lineno">  992</span> </div>
<div class="ttc" id="a__lapack__subroutines_8h_html_af01a903df7bdb7a494f5827e45bf3a2a"><div class="ttname"><a href="__lapack__subroutines_8h.html#af01a903df7bdb7a494f5827e45bf3a2a">f</a></div><div class="ttdeci">void int int int int npy_complex64 int int npy_complex64 float float npy_complex64 npy_complex64 * f</div><div class="ttdef"><b>Definition</b> _lapack_subroutines.h:262</div></div>
<div class="ttc" id="acallback_2foo_8f_html_a565fe2cc583df102f120752b0011c330"><div class="ttname"><a href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a></div><div class="ttdeci">subroutine func(a)</div><div class="ttdef"><b>Definition</b> foo.f:9</div></div>
</div><!-- fragment -->
</div>
</div>
<a id="a29e70c22da55461406fb1007327cf6cd" name="a29e70c22da55461406fb1007327cf6cd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a29e70c22da55461406fb1007327cf6cd">&#9670;&#160;</a></span>_test_tolerance()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge._test_tolerance </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>filter_</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1175</span><span class="keyword">def </span>_test_tolerance(filter_):</div>
<div class="line"><span class="lineno"> 1176</span>    ridge = Ridge(tol=1e-5, fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1177</span>    ridge.fit(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno"> 1178</span>    score = ridge.score(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno"> 1179</span> </div>
<div class="line"><span class="lineno"> 1180</span>    ridge2 = Ridge(tol=1e-3, fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1181</span>    ridge2.fit(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno"> 1182</span>    score2 = ridge2.score(filter_(X_diabetes), y_diabetes)</div>
<div class="line"><span class="lineno"> 1183</span> </div>
<div class="line"><span class="lineno"> 1184</span>    <span class="keyword">assert</span> score &gt;= score2</div>
<div class="line"><span class="lineno"> 1185</span> </div>
<div class="line"><span class="lineno"> 1186</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="afdc9d49a250c5466a873f93d8fc637c4" name="afdc9d49a250c5466a873f93d8fc637c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afdc9d49a250c5466a873f93d8fc637c4">&#9670;&#160;</a></span>check_dense_sparse()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.check_dense_sparse </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>test_func</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1187</span><span class="keyword">def </span>check_dense_sparse(test_func):</div>
<div class="line"><span class="lineno"> 1188</span>    <span class="comment"># test dense matrix</span></div>
<div class="line"><span class="lineno"> 1189</span>    ret_dense = test_func(DENSE_FILTER)</div>
<div class="line"><span class="lineno"> 1190</span>    <span class="comment"># test sparse matrix</span></div>
<div class="line"><span class="lineno"> 1191</span>    ret_sparse = test_func(SPARSE_FILTER)</div>
<div class="line"><span class="lineno"> 1192</span>    <span class="comment"># test that the outputs are the same</span></div>
<div class="line"><span class="lineno"> 1193</span>    <span class="keywordflow">if</span> ret_dense <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> ret_sparse <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno"> 1194</span>        assert_array_almost_equal(ret_dense, ret_sparse, decimal=3)</div>
<div class="line"><span class="lineno"> 1195</span> </div>
<div class="line"><span class="lineno"> 1196</span> </div>
<div class="line"><span class="lineno"> 1197</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1198</span>    <span class="stringliteral">&quot;test_func&quot;</span>,</div>
<div class="line"><span class="lineno"> 1199</span>    (</div>
<div class="line"><span class="lineno"> 1200</span>        _test_ridge_loo,</div>
<div class="line"><span class="lineno"> 1201</span>        _test_ridge_cv,</div>
<div class="line"><span class="lineno"> 1202</span>        _test_ridge_diabetes,</div>
<div class="line"><span class="lineno"> 1203</span>        _test_multi_ridge_diabetes,</div>
<div class="line"><span class="lineno"> 1204</span>        _test_ridge_classifiers,</div>
<div class="line"><span class="lineno"> 1205</span>        _test_tolerance,</div>
<div class="line"><span class="lineno"> 1206</span>    ),</div>
<div class="line"><span class="lineno"> 1207</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae7e934ffb91f7ec7239615ad1a93b533" name="ae7e934ffb91f7ec7239615ad1a93b533"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae7e934ffb91f7ec7239615ad1a93b533">&#9670;&#160;</a></span>DENSE_FILTER()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.DENSE_FILTER </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   70</span><span class="keyword">def </span>DENSE_FILTER(X):</div>
<div class="line"><span class="lineno">   71</span>    <span class="keywordflow">return</span> X</div>
<div class="line"><span class="lineno">   72</span> </div>
<div class="line"><span class="lineno">   73</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a90d507dbc605d30d323679761a6d808c" name="a90d507dbc605d30d323679761a6d808c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a90d507dbc605d30d323679761a6d808c">&#9670;&#160;</a></span>ols_ridge_dataset()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.ols_ridge_dataset </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>request</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Dataset with OLS and Ridge solutions, well conditioned X.

The construction is based on the SVD decomposition of X = U S V'.

Parameters
----------
type : {"long", "wide"}
    If "long", then n_samples &gt; n_features.
    If "wide", then n_features &gt; n_samples.

For "wide", we return the minimum norm solution w = X' (XX')^-1 y:

    min ||w||_2 subject to X w = y

Returns
-------
X : ndarray
    Last column of 1, i.e. intercept.
y : ndarray
coef_ols : ndarray of shape
    Minimum norm OLS solutions, i.e. min ||X w - y||_2_2 (with mininum ||w||_2 in
    case of ambiguity)
    Last coefficient is intercept.
coef_ridge : ndarray of shape (5,)
    Ridge solution with alpha=1, i.e. min ||X w - y||_2_2 + ||w||_2^2.
    Last coefficient is intercept.
</pre> <div class="fragment"><div class="line"><span class="lineno">   87</span><span class="keyword">def </span>ols_ridge_dataset(global_random_seed, request):</div>
<div class="line"><span class="lineno">   88</span>    <span class="stringliteral">&quot;&quot;&quot;Dataset with OLS and Ridge solutions, well conditioned X.</span></div>
<div class="line"><span class="lineno">   89</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   90</span><span class="stringliteral">    The construction is based on the SVD decomposition of X = U S V&#39;.</span></div>
<div class="line"><span class="lineno">   91</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   92</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   93</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   94</span><span class="stringliteral">    type : {&quot;long&quot;, &quot;wide&quot;}</span></div>
<div class="line"><span class="lineno">   95</span><span class="stringliteral">        If &quot;long&quot;, then n_samples &gt; n_features.</span></div>
<div class="line"><span class="lineno">   96</span><span class="stringliteral">        If &quot;wide&quot;, then n_features &gt; n_samples.</span></div>
<div class="line"><span class="lineno">   97</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   98</span><span class="stringliteral">    For &quot;wide&quot;, we return the minimum norm solution w = X&#39; (XX&#39;)^-1 y:</span></div>
<div class="line"><span class="lineno">   99</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  100</span><span class="stringliteral">        min ||w||_2 subject to X w = y</span></div>
<div class="line"><span class="lineno">  101</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  102</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  103</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  104</span><span class="stringliteral">    X : ndarray</span></div>
<div class="line"><span class="lineno">  105</span><span class="stringliteral">        Last column of 1, i.e. intercept.</span></div>
<div class="line"><span class="lineno">  106</span><span class="stringliteral">    y : ndarray</span></div>
<div class="line"><span class="lineno">  107</span><span class="stringliteral">    coef_ols : ndarray of shape</span></div>
<div class="line"><span class="lineno">  108</span><span class="stringliteral">        Minimum norm OLS solutions, i.e. min ||X w - y||_2_2 (with mininum ||w||_2 in</span></div>
<div class="line"><span class="lineno">  109</span><span class="stringliteral">        case of ambiguity)</span></div>
<div class="line"><span class="lineno">  110</span><span class="stringliteral">        Last coefficient is intercept.</span></div>
<div class="line"><span class="lineno">  111</span><span class="stringliteral">    coef_ridge : ndarray of shape (5,)</span></div>
<div class="line"><span class="lineno">  112</span><span class="stringliteral">        Ridge solution with alpha=1, i.e. min ||X w - y||_2_2 + ||w||_2^2.</span></div>
<div class="line"><span class="lineno">  113</span><span class="stringliteral">        Last coefficient is intercept.</span></div>
<div class="line"><span class="lineno">  114</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  115</span>    <span class="comment"># Make larger dim more than double as big as the smaller one.</span></div>
<div class="line"><span class="lineno">  116</span>    <span class="comment"># This helps when constructing singular matrices like (X, X).</span></div>
<div class="line"><span class="lineno">  117</span>    <span class="keywordflow">if</span> request.param == <span class="stringliteral">&quot;long&quot;</span>:</div>
<div class="line"><span class="lineno">  118</span>        n_samples, n_features = 12, 4</div>
<div class="line"><span class="lineno">  119</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  120</span>        n_samples, n_features = 4, 12</div>
<div class="line"><span class="lineno">  121</span>    k = min(n_samples, n_features)</div>
<div class="line"><span class="lineno">  122</span>    rng = np.random.RandomState(global_random_seed)</div>
<div class="line"><span class="lineno">  123</span>    X = make_low_rank_matrix(</div>
<div class="line"><span class="lineno">  124</span>        n_samples=n_samples, n_features=n_features, effective_rank=k, random_state=rng</div>
<div class="line"><span class="lineno">  125</span>    )</div>
<div class="line"><span class="lineno">  126</span>    X[:, -1] = 1  <span class="comment"># last columns acts as intercept</span></div>
<div class="line"><span class="lineno">  127</span>    U, s, Vt = linalg.svd(X)</div>
<div class="line"><span class="lineno">  128</span>    <span class="keyword">assert</span> np.all(s &gt; 1e-3)  <span class="comment"># to be sure</span></div>
<div class="line"><span class="lineno">  129</span>    U1, U2 = U[:, :k], U[:, k:]</div>
<div class="line"><span class="lineno">  130</span>    Vt1, _ = Vt[:k, :], Vt[k:, :]</div>
<div class="line"><span class="lineno">  131</span> </div>
<div class="line"><span class="lineno">  132</span>    <span class="keywordflow">if</span> request.param == <span class="stringliteral">&quot;long&quot;</span>:</div>
<div class="line"><span class="lineno">  133</span>        <span class="comment"># Add a term that vanishes in the product X&#39;y</span></div>
<div class="line"><span class="lineno">  134</span>        coef_ols = rng.uniform(low=-10, high=10, size=n_features)</div>
<div class="line"><span class="lineno">  135</span>        y = X @ coef_ols</div>
<div class="line"><span class="lineno">  136</span>        y += U2 @ rng.normal(size=n_samples - n_features) ** 2</div>
<div class="line"><span class="lineno">  137</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  138</span>        y = rng.uniform(low=-10, high=10, size=n_samples)</div>
<div class="line"><span class="lineno">  139</span>        <span class="comment"># w = X&#39;(XX&#39;)^-1 y = V s^-1 U&#39; y</span></div>
<div class="line"><span class="lineno">  140</span>        coef_ols = Vt1.T @ np.diag(1 / s) @ U1.T @ y</div>
<div class="line"><span class="lineno">  141</span> </div>
<div class="line"><span class="lineno">  142</span>    <span class="comment"># Add penalty alpha * ||coef||_2^2 for alpha=1 and solve via normal equations.</span></div>
<div class="line"><span class="lineno">  143</span>    <span class="comment"># Note that the problem is well conditioned such that we get accurate results.</span></div>
<div class="line"><span class="lineno">  144</span>    alpha = 1</div>
<div class="line"><span class="lineno">  145</span>    d = alpha * np.identity(n_features)</div>
<div class="line"><span class="lineno">  146</span>    d[-1, -1] = 0  <span class="comment"># intercept gets no penalty</span></div>
<div class="line"><span class="lineno">  147</span>    coef_ridge = linalg.solve(X.T @ X + d, X.T @ y)</div>
<div class="line"><span class="lineno">  148</span> </div>
<div class="line"><span class="lineno">  149</span>    <span class="comment"># To be sure</span></div>
<div class="line"><span class="lineno">  150</span>    R_OLS = y - X @ coef_ols</div>
<div class="line"><span class="lineno">  151</span>    R_Ridge = y - X @ coef_ridge</div>
<div class="line"><span class="lineno">  152</span>    <span class="keyword">assert</span> np.linalg.norm(R_OLS) &lt; np.linalg.norm(R_Ridge)</div>
<div class="line"><span class="lineno">  153</span> </div>
<div class="line"><span class="lineno">  154</span>    <span class="keywordflow">return</span> X, y, coef_ols, coef_ridge</div>
<div class="line"><span class="lineno">  155</span> </div>
<div class="line"><span class="lineno">  156</span> </div>
<div class="line"><span class="lineno">  157</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, SOLVERS)</span></div>
<div class="line"><span class="lineno">  158</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a8c2b7a860ad327adb8eea53a9f5f6863" name="a8c2b7a860ad327adb8eea53a9f5f6863"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8c2b7a860ad327adb8eea53a9f5f6863">&#9670;&#160;</a></span>SPARSE_FILTER()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.SPARSE_FILTER </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   74</span><span class="keyword">def </span>SPARSE_FILTER(X):</div>
<div class="line"><span class="lineno">   75</span>    <span class="keywordflow">return</span> sp.csr_matrix(X)</div>
<div class="line"><span class="lineno">   76</span> </div>
<div class="line"><span class="lineno">   77</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a4c67bbde5440f0f64d6439c0bf09618c" name="a4c67bbde5440f0f64d6439c0bf09618c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4c67bbde5440f0f64d6439c0bf09618c">&#9670;&#160;</a></span>test_check_gcv_mode_choice()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_check_gcv_mode_choice </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sparse</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>mode</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>mode_n_greater_than_p</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>mode_p_greater_than_n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  931</span>):</div>
<div class="line"><span class="lineno">  932</span>    X, _ = make_regression(n_samples=5, n_features=2)</div>
<div class="line"><span class="lineno">  933</span>    <span class="keywordflow">if</span> sparse:</div>
<div class="line"><span class="lineno">  934</span>        X = sp.csr_matrix(X)</div>
<div class="line"><span class="lineno">  935</span>    <span class="keyword">assert</span> _check_gcv_mode(X, mode) == mode_n_greater_than_p</div>
<div class="line"><span class="lineno">  936</span>    <span class="keyword">assert</span> _check_gcv_mode(X.T, mode) == mode_p_greater_than_n</div>
<div class="line"><span class="lineno">  937</span> </div>
<div class="line"><span class="lineno">  938</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a92161e8defa89312e4d69c9d429f20cd" name="a92161e8defa89312e4d69c9d429f20cd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a92161e8defa89312e4d69c9d429f20cd">&#9670;&#160;</a></span>test_class_weight_vs_sample_weight()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_class_weight_vs_sample_weight </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>reg</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check class_weights resemble sample_weights behavior.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1248</span><span class="keyword">def </span>test_class_weight_vs_sample_weight(reg):</div>
<div class="line"><span class="lineno"> 1249</span>    <span class="stringliteral">&quot;&quot;&quot;Check class_weights resemble sample_weights behavior.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1250</span> </div>
<div class="line"><span class="lineno"> 1251</span>    <span class="comment"># Iris is balanced, so no effect expected for using &#39;balanced&#39; weights</span></div>
<div class="line"><span class="lineno"> 1252</span>    reg1 = reg()</div>
<div class="line"><span class="lineno"> 1253</span>    reg1.fit(iris.data, iris.target)</div>
<div class="line"><span class="lineno"> 1254</span>    reg2 = reg(class_weight=<span class="stringliteral">&quot;balanced&quot;</span>)</div>
<div class="line"><span class="lineno"> 1255</span>    reg2.fit(iris.data, iris.target)</div>
<div class="line"><span class="lineno"> 1256</span>    assert_almost_equal(reg1.coef_, reg2.coef_)</div>
<div class="line"><span class="lineno"> 1257</span> </div>
<div class="line"><span class="lineno"> 1258</span>    <span class="comment"># Inflate importance of class 1, check against user-defined weights</span></div>
<div class="line"><span class="lineno"> 1259</span>    sample_weight = np.ones(iris.target.shape)</div>
<div class="line"><span class="lineno"> 1260</span>    sample_weight[iris.target == 1] *= 100</div>
<div class="line"><span class="lineno"> 1261</span>    class_weight = {0: 1.0, 1: 100.0, 2: 1.0}</div>
<div class="line"><span class="lineno"> 1262</span>    reg1 = reg()</div>
<div class="line"><span class="lineno"> 1263</span>    reg1.fit(iris.data, iris.target, sample_weight)</div>
<div class="line"><span class="lineno"> 1264</span>    reg2 = reg(class_weight=class_weight)</div>
<div class="line"><span class="lineno"> 1265</span>    reg2.fit(iris.data, iris.target)</div>
<div class="line"><span class="lineno"> 1266</span>    assert_almost_equal(reg1.coef_, reg2.coef_)</div>
<div class="line"><span class="lineno"> 1267</span> </div>
<div class="line"><span class="lineno"> 1268</span>    <span class="comment"># Check that sample_weight and class_weight are multiplicative</span></div>
<div class="line"><span class="lineno"> 1269</span>    reg1 = reg()</div>
<div class="line"><span class="lineno"> 1270</span>    reg1.fit(iris.data, iris.target, sample_weight**2)</div>
<div class="line"><span class="lineno"> 1271</span>    reg2 = reg(class_weight=class_weight)</div>
<div class="line"><span class="lineno"> 1272</span>    reg2.fit(iris.data, iris.target, sample_weight)</div>
<div class="line"><span class="lineno"> 1273</span>    assert_almost_equal(reg1.coef_, reg2.coef_)</div>
<div class="line"><span class="lineno"> 1274</span> </div>
<div class="line"><span class="lineno"> 1275</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ac54f07149e26ed6af2bf8ce9727c5b4e" name="ac54f07149e26ed6af2bf8ce9727c5b4e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac54f07149e26ed6af2bf8ce9727c5b4e">&#9670;&#160;</a></span>test_class_weights()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_class_weights </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1212</span><span class="keyword">def </span>test_class_weights():</div>
<div class="line"><span class="lineno"> 1213</span>    <span class="comment"># Test class weights.</span></div>
<div class="line"><span class="lineno"> 1214</span>    X = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])</div>
<div class="line"><span class="lineno"> 1215</span>    y = [1, 1, 1, -1, -1]</div>
<div class="line"><span class="lineno"> 1216</span> </div>
<div class="line"><span class="lineno"> 1217</span>    reg = RidgeClassifier(class_weight=<span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno"> 1218</span>    reg.fit(X, y)</div>
<div class="line"><span class="lineno"> 1219</span>    assert_array_equal(reg.predict([[0.2, -1.0]]), np.array([1]))</div>
<div class="line"><span class="lineno"> 1220</span> </div>
<div class="line"><span class="lineno"> 1221</span>    <span class="comment"># we give a small weights to class 1</span></div>
<div class="line"><span class="lineno"> 1222</span>    reg = RidgeClassifier(class_weight={1: 0.001})</div>
<div class="line"><span class="lineno"> 1223</span>    reg.fit(X, y)</div>
<div class="line"><span class="lineno"> 1224</span> </div>
<div class="line"><span class="lineno"> 1225</span>    <span class="comment"># now the hyperplane should rotate clock-wise and</span></div>
<div class="line"><span class="lineno"> 1226</span>    <span class="comment"># the prediction on this point should shift</span></div>
<div class="line"><span class="lineno"> 1227</span>    assert_array_equal(reg.predict([[0.2, -1.0]]), np.array([-1]))</div>
<div class="line"><span class="lineno"> 1228</span> </div>
<div class="line"><span class="lineno"> 1229</span>    <span class="comment"># check if class_weight = &#39;balanced&#39; can handle negative labels.</span></div>
<div class="line"><span class="lineno"> 1230</span>    reg = RidgeClassifier(class_weight=<span class="stringliteral">&quot;balanced&quot;</span>)</div>
<div class="line"><span class="lineno"> 1231</span>    reg.fit(X, y)</div>
<div class="line"><span class="lineno"> 1232</span>    assert_array_equal(reg.predict([[0.2, -1.0]]), np.array([1]))</div>
<div class="line"><span class="lineno"> 1233</span> </div>
<div class="line"><span class="lineno"> 1234</span>    <span class="comment"># class_weight = &#39;balanced&#39;, and class_weight = None should return</span></div>
<div class="line"><span class="lineno"> 1235</span>    <span class="comment"># same values when y has equal number of all labels</span></div>
<div class="line"><span class="lineno"> 1236</span>    X = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0]])</div>
<div class="line"><span class="lineno"> 1237</span>    y = [1, 1, -1, -1]</div>
<div class="line"><span class="lineno"> 1238</span>    reg = RidgeClassifier(class_weight=<span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno"> 1239</span>    reg.fit(X, y)</div>
<div class="line"><span class="lineno"> 1240</span>    rega = RidgeClassifier(class_weight=<span class="stringliteral">&quot;balanced&quot;</span>)</div>
<div class="line"><span class="lineno"> 1241</span>    rega.fit(X, y)</div>
<div class="line"><span class="lineno"> 1242</span>    <span class="keyword">assert</span> len(rega.classes_) == 2</div>
<div class="line"><span class="lineno"> 1243</span>    assert_array_almost_equal(reg.coef_, rega.coef_)</div>
<div class="line"><span class="lineno"> 1244</span>    assert_array_almost_equal(reg.intercept_, rega.intercept_)</div>
<div class="line"><span class="lineno"> 1245</span> </div>
<div class="line"><span class="lineno"> 1246</span> </div>
<div class="line"><span class="lineno"> 1247</span><span class="preprocessor">@pytest.mark.parametrize(&quot;reg&quot;, (RidgeClassifier, RidgeClassifierCV)</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="aeab3b250dd148ec587a138ab9fface36" name="aeab3b250dd148ec587a138ab9fface36"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeab3b250dd148ec587a138ab9fface36">&#9670;&#160;</a></span>test_class_weights_cv()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_class_weights_cv </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1276</span><span class="keyword">def </span>test_class_weights_cv():</div>
<div class="line"><span class="lineno"> 1277</span>    <span class="comment"># Test class weights for cross validated ridge classifier.</span></div>
<div class="line"><span class="lineno"> 1278</span>    X = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])</div>
<div class="line"><span class="lineno"> 1279</span>    y = [1, 1, 1, -1, -1]</div>
<div class="line"><span class="lineno"> 1280</span> </div>
<div class="line"><span class="lineno"> 1281</span>    reg = RidgeClassifierCV(class_weight=<span class="keywordtype">None</span>, alphas=[0.01, 0.1, 1])</div>
<div class="line"><span class="lineno"> 1282</span>    reg.fit(X, y)</div>
<div class="line"><span class="lineno"> 1283</span> </div>
<div class="line"><span class="lineno"> 1284</span>    <span class="comment"># we give a small weights to class 1</span></div>
<div class="line"><span class="lineno"> 1285</span>    reg = RidgeClassifierCV(class_weight={1: 0.001}, alphas=[0.01, 0.1, 1, 10])</div>
<div class="line"><span class="lineno"> 1286</span>    reg.fit(X, y)</div>
<div class="line"><span class="lineno"> 1287</span> </div>
<div class="line"><span class="lineno"> 1288</span>    assert_array_equal(reg.predict([[-0.2, 2]]), np.array([-1]))</div>
<div class="line"><span class="lineno"> 1289</span> </div>
<div class="line"><span class="lineno"> 1290</span> </div>
<div class="line"><span class="lineno"> 1291</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1292</span>    <span class="stringliteral">&quot;scoring&quot;</span>, [<span class="keywordtype">None</span>, <span class="stringliteral">&quot;neg_mean_squared_error&quot;</span>, _mean_squared_error_callable]</div>
<div class="line"><span class="lineno"> 1293</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="aaf17982f3ee6889ad5a43773e3887659" name="aaf17982f3ee6889ad5a43773e3887659"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aaf17982f3ee6889ad5a43773e3887659">&#9670;&#160;</a></span>test_compute_covariance()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_compute_covariance </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>uniform_weights</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  654</span><span class="keyword">def </span>test_compute_covariance(shape, uniform_weights):</div>
<div class="line"><span class="lineno">  655</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  656</span>    X = rng.randn(*shape)</div>
<div class="line"><span class="lineno">  657</span>    <span class="keywordflow">if</span> uniform_weights:</div>
<div class="line"><span class="lineno">  658</span>        sw = np.ones(X.shape[0])</div>
<div class="line"><span class="lineno">  659</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  660</span>        sw = rng.chisquare(1, shape[0])</div>
<div class="line"><span class="lineno">  661</span>    sqrt_sw = np.sqrt(sw)</div>
<div class="line"><span class="lineno">  662</span>    X_mean = np.average(X, axis=0, weights=sw)</div>
<div class="line"><span class="lineno">  663</span>    X_centered = (X - X_mean) * sqrt_sw[:, <span class="keywordtype">None</span>]</div>
<div class="line"><span class="lineno">  664</span>    true_covariance = X_centered.T.dot(X_centered)</div>
<div class="line"><span class="lineno">  665</span>    X_sparse = sp.csr_matrix(X * sqrt_sw[:, <span class="keywordtype">None</span>])</div>
<div class="line"><span class="lineno">  666</span>    gcv = _RidgeGCV(fit_intercept=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  667</span>    computed_cov, computed_mean = gcv._compute_covariance(X_sparse, sqrt_sw)</div>
<div class="line"><span class="lineno">  668</span>    assert_allclose(X_mean, computed_mean)</div>
<div class="line"><span class="lineno">  669</span>    assert_allclose(true_covariance, computed_cov)</div>
<div class="line"><span class="lineno">  670</span> </div>
<div class="line"><span class="lineno">  671</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ae719e559f5d94e0e61fac6245586f0a8" name="ae719e559f5d94e0e61fac6245586f0a8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae719e559f5d94e0e61fac6245586f0a8">&#9670;&#160;</a></span>test_compute_gram()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_compute_gram </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>uniform_weights</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  634</span><span class="keyword">def </span>test_compute_gram(shape, uniform_weights):</div>
<div class="line"><span class="lineno">  635</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  636</span>    X = rng.randn(*shape)</div>
<div class="line"><span class="lineno">  637</span>    <span class="keywordflow">if</span> uniform_weights:</div>
<div class="line"><span class="lineno">  638</span>        sw = np.ones(X.shape[0])</div>
<div class="line"><span class="lineno">  639</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  640</span>        sw = rng.chisquare(1, shape[0])</div>
<div class="line"><span class="lineno">  641</span>    sqrt_sw = np.sqrt(sw)</div>
<div class="line"><span class="lineno">  642</span>    X_mean = np.average(X, axis=0, weights=sw)</div>
<div class="line"><span class="lineno">  643</span>    X_centered = (X - X_mean) * sqrt_sw[:, <span class="keywordtype">None</span>]</div>
<div class="line"><span class="lineno">  644</span>    true_gram = X_centered.dot(X_centered.T)</div>
<div class="line"><span class="lineno">  645</span>    X_sparse = sp.csr_matrix(X * sqrt_sw[:, <span class="keywordtype">None</span>])</div>
<div class="line"><span class="lineno">  646</span>    gcv = _RidgeGCV(fit_intercept=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  647</span>    computed_gram, computed_mean = gcv._compute_gram(X_sparse, sqrt_sw)</div>
<div class="line"><span class="lineno">  648</span>    assert_allclose(X_mean, computed_mean)</div>
<div class="line"><span class="lineno">  649</span>    assert_allclose(true_gram, computed_gram)</div>
<div class="line"><span class="lineno">  650</span> </div>
<div class="line"><span class="lineno">  651</span> </div>
<div class="line"><span class="lineno">  652</span><span class="preprocessor">@pytest.mark.parametrize(&quot;shape&quot;, [(10, 1)</span>, (13, 9), (3, 7), (2, 2), (20, 20)])</div>
<div class="line"><span class="lineno">  653</span><span class="preprocessor">@pytest.mark.parametrize(&quot;uniform_weights&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="acd40e96f852f930a6ddbe797ad8aee88" name="acd40e96f852f930a6ddbe797ad8aee88"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acd40e96f852f930a6ddbe797ad8aee88">&#9670;&#160;</a></span>test_dense_sparse()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_dense_sparse </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>test_func</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1208</span><span class="keyword">def </span>test_dense_sparse(test_func):</div>
<div class="line"><span class="lineno"> 1209</span>    check_dense_sparse(test_func)</div>
<div class="line"><span class="lineno"> 1210</span> </div>
<div class="line"><span class="lineno"> 1211</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ab16e405feab6e57a76a3818674cb0879" name="ab16e405feab6e57a76a3818674cb0879"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab16e405feab6e57a76a3818674cb0879">&#9670;&#160;</a></span>test_dtype_match()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_dtype_match </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1707</span><span class="keyword">def </span>test_dtype_match(solver):</div>
<div class="line"><span class="lineno"> 1708</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno"> 1709</span>    alpha = 1.0</div>
<div class="line"><span class="lineno"> 1710</span>    positive = solver == <span class="stringliteral">&quot;lbfgs&quot;</span></div>
<div class="line"><span class="lineno"> 1711</span> </div>
<div class="line"><span class="lineno"> 1712</span>    n_samples, n_features = 6, 5</div>
<div class="line"><span class="lineno"> 1713</span>    X_64 = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1714</span>    y_64 = rng.randn(n_samples)</div>
<div class="line"><span class="lineno"> 1715</span>    X_32 = X_64.astype(np.float32)</div>
<div class="line"><span class="lineno"> 1716</span>    y_32 = y_64.astype(np.float32)</div>
<div class="line"><span class="lineno"> 1717</span> </div>
<div class="line"><span class="lineno"> 1718</span>    tol = 2 * np.finfo(np.float32).resolution</div>
<div class="line"><span class="lineno"> 1719</span>    <span class="comment"># Check type consistency 32bits</span></div>
<div class="line"><span class="lineno"> 1720</span>    ridge_32 = Ridge(</div>
<div class="line"><span class="lineno"> 1721</span>        alpha=alpha, solver=solver, max_iter=500, tol=tol, positive=positive</div>
<div class="line"><span class="lineno"> 1722</span>    )</div>
<div class="line"><span class="lineno"> 1723</span>    ridge_32.fit(X_32, y_32)</div>
<div class="line"><span class="lineno"> 1724</span>    coef_32 = ridge_32.coef_</div>
<div class="line"><span class="lineno"> 1725</span> </div>
<div class="line"><span class="lineno"> 1726</span>    <span class="comment"># Check type consistency 64 bits</span></div>
<div class="line"><span class="lineno"> 1727</span>    ridge_64 = Ridge(</div>
<div class="line"><span class="lineno"> 1728</span>        alpha=alpha, solver=solver, max_iter=500, tol=tol, positive=positive</div>
<div class="line"><span class="lineno"> 1729</span>    )</div>
<div class="line"><span class="lineno"> 1730</span>    ridge_64.fit(X_64, y_64)</div>
<div class="line"><span class="lineno"> 1731</span>    coef_64 = ridge_64.coef_</div>
<div class="line"><span class="lineno"> 1732</span> </div>
<div class="line"><span class="lineno"> 1733</span>    <span class="comment"># Do the actual checks at once for easier debug</span></div>
<div class="line"><span class="lineno"> 1734</span>    <span class="keyword">assert</span> coef_32.dtype == X_32.dtype</div>
<div class="line"><span class="lineno"> 1735</span>    <span class="keyword">assert</span> coef_64.dtype == X_64.dtype</div>
<div class="line"><span class="lineno"> 1736</span>    <span class="keyword">assert</span> ridge_32.predict(X_32).dtype == X_32.dtype</div>
<div class="line"><span class="lineno"> 1737</span>    <span class="keyword">assert</span> ridge_64.predict(X_64).dtype == X_64.dtype</div>
<div class="line"><span class="lineno"> 1738</span>    assert_allclose(ridge_32.coef_, ridge_64.coef_, rtol=1e-4, atol=5e-4)</div>
<div class="line"><span class="lineno"> 1739</span> </div>
<div class="line"><span class="lineno"> 1740</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a4f489b4afdb7a7d55b41194c3f882f49" name="a4f489b4afdb7a7d55b41194c3f882f49"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4f489b4afdb7a7d55b41194c3f882f49">&#9670;&#160;</a></span>test_dtype_match_cholesky()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_dtype_match_cholesky </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1741</span><span class="keyword">def </span>test_dtype_match_cholesky():</div>
<div class="line"><span class="lineno"> 1742</span>    <span class="comment"># Test different alphas in cholesky solver to ensure full coverage.</span></div>
<div class="line"><span class="lineno"> 1743</span>    <span class="comment"># This test is separated from test_dtype_match for clarity.</span></div>
<div class="line"><span class="lineno"> 1744</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno"> 1745</span>    alpha = np.array([1.0, 0.5])</div>
<div class="line"><span class="lineno"> 1746</span> </div>
<div class="line"><span class="lineno"> 1747</span>    n_samples, n_features, n_target = 6, 7, 2</div>
<div class="line"><span class="lineno"> 1748</span>    X_64 = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1749</span>    y_64 = rng.randn(n_samples, n_target)</div>
<div class="line"><span class="lineno"> 1750</span>    X_32 = X_64.astype(np.float32)</div>
<div class="line"><span class="lineno"> 1751</span>    y_32 = y_64.astype(np.float32)</div>
<div class="line"><span class="lineno"> 1752</span> </div>
<div class="line"><span class="lineno"> 1753</span>    <span class="comment"># Check type consistency 32bits</span></div>
<div class="line"><span class="lineno"> 1754</span>    ridge_32 = Ridge(alpha=alpha, solver=<span class="stringliteral">&quot;cholesky&quot;</span>)</div>
<div class="line"><span class="lineno"> 1755</span>    ridge_32.fit(X_32, y_32)</div>
<div class="line"><span class="lineno"> 1756</span>    coef_32 = ridge_32.coef_</div>
<div class="line"><span class="lineno"> 1757</span> </div>
<div class="line"><span class="lineno"> 1758</span>    <span class="comment"># Check type consistency 64 bits</span></div>
<div class="line"><span class="lineno"> 1759</span>    ridge_64 = Ridge(alpha=alpha, solver=<span class="stringliteral">&quot;cholesky&quot;</span>)</div>
<div class="line"><span class="lineno"> 1760</span>    ridge_64.fit(X_64, y_64)</div>
<div class="line"><span class="lineno"> 1761</span>    coef_64 = ridge_64.coef_</div>
<div class="line"><span class="lineno"> 1762</span> </div>
<div class="line"><span class="lineno"> 1763</span>    <span class="comment"># Do all the checks at once, like this is easier to debug</span></div>
<div class="line"><span class="lineno"> 1764</span>    <span class="keyword">assert</span> coef_32.dtype == X_32.dtype</div>
<div class="line"><span class="lineno"> 1765</span>    <span class="keyword">assert</span> coef_64.dtype == X_64.dtype</div>
<div class="line"><span class="lineno"> 1766</span>    <span class="keyword">assert</span> ridge_32.predict(X_32).dtype == X_32.dtype</div>
<div class="line"><span class="lineno"> 1767</span>    <span class="keyword">assert</span> ridge_64.predict(X_64).dtype == X_64.dtype</div>
<div class="line"><span class="lineno"> 1768</span>    assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)</div>
<div class="line"><span class="lineno"> 1769</span> </div>
<div class="line"><span class="lineno"> 1770</span> </div>
<div class="line"><span class="lineno"> 1771</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1772</span>    <span class="stringliteral">&quot;solver&quot;</span>, [<span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;cholesky&quot;</span>, <span class="stringliteral">&quot;lsqr&quot;</span>, <span class="stringliteral">&quot;sparse_cg&quot;</span>, <span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>, <span class="stringliteral">&quot;lbfgs&quot;</span>]</div>
<div class="line"><span class="lineno"> 1773</span>)</div>
<div class="line"><span class="lineno"> 1774</span><span class="preprocessor">@pytest.mark.parametrize(&quot;seed&quot;, range(1)</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a3fe9dd7400ed5201b6ae67b8bc6f3289" name="a3fe9dd7400ed5201b6ae67b8bc6f3289"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3fe9dd7400ed5201b6ae67b8bc6f3289">&#9670;&#160;</a></span>test_lbfgs_solver_consistency()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_lbfgs_solver_consistency </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that LBGFS gets almost the same coef of svd when positive=False.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1945</span><span class="keyword">def </span>test_lbfgs_solver_consistency(alpha):</div>
<div class="line"><span class="lineno"> 1946</span>    <span class="stringliteral">&quot;&quot;&quot;Test that LBGFS gets almost the same coef of svd when positive=False.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1947</span>    X, y = make_regression(n_samples=300, n_features=300, random_state=42)</div>
<div class="line"><span class="lineno"> 1948</span>    y = np.expand_dims(y, 1)</div>
<div class="line"><span class="lineno"> 1949</span>    alpha = np.asarray([alpha])</div>
<div class="line"><span class="lineno"> 1950</span>    config = {</div>
<div class="line"><span class="lineno"> 1951</span>        <span class="stringliteral">&quot;positive&quot;</span>: <span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1952</span>        <span class="stringliteral">&quot;tol&quot;</span>: 1e-16,</div>
<div class="line"><span class="lineno"> 1953</span>        <span class="stringliteral">&quot;max_iter&quot;</span>: 500000,</div>
<div class="line"><span class="lineno"> 1954</span>    }</div>
<div class="line"><span class="lineno"> 1955</span> </div>
<div class="line"><span class="lineno"> 1956</span>    coef_lbfgs = _solve_lbfgs(X, y, alpha, **config)</div>
<div class="line"><span class="lineno"> 1957</span>    coef_cholesky = _solve_svd(X, y, alpha)</div>
<div class="line"><span class="lineno"> 1958</span>    assert_allclose(coef_lbfgs, coef_cholesky, atol=1e-4, rtol=0)</div>
<div class="line"><span class="lineno"> 1959</span> </div>
<div class="line"><span class="lineno"> 1960</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a57e9e49ba271912fadf5547aaef71a59" name="a57e9e49ba271912fadf5547aaef71a59"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a57e9e49ba271912fadf5547aaef71a59">&#9670;&#160;</a></span>test_lbfgs_solver_error()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_lbfgs_solver_error </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that LBFGS solver raises ConvergenceWarning.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1961</span><span class="keyword">def </span>test_lbfgs_solver_error():</div>
<div class="line"><span class="lineno"> 1962</span>    <span class="stringliteral">&quot;&quot;&quot;Test that LBFGS solver raises ConvergenceWarning.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1963</span>    X = np.array([[1, -1], [1, 1]])</div>
<div class="line"><span class="lineno"> 1964</span>    y = np.array([-1e10, 1e10])</div>
<div class="line"><span class="lineno"> 1965</span> </div>
<div class="line"><span class="lineno"> 1966</span>    model = Ridge(</div>
<div class="line"><span class="lineno"> 1967</span>        alpha=0.01,</div>
<div class="line"><span class="lineno"> 1968</span>        solver=<span class="stringliteral">&quot;lbfgs&quot;</span>,</div>
<div class="line"><span class="lineno"> 1969</span>        fit_intercept=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1970</span>        tol=1e-12,</div>
<div class="line"><span class="lineno"> 1971</span>        positive=<span class="keyword">True</span>,</div>
<div class="line"><span class="lineno"> 1972</span>        max_iter=1,</div>
<div class="line"><span class="lineno"> 1973</span>    )</div>
<div class="line"><span class="lineno"> 1974</span>    <span class="keyword">with</span> pytest.warns(ConvergenceWarning, match=<span class="stringliteral">&quot;lbfgs solver did not converge&quot;</span>):</div>
<div class="line"><span class="lineno"> 1975</span>        model.fit(X, y)</div>
<div class="line"><span class="lineno"> 1976</span> </div>
<div class="line"><span class="lineno"> 1977</span> </div>
<div class="line"><span class="lineno"> 1978</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1979</span>    <span class="stringliteral">&quot;solver&quot;</span>, [<span class="stringliteral">&quot;cholesky&quot;</span>, <span class="stringliteral">&quot;lsqr&quot;</span>, <span class="stringliteral">&quot;sparse_cg&quot;</span>, <span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>, <span class="stringliteral">&quot;lbfgs&quot;</span>]</div>
<div class="line"><span class="lineno"> 1980</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a25ad1ec8243274949f216189dc2073f0" name="a25ad1ec8243274949f216189dc2073f0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a25ad1ec8243274949f216189dc2073f0">&#9670;&#160;</a></span>test_n_iter()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_n_iter </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1550</span><span class="keyword">def </span>test_n_iter():</div>
<div class="line"><span class="lineno"> 1551</span>    <span class="comment"># Test that self.n_iter_ is correct.</span></div>
<div class="line"><span class="lineno"> 1552</span>    n_targets = 2</div>
<div class="line"><span class="lineno"> 1553</span>    X, y = X_diabetes, y_diabetes</div>
<div class="line"><span class="lineno"> 1554</span>    y_n = np.tile(y, (n_targets, 1)).T</div>
<div class="line"><span class="lineno"> 1555</span> </div>
<div class="line"><span class="lineno"> 1556</span>    <span class="keywordflow">for</span> max_iter <span class="keywordflow">in</span> range(1, 4):</div>
<div class="line"><span class="lineno"> 1557</span>        <span class="keywordflow">for</span> solver <span class="keywordflow">in</span> (<span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>, <span class="stringliteral">&quot;lsqr&quot;</span>):</div>
<div class="line"><span class="lineno"> 1558</span>            reg = Ridge(solver=solver, max_iter=max_iter, tol=1e-12)</div>
<div class="line"><span class="lineno"> 1559</span>            reg.fit(X, y_n)</div>
<div class="line"><span class="lineno"> 1560</span>            assert_array_equal(reg.n_iter_, np.tile(max_iter, n_targets))</div>
<div class="line"><span class="lineno"> 1561</span> </div>
<div class="line"><span class="lineno"> 1562</span>    <span class="keywordflow">for</span> solver <span class="keywordflow">in</span> (<span class="stringliteral">&quot;sparse_cg&quot;</span>, <span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;cholesky&quot;</span>):</div>
<div class="line"><span class="lineno"> 1563</span>        reg = Ridge(solver=solver, max_iter=1, tol=1e-1)</div>
<div class="line"><span class="lineno"> 1564</span>        reg.fit(X, y_n)</div>
<div class="line"><span class="lineno"> 1565</span>        <span class="keyword">assert</span> reg.n_iter_ <span class="keywordflow">is</span> <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1566</span> </div>
<div class="line"><span class="lineno"> 1567</span> </div>
<div class="line"><span class="lineno"> 1568</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, [&quot;lsqr&quot;, &quot;sparse_cg&quot;, &quot;lbfgs&quot;, &quot;auto&quot;])</span></div>
<div class="line"><span class="lineno"> 1569</span><span class="preprocessor">@pytest.mark.parametrize(&quot;with_sample_weight&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="af482000e723069200afe81b80b64fc16" name="af482000e723069200afe81b80b64fc16"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af482000e723069200afe81b80b64fc16">&#9670;&#160;</a></span>test_positive_ridge_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_positive_ridge_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check ridge loss consistency when positive argument is enabled.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1907</span><span class="keyword">def </span>test_positive_ridge_loss(alpha):</div>
<div class="line"><span class="lineno"> 1908</span>    <span class="stringliteral">&quot;&quot;&quot;Check ridge loss consistency when positive argument is enabled.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1909</span>    X, y = make_regression(n_samples=300, n_features=300, random_state=42)</div>
<div class="line"><span class="lineno"> 1910</span>    alpha = 0.10</div>
<div class="line"><span class="lineno"> 1911</span>    n_checks = 100</div>
<div class="line"><span class="lineno"> 1912</span> </div>
<div class="line"><span class="lineno"> 1913</span>    <span class="keyword">def </span>ridge_loss(model, random_state=None, noise_scale=1e-8):</div>
<div class="line"><span class="lineno"> 1914</span>        intercept = model.intercept_</div>
<div class="line"><span class="lineno"> 1915</span>        <span class="keywordflow">if</span> random_state <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno"> 1916</span>            rng = np.random.RandomState(random_state)</div>
<div class="line"><span class="lineno"> 1917</span>            coef = model.coef_ + rng.uniform(0, noise_scale, size=model.coef_.shape)</div>
<div class="line"><span class="lineno"> 1918</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1919</span>            coef = model.coef_</div>
<div class="line"><span class="lineno"> 1920</span> </div>
<div class="line"><span class="lineno"> 1921</span>        <span class="keywordflow">return</span> 0.5 * np.sum((y - X @ coef - intercept) ** 2) + 0.5 * alpha * np.sum(</div>
<div class="line"><span class="lineno"> 1922</span>            coef**2</div>
<div class="line"><span class="lineno"> 1923</span>        )</div>
<div class="line"><span class="lineno"> 1924</span> </div>
<div class="line"><span class="lineno"> 1925</span>    model = Ridge(alpha=alpha).fit(X, y)</div>
<div class="line"><span class="lineno"> 1926</span>    model_positive = Ridge(alpha=alpha, positive=<span class="keyword">True</span>).fit(X, y)</div>
<div class="line"><span class="lineno"> 1927</span> </div>
<div class="line"><span class="lineno"> 1928</span>    <span class="comment"># Check 1:</span></div>
<div class="line"><span class="lineno"> 1929</span>    <span class="comment">#   Loss for solution found by Ridge(positive=False)</span></div>
<div class="line"><span class="lineno"> 1930</span>    <span class="comment">#   is lower than that for solution found by Ridge(positive=True)</span></div>
<div class="line"><span class="lineno"> 1931</span>    loss = ridge_loss(model)</div>
<div class="line"><span class="lineno"> 1932</span>    loss_positive = ridge_loss(model_positive)</div>
<div class="line"><span class="lineno"> 1933</span>    <span class="keyword">assert</span> loss &lt;= loss_positive</div>
<div class="line"><span class="lineno"> 1934</span> </div>
<div class="line"><span class="lineno"> 1935</span>    <span class="comment"># Check 2:</span></div>
<div class="line"><span class="lineno"> 1936</span>    <span class="comment">#   Loss for solution found by Ridge(positive=True)</span></div>
<div class="line"><span class="lineno"> 1937</span>    <span class="comment">#   is lower than that for small random positive perturbation</span></div>
<div class="line"><span class="lineno"> 1938</span>    <span class="comment">#   of the positive solution.</span></div>
<div class="line"><span class="lineno"> 1939</span>    <span class="keywordflow">for</span> random_state <span class="keywordflow">in</span> range(n_checks):</div>
<div class="line"><span class="lineno"> 1940</span>        loss_perturbed = ridge_loss(model_positive, random_state=random_state)</div>
<div class="line"><span class="lineno"> 1941</span>        <span class="keyword">assert</span> loss_positive &lt;= loss_perturbed</div>
<div class="line"><span class="lineno"> 1942</span> </div>
<div class="line"><span class="lineno"> 1943</span> </div>
<div class="line"><span class="lineno"> 1944</span><span class="preprocessor">@pytest.mark.parametrize(&quot;alpha&quot;, [1e-3, 1e-2, 0.1, 1.0])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="aef45afbbf6c1360d7566105b7861222a" name="aef45afbbf6c1360d7566105b7861222a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aef45afbbf6c1360d7566105b7861222a">&#9670;&#160;</a></span>test_primal_dual_relationship()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_primal_dual_relationship </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  497</span><span class="keyword">def </span>test_primal_dual_relationship():</div>
<div class="line"><span class="lineno">  498</span>    y = y_diabetes.reshape(-1, 1)</div>
<div class="line"><span class="lineno">  499</span>    coef = _solve_cholesky(X_diabetes, y, alpha=[1e-2])</div>
<div class="line"><span class="lineno">  500</span>    K = np.dot(X_diabetes, X_diabetes.T)</div>
<div class="line"><span class="lineno">  501</span>    dual_coef = _solve_cholesky_kernel(K, y, alpha=[1e-2])</div>
<div class="line"><span class="lineno">  502</span>    coef2 = np.dot(X_diabetes.T, dual_coef).T</div>
<div class="line"><span class="lineno">  503</span>    assert_array_almost_equal(coef, coef2)</div>
<div class="line"><span class="lineno">  504</span> </div>
<div class="line"><span class="lineno">  505</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a4ca9462e9f3a9da110d4c1614adecd64" name="a4ca9462e9f3a9da110d4c1614adecd64"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4ca9462e9f3a9da110d4c1614adecd64">&#9670;&#160;</a></span>test_raises_value_error_if_sample_weights_greater_than_1d()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_raises_value_error_if_sample_weights_greater_than_1d </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1397</span><span class="keyword">def </span>test_raises_value_error_if_sample_weights_greater_than_1d():</div>
<div class="line"><span class="lineno"> 1398</span>    <span class="comment"># Sample weights must be either scalar or 1D</span></div>
<div class="line"><span class="lineno"> 1399</span> </div>
<div class="line"><span class="lineno"> 1400</span>    n_sampless = [2, 3]</div>
<div class="line"><span class="lineno"> 1401</span>    n_featuress = [3, 2]</div>
<div class="line"><span class="lineno"> 1402</span> </div>
<div class="line"><span class="lineno"> 1403</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno"> 1404</span> </div>
<div class="line"><span class="lineno"> 1405</span>    <span class="keywordflow">for</span> n_samples, n_features <span class="keywordflow">in</span> zip(n_sampless, n_featuress):</div>
<div class="line"><span class="lineno"> 1406</span>        X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1407</span>        y = rng.randn(n_samples)</div>
<div class="line"><span class="lineno"> 1408</span>        sample_weights_OK = rng.randn(n_samples) ** 2 + 1</div>
<div class="line"><span class="lineno"> 1409</span>        sample_weights_OK_1 = 1.0</div>
<div class="line"><span class="lineno"> 1410</span>        sample_weights_OK_2 = 2.0</div>
<div class="line"><span class="lineno"> 1411</span>        sample_weights_not_OK = sample_weights_OK[:, np.newaxis]</div>
<div class="line"><span class="lineno"> 1412</span>        sample_weights_not_OK_2 = sample_weights_OK[np.newaxis, :]</div>
<div class="line"><span class="lineno"> 1413</span> </div>
<div class="line"><span class="lineno"> 1414</span>        ridge = Ridge(alpha=1)</div>
<div class="line"><span class="lineno"> 1415</span> </div>
<div class="line"><span class="lineno"> 1416</span>        <span class="comment"># make sure the &quot;OK&quot; sample weights actually work</span></div>
<div class="line"><span class="lineno"> 1417</span>        ridge.fit(X, y, sample_weights_OK)</div>
<div class="line"><span class="lineno"> 1418</span>        ridge.fit(X, y, sample_weights_OK_1)</div>
<div class="line"><span class="lineno"> 1419</span>        ridge.fit(X, y, sample_weights_OK_2)</div>
<div class="line"><span class="lineno"> 1420</span> </div>
<div class="line"><span class="lineno"> 1421</span>        <span class="keyword">def </span>fit_ridge_not_ok():</div>
<div class="line"><span class="lineno"> 1422</span>            ridge.fit(X, y, sample_weights_not_OK)</div>
<div class="line"><span class="lineno"> 1423</span> </div>
<div class="line"><span class="lineno"> 1424</span>        <span class="keyword">def </span>fit_ridge_not_ok_2():</div>
<div class="line"><span class="lineno"> 1425</span>            ridge.fit(X, y, sample_weights_not_OK_2)</div>
<div class="line"><span class="lineno"> 1426</span> </div>
<div class="line"><span class="lineno"> 1427</span>        err_msg = <span class="stringliteral">&quot;Sample weights must be 1D array or scalar&quot;</span></div>
<div class="line"><span class="lineno"> 1428</span>        <span class="keyword">with</span> pytest.raises(ValueError, match=err_msg):</div>
<div class="line"><span class="lineno"> 1429</span>            fit_ridge_not_ok()</div>
<div class="line"><span class="lineno"> 1430</span> </div>
<div class="line"><span class="lineno"> 1431</span>        err_msg = <span class="stringliteral">&quot;Sample weights must be 1D array or scalar&quot;</span></div>
<div class="line"><span class="lineno"> 1432</span>        <span class="keyword">with</span> pytest.raises(ValueError, match=err_msg):</div>
<div class="line"><span class="lineno"> 1433</span>            fit_ridge_not_ok_2()</div>
<div class="line"><span class="lineno"> 1434</span> </div>
<div class="line"><span class="lineno"> 1435</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ac2e3f4588951ecdbd7985115f47ae87a" name="ac2e3f4588951ecdbd7985115f47ae87a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac2e3f4588951ecdbd7985115f47ae87a">&#9670;&#160;</a></span>test_raises_value_error_if_solver_not_supported()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_raises_value_error_if_solver_not_supported </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1522</span><span class="keyword">def </span>test_raises_value_error_if_solver_not_supported():</div>
<div class="line"><span class="lineno"> 1523</span>    <span class="comment"># Tests whether a ValueError is raised if a non-identified solver</span></div>
<div class="line"><span class="lineno"> 1524</span>    <span class="comment"># is passed to ridge_regression</span></div>
<div class="line"><span class="lineno"> 1525</span> </div>
<div class="line"><span class="lineno"> 1526</span>    wrong_solver = <span class="stringliteral">&quot;This is not a solver (MagritteSolveCV QuantumBitcoin)&quot;</span></div>
<div class="line"><span class="lineno"> 1527</span> </div>
<div class="line"><span class="lineno"> 1528</span>    exception = ValueError</div>
<div class="line"><span class="lineno"> 1529</span>    message = (</div>
<div class="line"><span class="lineno"> 1530</span>        <span class="stringliteral">&quot;Known solvers are &#39;sparse_cg&#39;, &#39;cholesky&#39;, &#39;svd&#39;&quot;</span></div>
<div class="line"><span class="lineno"> 1531</span>        <span class="stringliteral">&quot; &#39;lsqr&#39;, &#39;sag&#39; or &#39;saga&#39;. Got %s.&quot;</span> % wrong_solver</div>
<div class="line"><span class="lineno"> 1532</span>    )</div>
<div class="line"><span class="lineno"> 1533</span> </div>
<div class="line"><span class="lineno"> 1534</span>    <span class="keyword">def </span><a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>():</div>
<div class="line"><span class="lineno"> 1535</span>        X = np.eye(3)</div>
<div class="line"><span class="lineno"> 1536</span>        y = np.ones(3)</div>
<div class="line"><span class="lineno"> 1537</span>        ridge_regression(X, y, alpha=1.0, solver=wrong_solver)</div>
<div class="line"><span class="lineno"> 1538</span> </div>
<div class="line"><span class="lineno"> 1539</span>        <span class="keyword">with</span> pytest.raises(exception, match=message):</div>
<div class="line"><span class="lineno"> 1540</span>            <a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>()</div>
<div class="line"><span class="lineno"> 1541</span> </div>
<div class="line"><span class="lineno"> 1542</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a2dc851e27e61bd1c849d28e7617ad565" name="a2dc851e27e61bd1c849d28e7617ad565"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2dc851e27e61bd1c849d28e7617ad565">&#9670;&#160;</a></span>test_ridge_best_score()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_best_score </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ridge</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>make_dataset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>cv</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1029</span><span class="keyword">def </span>test_ridge_best_score(ridge, make_dataset, cv):</div>
<div class="line"><span class="lineno"> 1030</span>    <span class="comment"># check that the best_score_ is store</span></div>
<div class="line"><span class="lineno"> 1031</span>    X, y = make_dataset(n_samples=6, random_state=42)</div>
<div class="line"><span class="lineno"> 1032</span>    ridge.set_params(store_cv_values=<span class="keyword">False</span>, cv=cv)</div>
<div class="line"><span class="lineno"> 1033</span>    ridge.fit(X, y)</div>
<div class="line"><span class="lineno"> 1034</span>    <span class="keyword">assert</span> hasattr(ridge, <span class="stringliteral">&quot;best_score_&quot;</span>)</div>
<div class="line"><span class="lineno"> 1035</span>    <span class="keyword">assert</span> isinstance(ridge.best_score_, float)</div>
<div class="line"><span class="lineno"> 1036</span> </div>
<div class="line"><span class="lineno"> 1037</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aa14d15118da8c34739f39001b8131431" name="aa14d15118da8c34739f39001b8131431"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa14d15118da8c34739f39001b8131431">&#9670;&#160;</a></span>test_ridge_classifier_cv_store_cv_values()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_classifier_cv_store_cv_values </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>scoring</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1324</span><span class="keyword">def </span>test_ridge_classifier_cv_store_cv_values(scoring):</div>
<div class="line"><span class="lineno"> 1325</span>    x = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])</div>
<div class="line"><span class="lineno"> 1326</span>    y = np.array([1, 1, 1, -1, -1])</div>
<div class="line"><span class="lineno"> 1327</span> </div>
<div class="line"><span class="lineno"> 1328</span>    n_samples = x.shape[0]</div>
<div class="line"><span class="lineno"> 1329</span>    alphas = [1e-1, 1e0, 1e1]</div>
<div class="line"><span class="lineno"> 1330</span>    n_alphas = len(alphas)</div>
<div class="line"><span class="lineno"> 1331</span> </div>
<div class="line"><span class="lineno"> 1332</span>    scoring_ = make_scorer(scoring) <span class="keywordflow">if</span> callable(scoring) <span class="keywordflow">else</span> scoring</div>
<div class="line"><span class="lineno"> 1333</span> </div>
<div class="line"><span class="lineno"> 1334</span>    r = RidgeClassifierCV(</div>
<div class="line"><span class="lineno"> 1335</span>        alphas=alphas, cv=<span class="keywordtype">None</span>, store_cv_values=<span class="keyword">True</span>, scoring=scoring_</div>
<div class="line"><span class="lineno"> 1336</span>    )</div>
<div class="line"><span class="lineno"> 1337</span> </div>
<div class="line"><span class="lineno"> 1338</span>    <span class="comment"># with len(y.shape) == 1</span></div>
<div class="line"><span class="lineno"> 1339</span>    n_targets = 1</div>
<div class="line"><span class="lineno"> 1340</span>    r.fit(x, y)</div>
<div class="line"><span class="lineno"> 1341</span>    <span class="keyword">assert</span> r.cv_values_.shape == (n_samples, n_targets, n_alphas)</div>
<div class="line"><span class="lineno"> 1342</span> </div>
<div class="line"><span class="lineno"> 1343</span>    <span class="comment"># with len(y.shape) == 2</span></div>
<div class="line"><span class="lineno"> 1344</span>    y = np.array(</div>
<div class="line"><span class="lineno"> 1345</span>        [[1, 1, 1, -1, -1], [1, -1, 1, -1, 1], [-1, -1, 1, -1, -1]]</div>
<div class="line"><span class="lineno"> 1346</span>    ).transpose()</div>
<div class="line"><span class="lineno"> 1347</span>    n_targets = y.shape[1]</div>
<div class="line"><span class="lineno"> 1348</span>    r.fit(x, y)</div>
<div class="line"><span class="lineno"> 1349</span>    <span class="keyword">assert</span> r.cv_values_.shape == (n_samples, n_targets, n_alphas)</div>
<div class="line"><span class="lineno"> 1350</span> </div>
<div class="line"><span class="lineno"> 1351</span> </div>
<div class="line"><span class="lineno"> 1352</span><span class="preprocessor">@pytest.mark.parametrize(&quot;Estimator&quot;, [RidgeCV, RidgeClassifierCV])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="ab5119ea42f2e00c2cd42eb1baf41141b" name="ab5119ea42f2e00c2cd42eb1baf41141b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab5119ea42f2e00c2cd42eb1baf41141b">&#9670;&#160;</a></span>test_ridge_classifier_with_scoring()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_classifier_with_scoring </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>filter_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>scoring</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>cv</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1148</span><span class="keyword">def </span>test_ridge_classifier_with_scoring(filter_, scoring, cv):</div>
<div class="line"><span class="lineno"> 1149</span>    <span class="comment"># non-regression test for #14672</span></div>
<div class="line"><span class="lineno"> 1150</span>    <span class="comment"># check that RidgeClassifierCV works with all sort of scoring and</span></div>
<div class="line"><span class="lineno"> 1151</span>    <span class="comment"># cross-validation</span></div>
<div class="line"><span class="lineno"> 1152</span>    scoring_ = make_scorer(scoring) <span class="keywordflow">if</span> callable(scoring) <span class="keywordflow">else</span> scoring</div>
<div class="line"><span class="lineno"> 1153</span>    clf = RidgeClassifierCV(scoring=scoring_, cv=cv)</div>
<div class="line"><span class="lineno"> 1154</span>    <span class="comment"># Smoke test to check that fit/predict does not raise error</span></div>
<div class="line"><span class="lineno"> 1155</span>    clf.fit(filter_(X_iris), y_iris).predict(filter_(X_iris))</div>
<div class="line"><span class="lineno"> 1156</span> </div>
<div class="line"><span class="lineno"> 1157</span> </div>
<div class="line"><span class="lineno"> 1158</span><span class="preprocessor">@pytest.mark.parametrize(&quot;cv&quot;, [None, KFold(5)</span>])</div>
<div class="line"><span class="lineno"> 1159</span><span class="preprocessor">@pytest.mark.parametrize(&quot;filter_&quot;, [DENSE_FILTER, SPARSE_FILTER])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="acb04ee82f1e72e8879915690eb95f493" name="acb04ee82f1e72e8879915690eb95f493"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acb04ee82f1e72e8879915690eb95f493">&#9670;&#160;</a></span>test_ridge_cv_individual_penalties()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_cv_individual_penalties </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1038</span><span class="keyword">def </span>test_ridge_cv_individual_penalties():</div>
<div class="line"><span class="lineno"> 1039</span>    <span class="comment"># Tests the ridge_cv object optimizing individual penalties for each target</span></div>
<div class="line"><span class="lineno"> 1040</span> </div>
<div class="line"><span class="lineno"> 1041</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno"> 1042</span> </div>
<div class="line"><span class="lineno"> 1043</span>    <span class="comment"># Create random dataset with multiple targets. Each target should have</span></div>
<div class="line"><span class="lineno"> 1044</span>    <span class="comment"># a different optimal alpha.</span></div>
<div class="line"><span class="lineno"> 1045</span>    n_samples, n_features, n_targets = 20, 5, 3</div>
<div class="line"><span class="lineno"> 1046</span>    y = rng.randn(n_samples, n_targets)</div>
<div class="line"><span class="lineno"> 1047</span>    X = (</div>
<div class="line"><span class="lineno"> 1048</span>        np.dot(y[:, [0]], np.ones((1, n_features)))</div>
<div class="line"><span class="lineno"> 1049</span>        + np.dot(y[:, [1]], 0.05 * np.ones((1, n_features)))</div>
<div class="line"><span class="lineno"> 1050</span>        + np.dot(y[:, [2]], 0.001 * np.ones((1, n_features)))</div>
<div class="line"><span class="lineno"> 1051</span>        + rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1052</span>    )</div>
<div class="line"><span class="lineno"> 1053</span> </div>
<div class="line"><span class="lineno"> 1054</span>    alphas = (1, 100, 1000)</div>
<div class="line"><span class="lineno"> 1055</span> </div>
<div class="line"><span class="lineno"> 1056</span>    <span class="comment"># Find optimal alpha for each target</span></div>
<div class="line"><span class="lineno"> 1057</span>    optimal_alphas = [RidgeCV(alphas=alphas).fit(X, target).alpha_ <span class="keywordflow">for</span> target <span class="keywordflow">in</span> y.T]</div>
<div class="line"><span class="lineno"> 1058</span> </div>
<div class="line"><span class="lineno"> 1059</span>    <span class="comment"># Find optimal alphas for all targets simultaneously</span></div>
<div class="line"><span class="lineno"> 1060</span>    ridge_cv = RidgeCV(alphas=alphas, alpha_per_target=<span class="keyword">True</span>).fit(X, y)</div>
<div class="line"><span class="lineno"> 1061</span>    assert_array_equal(optimal_alphas, ridge_cv.alpha_)</div>
<div class="line"><span class="lineno"> 1062</span> </div>
<div class="line"><span class="lineno"> 1063</span>    <span class="comment"># The resulting regression weights should incorporate the different</span></div>
<div class="line"><span class="lineno"> 1064</span>    <span class="comment"># alpha values.</span></div>
<div class="line"><span class="lineno"> 1065</span>    assert_array_almost_equal(</div>
<div class="line"><span class="lineno"> 1066</span>        Ridge(alpha=ridge_cv.alpha_).fit(X, y).coef_, ridge_cv.coef_</div>
<div class="line"><span class="lineno"> 1067</span>    )</div>
<div class="line"><span class="lineno"> 1068</span> </div>
<div class="line"><span class="lineno"> 1069</span>    <span class="comment"># Test shape of alpha_ and cv_values_</span></div>
<div class="line"><span class="lineno"> 1070</span>    ridge_cv = RidgeCV(alphas=alphas, alpha_per_target=<span class="keyword">True</span>, store_cv_values=<span class="keyword">True</span>).fit(</div>
<div class="line"><span class="lineno"> 1071</span>        X, y</div>
<div class="line"><span class="lineno"> 1072</span>    )</div>
<div class="line"><span class="lineno"> 1073</span>    <span class="keyword">assert</span> ridge_cv.alpha_.shape == (n_targets,)</div>
<div class="line"><span class="lineno"> 1074</span>    <span class="keyword">assert</span> ridge_cv.best_score_.shape == (n_targets,)</div>
<div class="line"><span class="lineno"> 1075</span>    <span class="keyword">assert</span> ridge_cv.cv_values_.shape == (n_samples, len(alphas), n_targets)</div>
<div class="line"><span class="lineno"> 1076</span> </div>
<div class="line"><span class="lineno"> 1077</span>    <span class="comment"># Test edge case of there being only one alpha value</span></div>
<div class="line"><span class="lineno"> 1078</span>    ridge_cv = RidgeCV(alphas=1, alpha_per_target=<span class="keyword">True</span>, store_cv_values=<span class="keyword">True</span>).fit(X, y)</div>
<div class="line"><span class="lineno"> 1079</span>    <span class="keyword">assert</span> ridge_cv.alpha_.shape == (n_targets,)</div>
<div class="line"><span class="lineno"> 1080</span>    <span class="keyword">assert</span> ridge_cv.best_score_.shape == (n_targets,)</div>
<div class="line"><span class="lineno"> 1081</span>    <span class="keyword">assert</span> ridge_cv.cv_values_.shape == (n_samples, n_targets, 1)</div>
<div class="line"><span class="lineno"> 1082</span> </div>
<div class="line"><span class="lineno"> 1083</span>    <span class="comment"># Test edge case of there being only one target</span></div>
<div class="line"><span class="lineno"> 1084</span>    ridge_cv = RidgeCV(alphas=alphas, alpha_per_target=<span class="keyword">True</span>, store_cv_values=<span class="keyword">True</span>).fit(</div>
<div class="line"><span class="lineno"> 1085</span>        X, y[:, 0]</div>
<div class="line"><span class="lineno"> 1086</span>    )</div>
<div class="line"><span class="lineno"> 1087</span>    <span class="keyword">assert</span> np.isscalar(ridge_cv.alpha_)</div>
<div class="line"><span class="lineno"> 1088</span>    <span class="keyword">assert</span> np.isscalar(ridge_cv.best_score_)</div>
<div class="line"><span class="lineno"> 1089</span>    <span class="keyword">assert</span> ridge_cv.cv_values_.shape == (n_samples, len(alphas))</div>
<div class="line"><span class="lineno"> 1090</span> </div>
<div class="line"><span class="lineno"> 1091</span>    <span class="comment"># Try with a custom scoring function</span></div>
<div class="line"><span class="lineno"> 1092</span>    ridge_cv = RidgeCV(alphas=alphas, alpha_per_target=<span class="keyword">True</span>, scoring=<span class="stringliteral">&quot;r2&quot;</span>).fit(X, y)</div>
<div class="line"><span class="lineno"> 1093</span>    assert_array_equal(optimal_alphas, ridge_cv.alpha_)</div>
<div class="line"><span class="lineno"> 1094</span>    assert_array_almost_equal(</div>
<div class="line"><span class="lineno"> 1095</span>        Ridge(alpha=ridge_cv.alpha_).fit(X, y).coef_, ridge_cv.coef_</div>
<div class="line"><span class="lineno"> 1096</span>    )</div>
<div class="line"><span class="lineno"> 1097</span> </div>
<div class="line"><span class="lineno"> 1098</span>    <span class="comment"># Using a custom CV object should throw an error in combination with</span></div>
<div class="line"><span class="lineno"> 1099</span>    <span class="comment"># alpha_per_target=True</span></div>
<div class="line"><span class="lineno"> 1100</span>    ridge_cv = RidgeCV(alphas=alphas, cv=LeaveOneOut(), alpha_per_target=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno"> 1101</span>    msg = <span class="stringliteral">&quot;cv!=None and alpha_per_target=True are incompatible&quot;</span></div>
<div class="line"><span class="lineno"> 1102</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=msg):</div>
<div class="line"><span class="lineno"> 1103</span>        ridge_cv.fit(X, y)</div>
<div class="line"><span class="lineno"> 1104</span>    ridge_cv = RidgeCV(alphas=alphas, cv=6, alpha_per_target=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno"> 1105</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=msg):</div>
<div class="line"><span class="lineno"> 1106</span>        ridge_cv.fit(X, y)</div>
<div class="line"><span class="lineno"> 1107</span> </div>
<div class="line"><span class="lineno"> 1108</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="abf6e654ea81704532a2f54aa7cb7630a" name="abf6e654ea81704532a2f54aa7cb7630a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abf6e654ea81704532a2f54aa7cb7630a">&#9670;&#160;</a></span>test_ridge_fit_intercept_sparse()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_fit_intercept_sparse </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>with_sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check that ridge finds the same coefs and intercept on dense and sparse input
in the presence of sample weights.

For now only sparse_cg and lbfgs can correctly fit an intercept
with sparse X with default tol and max_iter.
'sag' is tested separately in test_ridge_fit_intercept_sparse_sag because it
requires more iterations and should raise a warning if default max_iter is used.
Other solvers raise an exception, as checked in
test_ridge_fit_intercept_sparse_error
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1570</span><span class="keyword">def </span>test_ridge_fit_intercept_sparse(solver, with_sample_weight, global_random_seed):</div>
<div class="line"><span class="lineno"> 1571</span>    <span class="stringliteral">&quot;&quot;&quot;Check that ridge finds the same coefs and intercept on dense and sparse input</span></div>
<div class="line"><span class="lineno"> 1572</span><span class="stringliteral">    in the presence of sample weights.</span></div>
<div class="line"><span class="lineno"> 1573</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1574</span><span class="stringliteral">    For now only sparse_cg and lbfgs can correctly fit an intercept</span></div>
<div class="line"><span class="lineno"> 1575</span><span class="stringliteral">    with sparse X with default tol and max_iter.</span></div>
<div class="line"><span class="lineno"> 1576</span><span class="stringliteral">    &#39;sag&#39; is tested separately in test_ridge_fit_intercept_sparse_sag because it</span></div>
<div class="line"><span class="lineno"> 1577</span><span class="stringliteral">    requires more iterations and should raise a warning if default max_iter is used.</span></div>
<div class="line"><span class="lineno"> 1578</span><span class="stringliteral">    Other solvers raise an exception, as checked in</span></div>
<div class="line"><span class="lineno"> 1579</span><span class="stringliteral">    test_ridge_fit_intercept_sparse_error</span></div>
<div class="line"><span class="lineno"> 1580</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1581</span>    positive = solver == <span class="stringliteral">&quot;lbfgs&quot;</span></div>
<div class="line"><span class="lineno"> 1582</span>    X, y = _make_sparse_offset_regression(</div>
<div class="line"><span class="lineno"> 1583</span>        n_features=20, random_state=global_random_seed, positive=positive</div>
<div class="line"><span class="lineno"> 1584</span>    )</div>
<div class="line"><span class="lineno"> 1585</span> </div>
<div class="line"><span class="lineno"> 1586</span>    sample_weight = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1587</span>    <span class="keywordflow">if</span> with_sample_weight:</div>
<div class="line"><span class="lineno"> 1588</span>        rng = np.random.RandomState(global_random_seed)</div>
<div class="line"><span class="lineno"> 1589</span>        sample_weight = 1.0 + rng.uniform(size=X.shape[0])</div>
<div class="line"><span class="lineno"> 1590</span> </div>
<div class="line"><span class="lineno"> 1591</span>    <span class="comment"># &quot;auto&quot; should switch to &quot;sparse_cg&quot; when X is sparse</span></div>
<div class="line"><span class="lineno"> 1592</span>    <span class="comment"># so the reference we use for both (&quot;auto&quot; and &quot;sparse_cg&quot;) is</span></div>
<div class="line"><span class="lineno"> 1593</span>    <span class="comment"># Ridge(solver=&quot;sparse_cg&quot;), fitted using the dense representation (note</span></div>
<div class="line"><span class="lineno"> 1594</span>    <span class="comment"># that &quot;sparse_cg&quot; can fit sparse or dense data)</span></div>
<div class="line"><span class="lineno"> 1595</span>    dense_solver = <span class="stringliteral">&quot;sparse_cg&quot;</span> <span class="keywordflow">if</span> solver == <span class="stringliteral">&quot;auto&quot;</span> <span class="keywordflow">else</span> solver</div>
<div class="line"><span class="lineno"> 1596</span>    dense_ridge = Ridge(solver=dense_solver, tol=1e-12, positive=positive)</div>
<div class="line"><span class="lineno"> 1597</span>    sparse_ridge = Ridge(solver=solver, tol=1e-12, positive=positive)</div>
<div class="line"><span class="lineno"> 1598</span> </div>
<div class="line"><span class="lineno"> 1599</span>    dense_ridge.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1600</span>    sparse_ridge.fit(sp.csr_matrix(X), y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1601</span> </div>
<div class="line"><span class="lineno"> 1602</span>    assert_allclose(dense_ridge.intercept_, sparse_ridge.intercept_)</div>
<div class="line"><span class="lineno"> 1603</span>    assert_allclose(dense_ridge.coef_, sparse_ridge.coef_, rtol=5e-7)</div>
<div class="line"><span class="lineno"> 1604</span> </div>
<div class="line"><span class="lineno"> 1605</span> </div>
<div class="line"><span class="lineno"> 1606</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, [&quot;saga&quot;, &quot;svd&quot;, &quot;cholesky&quot;])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a203b521ba6a9af7d01cc7b36322d54c9" name="a203b521ba6a9af7d01cc7b36322d54c9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a203b521ba6a9af7d01cc7b36322d54c9">&#9670;&#160;</a></span>test_ridge_fit_intercept_sparse_error()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_fit_intercept_sparse_error </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1607</span><span class="keyword">def </span>test_ridge_fit_intercept_sparse_error(solver):</div>
<div class="line"><span class="lineno"> 1608</span>    X, y = _make_sparse_offset_regression(n_features=20, random_state=0)</div>
<div class="line"><span class="lineno"> 1609</span>    X_csr = sp.csr_matrix(X)</div>
<div class="line"><span class="lineno"> 1610</span>    sparse_ridge = Ridge(solver=solver)</div>
<div class="line"><span class="lineno"> 1611</span>    err_msg = <span class="stringliteral">&quot;solver=&#39;{}&#39; does not support&quot;</span>.format(solver)</div>
<div class="line"><span class="lineno"> 1612</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=err_msg):</div>
<div class="line"><span class="lineno"> 1613</span>        sparse_ridge.fit(X_csr, y)</div>
<div class="line"><span class="lineno"> 1614</span> </div>
<div class="line"><span class="lineno"> 1615</span> </div>
<div class="line"><span class="lineno"> 1616</span><span class="preprocessor">@pytest.mark.parametrize(&quot;with_sample_weight&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="af0fddc87a5ad9b0d6e712ac69be0382a" name="af0fddc87a5ad9b0d6e712ac69be0382a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af0fddc87a5ad9b0d6e712ac69be0382a">&#9670;&#160;</a></span>test_ridge_fit_intercept_sparse_sag()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_fit_intercept_sparse_sag </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>with_sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1617</span><span class="keyword">def </span>test_ridge_fit_intercept_sparse_sag(with_sample_weight, global_random_seed):</div>
<div class="line"><span class="lineno"> 1618</span>    X, y = _make_sparse_offset_regression(</div>
<div class="line"><span class="lineno"> 1619</span>        n_features=5, n_samples=20, random_state=global_random_seed, X_offset=5.0</div>
<div class="line"><span class="lineno"> 1620</span>    )</div>
<div class="line"><span class="lineno"> 1621</span>    <span class="keywordflow">if</span> with_sample_weight:</div>
<div class="line"><span class="lineno"> 1622</span>        rng = np.random.RandomState(global_random_seed)</div>
<div class="line"><span class="lineno"> 1623</span>        sample_weight = 1.0 + rng.uniform(size=X.shape[0])</div>
<div class="line"><span class="lineno"> 1624</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1625</span>        sample_weight = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1626</span>    X_csr = sp.csr_matrix(X)</div>
<div class="line"><span class="lineno"> 1627</span> </div>
<div class="line"><span class="lineno"> 1628</span>    params = dict(</div>
<div class="line"><span class="lineno"> 1629</span>        alpha=1.0, solver=<span class="stringliteral">&quot;sag&quot;</span>, fit_intercept=<span class="keyword">True</span>, tol=1e-10, max_iter=100000</div>
<div class="line"><span class="lineno"> 1630</span>    )</div>
<div class="line"><span class="lineno"> 1631</span>    dense_ridge = Ridge(**params)</div>
<div class="line"><span class="lineno"> 1632</span>    sparse_ridge = Ridge(**params)</div>
<div class="line"><span class="lineno"> 1633</span>    dense_ridge.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1634</span>    <span class="keyword">with</span> warnings.catch_warnings():</div>
<div class="line"><span class="lineno"> 1635</span>        warnings.simplefilter(<span class="stringliteral">&quot;error&quot;</span>, UserWarning)</div>
<div class="line"><span class="lineno"> 1636</span>        sparse_ridge.fit(X_csr, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1637</span>    assert_allclose(dense_ridge.intercept_, sparse_ridge.intercept_, rtol=1e-4)</div>
<div class="line"><span class="lineno"> 1638</span>    assert_allclose(dense_ridge.coef_, sparse_ridge.coef_, rtol=1e-4)</div>
<div class="line"><span class="lineno"> 1639</span>    <span class="keyword">with</span> pytest.warns(UserWarning, match=<span class="stringliteral">&#39;&quot;sag&quot; solver requires.*&#39;</span>):</div>
<div class="line"><span class="lineno"> 1640</span>        Ridge(solver=<span class="stringliteral">&quot;sag&quot;</span>, fit_intercept=<span class="keyword">True</span>, tol=1e-3, max_iter=<span class="keywordtype">None</span>).fit(X_csr, y)</div>
<div class="line"><span class="lineno"> 1641</span> </div>
<div class="line"><span class="lineno"> 1642</span> </div>
<div class="line"><span class="lineno"> 1643</span><span class="preprocessor">@pytest.mark.parametrize(&quot;return_intercept&quot;, [False, True])</span></div>
<div class="line"><span class="lineno"> 1644</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, np.ones(1000)</span>])</div>
<div class="line"><span class="lineno"> 1645</span><span class="preprocessor">@pytest.mark.parametrize(&quot;arr_type&quot;, [np.array, sp.csr_matrix])</span></div>
<div class="line"><span class="lineno"> 1646</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1647</span>    <span class="stringliteral">&quot;solver&quot;</span>, [<span class="stringliteral">&quot;auto&quot;</span>, <span class="stringliteral">&quot;sparse_cg&quot;</span>, <span class="stringliteral">&quot;cholesky&quot;</span>, <span class="stringliteral">&quot;lsqr&quot;</span>, <span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>, <span class="stringliteral">&quot;lbfgs&quot;</span>]</div>
<div class="line"><span class="lineno"> 1648</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a6edb0e4dc0ca2cd2555e1fb0445d0b3b" name="a6edb0e4dc0ca2cd2555e1fb0445d0b3b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6edb0e4dc0ca2cd2555e1fb0445d0b3b">&#9670;&#160;</a></span>test_ridge_gcv_cv_values_not_stored()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_gcv_cv_values_not_stored </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ridge</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>make_dataset</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1017</span><span class="keyword">def </span>test_ridge_gcv_cv_values_not_stored(ridge, make_dataset):</div>
<div class="line"><span class="lineno"> 1018</span>    <span class="comment"># Check that `cv_values_` is not stored when store_cv_values is False</span></div>
<div class="line"><span class="lineno"> 1019</span>    X, y = make_dataset(n_samples=6, random_state=42)</div>
<div class="line"><span class="lineno"> 1020</span>    ridge.fit(X, y)</div>
<div class="line"><span class="lineno"> 1021</span>    <span class="keyword">assert</span> <span class="keywordflow">not</span> hasattr(ridge, <span class="stringliteral">&quot;cv_values_&quot;</span>)</div>
<div class="line"><span class="lineno"> 1022</span> </div>
<div class="line"><span class="lineno"> 1023</span> </div>
<div class="line"><span class="lineno"> 1024</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1025</span>    <span class="stringliteral">&quot;ridge, make_dataset&quot;</span>,</div>
<div class="line"><span class="lineno"> 1026</span>    [(RidgeCV(), make_regression), (RidgeClassifierCV(), make_classification)],</div>
<div class="line"><span class="lineno"> 1027</span>)</div>
<div class="line"><span class="lineno"> 1028</span><span class="preprocessor">@pytest.mark.parametrize(&quot;cv&quot;, [None, 3])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="aba9cdead74873310486511879657c5cc" name="aba9cdead74873310486511879657c5cc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aba9cdead74873310486511879657c5cc">&#9670;&#160;</a></span>test_ridge_gcv_sample_weights()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_gcv_sample_weights </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>gcv_mode</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X_constructor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>noise</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  861</span>):</div>
<div class="line"><span class="lineno">  862</span>    alphas = [1e-3, 0.1, 1.0, 10.0, 1e3]</div>
<div class="line"><span class="lineno">  863</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  864</span>    n_targets = y_shape[-1] <span class="keywordflow">if</span> len(y_shape) == 2 <span class="keywordflow">else</span> 1</div>
<div class="line"><span class="lineno">  865</span>    X, y = _make_sparse_offset_regression(</div>
<div class="line"><span class="lineno">  866</span>        n_samples=11,</div>
<div class="line"><span class="lineno">  867</span>        n_features=n_features,</div>
<div class="line"><span class="lineno">  868</span>        n_targets=n_targets,</div>
<div class="line"><span class="lineno">  869</span>        random_state=0,</div>
<div class="line"><span class="lineno">  870</span>        shuffle=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno">  871</span>        noise=noise,</div>
<div class="line"><span class="lineno">  872</span>    )</div>
<div class="line"><span class="lineno">  873</span>    y = y.reshape(y_shape)</div>
<div class="line"><span class="lineno">  874</span> </div>
<div class="line"><span class="lineno">  875</span>    sample_weight = 3 * rng.randn(len(X))</div>
<div class="line"><span class="lineno">  876</span>    sample_weight = (sample_weight - sample_weight.min() + 1).astype(int)</div>
<div class="line"><span class="lineno">  877</span>    indices = np.repeat(np.arange(X.shape[0]), sample_weight)</div>
<div class="line"><span class="lineno">  878</span>    sample_weight = sample_weight.astype(float)</div>
<div class="line"><span class="lineno">  879</span>    X_tiled, y_tiled = X[indices], y[indices]</div>
<div class="line"><span class="lineno">  880</span> </div>
<div class="line"><span class="lineno">  881</span>    cv = GroupKFold(n_splits=X.shape[0])</div>
<div class="line"><span class="lineno">  882</span>    splits = cv.split(X_tiled, y_tiled, groups=indices)</div>
<div class="line"><span class="lineno">  883</span>    kfold = RidgeCV(</div>
<div class="line"><span class="lineno">  884</span>        alphas=alphas,</div>
<div class="line"><span class="lineno">  885</span>        cv=splits,</div>
<div class="line"><span class="lineno">  886</span>        scoring=<span class="stringliteral">&quot;neg_mean_squared_error&quot;</span>,</div>
<div class="line"><span class="lineno">  887</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  888</span>    )</div>
<div class="line"><span class="lineno">  889</span>    kfold.fit(X_tiled, y_tiled)</div>
<div class="line"><span class="lineno">  890</span> </div>
<div class="line"><span class="lineno">  891</span>    ridge_reg = Ridge(alpha=kfold.alpha_, fit_intercept=fit_intercept)</div>
<div class="line"><span class="lineno">  892</span>    splits = cv.split(X_tiled, y_tiled, groups=indices)</div>
<div class="line"><span class="lineno">  893</span>    predictions = cross_val_predict(ridge_reg, X_tiled, y_tiled, cv=splits)</div>
<div class="line"><span class="lineno">  894</span>    kfold_errors = (y_tiled - predictions) ** 2</div>
<div class="line"><span class="lineno">  895</span>    kfold_errors = [</div>
<div class="line"><span class="lineno">  896</span>        np.sum(kfold_errors[indices == i], axis=0) <span class="keywordflow">for</span> i <span class="keywordflow">in</span> np.arange(X.shape[0])</div>
<div class="line"><span class="lineno">  897</span>    ]</div>
<div class="line"><span class="lineno">  898</span>    kfold_errors = np.asarray(kfold_errors)</div>
<div class="line"><span class="lineno">  899</span> </div>
<div class="line"><span class="lineno">  900</span>    X_gcv = X_constructor(X)</div>
<div class="line"><span class="lineno">  901</span>    gcv_ridge = RidgeCV(</div>
<div class="line"><span class="lineno">  902</span>        alphas=alphas,</div>
<div class="line"><span class="lineno">  903</span>        store_cv_values=<span class="keyword">True</span>,</div>
<div class="line"><span class="lineno">  904</span>        gcv_mode=gcv_mode,</div>
<div class="line"><span class="lineno">  905</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  906</span>    )</div>
<div class="line"><span class="lineno">  907</span>    gcv_ridge.fit(X_gcv, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  908</span>    <span class="keywordflow">if</span> len(y_shape) == 2:</div>
<div class="line"><span class="lineno">  909</span>        gcv_errors = gcv_ridge.cv_values_[:, :, alphas.index(kfold.alpha_)]</div>
<div class="line"><span class="lineno">  910</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  911</span>        gcv_errors = gcv_ridge.cv_values_[:, alphas.index(kfold.alpha_)]</div>
<div class="line"><span class="lineno">  912</span> </div>
<div class="line"><span class="lineno">  913</span>    <span class="keyword">assert</span> kfold.alpha_ == pytest.approx(gcv_ridge.alpha_)</div>
<div class="line"><span class="lineno">  914</span>    assert_allclose(gcv_errors, kfold_errors, rtol=1e-3)</div>
<div class="line"><span class="lineno">  915</span>    assert_allclose(gcv_ridge.coef_, kfold.coef_, rtol=1e-3)</div>
<div class="line"><span class="lineno">  916</span>    assert_allclose(gcv_ridge.intercept_, kfold.intercept_, rtol=1e-3)</div>
<div class="line"><span class="lineno">  917</span> </div>
<div class="line"><span class="lineno">  918</span> </div>
<div class="line"><span class="lineno">  919</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sparse&quot;, [True, False])</span></div>
<div class="line"><span class="lineno">  920</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  921</span>    <span class="stringliteral">&quot;mode, mode_n_greater_than_p, mode_p_greater_than_n&quot;</span>,</div>
<div class="line"><span class="lineno">  922</span>    [</div>
<div class="line"><span class="lineno">  923</span>        (<span class="keywordtype">None</span>, <span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;eigen&quot;</span>),</div>
<div class="line"><span class="lineno">  924</span>        (<span class="stringliteral">&quot;auto&quot;</span>, <span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;eigen&quot;</span>),</div>
<div class="line"><span class="lineno">  925</span>        (<span class="stringliteral">&quot;eigen&quot;</span>, <span class="stringliteral">&quot;eigen&quot;</span>, <span class="stringliteral">&quot;eigen&quot;</span>),</div>
<div class="line"><span class="lineno">  926</span>        (<span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;svd&quot;</span>),</div>
<div class="line"><span class="lineno">  927</span>    ],</div>
<div class="line"><span class="lineno">  928</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="afb2365a5ae4d88b70d6f4db53961afdd" name="afb2365a5ae4d88b70d6f4db53961afdd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afb2365a5ae4d88b70d6f4db53961afdd">&#9670;&#160;</a></span>test_ridge_gcv_vs_ridge_loo_cv()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_gcv_vs_ridge_loo_cv </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>gcv_mode</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X_constructor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>noise</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  780</span>):</div>
<div class="line"><span class="lineno">  781</span>    n_samples, n_features = X_shape</div>
<div class="line"><span class="lineno">  782</span>    n_targets = y_shape[-1] <span class="keywordflow">if</span> len(y_shape) == 2 <span class="keywordflow">else</span> 1</div>
<div class="line"><span class="lineno">  783</span>    X, y = _make_sparse_offset_regression(</div>
<div class="line"><span class="lineno">  784</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  785</span>        n_features=n_features,</div>
<div class="line"><span class="lineno">  786</span>        n_targets=n_targets,</div>
<div class="line"><span class="lineno">  787</span>        random_state=0,</div>
<div class="line"><span class="lineno">  788</span>        shuffle=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno">  789</span>        noise=noise,</div>
<div class="line"><span class="lineno">  790</span>        n_informative=5,</div>
<div class="line"><span class="lineno">  791</span>    )</div>
<div class="line"><span class="lineno">  792</span>    y = y.reshape(y_shape)</div>
<div class="line"><span class="lineno">  793</span> </div>
<div class="line"><span class="lineno">  794</span>    alphas = [1e-3, 0.1, 1.0, 10.0, 1e3]</div>
<div class="line"><span class="lineno">  795</span>    loo_ridge = RidgeCV(</div>
<div class="line"><span class="lineno">  796</span>        cv=n_samples,</div>
<div class="line"><span class="lineno">  797</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  798</span>        alphas=alphas,</div>
<div class="line"><span class="lineno">  799</span>        scoring=<span class="stringliteral">&quot;neg_mean_squared_error&quot;</span>,</div>
<div class="line"><span class="lineno">  800</span>    )</div>
<div class="line"><span class="lineno">  801</span>    gcv_ridge = RidgeCV(</div>
<div class="line"><span class="lineno">  802</span>        gcv_mode=gcv_mode,</div>
<div class="line"><span class="lineno">  803</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  804</span>        alphas=alphas,</div>
<div class="line"><span class="lineno">  805</span>    )</div>
<div class="line"><span class="lineno">  806</span> </div>
<div class="line"><span class="lineno">  807</span>    loo_ridge.fit(X, y)</div>
<div class="line"><span class="lineno">  808</span> </div>
<div class="line"><span class="lineno">  809</span>    X_gcv = X_constructor(X)</div>
<div class="line"><span class="lineno">  810</span>    gcv_ridge.fit(X_gcv, y)</div>
<div class="line"><span class="lineno">  811</span> </div>
<div class="line"><span class="lineno">  812</span>    <span class="keyword">assert</span> gcv_ridge.alpha_ == pytest.approx(loo_ridge.alpha_)</div>
<div class="line"><span class="lineno">  813</span>    assert_allclose(gcv_ridge.coef_, loo_ridge.coef_, rtol=1e-3)</div>
<div class="line"><span class="lineno">  814</span>    assert_allclose(gcv_ridge.intercept_, loo_ridge.intercept_, rtol=1e-3)</div>
<div class="line"><span class="lineno">  815</span> </div>
<div class="line"><span class="lineno">  816</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a058873709e9c3e09aeef1652b848333e" name="a058873709e9c3e09aeef1652b848333e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a058873709e9c3e09aeef1652b848333e">&#9670;&#160;</a></span>test_ridge_ground_truth_positive_test()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_ground_truth_positive_test </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that Ridge w/wo positive converges to the same solution.

Ridge with positive=True and positive=False must give the same
when the ground truth coefs are all positive.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1861</span><span class="keyword">def </span>test_ridge_ground_truth_positive_test(fit_intercept, alpha):</div>
<div class="line"><span class="lineno"> 1862</span>    <span class="stringliteral">&quot;&quot;&quot;Test that Ridge w/wo positive converges to the same solution.</span></div>
<div class="line"><span class="lineno"> 1863</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1864</span><span class="stringliteral">    Ridge with positive=True and positive=False must give the same</span></div>
<div class="line"><span class="lineno"> 1865</span><span class="stringliteral">    when the ground truth coefs are all positive.</span></div>
<div class="line"><span class="lineno"> 1866</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1867</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno"> 1868</span>    X = rng.randn(300, 100)</div>
<div class="line"><span class="lineno"> 1869</span>    coef = rng.uniform(0.1, 1.0, size=X.shape[1])</div>
<div class="line"><span class="lineno"> 1870</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno"> 1871</span>        intercept = 1</div>
<div class="line"><span class="lineno"> 1872</span>        y = X @ coef + intercept</div>
<div class="line"><span class="lineno"> 1873</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1874</span>        y = X @ coef</div>
<div class="line"><span class="lineno"> 1875</span>    y += rng.normal(size=X.shape[0]) * 0.01</div>
<div class="line"><span class="lineno"> 1876</span> </div>
<div class="line"><span class="lineno"> 1877</span>    results = []</div>
<div class="line"><span class="lineno"> 1878</span>    <span class="keywordflow">for</span> positive <span class="keywordflow">in</span> [<span class="keyword">True</span>, <span class="keyword">False</span>]:</div>
<div class="line"><span class="lineno"> 1879</span>        model = Ridge(</div>
<div class="line"><span class="lineno"> 1880</span>            alpha=alpha, positive=positive, fit_intercept=fit_intercept, tol=1e-10</div>
<div class="line"><span class="lineno"> 1881</span>        )</div>
<div class="line"><span class="lineno"> 1882</span>        results.append(model.fit(X, y).coef_)</div>
<div class="line"><span class="lineno"> 1883</span>    assert_allclose(*results, atol=1e-6, rtol=0)</div>
<div class="line"><span class="lineno"> 1884</span> </div>
<div class="line"><span class="lineno"> 1885</span> </div>
<div class="line"><span class="lineno"> 1886</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1887</span>    <span class="stringliteral">&quot;solver&quot;</span>, [<span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;cholesky&quot;</span>, <span class="stringliteral">&quot;lsqr&quot;</span>, <span class="stringliteral">&quot;sparse_cg&quot;</span>, <span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>]</div>
<div class="line"><span class="lineno"> 1888</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a532d5f8767c3d9002edf7cbed411aec5" name="a532d5f8767c3d9002edf7cbed411aec5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a532d5f8767c3d9002edf7cbed411aec5">&#9670;&#160;</a></span>test_ridge_individual_penalties()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_individual_penalties </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  586</span><span class="keyword">def </span>test_ridge_individual_penalties():</div>
<div class="line"><span class="lineno">  587</span>    <span class="comment"># Tests the ridge object using individual penalties</span></div>
<div class="line"><span class="lineno">  588</span> </div>
<div class="line"><span class="lineno">  589</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno">  590</span> </div>
<div class="line"><span class="lineno">  591</span>    n_samples, n_features, n_targets = 20, 10, 5</div>
<div class="line"><span class="lineno">  592</span>    X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno">  593</span>    y = rng.randn(n_samples, n_targets)</div>
<div class="line"><span class="lineno">  594</span> </div>
<div class="line"><span class="lineno">  595</span>    penalties = np.arange(n_targets)</div>
<div class="line"><span class="lineno">  596</span> </div>
<div class="line"><span class="lineno">  597</span>    coef_cholesky = np.array(</div>
<div class="line"><span class="lineno">  598</span>        [</div>
<div class="line"><span class="lineno">  599</span>            Ridge(alpha=alpha, solver=<span class="stringliteral">&quot;cholesky&quot;</span>).fit(X, target).coef_</div>
<div class="line"><span class="lineno">  600</span>            <span class="keywordflow">for</span> alpha, target <span class="keywordflow">in</span> zip(penalties, y.T)</div>
<div class="line"><span class="lineno">  601</span>        ]</div>
<div class="line"><span class="lineno">  602</span>    )</div>
<div class="line"><span class="lineno">  603</span> </div>
<div class="line"><span class="lineno">  604</span>    coefs_indiv_pen = [</div>
<div class="line"><span class="lineno">  605</span>        Ridge(alpha=penalties, solver=solver, tol=1e-12).fit(X, y).coef_</div>
<div class="line"><span class="lineno">  606</span>        <span class="keywordflow">for</span> solver <span class="keywordflow">in</span> [<span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;sparse_cg&quot;</span>, <span class="stringliteral">&quot;lsqr&quot;</span>, <span class="stringliteral">&quot;cholesky&quot;</span>, <span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>]</div>
<div class="line"><span class="lineno">  607</span>    ]</div>
<div class="line"><span class="lineno">  608</span>    <span class="keywordflow">for</span> coef_indiv_pen <span class="keywordflow">in</span> coefs_indiv_pen:</div>
<div class="line"><span class="lineno">  609</span>        assert_array_almost_equal(coef_cholesky, coef_indiv_pen)</div>
<div class="line"><span class="lineno">  610</span> </div>
<div class="line"><span class="lineno">  611</span>    <span class="comment"># Test error is raised when number of targets and penalties do not match.</span></div>
<div class="line"><span class="lineno">  612</span>    ridge = Ridge(alpha=penalties[:-1])</div>
<div class="line"><span class="lineno">  613</span>    err_msg = <span class="stringliteral">&quot;Number of targets and number of penalties do not correspond: 4 != 5&quot;</span></div>
<div class="line"><span class="lineno">  614</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=err_msg):</div>
<div class="line"><span class="lineno">  615</span>        ridge.fit(X, y)</div>
<div class="line"><span class="lineno">  616</span> </div>
<div class="line"><span class="lineno">  617</span> </div>
<div class="line"><span class="lineno">  618</span><span class="preprocessor">@pytest.mark.parametrize(&quot;n_col&quot;, [()</span>, (1,), (3,)])</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae9662805b3391dc2e31a4baac38862ac" name="ae9662805b3391dc2e31a4baac38862ac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae9662805b3391dc2e31a4baac38862ac">&#9670;&#160;</a></span>test_ridge_intercept()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_intercept </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  547</span><span class="keyword">def </span>test_ridge_intercept():</div>
<div class="line"><span class="lineno">  548</span>    <span class="comment"># Test intercept with multiple targets GH issue #708</span></div>
<div class="line"><span class="lineno">  549</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  550</span>    n_samples, n_features = 5, 10</div>
<div class="line"><span class="lineno">  551</span>    X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno">  552</span>    y = rng.randn(n_samples)</div>
<div class="line"><span class="lineno">  553</span>    Y = np.c_[y, 1.0 + y]</div>
<div class="line"><span class="lineno">  554</span> </div>
<div class="line"><span class="lineno">  555</span>    ridge = Ridge()</div>
<div class="line"><span class="lineno">  556</span> </div>
<div class="line"><span class="lineno">  557</span>    ridge.fit(X, y)</div>
<div class="line"><span class="lineno">  558</span>    intercept = ridge.intercept_</div>
<div class="line"><span class="lineno">  559</span> </div>
<div class="line"><span class="lineno">  560</span>    ridge.fit(X, Y)</div>
<div class="line"><span class="lineno">  561</span>    assert_almost_equal(ridge.intercept_[0], intercept)</div>
<div class="line"><span class="lineno">  562</span>    assert_almost_equal(ridge.intercept_[1], intercept + 1.0)</div>
<div class="line"><span class="lineno">  563</span> </div>
<div class="line"><span class="lineno">  564</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a1270a62a81a6a4f2dec065e90967720b" name="a1270a62a81a6a4f2dec065e90967720b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1270a62a81a6a4f2dec065e90967720b">&#9670;&#160;</a></span>test_ridge_loo_cv_asym_scoring()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_loo_cv_asym_scoring </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  817</span><span class="keyword">def </span>test_ridge_loo_cv_asym_scoring():</div>
<div class="line"><span class="lineno">  818</span>    <span class="comment"># checking on asymmetric scoring</span></div>
<div class="line"><span class="lineno">  819</span>    scoring = <span class="stringliteral">&quot;explained_variance&quot;</span></div>
<div class="line"><span class="lineno">  820</span>    n_samples, n_features = 10, 5</div>
<div class="line"><span class="lineno">  821</span>    n_targets = 1</div>
<div class="line"><span class="lineno">  822</span>    X, y = _make_sparse_offset_regression(</div>
<div class="line"><span class="lineno">  823</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  824</span>        n_features=n_features,</div>
<div class="line"><span class="lineno">  825</span>        n_targets=n_targets,</div>
<div class="line"><span class="lineno">  826</span>        random_state=0,</div>
<div class="line"><span class="lineno">  827</span>        shuffle=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno">  828</span>        noise=1,</div>
<div class="line"><span class="lineno">  829</span>        n_informative=5,</div>
<div class="line"><span class="lineno">  830</span>    )</div>
<div class="line"><span class="lineno">  831</span> </div>
<div class="line"><span class="lineno">  832</span>    alphas = [1e-3, 0.1, 1.0, 10.0, 1e3]</div>
<div class="line"><span class="lineno">  833</span>    loo_ridge = RidgeCV(</div>
<div class="line"><span class="lineno">  834</span>        cv=n_samples, fit_intercept=<span class="keyword">True</span>, alphas=alphas, scoring=scoring</div>
<div class="line"><span class="lineno">  835</span>    )</div>
<div class="line"><span class="lineno">  836</span> </div>
<div class="line"><span class="lineno">  837</span>    gcv_ridge = RidgeCV(fit_intercept=<span class="keyword">True</span>, alphas=alphas, scoring=scoring)</div>
<div class="line"><span class="lineno">  838</span> </div>
<div class="line"><span class="lineno">  839</span>    loo_ridge.fit(X, y)</div>
<div class="line"><span class="lineno">  840</span>    gcv_ridge.fit(X, y)</div>
<div class="line"><span class="lineno">  841</span> </div>
<div class="line"><span class="lineno">  842</span>    <span class="keyword">assert</span> gcv_ridge.alpha_ == pytest.approx(loo_ridge.alpha_)</div>
<div class="line"><span class="lineno">  843</span>    assert_allclose(gcv_ridge.coef_, loo_ridge.coef_, rtol=1e-3)</div>
<div class="line"><span class="lineno">  844</span>    assert_allclose(gcv_ridge.intercept_, loo_ridge.intercept_, rtol=1e-3)</div>
<div class="line"><span class="lineno">  845</span> </div>
<div class="line"><span class="lineno">  846</span> </div>
<div class="line"><span class="lineno">  847</span><span class="preprocessor">@pytest.mark.parametrize(&quot;gcv_mode&quot;, [&quot;svd&quot;, &quot;eigen&quot;])</span></div>
<div class="line"><span class="lineno">  848</span><span class="preprocessor">@pytest.mark.parametrize(&quot;X_constructor&quot;, [np.asarray, sp.csr_matrix])</span></div>
<div class="line"><span class="lineno">  849</span><span class="preprocessor">@pytest.mark.parametrize(&quot;n_features&quot;, [8, 20])</span></div>
<div class="line"><span class="lineno">  850</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  851</span>    <span class="stringliteral">&quot;y_shape, fit_intercept, noise&quot;</span>,</div>
<div class="line"><span class="lineno">  852</span>    [</div>
<div class="line"><span class="lineno">  853</span>        ((11,), <span class="keyword">True</span>, 1.0),</div>
<div class="line"><span class="lineno">  854</span>        ((11, 1), <span class="keyword">True</span>, 20.0),</div>
<div class="line"><span class="lineno">  855</span>        ((11, 3), <span class="keyword">True</span>, 150.0),</div>
<div class="line"><span class="lineno">  856</span>        ((11, 3), <span class="keyword">False</span>, 30.0),</div>
<div class="line"><span class="lineno">  857</span>    ],</div>
<div class="line"><span class="lineno">  858</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae9757b138d0636d34eec2fd9d9b2cc4a" name="ae9757b138d0636d34eec2fd9d9b2cc4a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae9757b138d0636d34eec2fd9d9b2cc4a">&#9670;&#160;</a></span>test_ridge_positive_error_test()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_positive_error_test </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test input validation for positive argument in Ridge.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1889</span><span class="keyword">def </span>test_ridge_positive_error_test(solver):</div>
<div class="line"><span class="lineno"> 1890</span>    <span class="stringliteral">&quot;&quot;&quot;Test input validation for positive argument in Ridge.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1891</span>    alpha = 0.1</div>
<div class="line"><span class="lineno"> 1892</span>    X = np.array([[1, 2], [3, 4]])</div>
<div class="line"><span class="lineno"> 1893</span>    coef = np.array([1, -1])</div>
<div class="line"><span class="lineno"> 1894</span>    y = X @ coef</div>
<div class="line"><span class="lineno"> 1895</span> </div>
<div class="line"><span class="lineno"> 1896</span>    model = Ridge(alpha=alpha, positive=<span class="keyword">True</span>, solver=solver, fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1897</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=<span class="stringliteral">&quot;does not support positive&quot;</span>):</div>
<div class="line"><span class="lineno"> 1898</span>        model.fit(X, y)</div>
<div class="line"><span class="lineno"> 1899</span> </div>
<div class="line"><span class="lineno"> 1900</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=<span class="stringliteral">&quot;only &#39;lbfgs&#39; solver can be used&quot;</span>):</div>
<div class="line"><span class="lineno"> 1901</span>        _, _ = ridge_regression(</div>
<div class="line"><span class="lineno"> 1902</span>            X, y, alpha, positive=<span class="keyword">True</span>, solver=solver, return_intercept=<span class="keyword">False</span></div>
<div class="line"><span class="lineno"> 1903</span>        )</div>
<div class="line"><span class="lineno"> 1904</span> </div>
<div class="line"><span class="lineno"> 1905</span> </div>
<div class="line"><span class="lineno"> 1906</span><span class="preprocessor">@pytest.mark.parametrize(&quot;alpha&quot;, [1e-3, 1e-2, 0.1, 1.0])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a3d0d19f55663ab3e53f45047b8997a5f" name="a3d0d19f55663ab3e53f45047b8997a5f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3d0d19f55663ab3e53f45047b8997a5f">&#9670;&#160;</a></span>test_ridge_positive_regression_test()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_positive_regression_test </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that positive Ridge finds true positive coefficients.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1842</span><span class="keyword">def </span>test_ridge_positive_regression_test(solver, fit_intercept, alpha):</div>
<div class="line"><span class="lineno"> 1843</span>    <span class="stringliteral">&quot;&quot;&quot;Test that positive Ridge finds true positive coefficients.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1844</span>    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])</div>
<div class="line"><span class="lineno"> 1845</span>    coef = np.array([1, -10])</div>
<div class="line"><span class="lineno"> 1846</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno"> 1847</span>        intercept = 20</div>
<div class="line"><span class="lineno"> 1848</span>        y = X.dot(coef) + intercept</div>
<div class="line"><span class="lineno"> 1849</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1850</span>        y = X.dot(coef)</div>
<div class="line"><span class="lineno"> 1851</span> </div>
<div class="line"><span class="lineno"> 1852</span>    model = Ridge(</div>
<div class="line"><span class="lineno"> 1853</span>        alpha=alpha, positive=<span class="keyword">True</span>, solver=solver, fit_intercept=fit_intercept</div>
<div class="line"><span class="lineno"> 1854</span>    )</div>
<div class="line"><span class="lineno"> 1855</span>    model.fit(X, y)</div>
<div class="line"><span class="lineno"> 1856</span>    <span class="keyword">assert</span> np.all(model.coef_ &gt;= 0)</div>
<div class="line"><span class="lineno"> 1857</span> </div>
<div class="line"><span class="lineno"> 1858</span> </div>
<div class="line"><span class="lineno"> 1859</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
<div class="line"><span class="lineno"> 1860</span><span class="preprocessor">@pytest.mark.parametrize(&quot;alpha&quot;, [1e-3, 1e-2, 0.1, 1.0])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a4bf6b96f2996b73688507fce991f2622" name="a4bf6b96f2996b73688507fce991f2622"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4bf6b96f2996b73688507fce991f2622">&#9670;&#160;</a></span>test_ridge_regression()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ols_ridge_dataset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that Ridge converges for all solvers to correct solution.

We work with a simple constructed data set with known solution.
</pre> <div class="fragment"><div class="line"><span class="lineno">  159</span><span class="keyword">def </span>test_ridge_regression(solver, fit_intercept, ols_ridge_dataset, global_random_seed):</div>
<div class="line"><span class="lineno">  160</span>    <span class="stringliteral">&quot;&quot;&quot;Test that Ridge converges for all solvers to correct solution.</span></div>
<div class="line"><span class="lineno">  161</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  162</span><span class="stringliteral">    We work with a simple constructed data set with known solution.</span></div>
<div class="line"><span class="lineno">  163</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  164</span>    X, y, _, coef = ols_ridge_dataset</div>
<div class="line"><span class="lineno">  165</span>    alpha = 1.0  <span class="comment"># because ols_ridge_dataset uses this.</span></div>
<div class="line"><span class="lineno">  166</span>    params = dict(</div>
<div class="line"><span class="lineno">  167</span>        alpha=alpha,</div>
<div class="line"><span class="lineno">  168</span>        fit_intercept=<span class="keyword">True</span>,</div>
<div class="line"><span class="lineno">  169</span>        solver=solver,</div>
<div class="line"><span class="lineno">  170</span>        tol=1e-15 <span class="keywordflow">if</span> solver <span class="keywordflow">in</span> (<span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>) <span class="keywordflow">else</span> 1e-10,</div>
<div class="line"><span class="lineno">  171</span>        random_state=global_random_seed,</div>
<div class="line"><span class="lineno">  172</span>    )</div>
<div class="line"><span class="lineno">  173</span> </div>
<div class="line"><span class="lineno">  174</span>    <span class="comment"># Calculate residuals and R2.</span></div>
<div class="line"><span class="lineno">  175</span>    res_null = y - np.mean(y)</div>
<div class="line"><span class="lineno">  176</span>    res_Ridge = y - X @ coef</div>
<div class="line"><span class="lineno">  177</span>    R2_Ridge = 1 - np.sum(res_Ridge**2) / np.sum(res_null**2)</div>
<div class="line"><span class="lineno">  178</span> </div>
<div class="line"><span class="lineno">  179</span>    model = Ridge(**params)</div>
<div class="line"><span class="lineno">  180</span>    X = X[:, :-1]  <span class="comment"># remove intercept</span></div>
<div class="line"><span class="lineno">  181</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno">  182</span>        intercept = coef[-1]</div>
<div class="line"><span class="lineno">  183</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  184</span>        X = X - X.mean(axis=0)</div>
<div class="line"><span class="lineno">  185</span>        y = y - y.mean()</div>
<div class="line"><span class="lineno">  186</span>        intercept = 0</div>
<div class="line"><span class="lineno">  187</span>    model.fit(X, y)</div>
<div class="line"><span class="lineno">  188</span>    coef = coef[:-1]</div>
<div class="line"><span class="lineno">  189</span> </div>
<div class="line"><span class="lineno">  190</span>    <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  191</span>    assert_allclose(model.coef_, coef)</div>
<div class="line"><span class="lineno">  192</span>    <span class="keyword">assert</span> model.score(X, y) == pytest.approx(R2_Ridge)</div>
<div class="line"><span class="lineno">  193</span> </div>
<div class="line"><span class="lineno">  194</span>    <span class="comment"># Same with sample_weight.</span></div>
<div class="line"><span class="lineno">  195</span>    model = Ridge(**params).fit(X, y, sample_weight=np.ones(X.shape[0]))</div>
<div class="line"><span class="lineno">  196</span>    <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  197</span>    assert_allclose(model.coef_, coef)</div>
<div class="line"><span class="lineno">  198</span>    <span class="keyword">assert</span> model.score(X, y) == pytest.approx(R2_Ridge)</div>
<div class="line"><span class="lineno">  199</span> </div>
<div class="line"><span class="lineno">  200</span> </div>
<div class="line"><span class="lineno">  201</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, SOLVERS)</span></div>
<div class="line"><span class="lineno">  202</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="ac53ef9ec2530193f33c80ededa0c56fc" name="ac53ef9ec2530193f33c80ededa0c56fc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac53ef9ec2530193f33c80ededa0c56fc">&#9670;&#160;</a></span>test_ridge_regression_check_arguments_validity()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_check_arguments_validity </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>return_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>arr_type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">check if all combinations of arguments give valid estimations</pre> <div class="fragment"><div class="line"><span class="lineno"> 1651</span>):</div>
<div class="line"><span class="lineno"> 1652</span>    <span class="stringliteral">&quot;&quot;&quot;check if all combinations of arguments give valid estimations&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1653</span> </div>
<div class="line"><span class="lineno"> 1654</span>    <span class="comment"># test excludes &#39;svd&#39; solver because it raises exception for sparse inputs</span></div>
<div class="line"><span class="lineno"> 1655</span> </div>
<div class="line"><span class="lineno"> 1656</span>    rng = check_random_state(42)</div>
<div class="line"><span class="lineno"> 1657</span>    X = rng.rand(1000, 3)</div>
<div class="line"><span class="lineno"> 1658</span>    true_coefs = [1, 2, 0.1]</div>
<div class="line"><span class="lineno"> 1659</span>    y = np.dot(X, true_coefs)</div>
<div class="line"><span class="lineno"> 1660</span>    true_intercept = 0.0</div>
<div class="line"><span class="lineno"> 1661</span>    <span class="keywordflow">if</span> return_intercept:</div>
<div class="line"><span class="lineno"> 1662</span>        true_intercept = 10000.0</div>
<div class="line"><span class="lineno"> 1663</span>    y += true_intercept</div>
<div class="line"><span class="lineno"> 1664</span>    X_testing = arr_type(X)</div>
<div class="line"><span class="lineno"> 1665</span> </div>
<div class="line"><span class="lineno"> 1666</span>    alpha, tol = 1e-3, 1e-6</div>
<div class="line"><span class="lineno"> 1667</span>    atol = 1e-3 <span class="keywordflow">if</span> _IS_32BIT <span class="keywordflow">else</span> 1e-4</div>
<div class="line"><span class="lineno"> 1668</span> </div>
<div class="line"><span class="lineno"> 1669</span>    positive = solver == <span class="stringliteral">&quot;lbfgs&quot;</span></div>
<div class="line"><span class="lineno"> 1670</span> </div>
<div class="line"><span class="lineno"> 1671</span>    <span class="keywordflow">if</span> solver <span class="keywordflow">not</span> <span class="keywordflow">in</span> [<span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;auto&quot;</span>] <span class="keywordflow">and</span> return_intercept:</div>
<div class="line"><span class="lineno"> 1672</span>        <span class="keyword">with</span> pytest.raises(ValueError, match=<span class="stringliteral">&quot;In Ridge, only &#39;sag&#39; solver&quot;</span>):</div>
<div class="line"><span class="lineno"> 1673</span>            ridge_regression(</div>
<div class="line"><span class="lineno"> 1674</span>                X_testing,</div>
<div class="line"><span class="lineno"> 1675</span>                y,</div>
<div class="line"><span class="lineno"> 1676</span>                alpha=alpha,</div>
<div class="line"><span class="lineno"> 1677</span>                solver=solver,</div>
<div class="line"><span class="lineno"> 1678</span>                sample_weight=sample_weight,</div>
<div class="line"><span class="lineno"> 1679</span>                return_intercept=return_intercept,</div>
<div class="line"><span class="lineno"> 1680</span>                positive=positive,</div>
<div class="line"><span class="lineno"> 1681</span>                tol=tol,</div>
<div class="line"><span class="lineno"> 1682</span>            )</div>
<div class="line"><span class="lineno"> 1683</span>        <span class="keywordflow">return</span></div>
<div class="line"><span class="lineno"> 1684</span> </div>
<div class="line"><span class="lineno"> 1685</span>    out = ridge_regression(</div>
<div class="line"><span class="lineno"> 1686</span>        X_testing,</div>
<div class="line"><span class="lineno"> 1687</span>        y,</div>
<div class="line"><span class="lineno"> 1688</span>        alpha=alpha,</div>
<div class="line"><span class="lineno"> 1689</span>        solver=solver,</div>
<div class="line"><span class="lineno"> 1690</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno"> 1691</span>        positive=positive,</div>
<div class="line"><span class="lineno"> 1692</span>        return_intercept=return_intercept,</div>
<div class="line"><span class="lineno"> 1693</span>        tol=tol,</div>
<div class="line"><span class="lineno"> 1694</span>    )</div>
<div class="line"><span class="lineno"> 1695</span> </div>
<div class="line"><span class="lineno"> 1696</span>    <span class="keywordflow">if</span> return_intercept:</div>
<div class="line"><span class="lineno"> 1697</span>        coef, intercept = out</div>
<div class="line"><span class="lineno"> 1698</span>        assert_allclose(coef, true_coefs, rtol=0, atol=atol)</div>
<div class="line"><span class="lineno"> 1699</span>        assert_allclose(intercept, true_intercept, rtol=0, atol=atol)</div>
<div class="line"><span class="lineno"> 1700</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1701</span>        assert_allclose(out, true_coefs, rtol=0, atol=atol)</div>
<div class="line"><span class="lineno"> 1702</span> </div>
<div class="line"><span class="lineno"> 1703</span> </div>
<div class="line"><span class="lineno"> 1704</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1705</span>    <span class="stringliteral">&quot;solver&quot;</span>, [<span class="stringliteral">&quot;svd&quot;</span>, <span class="stringliteral">&quot;sparse_cg&quot;</span>, <span class="stringliteral">&quot;cholesky&quot;</span>, <span class="stringliteral">&quot;lsqr&quot;</span>, <span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>, <span class="stringliteral">&quot;lbfgs&quot;</span>]</div>
<div class="line"><span class="lineno"> 1706</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="af022883948abb70a94d06b8c03d55168" name="af022883948abb70a94d06b8c03d55168"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af022883948abb70a94d06b8c03d55168">&#9670;&#160;</a></span>test_ridge_regression_convergence_fail()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_convergence_fail </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  506</span><span class="keyword">def </span>test_ridge_regression_convergence_fail():</div>
<div class="line"><span class="lineno">  507</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  508</span>    y = rng.randn(5)</div>
<div class="line"><span class="lineno">  509</span>    X = rng.randn(5, 10)</div>
<div class="line"><span class="lineno">  510</span>    warning_message = <span class="stringliteral">r&quot;sparse_cg did not converge after&quot;</span> <span class="stringliteral">r&quot; [0-9]+ iterations.&quot;</span></div>
<div class="line"><span class="lineno">  511</span>    <span class="keyword">with</span> pytest.warns(ConvergenceWarning, match=warning_message):</div>
<div class="line"><span class="lineno">  512</span>        ridge_regression(</div>
<div class="line"><span class="lineno">  513</span>            X, y, alpha=1.0, solver=<span class="stringliteral">&quot;sparse_cg&quot;</span>, tol=0.0, max_iter=<span class="keywordtype">None</span>, verbose=1</div>
<div class="line"><span class="lineno">  514</span>        )</div>
<div class="line"><span class="lineno">  515</span> </div>
<div class="line"><span class="lineno">  516</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad070c287bef26ae83ed26eda493251ef" name="ad070c287bef26ae83ed26eda493251ef"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad070c287bef26ae83ed26eda493251ef">&#9670;&#160;</a></span>test_ridge_regression_custom_scoring()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_custom_scoring </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>filter_</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>cv</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1160</span><span class="keyword">def </span>test_ridge_regression_custom_scoring(filter_, cv):</div>
<div class="line"><span class="lineno"> 1161</span>    <span class="comment"># check that custom scoring is working as expected</span></div>
<div class="line"><span class="lineno"> 1162</span>    <span class="comment"># check the tie breaking strategy (keep the first alpha tried)</span></div>
<div class="line"><span class="lineno"> 1163</span> </div>
<div class="line"><span class="lineno"> 1164</span>    <span class="keyword">def </span>_dummy_score(y_test, y_pred):</div>
<div class="line"><span class="lineno"> 1165</span>        <span class="keywordflow">return</span> 0.42</div>
<div class="line"><span class="lineno"> 1166</span> </div>
<div class="line"><span class="lineno"> 1167</span>    alphas = np.logspace(-2, 2, num=5)</div>
<div class="line"><span class="lineno"> 1168</span>    clf = RidgeClassifierCV(alphas=alphas, scoring=make_scorer(_dummy_score), cv=cv)</div>
<div class="line"><span class="lineno"> 1169</span>    clf.fit(filter_(X_iris), y_iris)</div>
<div class="line"><span class="lineno"> 1170</span>    <span class="keyword">assert</span> clf.best_score_ == pytest.approx(0.42)</div>
<div class="line"><span class="lineno"> 1171</span>    <span class="comment"># In case of tie score, the first alphas will be kept</span></div>
<div class="line"><span class="lineno"> 1172</span>    <span class="keyword">assert</span> clf.alpha_ == pytest.approx(alphas[0])</div>
<div class="line"><span class="lineno"> 1173</span> </div>
<div class="line"><span class="lineno"> 1174</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ada3f213d5cfb5d6c212f1db5bfaa7dd2" name="ada3f213d5cfb5d6c212f1db5bfaa7dd2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ada3f213d5cfb5d6c212f1db5bfaa7dd2">&#9670;&#160;</a></span>test_ridge_regression_dtype_stability()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_dtype_stability </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1775</span><span class="keyword">def </span>test_ridge_regression_dtype_stability(solver, seed):</div>
<div class="line"><span class="lineno"> 1776</span>    random_state = np.random.RandomState(seed)</div>
<div class="line"><span class="lineno"> 1777</span>    n_samples, n_features = 6, 5</div>
<div class="line"><span class="lineno"> 1778</span>    X = random_state.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1779</span>    coef = random_state.randn(n_features)</div>
<div class="line"><span class="lineno"> 1780</span>    y = np.dot(X, coef) + 0.01 * random_state.randn(n_samples)</div>
<div class="line"><span class="lineno"> 1781</span>    alpha = 1.0</div>
<div class="line"><span class="lineno"> 1782</span>    positive = solver == <span class="stringliteral">&quot;lbfgs&quot;</span></div>
<div class="line"><span class="lineno"> 1783</span>    results = dict()</div>
<div class="line"><span class="lineno"> 1784</span>    <span class="comment"># XXX: Sparse CG seems to be far less numerically stable than the</span></div>
<div class="line"><span class="lineno"> 1785</span>    <span class="comment"># others, maybe we should not enable float32 for this one.</span></div>
<div class="line"><span class="lineno"> 1786</span>    atol = 1e-3 <span class="keywordflow">if</span> solver == <span class="stringliteral">&quot;sparse_cg&quot;</span> <span class="keywordflow">else</span> 1e-5</div>
<div class="line"><span class="lineno"> 1787</span>    <span class="keywordflow">for</span> current_dtype <span class="keywordflow">in</span> (np.float32, np.float64):</div>
<div class="line"><span class="lineno"> 1788</span>        results[current_dtype] = ridge_regression(</div>
<div class="line"><span class="lineno"> 1789</span>            X.astype(current_dtype),</div>
<div class="line"><span class="lineno"> 1790</span>            y.astype(current_dtype),</div>
<div class="line"><span class="lineno"> 1791</span>            alpha=alpha,</div>
<div class="line"><span class="lineno"> 1792</span>            solver=solver,</div>
<div class="line"><span class="lineno"> 1793</span>            random_state=random_state,</div>
<div class="line"><span class="lineno"> 1794</span>            sample_weight=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno"> 1795</span>            positive=positive,</div>
<div class="line"><span class="lineno"> 1796</span>            max_iter=500,</div>
<div class="line"><span class="lineno"> 1797</span>            tol=1e-10,</div>
<div class="line"><span class="lineno"> 1798</span>            return_n_iter=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1799</span>            return_intercept=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1800</span>        )</div>
<div class="line"><span class="lineno"> 1801</span> </div>
<div class="line"><span class="lineno"> 1802</span>    <span class="keyword">assert</span> results[np.float32].dtype == np.float32</div>
<div class="line"><span class="lineno"> 1803</span>    <span class="keyword">assert</span> results[np.float64].dtype == np.float64</div>
<div class="line"><span class="lineno"> 1804</span>    assert_allclose(results[np.float32], results[np.float64], atol=atol)</div>
<div class="line"><span class="lineno"> 1805</span> </div>
<div class="line"><span class="lineno"> 1806</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a42b3764169d6c2c4d40a8abedc81a472" name="a42b3764169d6c2c4d40a8abedc81a472"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a42b3764169d6c2c4d40a8abedc81a472">&#9670;&#160;</a></span>test_ridge_regression_hstacked_X()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_hstacked_X </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ols_ridge_dataset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that Ridge converges for all solvers to correct solution on hstacked data.

We work with a simple constructed data set with known solution.
Fit on [X] with alpha is the same as fit on [X, X]/2 with alpha/2.
For long X, [X, X] is a singular matrix.
</pre> <div class="fragment"><div class="line"><span class="lineno">  205</span>):</div>
<div class="line"><span class="lineno">  206</span>    <span class="stringliteral">&quot;&quot;&quot;Test that Ridge converges for all solvers to correct solution on hstacked data.</span></div>
<div class="line"><span class="lineno">  207</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  208</span><span class="stringliteral">    We work with a simple constructed data set with known solution.</span></div>
<div class="line"><span class="lineno">  209</span><span class="stringliteral">    Fit on [X] with alpha is the same as fit on [X, X]/2 with alpha/2.</span></div>
<div class="line"><span class="lineno">  210</span><span class="stringliteral">    For long X, [X, X] is a singular matrix.</span></div>
<div class="line"><span class="lineno">  211</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  212</span>    X, y, _, coef = ols_ridge_dataset</div>
<div class="line"><span class="lineno">  213</span>    n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno">  214</span>    alpha = 1.0  <span class="comment"># because ols_ridge_dataset uses this.</span></div>
<div class="line"><span class="lineno">  215</span> </div>
<div class="line"><span class="lineno">  216</span>    model = Ridge(</div>
<div class="line"><span class="lineno">  217</span>        alpha=alpha / 2,</div>
<div class="line"><span class="lineno">  218</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  219</span>        solver=solver,</div>
<div class="line"><span class="lineno">  220</span>        tol=1e-15 <span class="keywordflow">if</span> solver <span class="keywordflow">in</span> (<span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>) <span class="keywordflow">else</span> 1e-10,</div>
<div class="line"><span class="lineno">  221</span>        random_state=global_random_seed,</div>
<div class="line"><span class="lineno">  222</span>    )</div>
<div class="line"><span class="lineno">  223</span>    X = X[:, :-1]  <span class="comment"># remove intercept</span></div>
<div class="line"><span class="lineno">  224</span>    X = 0.5 * np.concatenate((X, X), axis=1)</div>
<div class="line"><span class="lineno">  225</span>    <span class="keyword">assert</span> np.linalg.matrix_rank(X) &lt;= min(n_samples, n_features - 1)</div>
<div class="line"><span class="lineno">  226</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno">  227</span>        intercept = coef[-1]</div>
<div class="line"><span class="lineno">  228</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  229</span>        X = X - X.mean(axis=0)</div>
<div class="line"><span class="lineno">  230</span>        y = y - y.mean()</div>
<div class="line"><span class="lineno">  231</span>        intercept = 0</div>
<div class="line"><span class="lineno">  232</span>    model.fit(X, y)</div>
<div class="line"><span class="lineno">  233</span>    coef = coef[:-1]</div>
<div class="line"><span class="lineno">  234</span> </div>
<div class="line"><span class="lineno">  235</span>    <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  236</span>    <span class="comment"># coefficients are not all on the same magnitude, adding a small atol to</span></div>
<div class="line"><span class="lineno">  237</span>    <span class="comment"># make this test less brittle</span></div>
<div class="line"><span class="lineno">  238</span>    assert_allclose(model.coef_, np.r_[coef, coef], atol=1e-8)</div>
<div class="line"><span class="lineno">  239</span> </div>
<div class="line"><span class="lineno">  240</span> </div>
<div class="line"><span class="lineno">  241</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, SOLVERS)</span></div>
<div class="line"><span class="lineno">  242</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="abf42c34730eabac1a5067679900ffb28" name="abf42c34730eabac1a5067679900ffb28"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abf42c34730eabac1a5067679900ffb28">&#9670;&#160;</a></span>test_ridge_regression_sample_weights()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_sample_weights </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sparseX</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ols_ridge_dataset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that Ridge with sample weights gives correct results.

We use the following trick:
    ||y - Xw||_2 = (z - Aw)' W (z - Aw)
for z=[y, y], A' = [X', X'] (vstacked), and W[:n/2] + W[n/2:] = 1, W=diag(W)
</pre> <div class="fragment"><div class="line"><span class="lineno">  454</span>):</div>
<div class="line"><span class="lineno">  455</span>    <span class="stringliteral">&quot;&quot;&quot;Test that Ridge with sample weights gives correct results.</span></div>
<div class="line"><span class="lineno">  456</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  457</span><span class="stringliteral">    We use the following trick:</span></div>
<div class="line"><span class="lineno">  458</span><span class="stringliteral">        ||y - Xw||_2 = (z - Aw)&#39; W (z - Aw)</span></div>
<div class="line"><span class="lineno">  459</span><span class="stringliteral">    for z=[y, y], A&#39; = [X&#39;, X&#39;] (vstacked), and W[:n/2] + W[n/2:] = 1, W=diag(W)</span></div>
<div class="line"><span class="lineno">  460</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  461</span>    <span class="keywordflow">if</span> sparseX:</div>
<div class="line"><span class="lineno">  462</span>        <span class="keywordflow">if</span> fit_intercept <span class="keywordflow">and</span> solver <span class="keywordflow">not</span> <span class="keywordflow">in</span> SPARSE_SOLVERS_WITH_INTERCEPT:</div>
<div class="line"><span class="lineno">  463</span>            pytest.skip()</div>
<div class="line"><span class="lineno">  464</span>        <span class="keywordflow">elif</span> <span class="keywordflow">not</span> fit_intercept <span class="keywordflow">and</span> solver <span class="keywordflow">not</span> <span class="keywordflow">in</span> SPARSE_SOLVERS_WITHOUT_INTERCEPT:</div>
<div class="line"><span class="lineno">  465</span>            pytest.skip()</div>
<div class="line"><span class="lineno">  466</span>    X, y, _, coef = ols_ridge_dataset</div>
<div class="line"><span class="lineno">  467</span>    n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno">  468</span>    sw = rng.uniform(low=0, high=1, size=n_samples)</div>
<div class="line"><span class="lineno">  469</span> </div>
<div class="line"><span class="lineno">  470</span>    model = Ridge(</div>
<div class="line"><span class="lineno">  471</span>        alpha=alpha,</div>
<div class="line"><span class="lineno">  472</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  473</span>        solver=solver,</div>
<div class="line"><span class="lineno">  474</span>        tol=1e-15 <span class="keywordflow">if</span> solver <span class="keywordflow">in</span> [<span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>] <span class="keywordflow">else</span> 1e-10,</div>
<div class="line"><span class="lineno">  475</span>        max_iter=100_000,</div>
<div class="line"><span class="lineno">  476</span>        random_state=global_random_seed,</div>
<div class="line"><span class="lineno">  477</span>    )</div>
<div class="line"><span class="lineno">  478</span>    X = X[:, :-1]  <span class="comment"># remove intercept</span></div>
<div class="line"><span class="lineno">  479</span>    X = np.concatenate((X, X), axis=0)</div>
<div class="line"><span class="lineno">  480</span>    y = np.r_[y, y]</div>
<div class="line"><span class="lineno">  481</span>    sw = np.r_[sw, 1 - sw] * alpha</div>
<div class="line"><span class="lineno">  482</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno">  483</span>        intercept = coef[-1]</div>
<div class="line"><span class="lineno">  484</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  485</span>        X = X - X.mean(axis=0)</div>
<div class="line"><span class="lineno">  486</span>        y = y - y.mean()</div>
<div class="line"><span class="lineno">  487</span>        intercept = 0</div>
<div class="line"><span class="lineno">  488</span>    <span class="keywordflow">if</span> sparseX:</div>
<div class="line"><span class="lineno">  489</span>        X = sp.csr_matrix(X)</div>
<div class="line"><span class="lineno">  490</span>    model.fit(X, y, sample_weight=sw)</div>
<div class="line"><span class="lineno">  491</span>    coef = coef[:-1]</div>
<div class="line"><span class="lineno">  492</span> </div>
<div class="line"><span class="lineno">  493</span>    <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  494</span>    assert_allclose(model.coef_, coef)</div>
<div class="line"><span class="lineno">  495</span> </div>
<div class="line"><span class="lineno">  496</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aa04f02536a05b012ab63d026d16272af" name="aa04f02536a05b012ab63d026d16272af"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa04f02536a05b012ab63d026d16272af">&#9670;&#160;</a></span>test_ridge_regression_unpenalized()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_unpenalized </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ols_ridge_dataset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that unpenalized Ridge = OLS converges for all solvers to correct solution.

We work with a simple constructed data set with known solution.
Note: This checks the minimum norm solution for wide X, i.e.
n_samples &lt; n_features:
    min ||w||_2 subject to X w = y
</pre> <div class="fragment"><div class="line"><span class="lineno">  287</span>):</div>
<div class="line"><span class="lineno">  288</span>    <span class="stringliteral">&quot;&quot;&quot;Test that unpenalized Ridge = OLS converges for all solvers to correct solution.</span></div>
<div class="line"><span class="lineno">  289</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  290</span><span class="stringliteral">    We work with a simple constructed data set with known solution.</span></div>
<div class="line"><span class="lineno">  291</span><span class="stringliteral">    Note: This checks the minimum norm solution for wide X, i.e.</span></div>
<div class="line"><span class="lineno">  292</span><span class="stringliteral">    n_samples &lt; n_features:</span></div>
<div class="line"><span class="lineno">  293</span><span class="stringliteral">        min ||w||_2 subject to X w = y</span></div>
<div class="line"><span class="lineno">  294</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  295</span>    X, y, coef, _ = ols_ridge_dataset</div>
<div class="line"><span class="lineno">  296</span>    n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno">  297</span>    alpha = 0  <span class="comment"># OLS</span></div>
<div class="line"><span class="lineno">  298</span>    params = dict(</div>
<div class="line"><span class="lineno">  299</span>        alpha=alpha,</div>
<div class="line"><span class="lineno">  300</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  301</span>        solver=solver,</div>
<div class="line"><span class="lineno">  302</span>        tol=1e-15 <span class="keywordflow">if</span> solver <span class="keywordflow">in</span> (<span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>) <span class="keywordflow">else</span> 1e-10,</div>
<div class="line"><span class="lineno">  303</span>        random_state=global_random_seed,</div>
<div class="line"><span class="lineno">  304</span>    )</div>
<div class="line"><span class="lineno">  305</span> </div>
<div class="line"><span class="lineno">  306</span>    model = Ridge(**params)</div>
<div class="line"><span class="lineno">  307</span>    <span class="comment"># Note that cholesky might give a warning: &quot;Singular matrix in solving dual</span></div>
<div class="line"><span class="lineno">  308</span>    <span class="comment"># problem. Using least-squares solution instead.&quot;</span></div>
<div class="line"><span class="lineno">  309</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno">  310</span>        X = X[:, :-1]  <span class="comment"># remove intercept</span></div>
<div class="line"><span class="lineno">  311</span>        intercept = coef[-1]</div>
<div class="line"><span class="lineno">  312</span>        coef = coef[:-1]</div>
<div class="line"><span class="lineno">  313</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  314</span>        intercept = 0</div>
<div class="line"><span class="lineno">  315</span>    model.fit(X, y)</div>
<div class="line"><span class="lineno">  316</span> </div>
<div class="line"><span class="lineno">  317</span>    <span class="comment"># FIXME: `assert_allclose(model.coef_, coef)` should work for all cases but fails</span></div>
<div class="line"><span class="lineno">  318</span>    <span class="comment"># for the wide/fat case with n_features &gt; n_samples. The current Ridge solvers do</span></div>
<div class="line"><span class="lineno">  319</span>    <span class="comment"># NOT return the minimum norm solution with fit_intercept=True.</span></div>
<div class="line"><span class="lineno">  320</span>    <span class="keywordflow">if</span> n_samples &gt; n_features <span class="keywordflow">or</span> <span class="keywordflow">not</span> fit_intercept:</div>
<div class="line"><span class="lineno">  321</span>        <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  322</span>        assert_allclose(model.coef_, coef)</div>
<div class="line"><span class="lineno">  323</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  324</span>        <span class="comment"># As it is an underdetermined problem, residuals = 0. This shows that we get</span></div>
<div class="line"><span class="lineno">  325</span>        <span class="comment"># a solution to X w = y ....</span></div>
<div class="line"><span class="lineno">  326</span>        assert_allclose(model.predict(X), y)</div>
<div class="line"><span class="lineno">  327</span>        assert_allclose(X @ coef + intercept, y)</div>
<div class="line"><span class="lineno">  328</span>        <span class="comment"># But it is not the minimum norm solution. (This should be equal.)</span></div>
<div class="line"><span class="lineno">  329</span>        <span class="keyword">assert</span> np.linalg.norm(np.r_[model.intercept_, model.coef_]) &gt; np.linalg.norm(</div>
<div class="line"><span class="lineno">  330</span>            np.r_[intercept, coef]</div>
<div class="line"><span class="lineno">  331</span>        )</div>
<div class="line"><span class="lineno">  332</span> </div>
<div class="line"><span class="lineno">  333</span>        pytest.xfail(reason=<span class="stringliteral">&quot;Ridge does not provide the minimum norm solution.&quot;</span>)</div>
<div class="line"><span class="lineno">  334</span>        <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  335</span>        assert_allclose(model.coef_, coef)</div>
<div class="line"><span class="lineno">  336</span> </div>
<div class="line"><span class="lineno">  337</span> </div>
<div class="line"><span class="lineno">  338</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, SOLVERS)</span></div>
<div class="line"><span class="lineno">  339</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a2fd19bb2eb4d843fadedaa48c626d0f6" name="a2fd19bb2eb4d843fadedaa48c626d0f6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2fd19bb2eb4d843fadedaa48c626d0f6">&#9670;&#160;</a></span>test_ridge_regression_unpenalized_hstacked_X()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_unpenalized_hstacked_X </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ols_ridge_dataset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that unpenalized Ridge = OLS converges for all solvers to correct solution.

We work with a simple constructed data set with known solution.
OLS fit on [X] is the same as fit on [X, X]/2.
For long X, [X, X] is a singular matrix and we check against the minimum norm
solution:
    min ||w||_2 subject to min ||X w - y||_2
</pre> <div class="fragment"><div class="line"><span class="lineno">  342</span>):</div>
<div class="line"><span class="lineno">  343</span>    <span class="stringliteral">&quot;&quot;&quot;Test that unpenalized Ridge = OLS converges for all solvers to correct solution.</span></div>
<div class="line"><span class="lineno">  344</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  345</span><span class="stringliteral">    We work with a simple constructed data set with known solution.</span></div>
<div class="line"><span class="lineno">  346</span><span class="stringliteral">    OLS fit on [X] is the same as fit on [X, X]/2.</span></div>
<div class="line"><span class="lineno">  347</span><span class="stringliteral">    For long X, [X, X] is a singular matrix and we check against the minimum norm</span></div>
<div class="line"><span class="lineno">  348</span><span class="stringliteral">    solution:</span></div>
<div class="line"><span class="lineno">  349</span><span class="stringliteral">        min ||w||_2 subject to min ||X w - y||_2</span></div>
<div class="line"><span class="lineno">  350</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  351</span>    X, y, coef, _ = ols_ridge_dataset</div>
<div class="line"><span class="lineno">  352</span>    n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno">  353</span>    alpha = 0  <span class="comment"># OLS</span></div>
<div class="line"><span class="lineno">  354</span> </div>
<div class="line"><span class="lineno">  355</span>    model = Ridge(</div>
<div class="line"><span class="lineno">  356</span>        alpha=alpha,</div>
<div class="line"><span class="lineno">  357</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  358</span>        solver=solver,</div>
<div class="line"><span class="lineno">  359</span>        tol=1e-15 <span class="keywordflow">if</span> solver <span class="keywordflow">in</span> (<span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>) <span class="keywordflow">else</span> 1e-10,</div>
<div class="line"><span class="lineno">  360</span>        random_state=global_random_seed,</div>
<div class="line"><span class="lineno">  361</span>    )</div>
<div class="line"><span class="lineno">  362</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno">  363</span>        X = X[:, :-1]  <span class="comment"># remove intercept</span></div>
<div class="line"><span class="lineno">  364</span>        intercept = coef[-1]</div>
<div class="line"><span class="lineno">  365</span>        coef = coef[:-1]</div>
<div class="line"><span class="lineno">  366</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  367</span>        intercept = 0</div>
<div class="line"><span class="lineno">  368</span>    X = 0.5 * np.concatenate((X, X), axis=1)</div>
<div class="line"><span class="lineno">  369</span>    <span class="keyword">assert</span> np.linalg.matrix_rank(X) &lt;= min(n_samples, n_features)</div>
<div class="line"><span class="lineno">  370</span>    model.fit(X, y)</div>
<div class="line"><span class="lineno">  371</span> </div>
<div class="line"><span class="lineno">  372</span>    <span class="keywordflow">if</span> n_samples &gt; n_features <span class="keywordflow">or</span> <span class="keywordflow">not</span> fit_intercept:</div>
<div class="line"><span class="lineno">  373</span>        <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  374</span>        <span class="keywordflow">if</span> solver == <span class="stringliteral">&quot;cholesky&quot;</span>:</div>
<div class="line"><span class="lineno">  375</span>            <span class="comment"># Cholesky is a bad choice for singular X.</span></div>
<div class="line"><span class="lineno">  376</span>            pytest.skip()</div>
<div class="line"><span class="lineno">  377</span>        assert_allclose(model.coef_, np.r_[coef, coef])</div>
<div class="line"><span class="lineno">  378</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  379</span>        <span class="comment"># FIXME: Same as in test_ridge_regression_unpenalized.</span></div>
<div class="line"><span class="lineno">  380</span>        <span class="comment"># As it is an underdetermined problem, residuals = 0. This shows that we get</span></div>
<div class="line"><span class="lineno">  381</span>        <span class="comment"># a solution to X w = y ....</span></div>
<div class="line"><span class="lineno">  382</span>        assert_allclose(model.predict(X), y)</div>
<div class="line"><span class="lineno">  383</span>        <span class="comment"># But it is not the minimum norm solution. (This should be equal.)</span></div>
<div class="line"><span class="lineno">  384</span>        <span class="keyword">assert</span> np.linalg.norm(np.r_[model.intercept_, model.coef_]) &gt; np.linalg.norm(</div>
<div class="line"><span class="lineno">  385</span>            np.r_[intercept, coef, coef]</div>
<div class="line"><span class="lineno">  386</span>        )</div>
<div class="line"><span class="lineno">  387</span> </div>
<div class="line"><span class="lineno">  388</span>        pytest.xfail(reason=<span class="stringliteral">&quot;Ridge does not provide the minimum norm solution.&quot;</span>)</div>
<div class="line"><span class="lineno">  389</span>        <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  390</span>        assert_allclose(model.coef_, np.r_[coef, coef])</div>
<div class="line"><span class="lineno">  391</span> </div>
<div class="line"><span class="lineno">  392</span> </div>
<div class="line"><span class="lineno">  393</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, SOLVERS)</span></div>
<div class="line"><span class="lineno">  394</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a0480dae63456e7f5917979d4bc43a4e8" name="a0480dae63456e7f5917979d4bc43a4e8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0480dae63456e7f5917979d4bc43a4e8">&#9670;&#160;</a></span>test_ridge_regression_unpenalized_vstacked_X()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_unpenalized_vstacked_X </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ols_ridge_dataset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that unpenalized Ridge = OLS converges for all solvers to correct solution.

We work with a simple constructed data set with known solution.
OLS fit on [X] is the same as fit on [X], [y]
                                     [X], [y].
For wide X, [X', X'] is a singular matrix and we check against the minimum norm
solution:
    min ||w||_2 subject to X w = y
</pre> <div class="fragment"><div class="line"><span class="lineno">  397</span>):</div>
<div class="line"><span class="lineno">  398</span>    <span class="stringliteral">&quot;&quot;&quot;Test that unpenalized Ridge = OLS converges for all solvers to correct solution.</span></div>
<div class="line"><span class="lineno">  399</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  400</span><span class="stringliteral">    We work with a simple constructed data set with known solution.</span></div>
<div class="line"><span class="lineno">  401</span><span class="stringliteral">    OLS fit on [X] is the same as fit on [X], [y]</span></div>
<div class="line"><span class="lineno">  402</span><span class="stringliteral">                                         [X], [y].</span></div>
<div class="line"><span class="lineno">  403</span><span class="stringliteral">    For wide X, [X&#39;, X&#39;] is a singular matrix and we check against the minimum norm</span></div>
<div class="line"><span class="lineno">  404</span><span class="stringliteral">    solution:</span></div>
<div class="line"><span class="lineno">  405</span><span class="stringliteral">        min ||w||_2 subject to X w = y</span></div>
<div class="line"><span class="lineno">  406</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  407</span>    X, y, coef, _ = ols_ridge_dataset</div>
<div class="line"><span class="lineno">  408</span>    n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno">  409</span>    alpha = 0  <span class="comment"># OLS</span></div>
<div class="line"><span class="lineno">  410</span> </div>
<div class="line"><span class="lineno">  411</span>    model = Ridge(</div>
<div class="line"><span class="lineno">  412</span>        alpha=alpha,</div>
<div class="line"><span class="lineno">  413</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  414</span>        solver=solver,</div>
<div class="line"><span class="lineno">  415</span>        tol=1e-15 <span class="keywordflow">if</span> solver <span class="keywordflow">in</span> (<span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>) <span class="keywordflow">else</span> 1e-10,</div>
<div class="line"><span class="lineno">  416</span>        random_state=global_random_seed,</div>
<div class="line"><span class="lineno">  417</span>    )</div>
<div class="line"><span class="lineno">  418</span> </div>
<div class="line"><span class="lineno">  419</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno">  420</span>        X = X[:, :-1]  <span class="comment"># remove intercept</span></div>
<div class="line"><span class="lineno">  421</span>        intercept = coef[-1]</div>
<div class="line"><span class="lineno">  422</span>        coef = coef[:-1]</div>
<div class="line"><span class="lineno">  423</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  424</span>        intercept = 0</div>
<div class="line"><span class="lineno">  425</span>    X = np.concatenate((X, X), axis=0)</div>
<div class="line"><span class="lineno">  426</span>    <span class="keyword">assert</span> np.linalg.matrix_rank(X) &lt;= min(n_samples, n_features)</div>
<div class="line"><span class="lineno">  427</span>    y = np.r_[y, y]</div>
<div class="line"><span class="lineno">  428</span>    model.fit(X, y)</div>
<div class="line"><span class="lineno">  429</span> </div>
<div class="line"><span class="lineno">  430</span>    <span class="keywordflow">if</span> n_samples &gt; n_features <span class="keywordflow">or</span> <span class="keywordflow">not</span> fit_intercept:</div>
<div class="line"><span class="lineno">  431</span>        <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  432</span>        assert_allclose(model.coef_, coef)</div>
<div class="line"><span class="lineno">  433</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  434</span>        <span class="comment"># FIXME: Same as in test_ridge_regression_unpenalized.</span></div>
<div class="line"><span class="lineno">  435</span>        <span class="comment"># As it is an underdetermined problem, residuals = 0. This shows that we get</span></div>
<div class="line"><span class="lineno">  436</span>        <span class="comment"># a solution to X w = y ....</span></div>
<div class="line"><span class="lineno">  437</span>        assert_allclose(model.predict(X), y)</div>
<div class="line"><span class="lineno">  438</span>        <span class="comment"># But it is not the minimum norm solution. (This should be equal.)</span></div>
<div class="line"><span class="lineno">  439</span>        <span class="keyword">assert</span> np.linalg.norm(np.r_[model.intercept_, model.coef_]) &gt; np.linalg.norm(</div>
<div class="line"><span class="lineno">  440</span>            np.r_[intercept, coef]</div>
<div class="line"><span class="lineno">  441</span>        )</div>
<div class="line"><span class="lineno">  442</span> </div>
<div class="line"><span class="lineno">  443</span>        pytest.xfail(reason=<span class="stringliteral">&quot;Ridge does not provide the minimum norm solution.&quot;</span>)</div>
<div class="line"><span class="lineno">  444</span>        <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  445</span>        assert_allclose(model.coef_, coef)</div>
<div class="line"><span class="lineno">  446</span> </div>
<div class="line"><span class="lineno">  447</span> </div>
<div class="line"><span class="lineno">  448</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, SOLVERS)</span></div>
<div class="line"><span class="lineno">  449</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
<div class="line"><span class="lineno">  450</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sparseX&quot;, [True, False])</span></div>
<div class="line"><span class="lineno">  451</span><span class="preprocessor">@pytest.mark.parametrize(&quot;alpha&quot;, [1.0, 1e-2])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="ae2eb503ce5ebda6d5d75221cbc22d49e" name="ae2eb503ce5ebda6d5d75221cbc22d49e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae2eb503ce5ebda6d5d75221cbc22d49e">&#9670;&#160;</a></span>test_ridge_regression_vstacked_X()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_regression_vstacked_X </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ols_ridge_dataset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that Ridge converges for all solvers to correct solution on vstacked data.

We work with a simple constructed data set with known solution.
Fit on [X] with alpha is the same as fit on [X], [y]
                                            [X], [y] with 2 * alpha.
For wide X, [X', X'] is a singular matrix.
</pre> <div class="fragment"><div class="line"><span class="lineno">  245</span>):</div>
<div class="line"><span class="lineno">  246</span>    <span class="stringliteral">&quot;&quot;&quot;Test that Ridge converges for all solvers to correct solution on vstacked data.</span></div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral">    We work with a simple constructed data set with known solution.</span></div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">    Fit on [X] with alpha is the same as fit on [X], [y]</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral">                                                [X], [y] with 2 * alpha.</span></div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral">    For wide X, [X&#39;, X&#39;] is a singular matrix.</span></div>
<div class="line"><span class="lineno">  252</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  253</span>    X, y, _, coef = ols_ridge_dataset</div>
<div class="line"><span class="lineno">  254</span>    n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno">  255</span>    alpha = 1.0  <span class="comment"># because ols_ridge_dataset uses this.</span></div>
<div class="line"><span class="lineno">  256</span> </div>
<div class="line"><span class="lineno">  257</span>    model = Ridge(</div>
<div class="line"><span class="lineno">  258</span>        alpha=2 * alpha,</div>
<div class="line"><span class="lineno">  259</span>        fit_intercept=fit_intercept,</div>
<div class="line"><span class="lineno">  260</span>        solver=solver,</div>
<div class="line"><span class="lineno">  261</span>        tol=1e-15 <span class="keywordflow">if</span> solver <span class="keywordflow">in</span> (<span class="stringliteral">&quot;sag&quot;</span>, <span class="stringliteral">&quot;saga&quot;</span>) <span class="keywordflow">else</span> 1e-10,</div>
<div class="line"><span class="lineno">  262</span>        random_state=global_random_seed,</div>
<div class="line"><span class="lineno">  263</span>    )</div>
<div class="line"><span class="lineno">  264</span>    X = X[:, :-1]  <span class="comment"># remove intercept</span></div>
<div class="line"><span class="lineno">  265</span>    X = np.concatenate((X, X), axis=0)</div>
<div class="line"><span class="lineno">  266</span>    <span class="keyword">assert</span> np.linalg.matrix_rank(X) &lt;= min(n_samples, n_features)</div>
<div class="line"><span class="lineno">  267</span>    y = np.r_[y, y]</div>
<div class="line"><span class="lineno">  268</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno">  269</span>        intercept = coef[-1]</div>
<div class="line"><span class="lineno">  270</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  271</span>        X = X - X.mean(axis=0)</div>
<div class="line"><span class="lineno">  272</span>        y = y - y.mean()</div>
<div class="line"><span class="lineno">  273</span>        intercept = 0</div>
<div class="line"><span class="lineno">  274</span>    model.fit(X, y)</div>
<div class="line"><span class="lineno">  275</span>    coef = coef[:-1]</div>
<div class="line"><span class="lineno">  276</span> </div>
<div class="line"><span class="lineno">  277</span>    <span class="keyword">assert</span> model.intercept_ == pytest.approx(intercept)</div>
<div class="line"><span class="lineno">  278</span>    <span class="comment"># coefficients are not all on the same magnitude, adding a small atol to</span></div>
<div class="line"><span class="lineno">  279</span>    <span class="comment"># make this test less brittle</span></div>
<div class="line"><span class="lineno">  280</span>    assert_allclose(model.coef_, coef, atol=1e-8)</div>
<div class="line"><span class="lineno">  281</span> </div>
<div class="line"><span class="lineno">  282</span> </div>
<div class="line"><span class="lineno">  283</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, SOLVERS)</span></div>
<div class="line"><span class="lineno">  284</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a18e49a9b79d318e4eda8b85962fc14be" name="a18e49a9b79d318e4eda8b85962fc14be"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a18e49a9b79d318e4eda8b85962fc14be">&#9670;&#160;</a></span>test_ridge_sag_with_X_fortran()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_sag_with_X_fortran </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1807</span><span class="keyword">def </span>test_ridge_sag_with_X_fortran():</div>
<div class="line"><span class="lineno"> 1808</span>    <span class="comment"># check that Fortran array are converted when using SAG solver</span></div>
<div class="line"><span class="lineno"> 1809</span>    X, y = make_regression(random_state=42)</div>
<div class="line"><span class="lineno"> 1810</span>    <span class="comment"># for the order of X and y to not be C-ordered arrays</span></div>
<div class="line"><span class="lineno"> 1811</span>    X = np.asfortranarray(X)</div>
<div class="line"><span class="lineno"> 1812</span>    X = X[::2, :]</div>
<div class="line"><span class="lineno"> 1813</span>    y = y[::2]</div>
<div class="line"><span class="lineno"> 1814</span>    Ridge(solver=<span class="stringliteral">&quot;sag&quot;</span>).fit(X, y)</div>
<div class="line"><span class="lineno"> 1815</span> </div>
<div class="line"><span class="lineno"> 1816</span> </div>
<div class="line"><span class="lineno"> 1817</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1818</span>    <span class="stringliteral">&quot;Classifier, params&quot;</span>,</div>
<div class="line"><span class="lineno"> 1819</span>    [</div>
<div class="line"><span class="lineno"> 1820</span>        (RidgeClassifier, {}),</div>
<div class="line"><span class="lineno"> 1821</span>        (RidgeClassifierCV, {<span class="stringliteral">&quot;cv&quot;</span>: <span class="keywordtype">None</span>}),</div>
<div class="line"><span class="lineno"> 1822</span>        (RidgeClassifierCV, {<span class="stringliteral">&quot;cv&quot;</span>: 3}),</div>
<div class="line"><span class="lineno"> 1823</span>    ],</div>
<div class="line"><span class="lineno"> 1824</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="aec975be623ef3c2979e9dbd3fb6fa085" name="aec975be623ef3c2979e9dbd3fb6fa085"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aec975be623ef3c2979e9dbd3fb6fa085">&#9670;&#160;</a></span>test_ridge_sample_weight_invariance()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_sample_weight_invariance </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that Ridge fulfils sample weight invariance.

Note that this test is stricter than the common test
check_sample_weights_invariance alone.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1981</span><span class="keyword">def </span>test_ridge_sample_weight_invariance(solver):</div>
<div class="line"><span class="lineno"> 1982</span>    <span class="stringliteral">&quot;&quot;&quot;Test that Ridge fulfils sample weight invariance.</span></div>
<div class="line"><span class="lineno"> 1983</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1984</span><span class="stringliteral">    Note that this test is stricter than the common test</span></div>
<div class="line"><span class="lineno"> 1985</span><span class="stringliteral">    check_sample_weights_invariance alone.</span></div>
<div class="line"><span class="lineno"> 1986</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1987</span>    params = dict(</div>
<div class="line"><span class="lineno"> 1988</span>        alpha=1.0,</div>
<div class="line"><span class="lineno"> 1989</span>        solver=solver,</div>
<div class="line"><span class="lineno"> 1990</span>        tol=1e-12,</div>
<div class="line"><span class="lineno"> 1991</span>        positive=(solver == <span class="stringliteral">&quot;lbfgs&quot;</span>),</div>
<div class="line"><span class="lineno"> 1992</span>    )</div>
<div class="line"><span class="lineno"> 1993</span>    reg = Ridge(**params)</div>
<div class="line"><span class="lineno"> 1994</span>    name = reg.__class__.__name__</div>
<div class="line"><span class="lineno"> 1995</span>    check_sample_weights_invariance(name, reg, kind=<span class="stringliteral">&quot;ones&quot;</span>)</div>
<div class="line"><span class="lineno"> 1996</span>    check_sample_weights_invariance(name, reg, kind=<span class="stringliteral">&quot;zeros&quot;</span>)</div>
<div class="line"><span class="lineno"> 1997</span> </div>
<div class="line"><span class="lineno"> 1998</span>    <span class="comment"># Check that duplicating the training dataset is equivalent to multiplying</span></div>
<div class="line"><span class="lineno"> 1999</span>    <span class="comment"># the weights by 2:</span></div>
<div class="line"><span class="lineno"> 2000</span> </div>
<div class="line"><span class="lineno"> 2001</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno"> 2002</span>    X, y = make_regression(</div>
<div class="line"><span class="lineno"> 2003</span>        n_samples=100,</div>
<div class="line"><span class="lineno"> 2004</span>        n_features=300,</div>
<div class="line"><span class="lineno"> 2005</span>        effective_rank=10,</div>
<div class="line"><span class="lineno"> 2006</span>        n_informative=50,</div>
<div class="line"><span class="lineno"> 2007</span>        random_state=rng,</div>
<div class="line"><span class="lineno"> 2008</span>    )</div>
<div class="line"><span class="lineno"> 2009</span>    sw = rng.uniform(low=0.01, high=2, size=X.shape[0])</div>
<div class="line"><span class="lineno"> 2010</span>    X_dup = np.concatenate([X, X], axis=0)</div>
<div class="line"><span class="lineno"> 2011</span>    y_dup = np.concatenate([y, y], axis=0)</div>
<div class="line"><span class="lineno"> 2012</span>    sw_dup = np.concatenate([sw, sw], axis=0)</div>
<div class="line"><span class="lineno"> 2013</span> </div>
<div class="line"><span class="lineno"> 2014</span>    ridge_2sw = Ridge(**params).fit(X, y, sample_weight=2 * sw)</div>
<div class="line"><span class="lineno"> 2015</span>    ridge_dup = Ridge(**params).fit(X_dup, y_dup, sample_weight=sw_dup)</div>
<div class="line"><span class="lineno"> 2016</span> </div>
<div class="line"><span class="lineno"> 2017</span>    assert_allclose(ridge_2sw.coef_, ridge_dup.coef_)</div>
<div class="line"><span class="lineno"> 2018</span>    assert_allclose(ridge_2sw.intercept_, ridge_dup.intercept_)</div>
</div><!-- fragment -->
</div>
</div>
<a id="aee6699e551c0586d545874d9421b6fa0" name="aee6699e551c0586d545874d9421b6fa0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aee6699e551c0586d545874d9421b6fa0">&#9670;&#160;</a></span>test_ridge_shapes_type()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_shapes_type </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  517</span><span class="keyword">def </span>test_ridge_shapes_type():</div>
<div class="line"><span class="lineno">  518</span>    <span class="comment"># Test shape of coef_ and intercept_</span></div>
<div class="line"><span class="lineno">  519</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  520</span>    n_samples, n_features = 5, 10</div>
<div class="line"><span class="lineno">  521</span>    X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno">  522</span>    y = rng.randn(n_samples)</div>
<div class="line"><span class="lineno">  523</span>    Y1 = y[:, np.newaxis]</div>
<div class="line"><span class="lineno">  524</span>    Y = np.c_[y, 1 + y]</div>
<div class="line"><span class="lineno">  525</span> </div>
<div class="line"><span class="lineno">  526</span>    ridge = Ridge()</div>
<div class="line"><span class="lineno">  527</span> </div>
<div class="line"><span class="lineno">  528</span>    ridge.fit(X, y)</div>
<div class="line"><span class="lineno">  529</span>    <span class="keyword">assert</span> ridge.coef_.shape == (n_features,)</div>
<div class="line"><span class="lineno">  530</span>    <span class="keyword">assert</span> ridge.intercept_.shape == ()</div>
<div class="line"><span class="lineno">  531</span>    <span class="keyword">assert</span> isinstance(ridge.coef_, np.ndarray)</div>
<div class="line"><span class="lineno">  532</span>    <span class="keyword">assert</span> isinstance(ridge.intercept_, float)</div>
<div class="line"><span class="lineno">  533</span> </div>
<div class="line"><span class="lineno">  534</span>    ridge.fit(X, Y1)</div>
<div class="line"><span class="lineno">  535</span>    <span class="keyword">assert</span> ridge.coef_.shape == (1, n_features)</div>
<div class="line"><span class="lineno">  536</span>    <span class="keyword">assert</span> ridge.intercept_.shape == (1,)</div>
<div class="line"><span class="lineno">  537</span>    <span class="keyword">assert</span> isinstance(ridge.coef_, np.ndarray)</div>
<div class="line"><span class="lineno">  538</span>    <span class="keyword">assert</span> isinstance(ridge.intercept_, np.ndarray)</div>
<div class="line"><span class="lineno">  539</span> </div>
<div class="line"><span class="lineno">  540</span>    ridge.fit(X, Y)</div>
<div class="line"><span class="lineno">  541</span>    <span class="keyword">assert</span> ridge.coef_.shape == (2, n_features)</div>
<div class="line"><span class="lineno">  542</span>    <span class="keyword">assert</span> ridge.intercept_.shape == (2,)</div>
<div class="line"><span class="lineno">  543</span>    <span class="keyword">assert</span> isinstance(ridge.coef_, np.ndarray)</div>
<div class="line"><span class="lineno">  544</span>    <span class="keyword">assert</span> isinstance(ridge.intercept_, np.ndarray)</div>
<div class="line"><span class="lineno">  545</span> </div>
<div class="line"><span class="lineno">  546</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a6e3a54e9990bd428910212a5d98e3289" name="a6e3a54e9990bd428910212a5d98e3289"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6e3a54e9990bd428910212a5d98e3289">&#9670;&#160;</a></span>test_ridge_vs_lstsq()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridge_vs_lstsq </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  565</span><span class="keyword">def </span>test_ridge_vs_lstsq():</div>
<div class="line"><span class="lineno">  566</span>    <span class="comment"># On alpha=0., Ridge and OLS yield the same solution.</span></div>
<div class="line"><span class="lineno">  567</span> </div>
<div class="line"><span class="lineno">  568</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  569</span>    <span class="comment"># we need more samples than features</span></div>
<div class="line"><span class="lineno">  570</span>    n_samples, n_features = 5, 4</div>
<div class="line"><span class="lineno">  571</span>    y = rng.randn(n_samples)</div>
<div class="line"><span class="lineno">  572</span>    X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno">  573</span> </div>
<div class="line"><span class="lineno">  574</span>    ridge = Ridge(alpha=0.0, fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  575</span>    ols = LinearRegression(fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  576</span> </div>
<div class="line"><span class="lineno">  577</span>    ridge.fit(X, y)</div>
<div class="line"><span class="lineno">  578</span>    ols.fit(X, y)</div>
<div class="line"><span class="lineno">  579</span>    assert_almost_equal(ridge.coef_, ols.coef_)</div>
<div class="line"><span class="lineno">  580</span> </div>
<div class="line"><span class="lineno">  581</span>    ridge.fit(X, y)</div>
<div class="line"><span class="lineno">  582</span>    ols.fit(X, y)</div>
<div class="line"><span class="lineno">  583</span>    assert_almost_equal(ridge.coef_, ols.coef_)</div>
<div class="line"><span class="lineno">  584</span> </div>
<div class="line"><span class="lineno">  585</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ab1572701f799d60456b1b005ff4e677f" name="ab1572701f799d60456b1b005ff4e677f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab1572701f799d60456b1b005ff4e677f">&#9670;&#160;</a></span>test_ridgeclassifier_multilabel()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridgeclassifier_multilabel </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Classifier</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>params</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check that multilabel classification is supported and give meaningful
results.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1825</span><span class="keyword">def </span>test_ridgeclassifier_multilabel(Classifier, params):</div>
<div class="line"><span class="lineno"> 1826</span>    <span class="stringliteral">&quot;&quot;&quot;Check that multilabel classification is supported and give meaningful</span></div>
<div class="line"><span class="lineno"> 1827</span><span class="stringliteral">    results.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1828</span>    X, y = make_multilabel_classification(n_classes=1, random_state=0)</div>
<div class="line"><span class="lineno"> 1829</span>    y = y.reshape(-1, 1)</div>
<div class="line"><span class="lineno"> 1830</span>    Y = np.concatenate([y, y], axis=1)</div>
<div class="line"><span class="lineno"> 1831</span>    clf = Classifier(**params).fit(X, Y)</div>
<div class="line"><span class="lineno"> 1832</span>    Y_pred = clf.predict(X)</div>
<div class="line"><span class="lineno"> 1833</span> </div>
<div class="line"><span class="lineno"> 1834</span>    <span class="keyword">assert</span> Y_pred.shape == Y.shape</div>
<div class="line"><span class="lineno"> 1835</span>    assert_array_equal(Y_pred[:, 0], Y_pred[:, 1])</div>
<div class="line"><span class="lineno"> 1836</span>    Ridge(solver=<span class="stringliteral">&quot;sag&quot;</span>).fit(X, y)</div>
<div class="line"><span class="lineno"> 1837</span> </div>
<div class="line"><span class="lineno"> 1838</span> </div>
<div class="line"><span class="lineno"> 1839</span><span class="preprocessor">@pytest.mark.parametrize(&quot;solver&quot;, [&quot;auto&quot;, &quot;lbfgs&quot;])</span></div>
<div class="line"><span class="lineno"> 1840</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
<div class="line"><span class="lineno"> 1841</span><span class="preprocessor">@pytest.mark.parametrize(&quot;alpha&quot;, [1e-3, 1e-2, 0.1, 1.0])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a33c1d21f50a8f9aa7aa6509ef01beca3" name="a33c1d21f50a8f9aa7aa6509ef01beca3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a33c1d21f50a8f9aa7aa6509ef01beca3">&#9670;&#160;</a></span>test_ridgecv_alphas_conversion()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridgecv_alphas_conversion </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Estimator</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1353</span><span class="keyword">def </span>test_ridgecv_alphas_conversion(Estimator):</div>
<div class="line"><span class="lineno"> 1354</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno"> 1355</span>    alphas = (0.1, 1.0, 10.0)</div>
<div class="line"><span class="lineno"> 1356</span> </div>
<div class="line"><span class="lineno"> 1357</span>    n_samples, n_features = 5, 5</div>
<div class="line"><span class="lineno"> 1358</span>    <span class="keywordflow">if</span> Estimator <span class="keywordflow">is</span> RidgeCV:</div>
<div class="line"><span class="lineno"> 1359</span>        y = rng.randn(n_samples)</div>
<div class="line"><span class="lineno"> 1360</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1361</span>        y = rng.randint(0, 2, n_samples)</div>
<div class="line"><span class="lineno"> 1362</span>    X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1363</span> </div>
<div class="line"><span class="lineno"> 1364</span>    ridge_est = Estimator(alphas=alphas)</div>
<div class="line"><span class="lineno"> 1365</span>    <span class="keyword">assert</span> (</div>
<div class="line"><span class="lineno"> 1366</span>        ridge_est.alphas <span class="keywordflow">is</span> alphas</div>
<div class="line"><span class="lineno"> 1367</span>    ), f<span class="stringliteral">&quot;`alphas` was mutated in `{Estimator.__name__}.__init__`&quot;</span></div>
<div class="line"><span class="lineno"> 1368</span> </div>
<div class="line"><span class="lineno"> 1369</span>    ridge_est.fit(X, y)</div>
<div class="line"><span class="lineno"> 1370</span>    assert_array_equal(ridge_est.alphas, np.asarray(alphas))</div>
<div class="line"><span class="lineno"> 1371</span> </div>
<div class="line"><span class="lineno"> 1372</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a0679a6715391a428a6d9d5809f0cb7e9" name="a0679a6715391a428a6d9d5809f0cb7e9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0679a6715391a428a6d9d5809f0cb7e9">&#9670;&#160;</a></span>test_ridgecv_alphas_scalar()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridgecv_alphas_scalar </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Estimator</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check the case when `alphas` is a scalar.
This case was supported in the past when `alphas` where converted
into array in `__init__`.
We add this test to ensure backward compatibility.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1505</span><span class="keyword">def </span>test_ridgecv_alphas_scalar(Estimator):</div>
<div class="line"><span class="lineno"> 1506</span>    <span class="stringliteral">&quot;&quot;&quot;Check the case when `alphas` is a scalar.</span></div>
<div class="line"><span class="lineno"> 1507</span><span class="stringliteral">    This case was supported in the past when `alphas` where converted</span></div>
<div class="line"><span class="lineno"> 1508</span><span class="stringliteral">    into array in `__init__`.</span></div>
<div class="line"><span class="lineno"> 1509</span><span class="stringliteral">    We add this test to ensure backward compatibility.</span></div>
<div class="line"><span class="lineno"> 1510</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1511</span> </div>
<div class="line"><span class="lineno"> 1512</span>    n_samples, n_features = 5, 5</div>
<div class="line"><span class="lineno"> 1513</span>    X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1514</span>    <span class="keywordflow">if</span> Estimator <span class="keywordflow">is</span> RidgeCV:</div>
<div class="line"><span class="lineno"> 1515</span>        y = rng.randn(n_samples)</div>
<div class="line"><span class="lineno"> 1516</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1517</span>        y = rng.randint(0, 2, n_samples)</div>
<div class="line"><span class="lineno"> 1518</span> </div>
<div class="line"><span class="lineno"> 1519</span>    Estimator(alphas=1).fit(X, y)</div>
<div class="line"><span class="lineno"> 1520</span> </div>
<div class="line"><span class="lineno"> 1521</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aa57728b84b22f95943c421330ac59721" name="aa57728b84b22f95943c421330ac59721"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa57728b84b22f95943c421330ac59721">&#9670;&#160;</a></span>test_ridgecv_alphas_validation()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridgecv_alphas_validation </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Estimator</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>err_type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>err_msg</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check the `alphas` validation in RidgeCV and RidgeClassifierCV.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1493</span><span class="keyword">def </span>test_ridgecv_alphas_validation(Estimator, params, err_type, err_msg):</div>
<div class="line"><span class="lineno"> 1494</span>    <span class="stringliteral">&quot;&quot;&quot;Check the `alphas` validation in RidgeCV and RidgeClassifierCV.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1495</span> </div>
<div class="line"><span class="lineno"> 1496</span>    n_samples, n_features = 5, 5</div>
<div class="line"><span class="lineno"> 1497</span>    X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1498</span>    y = rng.randint(0, 2, n_samples)</div>
<div class="line"><span class="lineno"> 1499</span> </div>
<div class="line"><span class="lineno"> 1500</span>    <span class="keyword">with</span> pytest.raises(err_type, match=err_msg):</div>
<div class="line"><span class="lineno"> 1501</span>        Estimator(**params).fit(X, y)</div>
<div class="line"><span class="lineno"> 1502</span> </div>
<div class="line"><span class="lineno"> 1503</span> </div>
<div class="line"><span class="lineno"> 1504</span><span class="preprocessor">@pytest.mark.parametrize(&quot;Estimator&quot;, [RidgeCV, RidgeClassifierCV])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a41e496e501ccba233f11b697979063d6" name="a41e496e501ccba233f11b697979063d6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a41e496e501ccba233f11b697979063d6">&#9670;&#160;</a></span>test_ridgecv_int_alphas()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridgecv_int_alphas </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1467</span><span class="keyword">def </span>test_ridgecv_int_alphas():</div>
<div class="line"><span class="lineno"> 1468</span>    X = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])</div>
<div class="line"><span class="lineno"> 1469</span>    y = [1, 1, 1, -1, -1]</div>
<div class="line"><span class="lineno"> 1470</span> </div>
<div class="line"><span class="lineno"> 1471</span>    <span class="comment"># Integers</span></div>
<div class="line"><span class="lineno"> 1472</span>    ridge = RidgeCV(alphas=(1, 10, 100))</div>
<div class="line"><span class="lineno"> 1473</span>    ridge.fit(X, y)</div>
<div class="line"><span class="lineno"> 1474</span> </div>
<div class="line"><span class="lineno"> 1475</span> </div>
<div class="line"><span class="lineno"> 1476</span><span class="preprocessor">@pytest.mark.parametrize(&quot;Estimator&quot;, [RidgeCV, RidgeClassifierCV])</span></div>
<div class="line"><span class="lineno"> 1477</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1478</span>    <span class="stringliteral">&quot;params, err_type, err_msg&quot;</span>,</div>
<div class="line"><span class="lineno"> 1479</span>    [</div>
<div class="line"><span class="lineno"> 1480</span>        ({<span class="stringliteral">&quot;alphas&quot;</span>: (1, -1, -100)}, ValueError, <span class="stringliteral">r&quot;alphas\[1\] == -1, must be &gt; 0.0&quot;</span>),</div>
<div class="line"><span class="lineno"> 1481</span>        (</div>
<div class="line"><span class="lineno"> 1482</span>            {<span class="stringliteral">&quot;alphas&quot;</span>: (-0.1, -1.0, -10.0)},</div>
<div class="line"><span class="lineno"> 1483</span>            ValueError,</div>
<div class="line"><span class="lineno"> 1484</span>            <span class="stringliteral">r&quot;alphas\[0\] == -0.1, must be &gt; 0.0&quot;</span>,</div>
<div class="line"><span class="lineno"> 1485</span>        ),</div>
<div class="line"><span class="lineno"> 1486</span>        (</div>
<div class="line"><span class="lineno"> 1487</span>            {<span class="stringliteral">&quot;alphas&quot;</span>: (1, 1.0, <span class="stringliteral">&quot;1&quot;</span>)},</div>
<div class="line"><span class="lineno"> 1488</span>            TypeError,</div>
<div class="line"><span class="lineno"> 1489</span>            <span class="stringliteral">r&quot;alphas\[2\] must be an instance of float, not str&quot;</span>,</div>
<div class="line"><span class="lineno"> 1490</span>        ),</div>
<div class="line"><span class="lineno"> 1491</span>    ],</div>
<div class="line"><span class="lineno"> 1492</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a99f7635bcd52d19cd46189f85e802e8a" name="a99f7635bcd52d19cd46189f85e802e8a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a99f7635bcd52d19cd46189f85e802e8a">&#9670;&#160;</a></span>test_ridgecv_sample_weight()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridgecv_sample_weight </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1373</span><span class="keyword">def </span>test_ridgecv_sample_weight():</div>
<div class="line"><span class="lineno"> 1374</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno"> 1375</span>    alphas = (0.1, 1.0, 10.0)</div>
<div class="line"><span class="lineno"> 1376</span> </div>
<div class="line"><span class="lineno"> 1377</span>    <span class="comment"># There are different algorithms for n_samples &gt; n_features</span></div>
<div class="line"><span class="lineno"> 1378</span>    <span class="comment"># and the opposite, so test them both.</span></div>
<div class="line"><span class="lineno"> 1379</span>    <span class="keywordflow">for</span> n_samples, n_features <span class="keywordflow">in</span> ((6, 5), (5, 10)):</div>
<div class="line"><span class="lineno"> 1380</span>        y = rng.randn(n_samples)</div>
<div class="line"><span class="lineno"> 1381</span>        X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1382</span>        sample_weight = 1.0 + rng.rand(n_samples)</div>
<div class="line"><span class="lineno"> 1383</span> </div>
<div class="line"><span class="lineno"> 1384</span>        cv = KFold(5)</div>
<div class="line"><span class="lineno"> 1385</span>        ridgecv = RidgeCV(alphas=alphas, cv=cv)</div>
<div class="line"><span class="lineno"> 1386</span>        ridgecv.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1387</span> </div>
<div class="line"><span class="lineno"> 1388</span>        <span class="comment"># Check using GridSearchCV directly</span></div>
<div class="line"><span class="lineno"> 1389</span>        parameters = {<span class="stringliteral">&quot;alpha&quot;</span>: alphas}</div>
<div class="line"><span class="lineno"> 1390</span>        gs = GridSearchCV(Ridge(), parameters, cv=cv)</div>
<div class="line"><span class="lineno"> 1391</span>        gs.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1392</span> </div>
<div class="line"><span class="lineno"> 1393</span>        <span class="keyword">assert</span> ridgecv.alpha_ == gs.best_estimator_.alpha</div>
<div class="line"><span class="lineno"> 1394</span>        assert_array_almost_equal(ridgecv.coef_, gs.best_estimator_.coef_)</div>
<div class="line"><span class="lineno"> 1395</span> </div>
<div class="line"><span class="lineno"> 1396</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="af98b5b52efe75b5d16a4bf1f482254df" name="af98b5b52efe75b5d16a4bf1f482254df"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af98b5b52efe75b5d16a4bf1f482254df">&#9670;&#160;</a></span>test_ridgecv_store_cv_values()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_ridgecv_store_cv_values </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>scoring</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1294</span><span class="keyword">def </span>test_ridgecv_store_cv_values(scoring):</div>
<div class="line"><span class="lineno"> 1295</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno"> 1296</span> </div>
<div class="line"><span class="lineno"> 1297</span>    n_samples = 8</div>
<div class="line"><span class="lineno"> 1298</span>    n_features = 5</div>
<div class="line"><span class="lineno"> 1299</span>    x = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1300</span>    alphas = [1e-1, 1e0, 1e1]</div>
<div class="line"><span class="lineno"> 1301</span>    n_alphas = len(alphas)</div>
<div class="line"><span class="lineno"> 1302</span> </div>
<div class="line"><span class="lineno"> 1303</span>    scoring_ = make_scorer(scoring) <span class="keywordflow">if</span> callable(scoring) <span class="keywordflow">else</span> scoring</div>
<div class="line"><span class="lineno"> 1304</span> </div>
<div class="line"><span class="lineno"> 1305</span>    r = RidgeCV(alphas=alphas, cv=<span class="keywordtype">None</span>, store_cv_values=<span class="keyword">True</span>, scoring=scoring_)</div>
<div class="line"><span class="lineno"> 1306</span> </div>
<div class="line"><span class="lineno"> 1307</span>    <span class="comment"># with len(y.shape) == 1</span></div>
<div class="line"><span class="lineno"> 1308</span>    y = rng.randn(n_samples)</div>
<div class="line"><span class="lineno"> 1309</span>    r.fit(x, y)</div>
<div class="line"><span class="lineno"> 1310</span>    <span class="keyword">assert</span> r.cv_values_.shape == (n_samples, n_alphas)</div>
<div class="line"><span class="lineno"> 1311</span> </div>
<div class="line"><span class="lineno"> 1312</span>    <span class="comment"># with len(y.shape) == 2</span></div>
<div class="line"><span class="lineno"> 1313</span>    n_targets = 3</div>
<div class="line"><span class="lineno"> 1314</span>    y = rng.randn(n_samples, n_targets)</div>
<div class="line"><span class="lineno"> 1315</span>    r.fit(x, y)</div>
<div class="line"><span class="lineno"> 1316</span>    <span class="keyword">assert</span> r.cv_values_.shape == (n_samples, n_targets, n_alphas)</div>
<div class="line"><span class="lineno"> 1317</span> </div>
<div class="line"><span class="lineno"> 1318</span>    r = RidgeCV(cv=3, store_cv_values=<span class="keyword">True</span>, scoring=scoring)</div>
<div class="line"><span class="lineno"> 1319</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=<span class="stringliteral">&quot;cv!=None and store_cv_values&quot;</span>):</div>
<div class="line"><span class="lineno"> 1320</span>        r.fit(x, y)</div>
<div class="line"><span class="lineno"> 1321</span> </div>
<div class="line"><span class="lineno"> 1322</span> </div>
<div class="line"><span class="lineno"> 1323</span><span class="preprocessor">@pytest.mark.parametrize(&quot;scoring&quot;, [None, &quot;accuracy&quot;, _accuracy_callable])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a3e1d30548804d649c9d64a2cb4175933" name="a3e1d30548804d649c9d64a2cb4175933"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3e1d30548804d649c9d64a2cb4175933">&#9670;&#160;</a></span>test_solver_consistency()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_solver_consistency </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>proportion_nonzero</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dtype</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sparse_X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  735</span>):</div>
<div class="line"><span class="lineno">  736</span>    alpha = 1.0</div>
<div class="line"><span class="lineno">  737</span>    noise = 50.0 <span class="keywordflow">if</span> proportion_nonzero &gt; 0.9 <span class="keywordflow">else</span> 500.0</div>
<div class="line"><span class="lineno">  738</span>    X, y = _make_sparse_offset_regression(</div>
<div class="line"><span class="lineno">  739</span>        bias=10,</div>
<div class="line"><span class="lineno">  740</span>        n_features=30,</div>
<div class="line"><span class="lineno">  741</span>        proportion_nonzero=proportion_nonzero,</div>
<div class="line"><span class="lineno">  742</span>        noise=noise,</div>
<div class="line"><span class="lineno">  743</span>        random_state=seed,</div>
<div class="line"><span class="lineno">  744</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  745</span>    )</div>
<div class="line"><span class="lineno">  746</span> </div>
<div class="line"><span class="lineno">  747</span>    <span class="comment"># Manually scale the data to avoid pathological cases. We use</span></div>
<div class="line"><span class="lineno">  748</span>    <span class="comment"># minmax_scale to deal with the sparse case without breaking</span></div>
<div class="line"><span class="lineno">  749</span>    <span class="comment"># the sparsity pattern.</span></div>
<div class="line"><span class="lineno">  750</span>    X = minmax_scale(X)</div>
<div class="line"><span class="lineno">  751</span> </div>
<div class="line"><span class="lineno">  752</span>    svd_ridge = Ridge(solver=<span class="stringliteral">&quot;svd&quot;</span>, alpha=alpha).fit(X, y)</div>
<div class="line"><span class="lineno">  753</span>    X = X.astype(dtype, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  754</span>    y = y.astype(dtype, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  755</span>    <span class="keywordflow">if</span> sparse_X:</div>
<div class="line"><span class="lineno">  756</span>        X = sp.csr_matrix(X)</div>
<div class="line"><span class="lineno">  757</span>    <span class="keywordflow">if</span> solver == <span class="stringliteral">&quot;ridgecv&quot;</span>:</div>
<div class="line"><span class="lineno">  758</span>        ridge = RidgeCV(alphas=[alpha])</div>
<div class="line"><span class="lineno">  759</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  760</span>        ridge = Ridge(solver=solver, tol=1e-10, alpha=alpha)</div>
<div class="line"><span class="lineno">  761</span>    ridge.fit(X, y)</div>
<div class="line"><span class="lineno">  762</span>    assert_allclose(ridge.coef_, svd_ridge.coef_, atol=1e-3, rtol=1e-3)</div>
<div class="line"><span class="lineno">  763</span>    assert_allclose(ridge.intercept_, svd_ridge.intercept_, atol=1e-3, rtol=1e-3)</div>
<div class="line"><span class="lineno">  764</span> </div>
<div class="line"><span class="lineno">  765</span> </div>
<div class="line"><span class="lineno">  766</span><span class="preprocessor">@pytest.mark.parametrize(&quot;gcv_mode&quot;, [&quot;svd&quot;, &quot;eigen&quot;])</span></div>
<div class="line"><span class="lineno">  767</span><span class="preprocessor">@pytest.mark.parametrize(&quot;X_constructor&quot;, [np.asarray, sp.csr_matrix])</span></div>
<div class="line"><span class="lineno">  768</span><span class="preprocessor">@pytest.mark.parametrize(&quot;X_shape&quot;, [(11, 8)</span>, (11, 20)])</div>
<div class="line"><span class="lineno">  769</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [True, False])</span></div>
<div class="line"><span class="lineno">  770</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  771</span>    <span class="stringliteral">&quot;y_shape, noise&quot;</span>,</div>
<div class="line"><span class="lineno">  772</span>    [</div>
<div class="line"><span class="lineno">  773</span>        ((11,), 1.0),</div>
<div class="line"><span class="lineno">  774</span>        ((11, 1), 30.0),</div>
<div class="line"><span class="lineno">  775</span>        ((11, 3), 150.0),</div>
<div class="line"><span class="lineno">  776</span>    ],</div>
<div class="line"><span class="lineno">  777</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ab4cb628654242257a120821b583200e1" name="ab4cb628654242257a120821b583200e1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab4cb628654242257a120821b583200e1">&#9670;&#160;</a></span>test_sparse_cg_max_iter()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_sparse_cg_max_iter </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1543</span><span class="keyword">def </span>test_sparse_cg_max_iter():</div>
<div class="line"><span class="lineno"> 1544</span>    reg = Ridge(solver=<span class="stringliteral">&quot;sparse_cg&quot;</span>, max_iter=1)</div>
<div class="line"><span class="lineno"> 1545</span>    reg.fit(X_diabetes, y_diabetes)</div>
<div class="line"><span class="lineno"> 1546</span>    <span class="keyword">assert</span> reg.coef_.shape[0] == X_diabetes.shape[1]</div>
<div class="line"><span class="lineno"> 1547</span> </div>
<div class="line"><span class="lineno"> 1548</span> </div>
<div class="line"><span class="lineno"> 1549</span><span class="preprocessor">@ignore_warnings</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="aff5883e4de00594746f76fb1336d46a2" name="aff5883e4de00594746f76fb1336d46a2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aff5883e4de00594746f76fb1336d46a2">&#9670;&#160;</a></span>test_sparse_design_with_sample_weights()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_sparse_design_with_sample_weights </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1436</span><span class="keyword">def </span>test_sparse_design_with_sample_weights():</div>
<div class="line"><span class="lineno"> 1437</span>    <span class="comment"># Sample weights must work with sparse matrices</span></div>
<div class="line"><span class="lineno"> 1438</span> </div>
<div class="line"><span class="lineno"> 1439</span>    n_sampless = [2, 3]</div>
<div class="line"><span class="lineno"> 1440</span>    n_featuress = [3, 2]</div>
<div class="line"><span class="lineno"> 1441</span> </div>
<div class="line"><span class="lineno"> 1442</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno"> 1443</span> </div>
<div class="line"><span class="lineno"> 1444</span>    sparse_matrix_converters = [</div>
<div class="line"><span class="lineno"> 1445</span>        sp.coo_matrix,</div>
<div class="line"><span class="lineno"> 1446</span>        sp.csr_matrix,</div>
<div class="line"><span class="lineno"> 1447</span>        sp.csc_matrix,</div>
<div class="line"><span class="lineno"> 1448</span>        sp.lil_matrix,</div>
<div class="line"><span class="lineno"> 1449</span>        sp.dok_matrix,</div>
<div class="line"><span class="lineno"> 1450</span>    ]</div>
<div class="line"><span class="lineno"> 1451</span> </div>
<div class="line"><span class="lineno"> 1452</span>    sparse_ridge = Ridge(alpha=1.0, fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1453</span>    dense_ridge = Ridge(alpha=1.0, fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1454</span> </div>
<div class="line"><span class="lineno"> 1455</span>    <span class="keywordflow">for</span> n_samples, n_features <span class="keywordflow">in</span> zip(n_sampless, n_featuress):</div>
<div class="line"><span class="lineno"> 1456</span>        X = rng.randn(n_samples, n_features)</div>
<div class="line"><span class="lineno"> 1457</span>        y = rng.randn(n_samples)</div>
<div class="line"><span class="lineno"> 1458</span>        sample_weights = rng.randn(n_samples) ** 2 + 1</div>
<div class="line"><span class="lineno"> 1459</span>        <span class="keywordflow">for</span> sparse_converter <span class="keywordflow">in</span> sparse_matrix_converters:</div>
<div class="line"><span class="lineno"> 1460</span>            X_sparse = sparse_converter(X)</div>
<div class="line"><span class="lineno"> 1461</span>            sparse_ridge.fit(X_sparse, y, sample_weight=sample_weights)</div>
<div class="line"><span class="lineno"> 1462</span>            dense_ridge.fit(X, y, sample_weight=sample_weights)</div>
<div class="line"><span class="lineno"> 1463</span> </div>
<div class="line"><span class="lineno"> 1464</span>            assert_array_almost_equal(sparse_ridge.coef_, dense_ridge.coef_, decimal=6)</div>
<div class="line"><span class="lineno"> 1465</span> </div>
<div class="line"><span class="lineno"> 1466</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad8cf170c1f38434250f72ee9b0162371" name="ad8cf170c1f38434250f72ee9b0162371"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad8cf170c1f38434250f72ee9b0162371">&#9670;&#160;</a></span>test_X_CenterStackOp()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.test_X_CenterStackOp </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_col</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  619</span><span class="keyword">def </span>test_X_CenterStackOp(n_col):</div>
<div class="line"><span class="lineno">  620</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  621</span>    X = rng.randn(11, 8)</div>
<div class="line"><span class="lineno">  622</span>    X_m = rng.randn(8)</div>
<div class="line"><span class="lineno">  623</span>    sqrt_sw = rng.randn(len(X))</div>
<div class="line"><span class="lineno">  624</span>    Y = rng.randn(11, *n_col)</div>
<div class="line"><span class="lineno">  625</span>    A = rng.randn(9, *n_col)</div>
<div class="line"><span class="lineno">  626</span>    operator = _X_CenterStackOp(sp.csr_matrix(X), X_m, sqrt_sw)</div>
<div class="line"><span class="lineno">  627</span>    reference_operator = np.hstack([X - sqrt_sw[:, <span class="keywordtype">None</span>] * X_m, sqrt_sw[:, <span class="keywordtype">None</span>]])</div>
<div class="line"><span class="lineno">  628</span>    assert_allclose(reference_operator.dot(A), operator.dot(A))</div>
<div class="line"><span class="lineno">  629</span>    assert_allclose(reference_operator.T.dot(Y), operator.T.dot(Y))</div>
<div class="line"><span class="lineno">  630</span> </div>
<div class="line"><span class="lineno">  631</span> </div>
<div class="line"><span class="lineno">  632</span><span class="preprocessor">@pytest.mark.parametrize(&quot;shape&quot;, [(10, 1)</span>, (13, 9), (3, 7), (2, 2), (20, 20)])</div>
<div class="line"><span class="lineno">  633</span><span class="preprocessor">@pytest.mark.parametrize(&quot;uniform_weights&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a919b2df8f10f777148a95d3f302b7c59" name="a919b2df8f10f777148a95d3f302b7c59"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a919b2df8f10f777148a95d3f302b7c59">&#9670;&#160;</a></span>diabetes</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.diabetes = datasets.load_diabetes()</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ab04905d865109ec38039a9a8020d9838" name="ab04905d865109ec38039a9a8020d9838"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab04905d865109ec38039a9a8020d9838">&#9670;&#160;</a></span>ind</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.ind = np.arange(X_diabetes.shape[0])</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aacddf4d95eb4c1544fd70e1b80afd45f" name="aacddf4d95eb4c1544fd70e1b80afd45f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aacddf4d95eb4c1544fd70e1b80afd45f">&#9670;&#160;</a></span>iris</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.iris = datasets.load_iris()</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="acd5259fc50fe976391436d21eda84abb" name="acd5259fc50fe976391436d21eda84abb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acd5259fc50fe976391436d21eda84abb">&#9670;&#160;</a></span>rng</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.rng = np.random.RandomState(0)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ad782224cbe16bce2c9ede7efac4e6c64" name="ad782224cbe16bce2c9ede7efac4e6c64"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad782224cbe16bce2c9ede7efac4e6c64">&#9670;&#160;</a></span>SOLVERS</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">tuple sklearn.linear_model.tests.test_ridge.SOLVERS = (&quot;svd&quot;, &quot;sparse_cg&quot;, &quot;cholesky&quot;, &quot;lsqr&quot;, &quot;sag&quot;, &quot;saga&quot;)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a41c17009ec8619bc9350ad8222099b32" name="a41c17009ec8619bc9350ad8222099b32"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a41c17009ec8619bc9350ad8222099b32">&#9670;&#160;</a></span>SPARSE_SOLVERS_WITH_INTERCEPT</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">tuple sklearn.linear_model.tests.test_ridge.SPARSE_SOLVERS_WITH_INTERCEPT = (&quot;sparse_cg&quot;, &quot;sag&quot;)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a6feb81e045a57ca37e2ba3a8f003634b" name="a6feb81e045a57ca37e2ba3a8f003634b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6feb81e045a57ca37e2ba3a8f003634b">&#9670;&#160;</a></span>SPARSE_SOLVERS_WITHOUT_INTERCEPT</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">tuple sklearn.linear_model.tests.test_ridge.SPARSE_SOLVERS_WITHOUT_INTERCEPT = (&quot;sparse_cg&quot;, &quot;cholesky&quot;, &quot;lsqr&quot;, &quot;sag&quot;, &quot;saga&quot;)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a4582a107b030dad4bcba71aff09fcb04" name="a4582a107b030dad4bcba71aff09fcb04"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4582a107b030dad4bcba71aff09fcb04">&#9670;&#160;</a></span>X_diabetes</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.X_diabetes</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a5a061a6f140fc3d42d96efa0f07edf6f" name="a5a061a6f140fc3d42d96efa0f07edf6f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5a061a6f140fc3d42d96efa0f07edf6f">&#9670;&#160;</a></span>X_iris</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.X_iris = sp.csr_matrix(iris.data)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a033fd3c4bf70e42b1469403dece2654d" name="a033fd3c4bf70e42b1469403dece2654d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a033fd3c4bf70e42b1469403dece2654d">&#9670;&#160;</a></span>y_diabetes</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.y_diabetes</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a5d065c6ae19f578db504b7f27e3974bf" name="a5d065c6ae19f578db504b7f27e3974bf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5d065c6ae19f578db504b7f27e3974bf">&#9670;&#160;</a></span>y_iris</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_ridge.y_iris = iris.target</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
