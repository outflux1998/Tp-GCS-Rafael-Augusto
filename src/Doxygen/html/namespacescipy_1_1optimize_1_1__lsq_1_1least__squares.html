<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: scipy.optimize._lsq.least_squares Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacescipy.html">scipy</a></li><li class="navelem"><a class="el" href="namespacescipy_1_1optimize.html">optimize</a></li><li class="navelem"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq.html">_lsq</a></li><li class="navelem"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html">least_squares</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">scipy.optimize._lsq.least_squares Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a567467e07e2f9931473123c379ad006f" id="r_a567467e07e2f9931473123c379ad006f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#a567467e07e2f9931473123c379ad006f">call_minpack</a> (fun, x0, jac, ftol, xtol, gtol, max_nfev, x_scale, diff_step)</td></tr>
<tr class="separator:a567467e07e2f9931473123c379ad006f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa579f28da46b588a00d171266ed94afd" id="r_aa579f28da46b588a00d171266ed94afd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#aa579f28da46b588a00d171266ed94afd">prepare_bounds</a> (bounds, <a class="el" href="__blas__subroutines_8h.html#a25eafceb38c8e75bc60701fea6623f71">n</a>)</td></tr>
<tr class="separator:aa579f28da46b588a00d171266ed94afd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaafc5f69f68b77aef122557c8b1b0fbb" id="r_aaafc5f69f68b77aef122557c8b1b0fbb"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#aaafc5f69f68b77aef122557c8b1b0fbb">check_tolerance</a> (ftol, xtol, gtol, method)</td></tr>
<tr class="separator:aaafc5f69f68b77aef122557c8b1b0fbb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab82a5c442e13e3cb7f0a84ad01f43811" id="r_ab82a5c442e13e3cb7f0a84ad01f43811"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#ab82a5c442e13e3cb7f0a84ad01f43811">check_x_scale</a> (x_scale, x0)</td></tr>
<tr class="separator:ab82a5c442e13e3cb7f0a84ad01f43811"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a41da3450226d2fbddadd9650223f6ef6" id="r_a41da3450226d2fbddadd9650223f6ef6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#a41da3450226d2fbddadd9650223f6ef6">check_jac_sparsity</a> (jac_sparsity, m, <a class="el" href="__blas__subroutines_8h.html#a25eafceb38c8e75bc60701fea6623f71">n</a>)</td></tr>
<tr class="separator:a41da3450226d2fbddadd9650223f6ef6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3c6f04ec523f173fdeabc6c0d5bb0bd" id="r_ab3c6f04ec523f173fdeabc6c0d5bb0bd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#ab3c6f04ec523f173fdeabc6c0d5bb0bd">huber</a> (<a class="el" href="__blas__subroutines_8h.html#a97fad269e85497afbd382321b3079d38">z</a>, <a class="el" href="__lapack__subroutines_8h.html#a9bb256f5d273cef048eb659e4ee52fe2">rho</a>, cost_only)</td></tr>
<tr class="separator:ab3c6f04ec523f173fdeabc6c0d5bb0bd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a20d3588c88af58fec7fe60b2c8f3f6c4" id="r_a20d3588c88af58fec7fe60b2c8f3f6c4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#a20d3588c88af58fec7fe60b2c8f3f6c4">soft_l1</a> (<a class="el" href="__blas__subroutines_8h.html#a97fad269e85497afbd382321b3079d38">z</a>, <a class="el" href="__lapack__subroutines_8h.html#a9bb256f5d273cef048eb659e4ee52fe2">rho</a>, cost_only)</td></tr>
<tr class="separator:a20d3588c88af58fec7fe60b2c8f3f6c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a40e5e1676d00dfbcd475cebb0f28d689" id="r_a40e5e1676d00dfbcd475cebb0f28d689"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#a40e5e1676d00dfbcd475cebb0f28d689">cauchy</a> (<a class="el" href="__blas__subroutines_8h.html#a97fad269e85497afbd382321b3079d38">z</a>, <a class="el" href="__lapack__subroutines_8h.html#a9bb256f5d273cef048eb659e4ee52fe2">rho</a>, cost_only)</td></tr>
<tr class="separator:a40e5e1676d00dfbcd475cebb0f28d689"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad01a81fd3f95b126f4389633735c2788" id="r_ad01a81fd3f95b126f4389633735c2788"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#ad01a81fd3f95b126f4389633735c2788">arctan</a> (<a class="el" href="__blas__subroutines_8h.html#a97fad269e85497afbd382321b3079d38">z</a>, <a class="el" href="__lapack__subroutines_8h.html#a9bb256f5d273cef048eb659e4ee52fe2">rho</a>, cost_only)</td></tr>
<tr class="separator:ad01a81fd3f95b126f4389633735c2788"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac19831d88907407fa1d1655f46c33d8d" id="r_ac19831d88907407fa1d1655f46c33d8d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#ac19831d88907407fa1d1655f46c33d8d">construct_loss_function</a> (m, loss, f_scale)</td></tr>
<tr class="separator:ac19831d88907407fa1d1655f46c33d8d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a491c9da22b8c9daa7b05e408aa9b0a2d" id="r_a491c9da22b8c9daa7b05e408aa9b0a2d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#a491c9da22b8c9daa7b05e408aa9b0a2d">least_squares</a> (fun, x0, jac='2-point', bounds=(-np.inf, np.inf), method='trf', ftol=1e-8, xtol=1e-8, gtol=1e-8, x_scale=1.0, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={})</td></tr>
<tr class="separator:a491c9da22b8c9daa7b05e408aa9b0a2d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a5782e898597b6780ad1a5ad4531a5daa" id="r_a5782e898597b6780ad1a5ad4531a5daa"><td class="memItemLeft" align="right" valign="top">dict&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#a5782e898597b6780ad1a5ad4531a5daa">TERMINATION_MESSAGES</a></td></tr>
<tr class="separator:a5782e898597b6780ad1a5ad4531a5daa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a36d1c38b591ad4cc3b077d04b978c181" id="r_a36d1c38b591ad4cc3b077d04b978c181"><td class="memItemLeft" align="right" valign="top">dict&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#a36d1c38b591ad4cc3b077d04b978c181">FROM_MINPACK_TO_COMMON</a></td></tr>
<tr class="separator:a36d1c38b591ad4cc3b077d04b978c181"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab6d87fc593a7f2fd5ae185cd95a9dec6" id="r_ab6d87fc593a7f2fd5ae185cd95a9dec6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__lsq_1_1least__squares.html#ab6d87fc593a7f2fd5ae185cd95a9dec6">IMPLEMENTED_LOSSES</a></td></tr>
<tr class="separator:ab6d87fc593a7f2fd5ae185cd95a9dec6"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Generic interface for least-squares minimization.</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="ad01a81fd3f95b126f4389633735c2788" name="ad01a81fd3f95b126f4389633735c2788"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad01a81fd3f95b126f4389633735c2788">&#9670;&#160;</a></span>arctan()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.arctan </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>rho</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>cost_only</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  198</span><span class="keyword">def </span>arctan(z, rho, cost_only):</div>
<div class="line"><span class="lineno">  199</span>    rho[0] = np.arctan(z)</div>
<div class="line"><span class="lineno">  200</span>    <span class="keywordflow">if</span> cost_only:</div>
<div class="line"><span class="lineno">  201</span>        <span class="keywordflow">return</span></div>
<div class="line"><span class="lineno">  202</span>    t = 1 + z**2</div>
<div class="line"><span class="lineno">  203</span>    rho[1] = 1 / t</div>
<div class="line"><span class="lineno">  204</span>    rho[2] = -2 * z / t**2</div>
<div class="line"><span class="lineno">  205</span> </div>
<div class="line"><span class="lineno">  206</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a567467e07e2f9931473123c379ad006f" name="a567467e07e2f9931473123c379ad006f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a567467e07e2f9931473123c379ad006f">&#9670;&#160;</a></span>call_minpack()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.call_minpack </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fun</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x0</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>jac</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ftol</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>xtol</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>gtol</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_nfev</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x_scale</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>diff_step</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   39</span><span class="keyword">def </span>call_minpack(fun, x0, jac, ftol, xtol, gtol, max_nfev, x_scale, diff_step):</div>
<div class="line"><span class="lineno">   40</span>    n = x0.size</div>
<div class="line"><span class="lineno">   41</span> </div>
<div class="line"><span class="lineno">   42</span>    <span class="keywordflow">if</span> diff_step <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">   43</span>        epsfcn = EPS</div>
<div class="line"><span class="lineno">   44</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   45</span>        epsfcn = diff_step**2</div>
<div class="line"><span class="lineno">   46</span> </div>
<div class="line"><span class="lineno">   47</span>    <span class="comment"># Compute MINPACK&#39;s `diag`, which is inverse of our `x_scale` and</span></div>
<div class="line"><span class="lineno">   48</span>    <span class="comment"># ``x_scale=&#39;jac&#39;`` corresponds to ``diag=None``.</span></div>
<div class="line"><span class="lineno">   49</span>    <span class="keywordflow">if</span> isinstance(x_scale, str) <span class="keywordflow">and</span> x_scale == <span class="stringliteral">&#39;jac&#39;</span>:</div>
<div class="line"><span class="lineno">   50</span>        diag = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">   51</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   52</span>        diag = 1 / x_scale</div>
<div class="line"><span class="lineno">   53</span> </div>
<div class="line"><span class="lineno">   54</span>    full_output = <span class="keyword">True</span></div>
<div class="line"><span class="lineno">   55</span>    col_deriv = <span class="keyword">False</span></div>
<div class="line"><span class="lineno">   56</span>    factor = 100.0</div>
<div class="line"><span class="lineno">   57</span> </div>
<div class="line"><span class="lineno">   58</span>    <span class="keywordflow">if</span> jac <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">   59</span>        <span class="keywordflow">if</span> max_nfev <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">   60</span>            <span class="comment"># n squared to account for Jacobian evaluations.</span></div>
<div class="line"><span class="lineno">   61</span>            max_nfev = 100 * n * (n + 1)</div>
<div class="line"><span class="lineno">   62</span>        x, info, status = _minpack._lmdif(</div>
<div class="line"><span class="lineno">   63</span>            fun, x0, (), full_output, ftol, xtol, gtol,</div>
<div class="line"><span class="lineno">   64</span>            max_nfev, epsfcn, factor, diag)</div>
<div class="line"><span class="lineno">   65</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   66</span>        <span class="keywordflow">if</span> max_nfev <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">   67</span>            max_nfev = 100 * n</div>
<div class="line"><span class="lineno">   68</span>        x, info, status = _minpack._lmder(</div>
<div class="line"><span class="lineno">   69</span>            fun, jac, x0, (), full_output, col_deriv,</div>
<div class="line"><span class="lineno">   70</span>            ftol, xtol, gtol, max_nfev, factor, diag)</div>
<div class="line"><span class="lineno">   71</span> </div>
<div class="line"><span class="lineno">   72</span>    f = info[<span class="stringliteral">&#39;fvec&#39;</span>]</div>
<div class="line"><span class="lineno">   73</span> </div>
<div class="line"><span class="lineno">   74</span>    <span class="keywordflow">if</span> callable(jac):</div>
<div class="line"><span class="lineno">   75</span>        J = jac(x)</div>
<div class="line"><span class="lineno">   76</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   77</span>        J = np.atleast_2d(approx_derivative(fun, x))</div>
<div class="line"><span class="lineno">   78</span> </div>
<div class="line"><span class="lineno">   79</span>    cost = 0.5 * np.dot(f, f)</div>
<div class="line"><span class="lineno">   80</span>    g = J.T.dot(f)</div>
<div class="line"><span class="lineno">   81</span>    g_norm = norm(g, ord=np.inf)</div>
<div class="line"><span class="lineno">   82</span> </div>
<div class="line"><span class="lineno">   83</span>    nfev = info[<span class="stringliteral">&#39;nfev&#39;</span>]</div>
<div class="line"><span class="lineno">   84</span>    njev = info.get(<span class="stringliteral">&#39;njev&#39;</span>, <span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">   85</span> </div>
<div class="line"><span class="lineno">   86</span>    status = FROM_MINPACK_TO_COMMON[status]</div>
<div class="line"><span class="lineno">   87</span>    active_mask = np.zeros_like(x0, dtype=int)</div>
<div class="line"><span class="lineno">   88</span> </div>
<div class="line"><span class="lineno">   89</span>    <span class="keywordflow">return</span> OptimizeResult(</div>
<div class="line"><span class="lineno">   90</span>        x=x, cost=cost, fun=f, jac=J, grad=g, optimality=g_norm,</div>
<div class="line"><span class="lineno">   91</span>        active_mask=active_mask, nfev=nfev, njev=njev, status=status)</div>
<div class="line"><span class="lineno">   92</span> </div>
<div class="line"><span class="lineno">   93</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a40e5e1676d00dfbcd475cebb0f28d689" name="a40e5e1676d00dfbcd475cebb0f28d689"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a40e5e1676d00dfbcd475cebb0f28d689">&#9670;&#160;</a></span>cauchy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.cauchy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>rho</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>cost_only</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  189</span><span class="keyword">def </span>cauchy(z, rho, cost_only):</div>
<div class="line"><span class="lineno">  190</span>    rho[0] = np.log1p(z)</div>
<div class="line"><span class="lineno">  191</span>    <span class="keywordflow">if</span> cost_only:</div>
<div class="line"><span class="lineno">  192</span>        <span class="keywordflow">return</span></div>
<div class="line"><span class="lineno">  193</span>    t = 1 + z</div>
<div class="line"><span class="lineno">  194</span>    rho[1] = 1 / t</div>
<div class="line"><span class="lineno">  195</span>    rho[2] = -1 / t**2</div>
<div class="line"><span class="lineno">  196</span> </div>
<div class="line"><span class="lineno">  197</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a41da3450226d2fbddadd9650223f6ef6" name="a41da3450226d2fbddadd9650223f6ef6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a41da3450226d2fbddadd9650223f6ef6">&#9670;&#160;</a></span>check_jac_sparsity()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.check_jac_sparsity </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>jac_sparsity</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>m</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  152</span><span class="keyword">def </span>check_jac_sparsity(jac_sparsity, m, n):</div>
<div class="line"><span class="lineno">  153</span>    <span class="keywordflow">if</span> jac_sparsity <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  154</span>        <span class="keywordflow">return</span> <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  155</span> </div>
<div class="line"><span class="lineno">  156</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> issparse(jac_sparsity):</div>
<div class="line"><span class="lineno">  157</span>        jac_sparsity = np.atleast_2d(jac_sparsity)</div>
<div class="line"><span class="lineno">  158</span> </div>
<div class="line"><span class="lineno">  159</span>    <span class="keywordflow">if</span> jac_sparsity.shape != (m, n):</div>
<div class="line"><span class="lineno">  160</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`jac_sparsity` has wrong shape.&quot;</span>)</div>
<div class="line"><span class="lineno">  161</span> </div>
<div class="line"><span class="lineno">  162</span>    <span class="keywordflow">return</span> jac_sparsity, group_columns(jac_sparsity)</div>
<div class="line"><span class="lineno">  163</span> </div>
<div class="line"><span class="lineno">  164</span> </div>
<div class="line"><span class="lineno">  165</span><span class="comment"># Loss functions.</span></div>
<div class="line"><span class="lineno">  166</span> </div>
<div class="line"><span class="lineno">  167</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aaafc5f69f68b77aef122557c8b1b0fbb" name="aaafc5f69f68b77aef122557c8b1b0fbb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aaafc5f69f68b77aef122557c8b1b0fbb">&#9670;&#160;</a></span>check_tolerance()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.check_tolerance </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ftol</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>xtol</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>gtol</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>method</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  105</span><span class="keyword">def </span>check_tolerance(ftol, xtol, gtol, method):</div>
<div class="line"><span class="lineno">  106</span>    <span class="keyword">def </span>check(tol, name):</div>
<div class="line"><span class="lineno">  107</span>        <span class="keywordflow">if</span> tol <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  108</span>            tol = 0</div>
<div class="line"><span class="lineno">  109</span>        <span class="keywordflow">elif</span> tol &lt; EPS:</div>
<div class="line"><span class="lineno">  110</span>            warn(<span class="stringliteral">&quot;Setting `{}` below the machine epsilon ({:.2e}) effectively &quot;</span></div>
<div class="line"><span class="lineno">  111</span>                 <span class="stringliteral">&quot;disables the corresponding termination condition.&quot;</span></div>
<div class="line"><span class="lineno">  112</span>                 .format(name, EPS))</div>
<div class="line"><span class="lineno">  113</span>        <span class="keywordflow">return</span> tol</div>
<div class="line"><span class="lineno">  114</span> </div>
<div class="line"><span class="lineno">  115</span>    ftol = check(ftol, <span class="stringliteral">&quot;ftol&quot;</span>)</div>
<div class="line"><span class="lineno">  116</span>    xtol = check(xtol, <span class="stringliteral">&quot;xtol&quot;</span>)</div>
<div class="line"><span class="lineno">  117</span>    gtol = check(gtol, <span class="stringliteral">&quot;gtol&quot;</span>)</div>
<div class="line"><span class="lineno">  118</span> </div>
<div class="line"><span class="lineno">  119</span>    <span class="keywordflow">if</span> method == <span class="stringliteral">&quot;lm&quot;</span> <span class="keywordflow">and</span> (ftol &lt; EPS <span class="keywordflow">or</span> xtol &lt; EPS <span class="keywordflow">or</span> gtol &lt; EPS):</div>
<div class="line"><span class="lineno">  120</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;All tolerances must be higher than machine epsilon &quot;</span></div>
<div class="line"><span class="lineno">  121</span>                         <span class="stringliteral">&quot;({:.2e}) for method &#39;lm&#39;.&quot;</span>.format(EPS))</div>
<div class="line"><span class="lineno">  122</span>    <span class="keywordflow">elif</span> ftol &lt; EPS <span class="keywordflow">and</span> xtol &lt; EPS <span class="keywordflow">and</span> gtol &lt; EPS:</div>
<div class="line"><span class="lineno">  123</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;At least one of the tolerances must be higher than &quot;</span></div>
<div class="line"><span class="lineno">  124</span>                         <span class="stringliteral">&quot;machine epsilon ({:.2e}).&quot;</span>.format(EPS))</div>
<div class="line"><span class="lineno">  125</span> </div>
<div class="line"><span class="lineno">  126</span>    <span class="keywordflow">return</span> ftol, xtol, gtol</div>
<div class="line"><span class="lineno">  127</span> </div>
<div class="line"><span class="lineno">  128</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ab82a5c442e13e3cb7f0a84ad01f43811" name="ab82a5c442e13e3cb7f0a84ad01f43811"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab82a5c442e13e3cb7f0a84ad01f43811">&#9670;&#160;</a></span>check_x_scale()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.check_x_scale </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x_scale</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x0</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  129</span><span class="keyword">def </span>check_x_scale(x_scale, x0):</div>
<div class="line"><span class="lineno">  130</span>    <span class="keywordflow">if</span> isinstance(x_scale, str) <span class="keywordflow">and</span> x_scale == <span class="stringliteral">&#39;jac&#39;</span>:</div>
<div class="line"><span class="lineno">  131</span>        <span class="keywordflow">return</span> x_scale</div>
<div class="line"><span class="lineno">  132</span> </div>
<div class="line"><span class="lineno">  133</span>    <span class="keywordflow">try</span>:</div>
<div class="line"><span class="lineno">  134</span>        x_scale = np.asarray(x_scale, dtype=float)</div>
<div class="line"><span class="lineno">  135</span>        valid = np.all(np.isfinite(x_scale)) <span class="keywordflow">and</span> np.all(x_scale &gt; 0)</div>
<div class="line"><span class="lineno">  136</span>    <span class="keywordflow">except</span> (ValueError, TypeError):</div>
<div class="line"><span class="lineno">  137</span>        valid = <span class="keyword">False</span></div>
<div class="line"><span class="lineno">  138</span> </div>
<div class="line"><span class="lineno">  139</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> valid:</div>
<div class="line"><span class="lineno">  140</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`x_scale` must be &#39;jac&#39; or array_like with &quot;</span></div>
<div class="line"><span class="lineno">  141</span>                         <span class="stringliteral">&quot;positive numbers.&quot;</span>)</div>
<div class="line"><span class="lineno">  142</span> </div>
<div class="line"><span class="lineno">  143</span>    <span class="keywordflow">if</span> x_scale.ndim == 0:</div>
<div class="line"><span class="lineno">  144</span>        x_scale = np.resize(x_scale, x0.shape)</div>
<div class="line"><span class="lineno">  145</span> </div>
<div class="line"><span class="lineno">  146</span>    <span class="keywordflow">if</span> x_scale.shape != x0.shape:</div>
<div class="line"><span class="lineno">  147</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Inconsistent shapes between `x_scale` and `x0`.&quot;</span>)</div>
<div class="line"><span class="lineno">  148</span> </div>
<div class="line"><span class="lineno">  149</span>    <span class="keywordflow">return</span> x_scale</div>
<div class="line"><span class="lineno">  150</span> </div>
<div class="line"><span class="lineno">  151</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ac19831d88907407fa1d1655f46c33d8d" name="ac19831d88907407fa1d1655f46c33d8d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac19831d88907407fa1d1655f46c33d8d">&#9670;&#160;</a></span>construct_loss_function()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.construct_loss_function </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>m</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>f_scale</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  211</span><span class="keyword">def </span>construct_loss_function(m, loss, f_scale):</div>
<div class="line"><span class="lineno">  212</span>    <span class="keywordflow">if</span> loss == <span class="stringliteral">&#39;linear&#39;</span>:</div>
<div class="line"><span class="lineno">  213</span>        <span class="keywordflow">return</span> <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  214</span> </div>
<div class="line"><span class="lineno">  215</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> callable(loss):</div>
<div class="line"><span class="lineno">  216</span>        loss = IMPLEMENTED_LOSSES[loss]</div>
<div class="line"><span class="lineno">  217</span>        rho = np.empty((3, m))</div>
<div class="line"><span class="lineno">  218</span> </div>
<div class="line"><span class="lineno">  219</span>        <span class="keyword">def </span>loss_function(f, cost_only=False):</div>
<div class="line"><span class="lineno">  220</span>            z = (f / f_scale) ** 2</div>
<div class="line"><span class="lineno">  221</span>            loss(z, rho, cost_only=cost_only)</div>
<div class="line"><span class="lineno">  222</span>            <span class="keywordflow">if</span> cost_only:</div>
<div class="line"><span class="lineno">  223</span>                <span class="keywordflow">return</span> 0.5 * f_scale ** 2 * np.sum(rho[0])</div>
<div class="line"><span class="lineno">  224</span>            rho[0] *= f_scale ** 2</div>
<div class="line"><span class="lineno">  225</span>            rho[2] /= f_scale ** 2</div>
<div class="line"><span class="lineno">  226</span>            <span class="keywordflow">return</span> rho</div>
<div class="line"><span class="lineno">  227</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  228</span>        <span class="keyword">def </span>loss_function(f, cost_only=False):</div>
<div class="line"><span class="lineno">  229</span>            z = (f / f_scale) ** 2</div>
<div class="line"><span class="lineno">  230</span>            rho = loss(z)</div>
<div class="line"><span class="lineno">  231</span>            <span class="keywordflow">if</span> cost_only:</div>
<div class="line"><span class="lineno">  232</span>                <span class="keywordflow">return</span> 0.5 * f_scale ** 2 * np.sum(rho[0])</div>
<div class="line"><span class="lineno">  233</span>            rho[0] *= f_scale ** 2</div>
<div class="line"><span class="lineno">  234</span>            rho[2] /= f_scale ** 2</div>
<div class="line"><span class="lineno">  235</span>            <span class="keywordflow">return</span> rho</div>
<div class="line"><span class="lineno">  236</span> </div>
<div class="line"><span class="lineno">  237</span>    <span class="keywordflow">return</span> loss_function</div>
<div class="line"><span class="lineno">  238</span> </div>
<div class="line"><span class="lineno">  239</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ab3c6f04ec523f173fdeabc6c0d5bb0bd" name="ab3c6f04ec523f173fdeabc6c0d5bb0bd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab3c6f04ec523f173fdeabc6c0d5bb0bd">&#9670;&#160;</a></span>huber()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.huber </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>rho</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>cost_only</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  168</span><span class="keyword">def </span>huber(z, rho, cost_only):</div>
<div class="line"><span class="lineno">  169</span>    mask = z &lt;= 1</div>
<div class="line"><span class="lineno">  170</span>    rho[0, mask] = z[mask]</div>
<div class="line"><span class="lineno">  171</span>    rho[0, ~mask] = 2 * z[~mask]**0.5 - 1</div>
<div class="line"><span class="lineno">  172</span>    <span class="keywordflow">if</span> cost_only:</div>
<div class="line"><span class="lineno">  173</span>        <span class="keywordflow">return</span></div>
<div class="line"><span class="lineno">  174</span>    rho[1, mask] = 1</div>
<div class="line"><span class="lineno">  175</span>    rho[1, ~mask] = z[~mask]**-0.5</div>
<div class="line"><span class="lineno">  176</span>    rho[2, mask] = 0</div>
<div class="line"><span class="lineno">  177</span>    rho[2, ~mask] = -0.5 * z[~mask]**-1.5</div>
<div class="line"><span class="lineno">  178</span> </div>
<div class="line"><span class="lineno">  179</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a491c9da22b8c9daa7b05e408aa9b0a2d" name="a491c9da22b8c9daa7b05e408aa9b0a2d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a491c9da22b8c9daa7b05e408aa9b0a2d">&#9670;&#160;</a></span>least_squares()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.least_squares </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fun</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x0</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>jac</em> = <code>'2-point'</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>bounds</em> = <code>(-np.inf,&#160;np.inf)</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>method</em> = <code>'trf'</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ftol</em> = <code>1e-8</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>xtol</em> = <code>1e-8</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>gtol</em> = <code>1e-8</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x_scale</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em> = <code>'linear'</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>f_scale</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>diff_step</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tr_solver</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tr_options</em> = <code>{}</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>jac_sparsity</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_nfev</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>args</em> = <code>()</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>kwargs</em> = <code>{}</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Solve a nonlinear least-squares problem with bounds on the variables.

Given the residuals f(x) (an m-D real function of n real
variables) and the loss function rho(s) (a scalar function), `least_squares`
finds a local minimum of the cost function F(x)::

    minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)
    subject to lb &lt;= x &lt;= ub

The purpose of the loss function rho(s) is to reduce the influence of
outliers on the solution.

Parameters
----------
fun : callable
    Function which computes the vector of residuals, with the signature
    ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with
    respect to its first argument. The argument ``x`` passed to this
    function is an ndarray of shape (n,) (never a scalar, even for n=1).
    It must allocate and return a 1-D array_like of shape (m,) or a scalar.
    If the argument ``x`` is complex or the function ``fun`` returns
    complex residuals, it must be wrapped in a real function of real
    arguments, as shown at the end of the Examples section.
x0 : array_like with shape (n,) or float
    Initial guess on independent variables. If float, it will be treated
    as a 1-D array with one element.
jac : {'2-point', '3-point', 'cs', callable}, optional
    Method of computing the Jacobian matrix (an m-by-n matrix, where
    element (i, j) is the partial derivative of f[i] with respect to
    x[j]). The keywords select a finite difference scheme for numerical
    estimation. The scheme '3-point' is more accurate, but requires
    twice as many operations as '2-point' (default). The scheme 'cs'
    uses complex steps, and while potentially the most accurate, it is
    applicable only when `fun` correctly handles complex inputs and
    can be analytically continued to the complex plane. Method 'lm'
    always uses the '2-point' scheme. If callable, it is used as
    ``jac(x, *args, **kwargs)`` and should return a good approximation
    (or the exact value) for the Jacobian as an array_like (np.atleast_2d
    is applied), a sparse matrix (csr_matrix preferred for performance) or
    a `scipy.sparse.linalg.LinearOperator`.
bounds : 2-tuple of array_like, optional
    Lower and upper bounds on independent variables. Defaults to no bounds.
    Each array must match the size of `x0` or be a scalar, in the latter
    case a bound will be the same for all variables. Use ``np.inf`` with
    an appropriate sign to disable bounds on all or some variables.
method : {'trf', 'dogbox', 'lm'}, optional
    Algorithm to perform minimization.

        * 'trf' : Trust Region Reflective algorithm, particularly suitable
          for large sparse problems with bounds. Generally robust method.
        * 'dogbox' : dogleg algorithm with rectangular trust regions,
          typical use case is small problems with bounds. Not recommended
          for problems with rank-deficient Jacobian.
        * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.
          Doesn't handle bounds and sparse Jacobians. Usually the most
          efficient method for small unconstrained problems.

    Default is 'trf'. See Notes for more information.
ftol : float or None, optional
    Tolerance for termination by the change of the cost function. Default
    is 1e-8. The optimization process is stopped when ``dF &lt; ftol * F``,
    and there was an adequate agreement between a local quadratic model and
    the true model in the last step.

    If None and 'method' is not 'lm', the termination by this condition is
    disabled. If 'method' is 'lm', this tolerance must be higher than
    machine epsilon.
xtol : float or None, optional
    Tolerance for termination by the change of the independent variables.
    Default is 1e-8. The exact condition depends on the `method` used:

        * For 'trf' and 'dogbox' : ``norm(dx) &lt; xtol * (xtol + norm(x))``.
        * For 'lm' : ``Delta &lt; xtol * norm(xs)``, where ``Delta`` is
          a trust-region radius and ``xs`` is the value of ``x``
          scaled according to `x_scale` parameter (see below).

    If None and 'method' is not 'lm', the termination by this condition is
    disabled. If 'method' is 'lm', this tolerance must be higher than
    machine epsilon.
gtol : float or None, optional
    Tolerance for termination by the norm of the gradient. Default is 1e-8.
    The exact condition depends on a `method` used:

        * For 'trf' : ``norm(g_scaled, ord=np.inf) &lt; gtol``, where
          ``g_scaled`` is the value of the gradient scaled to account for
          the presence of the bounds [STIR]_.
        * For 'dogbox' : ``norm(g_free, ord=np.inf) &lt; gtol``, where
          ``g_free`` is the gradient with respect to the variables which
          are not in the optimal state on the boundary.
        * For 'lm' : the maximum absolute value of the cosine of angles
          between columns of the Jacobian and the residual vector is less
          than `gtol`, or the residual vector is zero.

    If None and 'method' is not 'lm', the termination by this condition is
    disabled. If 'method' is 'lm', this tolerance must be higher than
    machine epsilon.
x_scale : array_like or 'jac', optional
    Characteristic scale of each variable. Setting `x_scale` is equivalent
    to reformulating the problem in scaled variables ``xs = x / x_scale``.
    An alternative view is that the size of a trust region along jth
    dimension is proportional to ``x_scale[j]``. Improved convergence may
    be achieved by setting `x_scale` such that a step of a given size
    along any of the scaled variables has a similar effect on the cost
    function. If set to 'jac', the scale is iteratively updated using the
    inverse norms of the columns of the Jacobian matrix (as described in
    [JJMore]_).
loss : str or callable, optional
    Determines the loss function. The following keyword values are allowed:

        * 'linear' (default) : ``rho(z) = z``. Gives a standard
          least-squares problem.
        * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth
          approximation of l1 (absolute value) loss. Usually a good
          choice for robust least squares.
        * 'huber' : ``rho(z) = z if z &lt;= 1 else 2*z**0.5 - 1``. Works
          similarly to 'soft_l1'.
        * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers
          influence, but may cause difficulties in optimization process.
        * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on
          a single residual, has properties similar to 'cauchy'.

    If callable, it must take a 1-D ndarray ``z=f**2`` and return an
    array_like with shape (3, m) where row 0 contains function values,
    row 1 contains first derivatives and row 2 contains second
    derivatives. Method 'lm' supports only 'linear' loss.
f_scale : float, optional
    Value of soft margin between inlier and outlier residuals, default
    is 1.0. The loss function is evaluated as follows
    ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,
    and ``rho`` is determined by `loss` parameter. This parameter has
    no effect with ``loss='linear'``, but for other `loss` values it is
    of crucial importance.
max_nfev : None or int, optional
    Maximum number of function evaluations before the termination.
    If None (default), the value is chosen automatically:

        * For 'trf' and 'dogbox' : 100 * n.
        * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)
          otherwise (because 'lm' counts function calls in Jacobian
          estimation).

diff_step : None or array_like, optional
    Determines the relative step size for the finite difference
    approximation of the Jacobian. The actual step is computed as
    ``x * diff_step``. If None (default), then `diff_step` is taken to be
    a conventional "optimal" power of machine epsilon for the finite
    difference scheme used [NR]_.
tr_solver : {None, 'exact', 'lsmr'}, optional
    Method for solving trust-region subproblems, relevant only for 'trf'
    and 'dogbox' methods.

        * 'exact' is suitable for not very large problems with dense
          Jacobian matrices. The computational complexity per iteration is
          comparable to a singular value decomposition of the Jacobian
          matrix.
        * 'lsmr' is suitable for problems with sparse and large Jacobian
          matrices. It uses the iterative procedure
          `scipy.sparse.linalg.lsmr` for finding a solution of a linear
          least-squares problem and only requires matrix-vector product
          evaluations.

    If None (default), the solver is chosen based on the type of Jacobian
    returned on the first iteration.
tr_options : dict, optional
    Keyword options passed to trust-region solver.

        * ``tr_solver='exact'``: `tr_options` are ignored.
        * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.
          Additionally,  ``method='trf'`` supports  'regularize' option
          (bool, default is True), which adds a regularization term to the
          normal equation, which improves convergence if the Jacobian is
          rank-deficient [Byrd]_ (eq. 3.4).

jac_sparsity : {None, array_like, sparse matrix}, optional
    Defines the sparsity structure of the Jacobian matrix for finite
    difference estimation, its shape must be (m, n). If the Jacobian has
    only few non-zero elements in *each* row, providing the sparsity
    structure will greatly speed up the computations [Curtis]_. A zero
    entry means that a corresponding element in the Jacobian is identically
    zero. If provided, forces the use of 'lsmr' trust-region solver.
    If None (default), then dense differencing will be used. Has no effect
    for 'lm' method.
verbose : {0, 1, 2}, optional
    Level of algorithm's verbosity:

        * 0 (default) : work silently.
        * 1 : display a termination report.
        * 2 : display progress during iterations (not supported by 'lm'
          method).

args, kwargs : tuple and dict, optional
    Additional arguments passed to `fun` and `jac`. Both empty by default.
    The calling signature is ``fun(x, *args, **kwargs)`` and the same for
    `jac`.

Returns
-------
result : OptimizeResult
    `OptimizeResult` with the following fields defined:

        x : ndarray, shape (n,)
            Solution found.
        cost : float
            Value of the cost function at the solution.
        fun : ndarray, shape (m,)
            Vector of residuals at the solution.
        jac : ndarray, sparse matrix or LinearOperator, shape (m, n)
            Modified Jacobian matrix at the solution, in the sense that J^T J
            is a Gauss-Newton approximation of the Hessian of the cost function.
            The type is the same as the one used by the algorithm.
        grad : ndarray, shape (m,)
            Gradient of the cost function at the solution.
        optimality : float
            First-order optimality measure. In unconstrained problems, it is
            always the uniform norm of the gradient. In constrained problems,
            it is the quantity which was compared with `gtol` during iterations.
        active_mask : ndarray of int, shape (n,)
            Each component shows whether a corresponding constraint is active
            (that is, whether a variable is at the bound):

                *  0 : a constraint is not active.
                * -1 : a lower bound is active.
                *  1 : an upper bound is active.

            Might be somewhat arbitrary for 'trf' method as it generates a
            sequence of strictly feasible iterates and `active_mask` is
            determined within a tolerance threshold.
        nfev : int
            Number of function evaluations done. Methods 'trf' and 'dogbox' do
            not count function calls for numerical Jacobian approximation, as
            opposed to 'lm' method.
        njev : int or None
            Number of Jacobian evaluations done. If numerical Jacobian
            approximation is used in 'lm' method, it is set to None.
        status : int
            The reason for algorithm termination:

                * -1 : improper input parameters status returned from MINPACK.
                *  0 : the maximum number of function evaluations is exceeded.
                *  1 : `gtol` termination condition is satisfied.
                *  2 : `ftol` termination condition is satisfied.
                *  3 : `xtol` termination condition is satisfied.
                *  4 : Both `ftol` and `xtol` termination conditions are satisfied.

        message : str
            Verbal description of the termination reason.
        success : bool
            True if one of the convergence criteria is satisfied (`status` &gt; 0).

See Also
--------
leastsq : A legacy wrapper for the MINPACK implementation of the
          Levenberg-Marquadt algorithm.
curve_fit : Least-squares minimization applied to a curve-fitting problem.

Notes
-----
Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares
algorithms implemented in MINPACK (lmder, lmdif). It runs the
Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.
The implementation is based on paper [JJMore]_, it is very robust and
efficient with a lot of smart tricks. It should be your first choice
for unconstrained problems. Note that it doesn't support bounds. Also,
it doesn't work when m &lt; n.

Method 'trf' (Trust Region Reflective) is motivated by the process of
solving a system of equations, which constitute the first-order optimality
condition for a bound-constrained minimization problem as formulated in
[STIR]_. The algorithm iteratively solves trust-region subproblems
augmented by a special diagonal quadratic term and with trust-region shape
determined by the distance from the bounds and the direction of the
gradient. This enhancements help to avoid making steps directly into bounds
and efficiently explore the whole space of variables. To further improve
convergence, the algorithm considers search directions reflected from the
bounds. To obey theoretical requirements, the algorithm keeps iterates
strictly feasible. With dense Jacobians trust-region subproblems are
solved by an exact method very similar to the one described in [JJMore]_
(and implemented in MINPACK). The difference from the MINPACK
implementation is that a singular value decomposition of a Jacobian
matrix is done once per iteration, instead of a QR decomposition and series
of Givens rotation eliminations. For large sparse Jacobians a 2-D subspace
approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.
The subspace is spanned by a scaled gradient and an approximate
Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no
constraints are imposed the algorithm is very similar to MINPACK and has
generally comparable performance. The algorithm works quite robust in
unbounded and bounded problems, thus it is chosen as a default algorithm.

Method 'dogbox' operates in a trust-region framework, but considers
rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.
The intersection of a current trust region and initial bounds is again
rectangular, so on each iteration a quadratic minimization problem subject
to bound constraints is solved approximately by Powell's dogleg method
[NumOpt]_. The required Gauss-Newton step can be computed exactly for
dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large
sparse Jacobians. The algorithm is likely to exhibit slow convergence when
the rank of Jacobian is less than the number of variables. The algorithm
often outperforms 'trf' in bounded problems with a small number of
variables.

Robust loss functions are implemented as described in [BA]_. The idea
is to modify a residual vector and a Jacobian matrix on each iteration
such that computed gradient and Gauss-Newton Hessian approximation match
the true gradient and Hessian approximation of the cost function. Then
the algorithm proceeds in a normal way, i.e., robust loss functions are
implemented as a simple wrapper over standard least-squares algorithms.

.. versionadded:: 0.17.0

References
----------
.. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, "A Subspace, Interior,
          and Conjugate Gradient Method for Large-Scale Bound-Constrained
          Minimization Problems," SIAM Journal on Scientific Computing,
          Vol. 21, Number 1, pp 1-23, 1999.
.. [NR] William H. Press et. al., "Numerical Recipes. The Art of Scientific
        Computing. 3rd edition", Sec. 5.7.
.. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, "Approximate
          solution of the trust region problem by minimization over
          two-dimensional subspaces", Math. Programming, 40, pp. 247-263,
          1988.
.. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, "On the estimation of
            sparse Jacobian matrices", Journal of the Institute of
            Mathematics and its Applications, 13, pp. 117-120, 1974.
.. [JJMore] J. J. More, "The Levenberg-Marquardt Algorithm: Implementation
            and Theory," Numerical Analysis, ed. G. A. Watson, Lecture
            Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.
.. [Voglis] C. Voglis and I. E. Lagaris, "A Rectangular Trust Region
            Dogleg Approach for Unconstrained and Bound Constrained
            Nonlinear Optimization", WSEAS International Conference on
            Applied Mathematics, Corfu, Greece, 2004.
.. [NumOpt] J. Nocedal and S. J. Wright, "Numerical optimization,
            2nd edition", Chapter 4.
.. [BA] B. Triggs et. al., "Bundle Adjustment - A Modern Synthesis",
        Proceedings of the International Workshop on Vision Algorithms:
        Theory and Practice, pp. 298-372, 1999.

Examples
--------
In this example we find a minimum of the Rosenbrock function without bounds
on independent variables.

&gt;&gt;&gt; def fun_rosenbrock(x):
...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])

Notice that we only provide the vector of the residuals. The algorithm
constructs the cost function as a sum of squares of the residuals, which
gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.

&gt;&gt;&gt; from scipy.optimize import least_squares
&gt;&gt;&gt; x0_rosenbrock = np.array([2, 2])
&gt;&gt;&gt; res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)
&gt;&gt;&gt; res_1.x
array([ 1.,  1.])
&gt;&gt;&gt; res_1.cost
9.8669242910846867e-30
&gt;&gt;&gt; res_1.optimality
8.8928864934219529e-14

We now constrain the variables, in such a way that the previous solution
becomes infeasible. Specifically, we require that ``x[1] &gt;= 1.5``, and
``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter
to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.

We also provide the analytic Jacobian:

&gt;&gt;&gt; def jac_rosenbrock(x):
...     return np.array([
...         [-20 * x[0], 10],
...         [-1, 0]])

Putting this all together, we see that the new solution lies on the bound:

&gt;&gt;&gt; res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,
...                       bounds=([-np.inf, 1.5], np.inf))
&gt;&gt;&gt; res_2.x
array([ 1.22437075,  1.5       ])
&gt;&gt;&gt; res_2.cost
0.025213093946805685
&gt;&gt;&gt; res_2.optimality
1.5885401433157753e-07

Now we solve a system of equations (i.e., the cost function should be zero
at a minimum) for a Broyden tridiagonal vector-valued function of 100000
variables:

&gt;&gt;&gt; def fun_broyden(x):
...     f = (3 - x) * x + 1
...     f[1:] -= x[:-1]
...     f[:-1] -= 2 * x[1:]
...     return f

The corresponding Jacobian matrix is sparse. We tell the algorithm to
estimate it by finite differences and provide the sparsity structure of
Jacobian to significantly speed up this process.

&gt;&gt;&gt; from scipy.sparse import lil_matrix
&gt;&gt;&gt; def sparsity_broyden(n):
...     sparsity = lil_matrix((n, n), dtype=int)
...     i = np.arange(n)
...     sparsity[i, i] = 1
...     i = np.arange(1, n)
...     sparsity[i, i - 1] = 1
...     i = np.arange(n - 1)
...     sparsity[i, i + 1] = 1
...     return sparsity
...
&gt;&gt;&gt; n = 100000
&gt;&gt;&gt; x0_broyden = -np.ones(n)
...
&gt;&gt;&gt; res_3 = least_squares(fun_broyden, x0_broyden,
...                       jac_sparsity=sparsity_broyden(n))
&gt;&gt;&gt; res_3.cost
4.5687069299604613e-23
&gt;&gt;&gt; res_3.optimality
1.1650454296851518e-11

Let's also solve a curve fitting problem using robust loss function to
take care of outliers in the data. Define the model function as
``y = a + b * exp(c * t)``, where t is a predictor variable, y is an
observation and a, b, c are parameters to estimate.

First, define the function which generates the data with noise and
outliers, define the model parameters, and generate data:

&gt;&gt;&gt; from numpy.random import default_rng
&gt;&gt;&gt; rng = default_rng()
&gt;&gt;&gt; def gen_data(t, a, b, c, noise=0., n_outliers=0, seed=None):
...     rng = default_rng(seed)
...
...     y = a + b * np.exp(t * c)
...
...     error = noise * rng.standard_normal(t.size)
...     outliers = rng.integers(0, t.size, n_outliers)
...     error[outliers] *= 10
...
...     return y + error
...
&gt;&gt;&gt; a = 0.5
&gt;&gt;&gt; b = 2.0
&gt;&gt;&gt; c = -1
&gt;&gt;&gt; t_min = 0
&gt;&gt;&gt; t_max = 10
&gt;&gt;&gt; n_points = 15
...
&gt;&gt;&gt; t_train = np.linspace(t_min, t_max, n_points)
&gt;&gt;&gt; y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)

Define function for computing residuals and initial estimate of
parameters.

&gt;&gt;&gt; def fun(x, t, y):
...     return x[0] + x[1] * np.exp(x[2] * t) - y
...
&gt;&gt;&gt; x0 = np.array([1.0, 1.0, 0.0])

Compute a standard least-squares solution:

&gt;&gt;&gt; res_lsq = least_squares(fun, x0, args=(t_train, y_train))

Now compute two solutions with two different robust loss functions. The
parameter `f_scale` is set to 0.1, meaning that inlier residuals should
not significantly exceed 0.1 (the noise level used).

&gt;&gt;&gt; res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,
...                             args=(t_train, y_train))
&gt;&gt;&gt; res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,
...                         args=(t_train, y_train))

And, finally, plot all the curves. We see that by selecting an appropriate
`loss`  we can get estimates close to optimal even in the presence of
strong outliers. But keep in mind that generally it is recommended to try
'soft_l1' or 'huber' losses first (if at all necessary) as the other two
options may cause difficulties in optimization process.

&gt;&gt;&gt; t_test = np.linspace(t_min, t_max, n_points * 10)
&gt;&gt;&gt; y_true = gen_data(t_test, a, b, c)
&gt;&gt;&gt; y_lsq = gen_data(t_test, *res_lsq.x)
&gt;&gt;&gt; y_soft_l1 = gen_data(t_test, *res_soft_l1.x)
&gt;&gt;&gt; y_log = gen_data(t_test, *res_log.x)
...
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; plt.plot(t_train, y_train, 'o')
&gt;&gt;&gt; plt.plot(t_test, y_true, 'k', linewidth=2, label='true')
&gt;&gt;&gt; plt.plot(t_test, y_lsq, label='linear loss')
&gt;&gt;&gt; plt.plot(t_test, y_soft_l1, label='soft_l1 loss')
&gt;&gt;&gt; plt.plot(t_test, y_log, label='cauchy loss')
&gt;&gt;&gt; plt.xlabel("t")
&gt;&gt;&gt; plt.ylabel("y")
&gt;&gt;&gt; plt.legend()
&gt;&gt;&gt; plt.show()

In the next example, we show how complex-valued residual functions of
complex variables can be optimized with ``least_squares()``. Consider the
following function:

&gt;&gt;&gt; def f(z):
...     return z - (0.5 + 0.5j)

We wrap it into a function of real variables that returns real residuals
by simply handling the real and imaginary parts as independent variables:

&gt;&gt;&gt; def f_wrap(x):
...     fx = f(x[0] + 1j*x[1])
...     return np.array([fx.real, fx.imag])

Thus, instead of the original m-D complex function of n complex
variables we optimize a 2m-D real function of 2n real variables:

&gt;&gt;&gt; from scipy.optimize import least_squares
&gt;&gt;&gt; res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))
&gt;&gt;&gt; z = res_wrapped.x[0] + res_wrapped.x[1]*1j
&gt;&gt;&gt; z
(0.49999999999925893+0.49999999999925893j)</pre> <div class="fragment"><div class="line"><span class="lineno">  244</span>        jac_sparsity=<span class="keywordtype">None</span>, max_nfev=<span class="keywordtype">None</span>, verbose=0, args=(), kwargs={}):</div>
<div class="line"><span class="lineno">  245</span>    <span class="stringliteral">&quot;&quot;&quot;Solve a nonlinear least-squares problem with bounds on the variables.</span></div>
<div class="line"><span class="lineno">  246</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral">    Given the residuals f(x) (an m-D real function of n real</span></div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral">    variables) and the loss function rho(s) (a scalar function), `least_squares`</span></div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">    finds a local minimum of the cost function F(x)::</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral">        minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)</span></div>
<div class="line"><span class="lineno">  252</span><span class="stringliteral">        subject to lb &lt;= x &lt;= ub</span></div>
<div class="line"><span class="lineno">  253</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  254</span><span class="stringliteral">    The purpose of the loss function rho(s) is to reduce the influence of</span></div>
<div class="line"><span class="lineno">  255</span><span class="stringliteral">    outliers on the solution.</span></div>
<div class="line"><span class="lineno">  256</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  257</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  258</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  259</span><span class="stringliteral">    fun : callable</span></div>
<div class="line"><span class="lineno">  260</span><span class="stringliteral">        Function which computes the vector of residuals, with the signature</span></div>
<div class="line"><span class="lineno">  261</span><span class="stringliteral">        ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with</span></div>
<div class="line"><span class="lineno">  262</span><span class="stringliteral">        respect to its first argument. The argument ``x`` passed to this</span></div>
<div class="line"><span class="lineno">  263</span><span class="stringliteral">        function is an ndarray of shape (n,) (never a scalar, even for n=1).</span></div>
<div class="line"><span class="lineno">  264</span><span class="stringliteral">        It must allocate and return a 1-D array_like of shape (m,) or a scalar.</span></div>
<div class="line"><span class="lineno">  265</span><span class="stringliteral">        If the argument ``x`` is complex or the function ``fun`` returns</span></div>
<div class="line"><span class="lineno">  266</span><span class="stringliteral">        complex residuals, it must be wrapped in a real function of real</span></div>
<div class="line"><span class="lineno">  267</span><span class="stringliteral">        arguments, as shown at the end of the Examples section.</span></div>
<div class="line"><span class="lineno">  268</span><span class="stringliteral">    x0 : array_like with shape (n,) or float</span></div>
<div class="line"><span class="lineno">  269</span><span class="stringliteral">        Initial guess on independent variables. If float, it will be treated</span></div>
<div class="line"><span class="lineno">  270</span><span class="stringliteral">        as a 1-D array with one element.</span></div>
<div class="line"><span class="lineno">  271</span><span class="stringliteral">    jac : {&#39;2-point&#39;, &#39;3-point&#39;, &#39;cs&#39;, callable}, optional</span></div>
<div class="line"><span class="lineno">  272</span><span class="stringliteral">        Method of computing the Jacobian matrix (an m-by-n matrix, where</span></div>
<div class="line"><span class="lineno">  273</span><span class="stringliteral">        element (i, j) is the partial derivative of f[i] with respect to</span></div>
<div class="line"><span class="lineno">  274</span><span class="stringliteral">        x[j]). The keywords select a finite difference scheme for numerical</span></div>
<div class="line"><span class="lineno">  275</span><span class="stringliteral">        estimation. The scheme &#39;3-point&#39; is more accurate, but requires</span></div>
<div class="line"><span class="lineno">  276</span><span class="stringliteral">        twice as many operations as &#39;2-point&#39; (default). The scheme &#39;cs&#39;</span></div>
<div class="line"><span class="lineno">  277</span><span class="stringliteral">        uses complex steps, and while potentially the most accurate, it is</span></div>
<div class="line"><span class="lineno">  278</span><span class="stringliteral">        applicable only when `fun` correctly handles complex inputs and</span></div>
<div class="line"><span class="lineno">  279</span><span class="stringliteral">        can be analytically continued to the complex plane. Method &#39;lm&#39;</span></div>
<div class="line"><span class="lineno">  280</span><span class="stringliteral">        always uses the &#39;2-point&#39; scheme. If callable, it is used as</span></div>
<div class="line"><span class="lineno">  281</span><span class="stringliteral">        ``jac(x, *args, **kwargs)`` and should return a good approximation</span></div>
<div class="line"><span class="lineno">  282</span><span class="stringliteral">        (or the exact value) for the Jacobian as an array_like (np.atleast_2d</span></div>
<div class="line"><span class="lineno">  283</span><span class="stringliteral">        is applied), a sparse matrix (csr_matrix preferred for performance) or</span></div>
<div class="line"><span class="lineno">  284</span><span class="stringliteral">        a `scipy.sparse.linalg.LinearOperator`.</span></div>
<div class="line"><span class="lineno">  285</span><span class="stringliteral">    bounds : 2-tuple of array_like, optional</span></div>
<div class="line"><span class="lineno">  286</span><span class="stringliteral">        Lower and upper bounds on independent variables. Defaults to no bounds.</span></div>
<div class="line"><span class="lineno">  287</span><span class="stringliteral">        Each array must match the size of `x0` or be a scalar, in the latter</span></div>
<div class="line"><span class="lineno">  288</span><span class="stringliteral">        case a bound will be the same for all variables. Use ``np.inf`` with</span></div>
<div class="line"><span class="lineno">  289</span><span class="stringliteral">        an appropriate sign to disable bounds on all or some variables.</span></div>
<div class="line"><span class="lineno">  290</span><span class="stringliteral">    method : {&#39;trf&#39;, &#39;dogbox&#39;, &#39;lm&#39;}, optional</span></div>
<div class="line"><span class="lineno">  291</span><span class="stringliteral">        Algorithm to perform minimization.</span></div>
<div class="line"><span class="lineno">  292</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  293</span><span class="stringliteral">            * &#39;trf&#39; : Trust Region Reflective algorithm, particularly suitable</span></div>
<div class="line"><span class="lineno">  294</span><span class="stringliteral">              for large sparse problems with bounds. Generally robust method.</span></div>
<div class="line"><span class="lineno">  295</span><span class="stringliteral">            * &#39;dogbox&#39; : dogleg algorithm with rectangular trust regions,</span></div>
<div class="line"><span class="lineno">  296</span><span class="stringliteral">              typical use case is small problems with bounds. Not recommended</span></div>
<div class="line"><span class="lineno">  297</span><span class="stringliteral">              for problems with rank-deficient Jacobian.</span></div>
<div class="line"><span class="lineno">  298</span><span class="stringliteral">            * &#39;lm&#39; : Levenberg-Marquardt algorithm as implemented in MINPACK.</span></div>
<div class="line"><span class="lineno">  299</span><span class="stringliteral">              Doesn&#39;t handle bounds and sparse Jacobians. Usually the most</span></div>
<div class="line"><span class="lineno">  300</span><span class="stringliteral">              efficient method for small unconstrained problems.</span></div>
<div class="line"><span class="lineno">  301</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  302</span><span class="stringliteral">        Default is &#39;trf&#39;. See Notes for more information.</span></div>
<div class="line"><span class="lineno">  303</span><span class="stringliteral">    ftol : float or None, optional</span></div>
<div class="line"><span class="lineno">  304</span><span class="stringliteral">        Tolerance for termination by the change of the cost function. Default</span></div>
<div class="line"><span class="lineno">  305</span><span class="stringliteral">        is 1e-8. The optimization process is stopped when ``dF &lt; ftol * F``,</span></div>
<div class="line"><span class="lineno">  306</span><span class="stringliteral">        and there was an adequate agreement between a local quadratic model and</span></div>
<div class="line"><span class="lineno">  307</span><span class="stringliteral">        the true model in the last step.</span></div>
<div class="line"><span class="lineno">  308</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  309</span><span class="stringliteral">        If None and &#39;method&#39; is not &#39;lm&#39;, the termination by this condition is</span></div>
<div class="line"><span class="lineno">  310</span><span class="stringliteral">        disabled. If &#39;method&#39; is &#39;lm&#39;, this tolerance must be higher than</span></div>
<div class="line"><span class="lineno">  311</span><span class="stringliteral">        machine epsilon.</span></div>
<div class="line"><span class="lineno">  312</span><span class="stringliteral">    xtol : float or None, optional</span></div>
<div class="line"><span class="lineno">  313</span><span class="stringliteral">        Tolerance for termination by the change of the independent variables.</span></div>
<div class="line"><span class="lineno">  314</span><span class="stringliteral">        Default is 1e-8. The exact condition depends on the `method` used:</span></div>
<div class="line"><span class="lineno">  315</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  316</span><span class="stringliteral">            * For &#39;trf&#39; and &#39;dogbox&#39; : ``norm(dx) &lt; xtol * (xtol + norm(x))``.</span></div>
<div class="line"><span class="lineno">  317</span><span class="stringliteral">            * For &#39;lm&#39; : ``Delta &lt; xtol * norm(xs)``, where ``Delta`` is</span></div>
<div class="line"><span class="lineno">  318</span><span class="stringliteral">              a trust-region radius and ``xs`` is the value of ``x``</span></div>
<div class="line"><span class="lineno">  319</span><span class="stringliteral">              scaled according to `x_scale` parameter (see below).</span></div>
<div class="line"><span class="lineno">  320</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  321</span><span class="stringliteral">        If None and &#39;method&#39; is not &#39;lm&#39;, the termination by this condition is</span></div>
<div class="line"><span class="lineno">  322</span><span class="stringliteral">        disabled. If &#39;method&#39; is &#39;lm&#39;, this tolerance must be higher than</span></div>
<div class="line"><span class="lineno">  323</span><span class="stringliteral">        machine epsilon.</span></div>
<div class="line"><span class="lineno">  324</span><span class="stringliteral">    gtol : float or None, optional</span></div>
<div class="line"><span class="lineno">  325</span><span class="stringliteral">        Tolerance for termination by the norm of the gradient. Default is 1e-8.</span></div>
<div class="line"><span class="lineno">  326</span><span class="stringliteral">        The exact condition depends on a `method` used:</span></div>
<div class="line"><span class="lineno">  327</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  328</span><span class="stringliteral">            * For &#39;trf&#39; : ``norm(g_scaled, ord=np.inf) &lt; gtol``, where</span></div>
<div class="line"><span class="lineno">  329</span><span class="stringliteral">              ``g_scaled`` is the value of the gradient scaled to account for</span></div>
<div class="line"><span class="lineno">  330</span><span class="stringliteral">              the presence of the bounds [STIR]_.</span></div>
<div class="line"><span class="lineno">  331</span><span class="stringliteral">            * For &#39;dogbox&#39; : ``norm(g_free, ord=np.inf) &lt; gtol``, where</span></div>
<div class="line"><span class="lineno">  332</span><span class="stringliteral">              ``g_free`` is the gradient with respect to the variables which</span></div>
<div class="line"><span class="lineno">  333</span><span class="stringliteral">              are not in the optimal state on the boundary.</span></div>
<div class="line"><span class="lineno">  334</span><span class="stringliteral">            * For &#39;lm&#39; : the maximum absolute value of the cosine of angles</span></div>
<div class="line"><span class="lineno">  335</span><span class="stringliteral">              between columns of the Jacobian and the residual vector is less</span></div>
<div class="line"><span class="lineno">  336</span><span class="stringliteral">              than `gtol`, or the residual vector is zero.</span></div>
<div class="line"><span class="lineno">  337</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  338</span><span class="stringliteral">        If None and &#39;method&#39; is not &#39;lm&#39;, the termination by this condition is</span></div>
<div class="line"><span class="lineno">  339</span><span class="stringliteral">        disabled. If &#39;method&#39; is &#39;lm&#39;, this tolerance must be higher than</span></div>
<div class="line"><span class="lineno">  340</span><span class="stringliteral">        machine epsilon.</span></div>
<div class="line"><span class="lineno">  341</span><span class="stringliteral">    x_scale : array_like or &#39;jac&#39;, optional</span></div>
<div class="line"><span class="lineno">  342</span><span class="stringliteral">        Characteristic scale of each variable. Setting `x_scale` is equivalent</span></div>
<div class="line"><span class="lineno">  343</span><span class="stringliteral">        to reformulating the problem in scaled variables ``xs = x / x_scale``.</span></div>
<div class="line"><span class="lineno">  344</span><span class="stringliteral">        An alternative view is that the size of a trust region along jth</span></div>
<div class="line"><span class="lineno">  345</span><span class="stringliteral">        dimension is proportional to ``x_scale[j]``. Improved convergence may</span></div>
<div class="line"><span class="lineno">  346</span><span class="stringliteral">        be achieved by setting `x_scale` such that a step of a given size</span></div>
<div class="line"><span class="lineno">  347</span><span class="stringliteral">        along any of the scaled variables has a similar effect on the cost</span></div>
<div class="line"><span class="lineno">  348</span><span class="stringliteral">        function. If set to &#39;jac&#39;, the scale is iteratively updated using the</span></div>
<div class="line"><span class="lineno">  349</span><span class="stringliteral">        inverse norms of the columns of the Jacobian matrix (as described in</span></div>
<div class="line"><span class="lineno">  350</span><span class="stringliteral">        [JJMore]_).</span></div>
<div class="line"><span class="lineno">  351</span><span class="stringliteral">    loss : str or callable, optional</span></div>
<div class="line"><span class="lineno">  352</span><span class="stringliteral">        Determines the loss function. The following keyword values are allowed:</span></div>
<div class="line"><span class="lineno">  353</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  354</span><span class="stringliteral">            * &#39;linear&#39; (default) : ``rho(z) = z``. Gives a standard</span></div>
<div class="line"><span class="lineno">  355</span><span class="stringliteral">              least-squares problem.</span></div>
<div class="line"><span class="lineno">  356</span><span class="stringliteral">            * &#39;soft_l1&#39; : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth</span></div>
<div class="line"><span class="lineno">  357</span><span class="stringliteral">              approximation of l1 (absolute value) loss. Usually a good</span></div>
<div class="line"><span class="lineno">  358</span><span class="stringliteral">              choice for robust least squares.</span></div>
<div class="line"><span class="lineno">  359</span><span class="stringliteral">            * &#39;huber&#39; : ``rho(z) = z if z &lt;= 1 else 2*z**0.5 - 1``. Works</span></div>
<div class="line"><span class="lineno">  360</span><span class="stringliteral">              similarly to &#39;soft_l1&#39;.</span></div>
<div class="line"><span class="lineno">  361</span><span class="stringliteral">            * &#39;cauchy&#39; : ``rho(z) = ln(1 + z)``. Severely weakens outliers</span></div>
<div class="line"><span class="lineno">  362</span><span class="stringliteral">              influence, but may cause difficulties in optimization process.</span></div>
<div class="line"><span class="lineno">  363</span><span class="stringliteral">            * &#39;arctan&#39; : ``rho(z) = arctan(z)``. Limits a maximum loss on</span></div>
<div class="line"><span class="lineno">  364</span><span class="stringliteral">              a single residual, has properties similar to &#39;cauchy&#39;.</span></div>
<div class="line"><span class="lineno">  365</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  366</span><span class="stringliteral">        If callable, it must take a 1-D ndarray ``z=f**2`` and return an</span></div>
<div class="line"><span class="lineno">  367</span><span class="stringliteral">        array_like with shape (3, m) where row 0 contains function values,</span></div>
<div class="line"><span class="lineno">  368</span><span class="stringliteral">        row 1 contains first derivatives and row 2 contains second</span></div>
<div class="line"><span class="lineno">  369</span><span class="stringliteral">        derivatives. Method &#39;lm&#39; supports only &#39;linear&#39; loss.</span></div>
<div class="line"><span class="lineno">  370</span><span class="stringliteral">    f_scale : float, optional</span></div>
<div class="line"><span class="lineno">  371</span><span class="stringliteral">        Value of soft margin between inlier and outlier residuals, default</span></div>
<div class="line"><span class="lineno">  372</span><span class="stringliteral">        is 1.0. The loss function is evaluated as follows</span></div>
<div class="line"><span class="lineno">  373</span><span class="stringliteral">        ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,</span></div>
<div class="line"><span class="lineno">  374</span><span class="stringliteral">        and ``rho`` is determined by `loss` parameter. This parameter has</span></div>
<div class="line"><span class="lineno">  375</span><span class="stringliteral">        no effect with ``loss=&#39;linear&#39;``, but for other `loss` values it is</span></div>
<div class="line"><span class="lineno">  376</span><span class="stringliteral">        of crucial importance.</span></div>
<div class="line"><span class="lineno">  377</span><span class="stringliteral">    max_nfev : None or int, optional</span></div>
<div class="line"><span class="lineno">  378</span><span class="stringliteral">        Maximum number of function evaluations before the termination.</span></div>
<div class="line"><span class="lineno">  379</span><span class="stringliteral">        If None (default), the value is chosen automatically:</span></div>
<div class="line"><span class="lineno">  380</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  381</span><span class="stringliteral">            * For &#39;trf&#39; and &#39;dogbox&#39; : 100 * n.</span></div>
<div class="line"><span class="lineno">  382</span><span class="stringliteral">            * For &#39;lm&#39; :  100 * n if `jac` is callable and 100 * n * (n + 1)</span></div>
<div class="line"><span class="lineno">  383</span><span class="stringliteral">              otherwise (because &#39;lm&#39; counts function calls in Jacobian</span></div>
<div class="line"><span class="lineno">  384</span><span class="stringliteral">              estimation).</span></div>
<div class="line"><span class="lineno">  385</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  386</span><span class="stringliteral">    diff_step : None or array_like, optional</span></div>
<div class="line"><span class="lineno">  387</span><span class="stringliteral">        Determines the relative step size for the finite difference</span></div>
<div class="line"><span class="lineno">  388</span><span class="stringliteral">        approximation of the Jacobian. The actual step is computed as</span></div>
<div class="line"><span class="lineno">  389</span><span class="stringliteral">        ``x * diff_step``. If None (default), then `diff_step` is taken to be</span></div>
<div class="line"><span class="lineno">  390</span><span class="stringliteral">        a conventional &quot;optimal&quot; power of machine epsilon for the finite</span></div>
<div class="line"><span class="lineno">  391</span><span class="stringliteral">        difference scheme used [NR]_.</span></div>
<div class="line"><span class="lineno">  392</span><span class="stringliteral">    tr_solver : {None, &#39;exact&#39;, &#39;lsmr&#39;}, optional</span></div>
<div class="line"><span class="lineno">  393</span><span class="stringliteral">        Method for solving trust-region subproblems, relevant only for &#39;trf&#39;</span></div>
<div class="line"><span class="lineno">  394</span><span class="stringliteral">        and &#39;dogbox&#39; methods.</span></div>
<div class="line"><span class="lineno">  395</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  396</span><span class="stringliteral">            * &#39;exact&#39; is suitable for not very large problems with dense</span></div>
<div class="line"><span class="lineno">  397</span><span class="stringliteral">              Jacobian matrices. The computational complexity per iteration is</span></div>
<div class="line"><span class="lineno">  398</span><span class="stringliteral">              comparable to a singular value decomposition of the Jacobian</span></div>
<div class="line"><span class="lineno">  399</span><span class="stringliteral">              matrix.</span></div>
<div class="line"><span class="lineno">  400</span><span class="stringliteral">            * &#39;lsmr&#39; is suitable for problems with sparse and large Jacobian</span></div>
<div class="line"><span class="lineno">  401</span><span class="stringliteral">              matrices. It uses the iterative procedure</span></div>
<div class="line"><span class="lineno">  402</span><span class="stringliteral">              `scipy.sparse.linalg.lsmr` for finding a solution of a linear</span></div>
<div class="line"><span class="lineno">  403</span><span class="stringliteral">              least-squares problem and only requires matrix-vector product</span></div>
<div class="line"><span class="lineno">  404</span><span class="stringliteral">              evaluations.</span></div>
<div class="line"><span class="lineno">  405</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  406</span><span class="stringliteral">        If None (default), the solver is chosen based on the type of Jacobian</span></div>
<div class="line"><span class="lineno">  407</span><span class="stringliteral">        returned on the first iteration.</span></div>
<div class="line"><span class="lineno">  408</span><span class="stringliteral">    tr_options : dict, optional</span></div>
<div class="line"><span class="lineno">  409</span><span class="stringliteral">        Keyword options passed to trust-region solver.</span></div>
<div class="line"><span class="lineno">  410</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  411</span><span class="stringliteral">            * ``tr_solver=&#39;exact&#39;``: `tr_options` are ignored.</span></div>
<div class="line"><span class="lineno">  412</span><span class="stringliteral">            * ``tr_solver=&#39;lsmr&#39;``: options for `scipy.sparse.linalg.lsmr`.</span></div>
<div class="line"><span class="lineno">  413</span><span class="stringliteral">              Additionally,  ``method=&#39;trf&#39;`` supports  &#39;regularize&#39; option</span></div>
<div class="line"><span class="lineno">  414</span><span class="stringliteral">              (bool, default is True), which adds a regularization term to the</span></div>
<div class="line"><span class="lineno">  415</span><span class="stringliteral">              normal equation, which improves convergence if the Jacobian is</span></div>
<div class="line"><span class="lineno">  416</span><span class="stringliteral">              rank-deficient [Byrd]_ (eq. 3.4).</span></div>
<div class="line"><span class="lineno">  417</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  418</span><span class="stringliteral">    jac_sparsity : {None, array_like, sparse matrix}, optional</span></div>
<div class="line"><span class="lineno">  419</span><span class="stringliteral">        Defines the sparsity structure of the Jacobian matrix for finite</span></div>
<div class="line"><span class="lineno">  420</span><span class="stringliteral">        difference estimation, its shape must be (m, n). If the Jacobian has</span></div>
<div class="line"><span class="lineno">  421</span><span class="stringliteral">        only few non-zero elements in *each* row, providing the sparsity</span></div>
<div class="line"><span class="lineno">  422</span><span class="stringliteral">        structure will greatly speed up the computations [Curtis]_. A zero</span></div>
<div class="line"><span class="lineno">  423</span><span class="stringliteral">        entry means that a corresponding element in the Jacobian is identically</span></div>
<div class="line"><span class="lineno">  424</span><span class="stringliteral">        zero. If provided, forces the use of &#39;lsmr&#39; trust-region solver.</span></div>
<div class="line"><span class="lineno">  425</span><span class="stringliteral">        If None (default), then dense differencing will be used. Has no effect</span></div>
<div class="line"><span class="lineno">  426</span><span class="stringliteral">        for &#39;lm&#39; method.</span></div>
<div class="line"><span class="lineno">  427</span><span class="stringliteral">    verbose : {0, 1, 2}, optional</span></div>
<div class="line"><span class="lineno">  428</span><span class="stringliteral">        Level of algorithm&#39;s verbosity:</span></div>
<div class="line"><span class="lineno">  429</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  430</span><span class="stringliteral">            * 0 (default) : work silently.</span></div>
<div class="line"><span class="lineno">  431</span><span class="stringliteral">            * 1 : display a termination report.</span></div>
<div class="line"><span class="lineno">  432</span><span class="stringliteral">            * 2 : display progress during iterations (not supported by &#39;lm&#39;</span></div>
<div class="line"><span class="lineno">  433</span><span class="stringliteral">              method).</span></div>
<div class="line"><span class="lineno">  434</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  435</span><span class="stringliteral">    args, kwargs : tuple and dict, optional</span></div>
<div class="line"><span class="lineno">  436</span><span class="stringliteral">        Additional arguments passed to `fun` and `jac`. Both empty by default.</span></div>
<div class="line"><span class="lineno">  437</span><span class="stringliteral">        The calling signature is ``fun(x, *args, **kwargs)`` and the same for</span></div>
<div class="line"><span class="lineno">  438</span><span class="stringliteral">        `jac`.</span></div>
<div class="line"><span class="lineno">  439</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  440</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  441</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  442</span><span class="stringliteral">    result : OptimizeResult</span></div>
<div class="line"><span class="lineno">  443</span><span class="stringliteral">        `OptimizeResult` with the following fields defined:</span></div>
<div class="line"><span class="lineno">  444</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  445</span><span class="stringliteral">            x : ndarray, shape (n,)</span></div>
<div class="line"><span class="lineno">  446</span><span class="stringliteral">                Solution found.</span></div>
<div class="line"><span class="lineno">  447</span><span class="stringliteral">            cost : float</span></div>
<div class="line"><span class="lineno">  448</span><span class="stringliteral">                Value of the cost function at the solution.</span></div>
<div class="line"><span class="lineno">  449</span><span class="stringliteral">            fun : ndarray, shape (m,)</span></div>
<div class="line"><span class="lineno">  450</span><span class="stringliteral">                Vector of residuals at the solution.</span></div>
<div class="line"><span class="lineno">  451</span><span class="stringliteral">            jac : ndarray, sparse matrix or LinearOperator, shape (m, n)</span></div>
<div class="line"><span class="lineno">  452</span><span class="stringliteral">                Modified Jacobian matrix at the solution, in the sense that J^T J</span></div>
<div class="line"><span class="lineno">  453</span><span class="stringliteral">                is a Gauss-Newton approximation of the Hessian of the cost function.</span></div>
<div class="line"><span class="lineno">  454</span><span class="stringliteral">                The type is the same as the one used by the algorithm.</span></div>
<div class="line"><span class="lineno">  455</span><span class="stringliteral">            grad : ndarray, shape (m,)</span></div>
<div class="line"><span class="lineno">  456</span><span class="stringliteral">                Gradient of the cost function at the solution.</span></div>
<div class="line"><span class="lineno">  457</span><span class="stringliteral">            optimality : float</span></div>
<div class="line"><span class="lineno">  458</span><span class="stringliteral">                First-order optimality measure. In unconstrained problems, it is</span></div>
<div class="line"><span class="lineno">  459</span><span class="stringliteral">                always the uniform norm of the gradient. In constrained problems,</span></div>
<div class="line"><span class="lineno">  460</span><span class="stringliteral">                it is the quantity which was compared with `gtol` during iterations.</span></div>
<div class="line"><span class="lineno">  461</span><span class="stringliteral">            active_mask : ndarray of int, shape (n,)</span></div>
<div class="line"><span class="lineno">  462</span><span class="stringliteral">                Each component shows whether a corresponding constraint is active</span></div>
<div class="line"><span class="lineno">  463</span><span class="stringliteral">                (that is, whether a variable is at the bound):</span></div>
<div class="line"><span class="lineno">  464</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  465</span><span class="stringliteral">                    *  0 : a constraint is not active.</span></div>
<div class="line"><span class="lineno">  466</span><span class="stringliteral">                    * -1 : a lower bound is active.</span></div>
<div class="line"><span class="lineno">  467</span><span class="stringliteral">                    *  1 : an upper bound is active.</span></div>
<div class="line"><span class="lineno">  468</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  469</span><span class="stringliteral">                Might be somewhat arbitrary for &#39;trf&#39; method as it generates a</span></div>
<div class="line"><span class="lineno">  470</span><span class="stringliteral">                sequence of strictly feasible iterates and `active_mask` is</span></div>
<div class="line"><span class="lineno">  471</span><span class="stringliteral">                determined within a tolerance threshold.</span></div>
<div class="line"><span class="lineno">  472</span><span class="stringliteral">            nfev : int</span></div>
<div class="line"><span class="lineno">  473</span><span class="stringliteral">                Number of function evaluations done. Methods &#39;trf&#39; and &#39;dogbox&#39; do</span></div>
<div class="line"><span class="lineno">  474</span><span class="stringliteral">                not count function calls for numerical Jacobian approximation, as</span></div>
<div class="line"><span class="lineno">  475</span><span class="stringliteral">                opposed to &#39;lm&#39; method.</span></div>
<div class="line"><span class="lineno">  476</span><span class="stringliteral">            njev : int or None</span></div>
<div class="line"><span class="lineno">  477</span><span class="stringliteral">                Number of Jacobian evaluations done. If numerical Jacobian</span></div>
<div class="line"><span class="lineno">  478</span><span class="stringliteral">                approximation is used in &#39;lm&#39; method, it is set to None.</span></div>
<div class="line"><span class="lineno">  479</span><span class="stringliteral">            status : int</span></div>
<div class="line"><span class="lineno">  480</span><span class="stringliteral">                The reason for algorithm termination:</span></div>
<div class="line"><span class="lineno">  481</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  482</span><span class="stringliteral">                    * -1 : improper input parameters status returned from MINPACK.</span></div>
<div class="line"><span class="lineno">  483</span><span class="stringliteral">                    *  0 : the maximum number of function evaluations is exceeded.</span></div>
<div class="line"><span class="lineno">  484</span><span class="stringliteral">                    *  1 : `gtol` termination condition is satisfied.</span></div>
<div class="line"><span class="lineno">  485</span><span class="stringliteral">                    *  2 : `ftol` termination condition is satisfied.</span></div>
<div class="line"><span class="lineno">  486</span><span class="stringliteral">                    *  3 : `xtol` termination condition is satisfied.</span></div>
<div class="line"><span class="lineno">  487</span><span class="stringliteral">                    *  4 : Both `ftol` and `xtol` termination conditions are satisfied.</span></div>
<div class="line"><span class="lineno">  488</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  489</span><span class="stringliteral">            message : str</span></div>
<div class="line"><span class="lineno">  490</span><span class="stringliteral">                Verbal description of the termination reason.</span></div>
<div class="line"><span class="lineno">  491</span><span class="stringliteral">            success : bool</span></div>
<div class="line"><span class="lineno">  492</span><span class="stringliteral">                True if one of the convergence criteria is satisfied (`status` &gt; 0).</span></div>
<div class="line"><span class="lineno">  493</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  494</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  495</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  496</span><span class="stringliteral">    leastsq : A legacy wrapper for the MINPACK implementation of the</span></div>
<div class="line"><span class="lineno">  497</span><span class="stringliteral">              Levenberg-Marquadt algorithm.</span></div>
<div class="line"><span class="lineno">  498</span><span class="stringliteral">    curve_fit : Least-squares minimization applied to a curve-fitting problem.</span></div>
<div class="line"><span class="lineno">  499</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  500</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  501</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  502</span><span class="stringliteral">    Method &#39;lm&#39; (Levenberg-Marquardt) calls a wrapper over least-squares</span></div>
<div class="line"><span class="lineno">  503</span><span class="stringliteral">    algorithms implemented in MINPACK (lmder, lmdif). It runs the</span></div>
<div class="line"><span class="lineno">  504</span><span class="stringliteral">    Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.</span></div>
<div class="line"><span class="lineno">  505</span><span class="stringliteral">    The implementation is based on paper [JJMore]_, it is very robust and</span></div>
<div class="line"><span class="lineno">  506</span><span class="stringliteral">    efficient with a lot of smart tricks. It should be your first choice</span></div>
<div class="line"><span class="lineno">  507</span><span class="stringliteral">    for unconstrained problems. Note that it doesn&#39;t support bounds. Also,</span></div>
<div class="line"><span class="lineno">  508</span><span class="stringliteral">    it doesn&#39;t work when m &lt; n.</span></div>
<div class="line"><span class="lineno">  509</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  510</span><span class="stringliteral">    Method &#39;trf&#39; (Trust Region Reflective) is motivated by the process of</span></div>
<div class="line"><span class="lineno">  511</span><span class="stringliteral">    solving a system of equations, which constitute the first-order optimality</span></div>
<div class="line"><span class="lineno">  512</span><span class="stringliteral">    condition for a bound-constrained minimization problem as formulated in</span></div>
<div class="line"><span class="lineno">  513</span><span class="stringliteral">    [STIR]_. The algorithm iteratively solves trust-region subproblems</span></div>
<div class="line"><span class="lineno">  514</span><span class="stringliteral">    augmented by a special diagonal quadratic term and with trust-region shape</span></div>
<div class="line"><span class="lineno">  515</span><span class="stringliteral">    determined by the distance from the bounds and the direction of the</span></div>
<div class="line"><span class="lineno">  516</span><span class="stringliteral">    gradient. This enhancements help to avoid making steps directly into bounds</span></div>
<div class="line"><span class="lineno">  517</span><span class="stringliteral">    and efficiently explore the whole space of variables. To further improve</span></div>
<div class="line"><span class="lineno">  518</span><span class="stringliteral">    convergence, the algorithm considers search directions reflected from the</span></div>
<div class="line"><span class="lineno">  519</span><span class="stringliteral">    bounds. To obey theoretical requirements, the algorithm keeps iterates</span></div>
<div class="line"><span class="lineno">  520</span><span class="stringliteral">    strictly feasible. With dense Jacobians trust-region subproblems are</span></div>
<div class="line"><span class="lineno">  521</span><span class="stringliteral">    solved by an exact method very similar to the one described in [JJMore]_</span></div>
<div class="line"><span class="lineno">  522</span><span class="stringliteral">    (and implemented in MINPACK). The difference from the MINPACK</span></div>
<div class="line"><span class="lineno">  523</span><span class="stringliteral">    implementation is that a singular value decomposition of a Jacobian</span></div>
<div class="line"><span class="lineno">  524</span><span class="stringliteral">    matrix is done once per iteration, instead of a QR decomposition and series</span></div>
<div class="line"><span class="lineno">  525</span><span class="stringliteral">    of Givens rotation eliminations. For large sparse Jacobians a 2-D subspace</span></div>
<div class="line"><span class="lineno">  526</span><span class="stringliteral">    approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.</span></div>
<div class="line"><span class="lineno">  527</span><span class="stringliteral">    The subspace is spanned by a scaled gradient and an approximate</span></div>
<div class="line"><span class="lineno">  528</span><span class="stringliteral">    Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no</span></div>
<div class="line"><span class="lineno">  529</span><span class="stringliteral">    constraints are imposed the algorithm is very similar to MINPACK and has</span></div>
<div class="line"><span class="lineno">  530</span><span class="stringliteral">    generally comparable performance. The algorithm works quite robust in</span></div>
<div class="line"><span class="lineno">  531</span><span class="stringliteral">    unbounded and bounded problems, thus it is chosen as a default algorithm.</span></div>
<div class="line"><span class="lineno">  532</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  533</span><span class="stringliteral">    Method &#39;dogbox&#39; operates in a trust-region framework, but considers</span></div>
<div class="line"><span class="lineno">  534</span><span class="stringliteral">    rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.</span></div>
<div class="line"><span class="lineno">  535</span><span class="stringliteral">    The intersection of a current trust region and initial bounds is again</span></div>
<div class="line"><span class="lineno">  536</span><span class="stringliteral">    rectangular, so on each iteration a quadratic minimization problem subject</span></div>
<div class="line"><span class="lineno">  537</span><span class="stringliteral">    to bound constraints is solved approximately by Powell&#39;s dogleg method</span></div>
<div class="line"><span class="lineno">  538</span><span class="stringliteral">    [NumOpt]_. The required Gauss-Newton step can be computed exactly for</span></div>
<div class="line"><span class="lineno">  539</span><span class="stringliteral">    dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large</span></div>
<div class="line"><span class="lineno">  540</span><span class="stringliteral">    sparse Jacobians. The algorithm is likely to exhibit slow convergence when</span></div>
<div class="line"><span class="lineno">  541</span><span class="stringliteral">    the rank of Jacobian is less than the number of variables. The algorithm</span></div>
<div class="line"><span class="lineno">  542</span><span class="stringliteral">    often outperforms &#39;trf&#39; in bounded problems with a small number of</span></div>
<div class="line"><span class="lineno">  543</span><span class="stringliteral">    variables.</span></div>
<div class="line"><span class="lineno">  544</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  545</span><span class="stringliteral">    Robust loss functions are implemented as described in [BA]_. The idea</span></div>
<div class="line"><span class="lineno">  546</span><span class="stringliteral">    is to modify a residual vector and a Jacobian matrix on each iteration</span></div>
<div class="line"><span class="lineno">  547</span><span class="stringliteral">    such that computed gradient and Gauss-Newton Hessian approximation match</span></div>
<div class="line"><span class="lineno">  548</span><span class="stringliteral">    the true gradient and Hessian approximation of the cost function. Then</span></div>
<div class="line"><span class="lineno">  549</span><span class="stringliteral">    the algorithm proceeds in a normal way, i.e., robust loss functions are</span></div>
<div class="line"><span class="lineno">  550</span><span class="stringliteral">    implemented as a simple wrapper over standard least-squares algorithms.</span></div>
<div class="line"><span class="lineno">  551</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  552</span><span class="stringliteral">    .. versionadded:: 0.17.0</span></div>
<div class="line"><span class="lineno">  553</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  554</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  555</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  556</span><span class="stringliteral">    .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, &quot;A Subspace, Interior,</span></div>
<div class="line"><span class="lineno">  557</span><span class="stringliteral">              and Conjugate Gradient Method for Large-Scale Bound-Constrained</span></div>
<div class="line"><span class="lineno">  558</span><span class="stringliteral">              Minimization Problems,&quot; SIAM Journal on Scientific Computing,</span></div>
<div class="line"><span class="lineno">  559</span><span class="stringliteral">              Vol. 21, Number 1, pp 1-23, 1999.</span></div>
<div class="line"><span class="lineno">  560</span><span class="stringliteral">    .. [NR] William H. Press et. al., &quot;Numerical Recipes. The Art of Scientific</span></div>
<div class="line"><span class="lineno">  561</span><span class="stringliteral">            Computing. 3rd edition&quot;, Sec. 5.7.</span></div>
<div class="line"><span class="lineno">  562</span><span class="stringliteral">    .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, &quot;Approximate</span></div>
<div class="line"><span class="lineno">  563</span><span class="stringliteral">              solution of the trust region problem by minimization over</span></div>
<div class="line"><span class="lineno">  564</span><span class="stringliteral">              two-dimensional subspaces&quot;, Math. Programming, 40, pp. 247-263,</span></div>
<div class="line"><span class="lineno">  565</span><span class="stringliteral">              1988.</span></div>
<div class="line"><span class="lineno">  566</span><span class="stringliteral">    .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, &quot;On the estimation of</span></div>
<div class="line"><span class="lineno">  567</span><span class="stringliteral">                sparse Jacobian matrices&quot;, Journal of the Institute of</span></div>
<div class="line"><span class="lineno">  568</span><span class="stringliteral">                Mathematics and its Applications, 13, pp. 117-120, 1974.</span></div>
<div class="line"><span class="lineno">  569</span><span class="stringliteral">    .. [JJMore] J. J. More, &quot;The Levenberg-Marquardt Algorithm: Implementation</span></div>
<div class="line"><span class="lineno">  570</span><span class="stringliteral">                and Theory,&quot; Numerical Analysis, ed. G. A. Watson, Lecture</span></div>
<div class="line"><span class="lineno">  571</span><span class="stringliteral">                Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.</span></div>
<div class="line"><span class="lineno">  572</span><span class="stringliteral">    .. [Voglis] C. Voglis and I. E. Lagaris, &quot;A Rectangular Trust Region</span></div>
<div class="line"><span class="lineno">  573</span><span class="stringliteral">                Dogleg Approach for Unconstrained and Bound Constrained</span></div>
<div class="line"><span class="lineno">  574</span><span class="stringliteral">                Nonlinear Optimization&quot;, WSEAS International Conference on</span></div>
<div class="line"><span class="lineno">  575</span><span class="stringliteral">                Applied Mathematics, Corfu, Greece, 2004.</span></div>
<div class="line"><span class="lineno">  576</span><span class="stringliteral">    .. [NumOpt] J. Nocedal and S. J. Wright, &quot;Numerical optimization,</span></div>
<div class="line"><span class="lineno">  577</span><span class="stringliteral">                2nd edition&quot;, Chapter 4.</span></div>
<div class="line"><span class="lineno">  578</span><span class="stringliteral">    .. [BA] B. Triggs et. al., &quot;Bundle Adjustment - A Modern Synthesis&quot;,</span></div>
<div class="line"><span class="lineno">  579</span><span class="stringliteral">            Proceedings of the International Workshop on Vision Algorithms:</span></div>
<div class="line"><span class="lineno">  580</span><span class="stringliteral">            Theory and Practice, pp. 298-372, 1999.</span></div>
<div class="line"><span class="lineno">  581</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  582</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno">  583</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  584</span><span class="stringliteral">    In this example we find a minimum of the Rosenbrock function without bounds</span></div>
<div class="line"><span class="lineno">  585</span><span class="stringliteral">    on independent variables.</span></div>
<div class="line"><span class="lineno">  586</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  587</span><span class="stringliteral">    &gt;&gt;&gt; def fun_rosenbrock(x):</span></div>
<div class="line"><span class="lineno">  588</span><span class="stringliteral">    ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])</span></div>
<div class="line"><span class="lineno">  589</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  590</span><span class="stringliteral">    Notice that we only provide the vector of the residuals. The algorithm</span></div>
<div class="line"><span class="lineno">  591</span><span class="stringliteral">    constructs the cost function as a sum of squares of the residuals, which</span></div>
<div class="line"><span class="lineno">  592</span><span class="stringliteral">    gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.</span></div>
<div class="line"><span class="lineno">  593</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  594</span><span class="stringliteral">    &gt;&gt;&gt; from scipy.optimize import least_squares</span></div>
<div class="line"><span class="lineno">  595</span><span class="stringliteral">    &gt;&gt;&gt; x0_rosenbrock = np.array([2, 2])</span></div>
<div class="line"><span class="lineno">  596</span><span class="stringliteral">    &gt;&gt;&gt; res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)</span></div>
<div class="line"><span class="lineno">  597</span><span class="stringliteral">    &gt;&gt;&gt; res_1.x</span></div>
<div class="line"><span class="lineno">  598</span><span class="stringliteral">    array([ 1.,  1.])</span></div>
<div class="line"><span class="lineno">  599</span><span class="stringliteral">    &gt;&gt;&gt; res_1.cost</span></div>
<div class="line"><span class="lineno">  600</span><span class="stringliteral">    9.8669242910846867e-30</span></div>
<div class="line"><span class="lineno">  601</span><span class="stringliteral">    &gt;&gt;&gt; res_1.optimality</span></div>
<div class="line"><span class="lineno">  602</span><span class="stringliteral">    8.8928864934219529e-14</span></div>
<div class="line"><span class="lineno">  603</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  604</span><span class="stringliteral">    We now constrain the variables, in such a way that the previous solution</span></div>
<div class="line"><span class="lineno">  605</span><span class="stringliteral">    becomes infeasible. Specifically, we require that ``x[1] &gt;= 1.5``, and</span></div>
<div class="line"><span class="lineno">  606</span><span class="stringliteral">    ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter</span></div>
<div class="line"><span class="lineno">  607</span><span class="stringliteral">    to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.</span></div>
<div class="line"><span class="lineno">  608</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  609</span><span class="stringliteral">    We also provide the analytic Jacobian:</span></div>
<div class="line"><span class="lineno">  610</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  611</span><span class="stringliteral">    &gt;&gt;&gt; def jac_rosenbrock(x):</span></div>
<div class="line"><span class="lineno">  612</span><span class="stringliteral">    ...     return np.array([</span></div>
<div class="line"><span class="lineno">  613</span><span class="stringliteral">    ...         [-20 * x[0], 10],</span></div>
<div class="line"><span class="lineno">  614</span><span class="stringliteral">    ...         [-1, 0]])</span></div>
<div class="line"><span class="lineno">  615</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  616</span><span class="stringliteral">    Putting this all together, we see that the new solution lies on the bound:</span></div>
<div class="line"><span class="lineno">  617</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  618</span><span class="stringliteral">    &gt;&gt;&gt; res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,</span></div>
<div class="line"><span class="lineno">  619</span><span class="stringliteral">    ...                       bounds=([-np.inf, 1.5], np.inf))</span></div>
<div class="line"><span class="lineno">  620</span><span class="stringliteral">    &gt;&gt;&gt; res_2.x</span></div>
<div class="line"><span class="lineno">  621</span><span class="stringliteral">    array([ 1.22437075,  1.5       ])</span></div>
<div class="line"><span class="lineno">  622</span><span class="stringliteral">    &gt;&gt;&gt; res_2.cost</span></div>
<div class="line"><span class="lineno">  623</span><span class="stringliteral">    0.025213093946805685</span></div>
<div class="line"><span class="lineno">  624</span><span class="stringliteral">    &gt;&gt;&gt; res_2.optimality</span></div>
<div class="line"><span class="lineno">  625</span><span class="stringliteral">    1.5885401433157753e-07</span></div>
<div class="line"><span class="lineno">  626</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  627</span><span class="stringliteral">    Now we solve a system of equations (i.e., the cost function should be zero</span></div>
<div class="line"><span class="lineno">  628</span><span class="stringliteral">    at a minimum) for a Broyden tridiagonal vector-valued function of 100000</span></div>
<div class="line"><span class="lineno">  629</span><span class="stringliteral">    variables:</span></div>
<div class="line"><span class="lineno">  630</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  631</span><span class="stringliteral">    &gt;&gt;&gt; def fun_broyden(x):</span></div>
<div class="line"><span class="lineno">  632</span><span class="stringliteral">    ...     f = (3 - x) * x + 1</span></div>
<div class="line"><span class="lineno">  633</span><span class="stringliteral">    ...     f[1:] -= x[:-1]</span></div>
<div class="line"><span class="lineno">  634</span><span class="stringliteral">    ...     f[:-1] -= 2 * x[1:]</span></div>
<div class="line"><span class="lineno">  635</span><span class="stringliteral">    ...     return f</span></div>
<div class="line"><span class="lineno">  636</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  637</span><span class="stringliteral">    The corresponding Jacobian matrix is sparse. We tell the algorithm to</span></div>
<div class="line"><span class="lineno">  638</span><span class="stringliteral">    estimate it by finite differences and provide the sparsity structure of</span></div>
<div class="line"><span class="lineno">  639</span><span class="stringliteral">    Jacobian to significantly speed up this process.</span></div>
<div class="line"><span class="lineno">  640</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  641</span><span class="stringliteral">    &gt;&gt;&gt; from scipy.sparse import lil_matrix</span></div>
<div class="line"><span class="lineno">  642</span><span class="stringliteral">    &gt;&gt;&gt; def sparsity_broyden(n):</span></div>
<div class="line"><span class="lineno">  643</span><span class="stringliteral">    ...     sparsity = lil_matrix((n, n), dtype=int)</span></div>
<div class="line"><span class="lineno">  644</span><span class="stringliteral">    ...     i = np.arange(n)</span></div>
<div class="line"><span class="lineno">  645</span><span class="stringliteral">    ...     sparsity[i, i] = 1</span></div>
<div class="line"><span class="lineno">  646</span><span class="stringliteral">    ...     i = np.arange(1, n)</span></div>
<div class="line"><span class="lineno">  647</span><span class="stringliteral">    ...     sparsity[i, i - 1] = 1</span></div>
<div class="line"><span class="lineno">  648</span><span class="stringliteral">    ...     i = np.arange(n - 1)</span></div>
<div class="line"><span class="lineno">  649</span><span class="stringliteral">    ...     sparsity[i, i + 1] = 1</span></div>
<div class="line"><span class="lineno">  650</span><span class="stringliteral">    ...     return sparsity</span></div>
<div class="line"><span class="lineno">  651</span><span class="stringliteral">    ...</span></div>
<div class="line"><span class="lineno">  652</span><span class="stringliteral">    &gt;&gt;&gt; n = 100000</span></div>
<div class="line"><span class="lineno">  653</span><span class="stringliteral">    &gt;&gt;&gt; x0_broyden = -np.ones(n)</span></div>
<div class="line"><span class="lineno">  654</span><span class="stringliteral">    ...</span></div>
<div class="line"><span class="lineno">  655</span><span class="stringliteral">    &gt;&gt;&gt; res_3 = least_squares(fun_broyden, x0_broyden,</span></div>
<div class="line"><span class="lineno">  656</span><span class="stringliteral">    ...                       jac_sparsity=sparsity_broyden(n))</span></div>
<div class="line"><span class="lineno">  657</span><span class="stringliteral">    &gt;&gt;&gt; res_3.cost</span></div>
<div class="line"><span class="lineno">  658</span><span class="stringliteral">    4.5687069299604613e-23</span></div>
<div class="line"><span class="lineno">  659</span><span class="stringliteral">    &gt;&gt;&gt; res_3.optimality</span></div>
<div class="line"><span class="lineno">  660</span><span class="stringliteral">    1.1650454296851518e-11</span></div>
<div class="line"><span class="lineno">  661</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  662</span><span class="stringliteral">    Let&#39;s also solve a curve fitting problem using robust loss function to</span></div>
<div class="line"><span class="lineno">  663</span><span class="stringliteral">    take care of outliers in the data. Define the model function as</span></div>
<div class="line"><span class="lineno">  664</span><span class="stringliteral">    ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an</span></div>
<div class="line"><span class="lineno">  665</span><span class="stringliteral">    observation and a, b, c are parameters to estimate.</span></div>
<div class="line"><span class="lineno">  666</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  667</span><span class="stringliteral">    First, define the function which generates the data with noise and</span></div>
<div class="line"><span class="lineno">  668</span><span class="stringliteral">    outliers, define the model parameters, and generate data:</span></div>
<div class="line"><span class="lineno">  669</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  670</span><span class="stringliteral">    &gt;&gt;&gt; from numpy.random import default_rng</span></div>
<div class="line"><span class="lineno">  671</span><span class="stringliteral">    &gt;&gt;&gt; rng = default_rng()</span></div>
<div class="line"><span class="lineno">  672</span><span class="stringliteral">    &gt;&gt;&gt; def gen_data(t, a, b, c, noise=0., n_outliers=0, seed=None):</span></div>
<div class="line"><span class="lineno">  673</span><span class="stringliteral">    ...     rng = default_rng(seed)</span></div>
<div class="line"><span class="lineno">  674</span><span class="stringliteral">    ...</span></div>
<div class="line"><span class="lineno">  675</span><span class="stringliteral">    ...     y = a + b * np.exp(t * c)</span></div>
<div class="line"><span class="lineno">  676</span><span class="stringliteral">    ...</span></div>
<div class="line"><span class="lineno">  677</span><span class="stringliteral">    ...     error = noise * rng.standard_normal(t.size)</span></div>
<div class="line"><span class="lineno">  678</span><span class="stringliteral">    ...     outliers = rng.integers(0, t.size, n_outliers)</span></div>
<div class="line"><span class="lineno">  679</span><span class="stringliteral">    ...     error[outliers] *= 10</span></div>
<div class="line"><span class="lineno">  680</span><span class="stringliteral">    ...</span></div>
<div class="line"><span class="lineno">  681</span><span class="stringliteral">    ...     return y + error</span></div>
<div class="line"><span class="lineno">  682</span><span class="stringliteral">    ...</span></div>
<div class="line"><span class="lineno">  683</span><span class="stringliteral">    &gt;&gt;&gt; a = 0.5</span></div>
<div class="line"><span class="lineno">  684</span><span class="stringliteral">    &gt;&gt;&gt; b = 2.0</span></div>
<div class="line"><span class="lineno">  685</span><span class="stringliteral">    &gt;&gt;&gt; c = -1</span></div>
<div class="line"><span class="lineno">  686</span><span class="stringliteral">    &gt;&gt;&gt; t_min = 0</span></div>
<div class="line"><span class="lineno">  687</span><span class="stringliteral">    &gt;&gt;&gt; t_max = 10</span></div>
<div class="line"><span class="lineno">  688</span><span class="stringliteral">    &gt;&gt;&gt; n_points = 15</span></div>
<div class="line"><span class="lineno">  689</span><span class="stringliteral">    ...</span></div>
<div class="line"><span class="lineno">  690</span><span class="stringliteral">    &gt;&gt;&gt; t_train = np.linspace(t_min, t_max, n_points)</span></div>
<div class="line"><span class="lineno">  691</span><span class="stringliteral">    &gt;&gt;&gt; y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)</span></div>
<div class="line"><span class="lineno">  692</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  693</span><span class="stringliteral">    Define function for computing residuals and initial estimate of</span></div>
<div class="line"><span class="lineno">  694</span><span class="stringliteral">    parameters.</span></div>
<div class="line"><span class="lineno">  695</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  696</span><span class="stringliteral">    &gt;&gt;&gt; def fun(x, t, y):</span></div>
<div class="line"><span class="lineno">  697</span><span class="stringliteral">    ...     return x[0] + x[1] * np.exp(x[2] * t) - y</span></div>
<div class="line"><span class="lineno">  698</span><span class="stringliteral">    ...</span></div>
<div class="line"><span class="lineno">  699</span><span class="stringliteral">    &gt;&gt;&gt; x0 = np.array([1.0, 1.0, 0.0])</span></div>
<div class="line"><span class="lineno">  700</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  701</span><span class="stringliteral">    Compute a standard least-squares solution:</span></div>
<div class="line"><span class="lineno">  702</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  703</span><span class="stringliteral">    &gt;&gt;&gt; res_lsq = least_squares(fun, x0, args=(t_train, y_train))</span></div>
<div class="line"><span class="lineno">  704</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  705</span><span class="stringliteral">    Now compute two solutions with two different robust loss functions. The</span></div>
<div class="line"><span class="lineno">  706</span><span class="stringliteral">    parameter `f_scale` is set to 0.1, meaning that inlier residuals should</span></div>
<div class="line"><span class="lineno">  707</span><span class="stringliteral">    not significantly exceed 0.1 (the noise level used).</span></div>
<div class="line"><span class="lineno">  708</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  709</span><span class="stringliteral">    &gt;&gt;&gt; res_soft_l1 = least_squares(fun, x0, loss=&#39;soft_l1&#39;, f_scale=0.1,</span></div>
<div class="line"><span class="lineno">  710</span><span class="stringliteral">    ...                             args=(t_train, y_train))</span></div>
<div class="line"><span class="lineno">  711</span><span class="stringliteral">    &gt;&gt;&gt; res_log = least_squares(fun, x0, loss=&#39;cauchy&#39;, f_scale=0.1,</span></div>
<div class="line"><span class="lineno">  712</span><span class="stringliteral">    ...                         args=(t_train, y_train))</span></div>
<div class="line"><span class="lineno">  713</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  714</span><span class="stringliteral">    And, finally, plot all the curves. We see that by selecting an appropriate</span></div>
<div class="line"><span class="lineno">  715</span><span class="stringliteral">    `loss`  we can get estimates close to optimal even in the presence of</span></div>
<div class="line"><span class="lineno">  716</span><span class="stringliteral">    strong outliers. But keep in mind that generally it is recommended to try</span></div>
<div class="line"><span class="lineno">  717</span><span class="stringliteral">    &#39;soft_l1&#39; or &#39;huber&#39; losses first (if at all necessary) as the other two</span></div>
<div class="line"><span class="lineno">  718</span><span class="stringliteral">    options may cause difficulties in optimization process.</span></div>
<div class="line"><span class="lineno">  719</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  720</span><span class="stringliteral">    &gt;&gt;&gt; t_test = np.linspace(t_min, t_max, n_points * 10)</span></div>
<div class="line"><span class="lineno">  721</span><span class="stringliteral">    &gt;&gt;&gt; y_true = gen_data(t_test, a, b, c)</span></div>
<div class="line"><span class="lineno">  722</span><span class="stringliteral">    &gt;&gt;&gt; y_lsq = gen_data(t_test, *res_lsq.x)</span></div>
<div class="line"><span class="lineno">  723</span><span class="stringliteral">    &gt;&gt;&gt; y_soft_l1 = gen_data(t_test, *res_soft_l1.x)</span></div>
<div class="line"><span class="lineno">  724</span><span class="stringliteral">    &gt;&gt;&gt; y_log = gen_data(t_test, *res_log.x)</span></div>
<div class="line"><span class="lineno">  725</span><span class="stringliteral">    ...</span></div>
<div class="line"><span class="lineno">  726</span><span class="stringliteral">    &gt;&gt;&gt; import matplotlib.pyplot as plt</span></div>
<div class="line"><span class="lineno">  727</span><span class="stringliteral">    &gt;&gt;&gt; plt.plot(t_train, y_train, &#39;o&#39;)</span></div>
<div class="line"><span class="lineno">  728</span><span class="stringliteral">    &gt;&gt;&gt; plt.plot(t_test, y_true, &#39;k&#39;, linewidth=2, label=&#39;true&#39;)</span></div>
<div class="line"><span class="lineno">  729</span><span class="stringliteral">    &gt;&gt;&gt; plt.plot(t_test, y_lsq, label=&#39;linear loss&#39;)</span></div>
<div class="line"><span class="lineno">  730</span><span class="stringliteral">    &gt;&gt;&gt; plt.plot(t_test, y_soft_l1, label=&#39;soft_l1 loss&#39;)</span></div>
<div class="line"><span class="lineno">  731</span><span class="stringliteral">    &gt;&gt;&gt; plt.plot(t_test, y_log, label=&#39;cauchy loss&#39;)</span></div>
<div class="line"><span class="lineno">  732</span><span class="stringliteral">    &gt;&gt;&gt; plt.xlabel(&quot;t&quot;)</span></div>
<div class="line"><span class="lineno">  733</span><span class="stringliteral">    &gt;&gt;&gt; plt.ylabel(&quot;y&quot;)</span></div>
<div class="line"><span class="lineno">  734</span><span class="stringliteral">    &gt;&gt;&gt; plt.legend()</span></div>
<div class="line"><span class="lineno">  735</span><span class="stringliteral">    &gt;&gt;&gt; plt.show()</span></div>
<div class="line"><span class="lineno">  736</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  737</span><span class="stringliteral">    In the next example, we show how complex-valued residual functions of</span></div>
<div class="line"><span class="lineno">  738</span><span class="stringliteral">    complex variables can be optimized with ``least_squares()``. Consider the</span></div>
<div class="line"><span class="lineno">  739</span><span class="stringliteral">    following function:</span></div>
<div class="line"><span class="lineno">  740</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  741</span><span class="stringliteral">    &gt;&gt;&gt; def f(z):</span></div>
<div class="line"><span class="lineno">  742</span><span class="stringliteral">    ...     return z - (0.5 + 0.5j)</span></div>
<div class="line"><span class="lineno">  743</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  744</span><span class="stringliteral">    We wrap it into a function of real variables that returns real residuals</span></div>
<div class="line"><span class="lineno">  745</span><span class="stringliteral">    by simply handling the real and imaginary parts as independent variables:</span></div>
<div class="line"><span class="lineno">  746</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  747</span><span class="stringliteral">    &gt;&gt;&gt; def f_wrap(x):</span></div>
<div class="line"><span class="lineno">  748</span><span class="stringliteral">    ...     fx = f(x[0] + 1j*x[1])</span></div>
<div class="line"><span class="lineno">  749</span><span class="stringliteral">    ...     return np.array([fx.real, fx.imag])</span></div>
<div class="line"><span class="lineno">  750</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  751</span><span class="stringliteral">    Thus, instead of the original m-D complex function of n complex</span></div>
<div class="line"><span class="lineno">  752</span><span class="stringliteral">    variables we optimize a 2m-D real function of 2n real variables:</span></div>
<div class="line"><span class="lineno">  753</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  754</span><span class="stringliteral">    &gt;&gt;&gt; from scipy.optimize import least_squares</span></div>
<div class="line"><span class="lineno">  755</span><span class="stringliteral">    &gt;&gt;&gt; res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))</span></div>
<div class="line"><span class="lineno">  756</span><span class="stringliteral">    &gt;&gt;&gt; z = res_wrapped.x[0] + res_wrapped.x[1]*1j</span></div>
<div class="line"><span class="lineno">  757</span><span class="stringliteral">    &gt;&gt;&gt; z</span></div>
<div class="line"><span class="lineno">  758</span><span class="stringliteral">    (0.49999999999925893+0.49999999999925893j)</span></div>
<div class="line"><span class="lineno">  759</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  760</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  761</span>    <span class="keywordflow">if</span> method <span class="keywordflow">not</span> <span class="keywordflow">in</span> [<span class="stringliteral">&#39;trf&#39;</span>, <span class="stringliteral">&#39;dogbox&#39;</span>, <span class="stringliteral">&#39;lm&#39;</span>]:</div>
<div class="line"><span class="lineno">  762</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`method` must be &#39;trf&#39;, &#39;dogbox&#39; or &#39;lm&#39;.&quot;</span>)</div>
<div class="line"><span class="lineno">  763</span> </div>
<div class="line"><span class="lineno">  764</span>    <span class="keywordflow">if</span> jac <span class="keywordflow">not</span> <span class="keywordflow">in</span> [<span class="stringliteral">&#39;2-point&#39;</span>, <span class="stringliteral">&#39;3-point&#39;</span>, <span class="stringliteral">&#39;cs&#39;</span>] <span class="keywordflow">and</span> <span class="keywordflow">not</span> callable(jac):</div>
<div class="line"><span class="lineno">  765</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`jac` must be &#39;2-point&#39;, &#39;3-point&#39;, &#39;cs&#39; or &quot;</span></div>
<div class="line"><span class="lineno">  766</span>                         <span class="stringliteral">&quot;callable.&quot;</span>)</div>
<div class="line"><span class="lineno">  767</span> </div>
<div class="line"><span class="lineno">  768</span>    <span class="keywordflow">if</span> tr_solver <span class="keywordflow">not</span> <span class="keywordflow">in</span> [<span class="keywordtype">None</span>, <span class="stringliteral">&#39;exact&#39;</span>, <span class="stringliteral">&#39;lsmr&#39;</span>]:</div>
<div class="line"><span class="lineno">  769</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`tr_solver` must be None, &#39;exact&#39; or &#39;lsmr&#39;.&quot;</span>)</div>
<div class="line"><span class="lineno">  770</span> </div>
<div class="line"><span class="lineno">  771</span>    <span class="keywordflow">if</span> loss <span class="keywordflow">not</span> <span class="keywordflow">in</span> IMPLEMENTED_LOSSES <span class="keywordflow">and</span> <span class="keywordflow">not</span> callable(loss):</div>
<div class="line"><span class="lineno">  772</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`loss` must be one of {0} or a callable.&quot;</span></div>
<div class="line"><span class="lineno">  773</span>                         .format(IMPLEMENTED_LOSSES.keys()))</div>
<div class="line"><span class="lineno">  774</span> </div>
<div class="line"><span class="lineno">  775</span>    <span class="keywordflow">if</span> method == <span class="stringliteral">&#39;lm&#39;</span> <span class="keywordflow">and</span> loss != <span class="stringliteral">&#39;linear&#39;</span>:</div>
<div class="line"><span class="lineno">  776</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;method=&#39;lm&#39; supports only &#39;linear&#39; loss function.&quot;</span>)</div>
<div class="line"><span class="lineno">  777</span> </div>
<div class="line"><span class="lineno">  778</span>    <span class="keywordflow">if</span> verbose <span class="keywordflow">not</span> <span class="keywordflow">in</span> [0, 1, 2]:</div>
<div class="line"><span class="lineno">  779</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`verbose` must be in [0, 1, 2].&quot;</span>)</div>
<div class="line"><span class="lineno">  780</span> </div>
<div class="line"><span class="lineno">  781</span>    <span class="keywordflow">if</span> len(bounds) != 2:</div>
<div class="line"><span class="lineno">  782</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`bounds` must contain 2 elements.&quot;</span>)</div>
<div class="line"><span class="lineno">  783</span> </div>
<div class="line"><span class="lineno">  784</span>    <span class="keywordflow">if</span> max_nfev <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> max_nfev &lt;= 0:</div>
<div class="line"><span class="lineno">  785</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`max_nfev` must be None or positive integer.&quot;</span>)</div>
<div class="line"><span class="lineno">  786</span> </div>
<div class="line"><span class="lineno">  787</span>    <span class="keywordflow">if</span> np.iscomplexobj(x0):</div>
<div class="line"><span class="lineno">  788</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`x0` must be real.&quot;</span>)</div>
<div class="line"><span class="lineno">  789</span> </div>
<div class="line"><span class="lineno">  790</span>    x0 = np.atleast_1d(x0).astype(float)</div>
<div class="line"><span class="lineno">  791</span> </div>
<div class="line"><span class="lineno">  792</span>    <span class="keywordflow">if</span> x0.ndim &gt; 1:</div>
<div class="line"><span class="lineno">  793</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`x0` must have at most 1 dimension.&quot;</span>)</div>
<div class="line"><span class="lineno">  794</span> </div>
<div class="line"><span class="lineno">  795</span>    lb, ub = prepare_bounds(bounds, x0.shape[0])</div>
<div class="line"><span class="lineno">  796</span> </div>
<div class="line"><span class="lineno">  797</span>    <span class="keywordflow">if</span> method == <span class="stringliteral">&#39;lm&#39;</span> <span class="keywordflow">and</span> <span class="keywordflow">not</span> np.all((lb == -np.inf) &amp; (ub == np.inf)):</div>
<div class="line"><span class="lineno">  798</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Method &#39;lm&#39; doesn&#39;t support bounds.&quot;</span>)</div>
<div class="line"><span class="lineno">  799</span> </div>
<div class="line"><span class="lineno">  800</span>    <span class="keywordflow">if</span> lb.shape != x0.shape <span class="keywordflow">or</span> ub.shape != x0.shape:</div>
<div class="line"><span class="lineno">  801</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Inconsistent shapes between bounds and `x0`.&quot;</span>)</div>
<div class="line"><span class="lineno">  802</span> </div>
<div class="line"><span class="lineno">  803</span>    <span class="keywordflow">if</span> np.any(lb &gt;= ub):</div>
<div class="line"><span class="lineno">  804</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Each lower bound must be strictly less than each &quot;</span></div>
<div class="line"><span class="lineno">  805</span>                         <span class="stringliteral">&quot;upper bound.&quot;</span>)</div>
<div class="line"><span class="lineno">  806</span> </div>
<div class="line"><span class="lineno">  807</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> in_bounds(x0, lb, ub):</div>
<div class="line"><span class="lineno">  808</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`x0` is infeasible.&quot;</span>)</div>
<div class="line"><span class="lineno">  809</span> </div>
<div class="line"><span class="lineno">  810</span>    x_scale = check_x_scale(x_scale, x0)</div>
<div class="line"><span class="lineno">  811</span> </div>
<div class="line"><span class="lineno">  812</span>    ftol, xtol, gtol = check_tolerance(ftol, xtol, gtol, method)</div>
<div class="line"><span class="lineno">  813</span> </div>
<div class="line"><span class="lineno">  814</span>    <span class="keyword">def </span>fun_wrapped(x):</div>
<div class="line"><span class="lineno">  815</span>        <span class="keywordflow">return</span> np.atleast_1d(fun(x, *args, **kwargs))</div>
<div class="line"><span class="lineno">  816</span> </div>
<div class="line"><span class="lineno">  817</span>    <span class="keywordflow">if</span> method == <span class="stringliteral">&#39;trf&#39;</span>:</div>
<div class="line"><span class="lineno">  818</span>        x0 = make_strictly_feasible(x0, lb, ub)</div>
<div class="line"><span class="lineno">  819</span> </div>
<div class="line"><span class="lineno">  820</span>    f0 = fun_wrapped(x0)</div>
<div class="line"><span class="lineno">  821</span> </div>
<div class="line"><span class="lineno">  822</span>    <span class="keywordflow">if</span> f0.ndim != 1:</div>
<div class="line"><span class="lineno">  823</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`fun` must return at most 1-d array_like. &quot;</span></div>
<div class="line"><span class="lineno">  824</span>                         <span class="stringliteral">&quot;f0.shape: {0}&quot;</span>.format(f0.shape))</div>
<div class="line"><span class="lineno">  825</span> </div>
<div class="line"><span class="lineno">  826</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> np.all(np.isfinite(f0)):</div>
<div class="line"><span class="lineno">  827</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Residuals are not finite in the initial point.&quot;</span>)</div>
<div class="line"><span class="lineno">  828</span> </div>
<div class="line"><span class="lineno">  829</span>    n = x0.size</div>
<div class="line"><span class="lineno">  830</span>    m = f0.size</div>
<div class="line"><span class="lineno">  831</span> </div>
<div class="line"><span class="lineno">  832</span>    <span class="keywordflow">if</span> method == <span class="stringliteral">&#39;lm&#39;</span> <span class="keywordflow">and</span> m &lt; n:</div>
<div class="line"><span class="lineno">  833</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Method &#39;lm&#39; doesn&#39;t work when the number of &quot;</span></div>
<div class="line"><span class="lineno">  834</span>                         <span class="stringliteral">&quot;residuals is less than the number of variables.&quot;</span>)</div>
<div class="line"><span class="lineno">  835</span> </div>
<div class="line"><span class="lineno">  836</span>    loss_function = construct_loss_function(m, loss, f_scale)</div>
<div class="line"><span class="lineno">  837</span>    <span class="keywordflow">if</span> callable(loss):</div>
<div class="line"><span class="lineno">  838</span>        rho = loss_function(f0)</div>
<div class="line"><span class="lineno">  839</span>        <span class="keywordflow">if</span> rho.shape != (3, m):</div>
<div class="line"><span class="lineno">  840</span>            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;The return value of `loss` callable has wrong &quot;</span></div>
<div class="line"><span class="lineno">  841</span>                             <span class="stringliteral">&quot;shape.&quot;</span>)</div>
<div class="line"><span class="lineno">  842</span>        initial_cost = 0.5 * np.sum(rho[0])</div>
<div class="line"><span class="lineno">  843</span>    <span class="keywordflow">elif</span> loss_function <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  844</span>        initial_cost = loss_function(f0, cost_only=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  845</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  846</span>        initial_cost = 0.5 * np.dot(f0, f0)</div>
<div class="line"><span class="lineno">  847</span> </div>
<div class="line"><span class="lineno">  848</span>    <span class="keywordflow">if</span> callable(jac):</div>
<div class="line"><span class="lineno">  849</span>        J0 = jac(x0, *args, **kwargs)</div>
<div class="line"><span class="lineno">  850</span> </div>
<div class="line"><span class="lineno">  851</span>        <span class="keywordflow">if</span> issparse(J0):</div>
<div class="line"><span class="lineno">  852</span>            J0 = J0.tocsr()</div>
<div class="line"><span class="lineno">  853</span> </div>
<div class="line"><span class="lineno">  854</span>            <span class="keyword">def </span>jac_wrapped(x, _=None):</div>
<div class="line"><span class="lineno">  855</span>                <span class="keywordflow">return</span> jac(x, *args, **kwargs).tocsr()</div>
<div class="line"><span class="lineno">  856</span> </div>
<div class="line"><span class="lineno">  857</span>        <span class="keywordflow">elif</span> isinstance(J0, LinearOperator):</div>
<div class="line"><span class="lineno">  858</span>            <span class="keyword">def </span>jac_wrapped(x, _=None):</div>
<div class="line"><span class="lineno">  859</span>                <span class="keywordflow">return</span> jac(x, *args, **kwargs)</div>
<div class="line"><span class="lineno">  860</span> </div>
<div class="line"><span class="lineno">  861</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  862</span>            J0 = np.atleast_2d(J0)</div>
<div class="line"><span class="lineno">  863</span> </div>
<div class="line"><span class="lineno">  864</span>            <span class="keyword">def </span>jac_wrapped(x, _=None):</div>
<div class="line"><span class="lineno">  865</span>                <span class="keywordflow">return</span> np.atleast_2d(jac(x, *args, **kwargs))</div>
<div class="line"><span class="lineno">  866</span> </div>
<div class="line"><span class="lineno">  867</span>    <span class="keywordflow">else</span>:  <span class="comment"># Estimate Jacobian by finite differences.</span></div>
<div class="line"><span class="lineno">  868</span>        <span class="keywordflow">if</span> method == <span class="stringliteral">&#39;lm&#39;</span>:</div>
<div class="line"><span class="lineno">  869</span>            <span class="keywordflow">if</span> jac_sparsity <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  870</span>                <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;method=&#39;lm&#39; does not support &quot;</span></div>
<div class="line"><span class="lineno">  871</span>                                 <span class="stringliteral">&quot;`jac_sparsity`.&quot;</span>)</div>
<div class="line"><span class="lineno">  872</span> </div>
<div class="line"><span class="lineno">  873</span>            <span class="keywordflow">if</span> jac != <span class="stringliteral">&#39;2-point&#39;</span>:</div>
<div class="line"><span class="lineno">  874</span>                warn(<span class="stringliteral">&quot;jac=&#39;{0}&#39; works equivalently to &#39;2-point&#39; &quot;</span></div>
<div class="line"><span class="lineno">  875</span>                     <span class="stringliteral">&quot;for method=&#39;lm&#39;.&quot;</span>.format(jac))</div>
<div class="line"><span class="lineno">  876</span> </div>
<div class="line"><span class="lineno">  877</span>            J0 = jac_wrapped = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  878</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  879</span>            <span class="keywordflow">if</span> jac_sparsity <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> tr_solver == <span class="stringliteral">&#39;exact&#39;</span>:</div>
<div class="line"><span class="lineno">  880</span>                <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;tr_solver=&#39;exact&#39; is incompatible &quot;</span></div>
<div class="line"><span class="lineno">  881</span>                                 <span class="stringliteral">&quot;with `jac_sparsity`.&quot;</span>)</div>
<div class="line"><span class="lineno">  882</span> </div>
<div class="line"><span class="lineno">  883</span>            jac_sparsity = check_jac_sparsity(jac_sparsity, m, n)</div>
<div class="line"><span class="lineno">  884</span> </div>
<div class="line"><span class="lineno">  885</span>            <span class="keyword">def </span>jac_wrapped(x, f):</div>
<div class="line"><span class="lineno">  886</span>                J = approx_derivative(fun, x, rel_step=diff_step, method=jac,</div>
<div class="line"><span class="lineno">  887</span>                                      f0=f, bounds=bounds, args=args,</div>
<div class="line"><span class="lineno">  888</span>                                      kwargs=kwargs, sparsity=jac_sparsity)</div>
<div class="line"><span class="lineno">  889</span>                <span class="keywordflow">if</span> J.ndim != 2:  <span class="comment"># J is guaranteed not sparse.</span></div>
<div class="line"><span class="lineno">  890</span>                    J = np.atleast_2d(J)</div>
<div class="line"><span class="lineno">  891</span> </div>
<div class="line"><span class="lineno">  892</span>                <span class="keywordflow">return</span> J</div>
<div class="line"><span class="lineno">  893</span> </div>
<div class="line"><span class="lineno">  894</span>            J0 = jac_wrapped(x0, f0)</div>
<div class="line"><span class="lineno">  895</span> </div>
<div class="line"><span class="lineno">  896</span>    <span class="keywordflow">if</span> J0 <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  897</span>        <span class="keywordflow">if</span> J0.shape != (m, n):</div>
<div class="line"><span class="lineno">  898</span>            <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">  899</span>                <span class="stringliteral">&quot;The return value of `jac` has wrong shape: expected {0}, &quot;</span></div>
<div class="line"><span class="lineno">  900</span>                <span class="stringliteral">&quot;actual {1}.&quot;</span>.format((m, n), J0.shape))</div>
<div class="line"><span class="lineno">  901</span> </div>
<div class="line"><span class="lineno">  902</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(J0, np.ndarray):</div>
<div class="line"><span class="lineno">  903</span>            <span class="keywordflow">if</span> method == <span class="stringliteral">&#39;lm&#39;</span>:</div>
<div class="line"><span class="lineno">  904</span>                <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;method=&#39;lm&#39; works only with dense &quot;</span></div>
<div class="line"><span class="lineno">  905</span>                                 <span class="stringliteral">&quot;Jacobian matrices.&quot;</span>)</div>
<div class="line"><span class="lineno">  906</span> </div>
<div class="line"><span class="lineno">  907</span>            <span class="keywordflow">if</span> tr_solver == <span class="stringliteral">&#39;exact&#39;</span>:</div>
<div class="line"><span class="lineno">  908</span>                <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">  909</span>                    <span class="stringliteral">&quot;tr_solver=&#39;exact&#39; works only with dense &quot;</span></div>
<div class="line"><span class="lineno">  910</span>                    <span class="stringliteral">&quot;Jacobian matrices.&quot;</span>)</div>
<div class="line"><span class="lineno">  911</span> </div>
<div class="line"><span class="lineno">  912</span>        jac_scale = isinstance(x_scale, str) <span class="keywordflow">and</span> x_scale == <span class="stringliteral">&#39;jac&#39;</span></div>
<div class="line"><span class="lineno">  913</span>        <span class="keywordflow">if</span> isinstance(J0, LinearOperator) <span class="keywordflow">and</span> jac_scale:</div>
<div class="line"><span class="lineno">  914</span>            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;x_scale=&#39;jac&#39; can&#39;t be used when `jac` &quot;</span></div>
<div class="line"><span class="lineno">  915</span>                             <span class="stringliteral">&quot;returns LinearOperator.&quot;</span>)</div>
<div class="line"><span class="lineno">  916</span> </div>
<div class="line"><span class="lineno">  917</span>        <span class="keywordflow">if</span> tr_solver <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  918</span>            <span class="keywordflow">if</span> isinstance(J0, np.ndarray):</div>
<div class="line"><span class="lineno">  919</span>                tr_solver = <span class="stringliteral">&#39;exact&#39;</span></div>
<div class="line"><span class="lineno">  920</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  921</span>                tr_solver = <span class="stringliteral">&#39;lsmr&#39;</span></div>
<div class="line"><span class="lineno">  922</span> </div>
<div class="line"><span class="lineno">  923</span>    <span class="keywordflow">if</span> method == <span class="stringliteral">&#39;lm&#39;</span>:</div>
<div class="line"><span class="lineno">  924</span>        result = call_minpack(fun_wrapped, x0, jac_wrapped, ftol, xtol, gtol,</div>
<div class="line"><span class="lineno">  925</span>                              max_nfev, x_scale, diff_step)</div>
<div class="line"><span class="lineno">  926</span> </div>
<div class="line"><span class="lineno">  927</span>    <span class="keywordflow">elif</span> method == <span class="stringliteral">&#39;trf&#39;</span>:</div>
<div class="line"><span class="lineno">  928</span>        result = trf(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol, xtol,</div>
<div class="line"><span class="lineno">  929</span>                     gtol, max_nfev, x_scale, loss_function, tr_solver,</div>
<div class="line"><span class="lineno">  930</span>                     tr_options.copy(), verbose)</div>
<div class="line"><span class="lineno">  931</span> </div>
<div class="line"><span class="lineno">  932</span>    <span class="keywordflow">elif</span> method == <span class="stringliteral">&#39;dogbox&#39;</span>:</div>
<div class="line"><span class="lineno">  933</span>        <span class="keywordflow">if</span> tr_solver == <span class="stringliteral">&#39;lsmr&#39;</span> <span class="keywordflow">and</span> <span class="stringliteral">&#39;regularize&#39;</span> <span class="keywordflow">in</span> tr_options:</div>
<div class="line"><span class="lineno">  934</span>            warn(<span class="stringliteral">&quot;The keyword &#39;regularize&#39; in `tr_options` is not relevant &quot;</span></div>
<div class="line"><span class="lineno">  935</span>                 <span class="stringliteral">&quot;for &#39;dogbox&#39; method.&quot;</span>)</div>
<div class="line"><span class="lineno">  936</span>            tr_options = tr_options.copy()</div>
<div class="line"><span class="lineno">  937</span>            del tr_options[<span class="stringliteral">&#39;regularize&#39;</span>]</div>
<div class="line"><span class="lineno">  938</span> </div>
<div class="line"><span class="lineno">  939</span>        result = dogbox(fun_wrapped, jac_wrapped, x0, f0, J0, lb, ub, ftol,</div>
<div class="line"><span class="lineno">  940</span>                        xtol, gtol, max_nfev, x_scale, loss_function,</div>
<div class="line"><span class="lineno">  941</span>                        tr_solver, tr_options, verbose)</div>
<div class="line"><span class="lineno">  942</span> </div>
<div class="line"><span class="lineno">  943</span>    result.message = TERMINATION_MESSAGES[result.status]</div>
<div class="line"><span class="lineno">  944</span>    result.success = result.status &gt; 0</div>
<div class="line"><span class="lineno">  945</span> </div>
<div class="line"><span class="lineno">  946</span>    <span class="keywordflow">if</span> verbose &gt;= 1:</div>
<div class="line"><span class="lineno">  947</span>        print(result.message)</div>
<div class="line"><span class="lineno">  948</span>        print(<span class="stringliteral">&quot;Function evaluations {0}, initial cost {1:.4e}, final cost &quot;</span></div>
<div class="line"><span class="lineno">  949</span>              <span class="stringliteral">&quot;{2:.4e}, first-order optimality {3:.2e}.&quot;</span></div>
<div class="line"><span class="lineno">  950</span>              .format(result.nfev, initial_cost, result.cost,</div>
<div class="line"><span class="lineno">  951</span>                      result.optimality))</div>
<div class="line"><span class="lineno">  952</span> </div>
<div class="line"><span class="lineno">  953</span>    <span class="keywordflow">return</span> result</div>
</div><!-- fragment -->
</div>
</div>
<a id="aa579f28da46b588a00d171266ed94afd" name="aa579f28da46b588a00d171266ed94afd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa579f28da46b588a00d171266ed94afd">&#9670;&#160;</a></span>prepare_bounds()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.prepare_bounds </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>bounds</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   94</span><span class="keyword">def </span>prepare_bounds(bounds, n):</div>
<div class="line"><span class="lineno">   95</span>    lb, ub = [np.asarray(b, dtype=float) <span class="keywordflow">for</span> b <span class="keywordflow">in</span> bounds]</div>
<div class="line"><span class="lineno">   96</span>    <span class="keywordflow">if</span> lb.ndim == 0:</div>
<div class="line"><span class="lineno">   97</span>        lb = np.resize(lb, n)</div>
<div class="line"><span class="lineno">   98</span> </div>
<div class="line"><span class="lineno">   99</span>    <span class="keywordflow">if</span> ub.ndim == 0:</div>
<div class="line"><span class="lineno">  100</span>        ub = np.resize(ub, n)</div>
<div class="line"><span class="lineno">  101</span> </div>
<div class="line"><span class="lineno">  102</span>    <span class="keywordflow">return</span> lb, ub</div>
<div class="line"><span class="lineno">  103</span> </div>
<div class="line"><span class="lineno">  104</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a20d3588c88af58fec7fe60b2c8f3f6c4" name="a20d3588c88af58fec7fe60b2c8f3f6c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a20d3588c88af58fec7fe60b2c8f3f6c4">&#9670;&#160;</a></span>soft_l1()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.soft_l1 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>rho</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>cost_only</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  180</span><span class="keyword">def </span>soft_l1(z, rho, cost_only):</div>
<div class="line"><span class="lineno">  181</span>    t = 1 + z</div>
<div class="line"><span class="lineno">  182</span>    rho[0] = 2 * (t**0.5 - 1)</div>
<div class="line"><span class="lineno">  183</span>    <span class="keywordflow">if</span> cost_only:</div>
<div class="line"><span class="lineno">  184</span>        <span class="keywordflow">return</span></div>
<div class="line"><span class="lineno">  185</span>    rho[1] = t**-0.5</div>
<div class="line"><span class="lineno">  186</span>    rho[2] = -0.5 * t**-1.5</div>
<div class="line"><span class="lineno">  187</span> </div>
<div class="line"><span class="lineno">  188</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a36d1c38b591ad4cc3b077d04b978c181" name="a36d1c38b591ad4cc3b077d04b978c181"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a36d1c38b591ad4cc3b077d04b978c181">&#9670;&#160;</a></span>FROM_MINPACK_TO_COMMON</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">dict scipy.optimize._lsq.least_squares.FROM_MINPACK_TO_COMMON</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><span class="lineno">    1</span>=  {</div>
<div class="line"><span class="lineno">    2</span>    0: -1,  <span class="comment"># Improper input parameters from MINPACK.</span></div>
<div class="line"><span class="lineno">    3</span>    1: 2,</div>
<div class="line"><span class="lineno">    4</span>    2: 3,</div>
<div class="line"><span class="lineno">    5</span>    3: 4,</div>
<div class="line"><span class="lineno">    6</span>    4: 1,</div>
<div class="line"><span class="lineno">    7</span>    5: 0</div>
<div class="line"><span class="lineno">    8</span>    <span class="comment"># There are 6, 7, 8 for too small tolerance parameters,</span></div>
<div class="line"><span class="lineno">    9</span>    <span class="comment"># but we guard against it by checking ftol, xtol, gtol beforehand.</span></div>
<div class="line"><span class="lineno">   10</span>}</div>
</div><!-- fragment -->
</div>
</div>
<a id="ab6d87fc593a7f2fd5ae185cd95a9dec6" name="ab6d87fc593a7f2fd5ae185cd95a9dec6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab6d87fc593a7f2fd5ae185cd95a9dec6">&#9670;&#160;</a></span>IMPLEMENTED_LOSSES</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._lsq.least_squares.IMPLEMENTED_LOSSES</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><span class="lineno">    1</span>=  dict(linear=<span class="keywordtype">None</span>, huber=huber, soft_l1=soft_l1,</div>
<div class="line"><span class="lineno">    2</span>                          cauchy=cauchy, arctan=arctan)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a5782e898597b6780ad1a5ad4531a5daa" name="a5782e898597b6780ad1a5ad4531a5daa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5782e898597b6780ad1a5ad4531a5daa">&#9670;&#160;</a></span>TERMINATION_MESSAGES</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">dict scipy.optimize._lsq.least_squares.TERMINATION_MESSAGES</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><span class="lineno">    1</span>=  {</div>
<div class="line"><span class="lineno">    2</span>    -1: <span class="stringliteral">&quot;Improper input parameters status returned from `leastsq`&quot;</span>,</div>
<div class="line"><span class="lineno">    3</span>    0: <span class="stringliteral">&quot;The maximum number of function evaluations is exceeded.&quot;</span>,</div>
<div class="line"><span class="lineno">    4</span>    1: <span class="stringliteral">&quot;`gtol` termination condition is satisfied.&quot;</span>,</div>
<div class="line"><span class="lineno">    5</span>    2: <span class="stringliteral">&quot;`ftol` termination condition is satisfied.&quot;</span>,</div>
<div class="line"><span class="lineno">    6</span>    3: <span class="stringliteral">&quot;`xtol` termination condition is satisfied.&quot;</span>,</div>
<div class="line"><span class="lineno">    7</span>    4: <span class="stringliteral">&quot;Both `ftol` and `xtol` termination conditions are satisfied.&quot;</span></div>
<div class="line"><span class="lineno">    8</span>}</div>
</div><!-- fragment -->
</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
