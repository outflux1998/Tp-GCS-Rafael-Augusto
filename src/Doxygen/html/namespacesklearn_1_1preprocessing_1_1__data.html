<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.preprocessing._data Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1preprocessing.html">preprocessing</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html">_data</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">sklearn.preprocessing._data Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1preprocessing_1_1__data_1_1_binarizer.html">Binarizer</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1preprocessing_1_1__data_1_1_kernel_centerer.html">KernelCenterer</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1preprocessing_1_1__data_1_1_max_abs_scaler.html">MaxAbsScaler</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1preprocessing_1_1__data_1_1_min_max_scaler.html">MinMaxScaler</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1preprocessing_1_1__data_1_1_normalizer.html">Normalizer</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1preprocessing_1_1__data_1_1_power_transformer.html">PowerTransformer</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1preprocessing_1_1__data_1_1_quantile_transformer.html">QuantileTransformer</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1preprocessing_1_1__data_1_1_robust_scaler.html">RobustScaler</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1preprocessing_1_1__data_1_1_standard_scaler.html">StandardScaler</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a85c2142a945808a2a79251941829b917" id="r_a85c2142a945808a2a79251941829b917"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#a85c2142a945808a2a79251941829b917">_is_constant_feature</a> (var, mean, n_samples)</td></tr>
<tr class="separator:a85c2142a945808a2a79251941829b917"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a41b955186893e66f5dd05aaea89abd9e" id="r_a41b955186893e66f5dd05aaea89abd9e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#a41b955186893e66f5dd05aaea89abd9e">_handle_zeros_in_scale</a> (<a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#acdc48832c8ced035403f3ee58b8f0b01">scale</a>, copy=True, constant_mask=None)</td></tr>
<tr class="separator:a41b955186893e66f5dd05aaea89abd9e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acdc48832c8ced035403f3ee58b8f0b01" id="r_acdc48832c8ced035403f3ee58b8f0b01"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#acdc48832c8ced035403f3ee58b8f0b01">scale</a> (X, *axis=0, with_mean=True, with_std=True, copy=True)</td></tr>
<tr class="separator:acdc48832c8ced035403f3ee58b8f0b01"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace15f3fc63a7ec0af22fb230ab23cc30" id="r_ace15f3fc63a7ec0af22fb230ab23cc30"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#ace15f3fc63a7ec0af22fb230ab23cc30">minmax_scale</a> (X, feature_range=(0, 1), *axis=0, copy=True)</td></tr>
<tr class="separator:ace15f3fc63a7ec0af22fb230ab23cc30"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac4b760081274b32aa343cc1d269c2165" id="r_ac4b760081274b32aa343cc1d269c2165"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#ac4b760081274b32aa343cc1d269c2165">maxabs_scale</a> (X, *axis=0, copy=True)</td></tr>
<tr class="separator:ac4b760081274b32aa343cc1d269c2165"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8707cbc7bec0ada5d7def37adc313ded" id="r_a8707cbc7bec0ada5d7def37adc313ded"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#a8707cbc7bec0ada5d7def37adc313ded">robust_scale</a> (X, *axis=0, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True, unit_variance=False)</td></tr>
<tr class="separator:a8707cbc7bec0ada5d7def37adc313ded"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7bacbfb60137f112c90028a3cc917767" id="r_a7bacbfb60137f112c90028a3cc917767"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#a7bacbfb60137f112c90028a3cc917767">normalize</a> (X, norm=&quot;l2&quot;, *axis=1, copy=True, return_norm=False)</td></tr>
<tr class="separator:a7bacbfb60137f112c90028a3cc917767"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a05d8e2d47929e6c7470d93cd18526af4" id="r_a05d8e2d47929e6c7470d93cd18526af4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#a05d8e2d47929e6c7470d93cd18526af4">binarize</a> (X, *threshold=0.0, copy=True)</td></tr>
<tr class="separator:a05d8e2d47929e6c7470d93cd18526af4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a677a2a464f2e3da561dbea927253743a" id="r_a677a2a464f2e3da561dbea927253743a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#a677a2a464f2e3da561dbea927253743a">add_dummy_feature</a> (X, value=1.0)</td></tr>
<tr class="separator:a677a2a464f2e3da561dbea927253743a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6839a4a003c881de83d7b1794b8399a4" id="r_a6839a4a003c881de83d7b1794b8399a4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#a6839a4a003c881de83d7b1794b8399a4">quantile_transform</a> (X, *axis=0, n_quantiles=1000, output_distribution=&quot;uniform&quot;, ignore_implicit_zeros=False, subsample=int(1e5), random_state=None, copy=True)</td></tr>
<tr class="separator:a6839a4a003c881de83d7b1794b8399a4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3b2d76087caf444ecc19dbf34960fbd7" id="r_a3b2d76087caf444ecc19dbf34960fbd7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#a3b2d76087caf444ecc19dbf34960fbd7">power_transform</a> (X, method=&quot;yeo-johnson&quot;, *standardize=True, copy=True)</td></tr>
<tr class="separator:a3b2d76087caf444ecc19dbf34960fbd7"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a1ed07fa0a5dd1bd2f5d841da47681820" id="r_a1ed07fa0a5dd1bd2f5d841da47681820"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1preprocessing_1_1__data.html#a1ed07fa0a5dd1bd2f5d841da47681820">BOUNDS_THRESHOLD</a> = 1e-7</td></tr>
<tr class="separator:a1ed07fa0a5dd1bd2f5d841da47681820"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Function Documentation</h2>
<a id="a41b955186893e66f5dd05aaea89abd9e" name="a41b955186893e66f5dd05aaea89abd9e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a41b955186893e66f5dd05aaea89abd9e">&#9670;&#160;</a></span>_handle_zeros_in_scale()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data._handle_zeros_in_scale </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>scale</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>constant_mask</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Set scales of near constant features to 1.

The goal is to avoid division by very small or zero values.

Near constant features are detected automatically by identifying
scales close to machine precision unless they are precomputed by
the caller and passed with the `constant_mask` kwarg.

Typically for standard scaling, the scales are the standard
deviation while near constant features are better detected on the
computed variances which are closer to machine precision by
construction.
</pre> <div class="fragment"><div class="line"><span class="lineno">   90</span><span class="keyword">def </span>_handle_zeros_in_scale(scale, copy=True, constant_mask=None):</div>
<div class="line"><span class="lineno">   91</span>    <span class="stringliteral">&quot;&quot;&quot;Set scales of near constant features to 1.</span></div>
<div class="line"><span class="lineno">   92</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   93</span><span class="stringliteral">    The goal is to avoid division by very small or zero values.</span></div>
<div class="line"><span class="lineno">   94</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   95</span><span class="stringliteral">    Near constant features are detected automatically by identifying</span></div>
<div class="line"><span class="lineno">   96</span><span class="stringliteral">    scales close to machine precision unless they are precomputed by</span></div>
<div class="line"><span class="lineno">   97</span><span class="stringliteral">    the caller and passed with the `constant_mask` kwarg.</span></div>
<div class="line"><span class="lineno">   98</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   99</span><span class="stringliteral">    Typically for standard scaling, the scales are the standard</span></div>
<div class="line"><span class="lineno">  100</span><span class="stringliteral">    deviation while near constant features are better detected on the</span></div>
<div class="line"><span class="lineno">  101</span><span class="stringliteral">    computed variances which are closer to machine precision by</span></div>
<div class="line"><span class="lineno">  102</span><span class="stringliteral">    construction.</span></div>
<div class="line"><span class="lineno">  103</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  104</span>    <span class="comment"># if we are fitting on 1D arrays, scale might be a scalar</span></div>
<div class="line"><span class="lineno">  105</span>    <span class="keywordflow">if</span> np.isscalar(scale):</div>
<div class="line"><span class="lineno">  106</span>        <span class="keywordflow">if</span> scale == 0.0:</div>
<div class="line"><span class="lineno">  107</span>            scale = 1.0</div>
<div class="line"><span class="lineno">  108</span>        <span class="keywordflow">return</span> scale</div>
<div class="line"><span class="lineno">  109</span>    <span class="keywordflow">elif</span> isinstance(scale, np.ndarray):</div>
<div class="line"><span class="lineno">  110</span>        <span class="keywordflow">if</span> constant_mask <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  111</span>            <span class="comment"># Detect near constant values to avoid dividing by a very small</span></div>
<div class="line"><span class="lineno">  112</span>            <span class="comment"># value that could lead to surprising results and numerical</span></div>
<div class="line"><span class="lineno">  113</span>            <span class="comment"># stability issues.</span></div>
<div class="line"><span class="lineno">  114</span>            constant_mask = scale &lt; 10 * np.finfo(scale.dtype).eps</div>
<div class="line"><span class="lineno">  115</span> </div>
<div class="line"><span class="lineno">  116</span>        <span class="keywordflow">if</span> copy:</div>
<div class="line"><span class="lineno">  117</span>            <span class="comment"># New array to avoid side-effects</span></div>
<div class="line"><span class="lineno">  118</span>            scale = scale.copy()</div>
<div class="line"><span class="lineno">  119</span>        scale[constant_mask] = 1.0</div>
<div class="line"><span class="lineno">  120</span>        <span class="keywordflow">return</span> scale</div>
<div class="line"><span class="lineno">  121</span> </div>
<div class="line"><span class="lineno">  122</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a85c2142a945808a2a79251941829b917" name="a85c2142a945808a2a79251941829b917"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a85c2142a945808a2a79251941829b917">&#9670;&#160;</a></span>_is_constant_feature()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data._is_constant_feature </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>var</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>mean</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Detect if a feature is indistinguishable from a constant feature.

The detection is based on its computed variance and on the theoretical
error bounds of the '2 pass algorithm' for variance computation.

See "Algorithms for computing the sample variance: analysis and
recommendations", by Chan, Golub, and LeVeque.
</pre> <div class="fragment"><div class="line"><span class="lineno">   74</span><span class="keyword">def </span>_is_constant_feature(var, mean, n_samples):</div>
<div class="line"><span class="lineno">   75</span>    <span class="stringliteral">&quot;&quot;&quot;Detect if a feature is indistinguishable from a constant feature.</span></div>
<div class="line"><span class="lineno">   76</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   77</span><span class="stringliteral">    The detection is based on its computed variance and on the theoretical</span></div>
<div class="line"><span class="lineno">   78</span><span class="stringliteral">    error bounds of the &#39;2 pass algorithm&#39; for variance computation.</span></div>
<div class="line"><span class="lineno">   79</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   80</span><span class="stringliteral">    See &quot;Algorithms for computing the sample variance: analysis and</span></div>
<div class="line"><span class="lineno">   81</span><span class="stringliteral">    recommendations&quot;, by Chan, Golub, and LeVeque.</span></div>
<div class="line"><span class="lineno">   82</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   83</span>    <span class="comment"># In scikit-learn, variance is always computed using float64 accumulators.</span></div>
<div class="line"><span class="lineno">   84</span>    eps = np.finfo(np.float64).eps</div>
<div class="line"><span class="lineno">   85</span> </div>
<div class="line"><span class="lineno">   86</span>    upper_bound = n_samples * eps * var + (n_samples * mean * eps) ** 2</div>
<div class="line"><span class="lineno">   87</span>    <span class="keywordflow">return</span> var &lt;= upper_bound</div>
<div class="line"><span class="lineno">   88</span> </div>
<div class="line"><span class="lineno">   89</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a677a2a464f2e3da561dbea927253743a" name="a677a2a464f2e3da561dbea927253743a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a677a2a464f2e3da561dbea927253743a">&#9670;&#160;</a></span>add_dummy_feature()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data.add_dummy_feature </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>value</em> = <code>1.0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Augment dataset with an additional dummy feature.

This is useful for fitting an intercept term with implementations which
cannot otherwise fit it directly.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Data.

value : float
    Value to use for the dummy feature.

Returns
-------
X : {ndarray, sparse matrix} of shape (n_samples, n_features + 1)
    Same data with dummy feature added as first column.

Examples
--------
&gt;&gt;&gt; from sklearn.preprocessing import add_dummy_feature
&gt;&gt;&gt; add_dummy_feature([[0, 1], [1, 0]])
array([[1., 0., 1.],
       [1., 1., 0.]])
</pre> <div class="fragment"><div class="line"><span class="lineno"> 2321</span><span class="keyword">def </span>add_dummy_feature(X, value=1.0):</div>
<div class="line"><span class="lineno"> 2322</span>    <span class="stringliteral">&quot;&quot;&quot;Augment dataset with an additional dummy feature.</span></div>
<div class="line"><span class="lineno"> 2323</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2324</span><span class="stringliteral">    This is useful for fitting an intercept term with implementations which</span></div>
<div class="line"><span class="lineno"> 2325</span><span class="stringliteral">    cannot otherwise fit it directly.</span></div>
<div class="line"><span class="lineno"> 2326</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2327</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 2328</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 2329</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 2330</span><span class="stringliteral">        Data.</span></div>
<div class="line"><span class="lineno"> 2331</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2332</span><span class="stringliteral">    value : float</span></div>
<div class="line"><span class="lineno"> 2333</span><span class="stringliteral">        Value to use for the dummy feature.</span></div>
<div class="line"><span class="lineno"> 2334</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2335</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 2336</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 2337</span><span class="stringliteral">    X : {ndarray, sparse matrix} of shape (n_samples, n_features + 1)</span></div>
<div class="line"><span class="lineno"> 2338</span><span class="stringliteral">        Same data with dummy feature added as first column.</span></div>
<div class="line"><span class="lineno"> 2339</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2340</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno"> 2341</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 2342</span><span class="stringliteral">    &gt;&gt;&gt; from sklearn.preprocessing import add_dummy_feature</span></div>
<div class="line"><span class="lineno"> 2343</span><span class="stringliteral">    &gt;&gt;&gt; add_dummy_feature([[0, 1], [1, 0]])</span></div>
<div class="line"><span class="lineno"> 2344</span><span class="stringliteral">    array([[1., 0., 1.],</span></div>
<div class="line"><span class="lineno"> 2345</span><span class="stringliteral">           [1., 1., 0.]])</span></div>
<div class="line"><span class="lineno"> 2346</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 2347</span>    X = check_array(X, accept_sparse=[<span class="stringliteral">&quot;csc&quot;</span>, <span class="stringliteral">&quot;csr&quot;</span>, <span class="stringliteral">&quot;coo&quot;</span>], dtype=FLOAT_DTYPES)</div>
<div class="line"><span class="lineno"> 2348</span>    n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno"> 2349</span>    shape = (n_samples, n_features + 1)</div>
<div class="line"><span class="lineno"> 2350</span>    <span class="keywordflow">if</span> sparse.issparse(X):</div>
<div class="line"><span class="lineno"> 2351</span>        <span class="keywordflow">if</span> sparse.isspmatrix_coo(X):</div>
<div class="line"><span class="lineno"> 2352</span>            <span class="comment"># Shift columns to the right.</span></div>
<div class="line"><span class="lineno"> 2353</span>            col = X.col + 1</div>
<div class="line"><span class="lineno"> 2354</span>            <span class="comment"># Column indices of dummy feature are 0 everywhere.</span></div>
<div class="line"><span class="lineno"> 2355</span>            col = np.concatenate((np.zeros(n_samples), col))</div>
<div class="line"><span class="lineno"> 2356</span>            <span class="comment"># Row indices of dummy feature are 0, ..., n_samples-1.</span></div>
<div class="line"><span class="lineno"> 2357</span>            row = np.concatenate((np.arange(n_samples), X.row))</div>
<div class="line"><span class="lineno"> 2358</span>            <span class="comment"># Prepend the dummy feature n_samples times.</span></div>
<div class="line"><span class="lineno"> 2359</span>            data = np.concatenate((np.full(n_samples, value), X.data))</div>
<div class="line"><span class="lineno"> 2360</span>            <span class="keywordflow">return</span> sparse.coo_matrix((data, (row, col)), shape)</div>
<div class="line"><span class="lineno"> 2361</span>        <span class="keywordflow">elif</span> sparse.isspmatrix_csc(X):</div>
<div class="line"><span class="lineno"> 2362</span>            <span class="comment"># Shift index pointers since we need to add n_samples elements.</span></div>
<div class="line"><span class="lineno"> 2363</span>            indptr = X.indptr + n_samples</div>
<div class="line"><span class="lineno"> 2364</span>            <span class="comment"># indptr[0] must be 0.</span></div>
<div class="line"><span class="lineno"> 2365</span>            indptr = np.concatenate((np.array([0]), indptr))</div>
<div class="line"><span class="lineno"> 2366</span>            <span class="comment"># Row indices of dummy feature are 0, ..., n_samples-1.</span></div>
<div class="line"><span class="lineno"> 2367</span>            indices = np.concatenate((np.arange(n_samples), X.indices))</div>
<div class="line"><span class="lineno"> 2368</span>            <span class="comment"># Prepend the dummy feature n_samples times.</span></div>
<div class="line"><span class="lineno"> 2369</span>            data = np.concatenate((np.full(n_samples, value), X.data))</div>
<div class="line"><span class="lineno"> 2370</span>            <span class="keywordflow">return</span> sparse.csc_matrix((data, indices, indptr), shape)</div>
<div class="line"><span class="lineno"> 2371</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 2372</span>            klass = X.__class__</div>
<div class="line"><span class="lineno"> 2373</span>            <span class="keywordflow">return</span> klass(add_dummy_feature(X.tocoo(), value))</div>
<div class="line"><span class="lineno"> 2374</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 2375</span>        <span class="keywordflow">return</span> np.hstack((np.full((n_samples, 1), value), X))</div>
<div class="line"><span class="lineno"> 2376</span> </div>
<div class="line"><span class="lineno"> 2377</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a05d8e2d47929e6c7470d93cd18526af4" name="a05d8e2d47929e6c7470d93cd18526af4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a05d8e2d47929e6c7470d93cd18526af4">&#9670;&#160;</a></span>binarize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data.binarize </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>threshold</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Boolean thresholding of array-like or scipy.sparse matrix.

Read more in the :ref:`User Guide &lt;preprocessing_binarization&gt;`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data to binarize, element by element.
    scipy.sparse matrices should be in CSR or CSC format to avoid an
    un-necessary copy.

threshold : float, default=0.0
    Feature values below or equal to this are replaced by 0, above it by 1.
    Threshold may not be less than 0 for operations on sparse matrices.

copy : bool, default=True
    Set to False to perform inplace binarization and avoid a copy
    (if the input is already a numpy array or a scipy.sparse CSR / CSC
    matrix and if axis is 1).

Returns
-------
X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The transformed data.

See Also
--------
Binarizer : Performs binarization using the Transformer API
    (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1992</span><span class="keyword">def </span>binarize(X, *, threshold=0.0, copy=True):</div>
<div class="line"><span class="lineno"> 1993</span>    <span class="stringliteral">&quot;&quot;&quot;Boolean thresholding of array-like or scipy.sparse matrix.</span></div>
<div class="line"><span class="lineno"> 1994</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1995</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;preprocessing_binarization&gt;`.</span></div>
<div class="line"><span class="lineno"> 1996</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1997</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 1998</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 1999</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 2000</span><span class="stringliteral">        The data to binarize, element by element.</span></div>
<div class="line"><span class="lineno"> 2001</span><span class="stringliteral">        scipy.sparse matrices should be in CSR or CSC format to avoid an</span></div>
<div class="line"><span class="lineno"> 2002</span><span class="stringliteral">        un-necessary copy.</span></div>
<div class="line"><span class="lineno"> 2003</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2004</span><span class="stringliteral">    threshold : float, default=0.0</span></div>
<div class="line"><span class="lineno"> 2005</span><span class="stringliteral">        Feature values below or equal to this are replaced by 0, above it by 1.</span></div>
<div class="line"><span class="lineno"> 2006</span><span class="stringliteral">        Threshold may not be less than 0 for operations on sparse matrices.</span></div>
<div class="line"><span class="lineno"> 2007</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2008</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno"> 2009</span><span class="stringliteral">        Set to False to perform inplace binarization and avoid a copy</span></div>
<div class="line"><span class="lineno"> 2010</span><span class="stringliteral">        (if the input is already a numpy array or a scipy.sparse CSR / CSC</span></div>
<div class="line"><span class="lineno"> 2011</span><span class="stringliteral">        matrix and if axis is 1).</span></div>
<div class="line"><span class="lineno"> 2012</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2013</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 2014</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 2015</span><span class="stringliteral">    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 2016</span><span class="stringliteral">        The transformed data.</span></div>
<div class="line"><span class="lineno"> 2017</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2018</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno"> 2019</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 2020</span><span class="stringliteral">    Binarizer : Performs binarization using the Transformer API</span></div>
<div class="line"><span class="lineno"> 2021</span><span class="stringliteral">        (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).</span></div>
<div class="line"><span class="lineno"> 2022</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 2023</span>    X = check_array(X, accept_sparse=[<span class="stringliteral">&quot;csr&quot;</span>, <span class="stringliteral">&quot;csc&quot;</span>], copy=copy)</div>
<div class="line"><span class="lineno"> 2024</span>    <span class="keywordflow">if</span> sparse.issparse(X):</div>
<div class="line"><span class="lineno"> 2025</span>        <span class="keywordflow">if</span> threshold &lt; 0:</div>
<div class="line"><span class="lineno"> 2026</span>            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Cannot binarize a sparse matrix with threshold &lt; 0&quot;</span>)</div>
<div class="line"><span class="lineno"> 2027</span>        cond = X.data &gt; threshold</div>
<div class="line"><span class="lineno"> 2028</span>        not_cond = np.logical_not(cond)</div>
<div class="line"><span class="lineno"> 2029</span>        X.data[cond] = 1</div>
<div class="line"><span class="lineno"> 2030</span>        X.data[not_cond] = 0</div>
<div class="line"><span class="lineno"> 2031</span>        X.eliminate_zeros()</div>
<div class="line"><span class="lineno"> 2032</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 2033</span>        cond = X &gt; threshold</div>
<div class="line"><span class="lineno"> 2034</span>        not_cond = np.logical_not(cond)</div>
<div class="line"><span class="lineno"> 2035</span>        X[cond] = 1</div>
<div class="line"><span class="lineno"> 2036</span>        X[not_cond] = 0</div>
<div class="line"><span class="lineno"> 2037</span>    <span class="keywordflow">return</span> X</div>
<div class="line"><span class="lineno"> 2038</span> </div>
<div class="line"><span class="lineno"> 2039</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ac4b760081274b32aa343cc1d269c2165" name="ac4b760081274b32aa343cc1d269c2165"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac4b760081274b32aa343cc1d269c2165">&#9670;&#160;</a></span>maxabs_scale()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data.maxabs_scale </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>axis</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Scale each feature to the [-1, 1] range without breaking the sparsity.

This estimator scales each feature individually such
that the maximal absolute value of each feature in the
training set will be 1.0.

This scaler can also be applied to sparse CSR or CSC matrices.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data.

axis : int, default=0
    Axis used to scale along. If 0, independently scale each feature,
    otherwise (if 1) scale each sample.

copy : bool, default=True
    Set to False to perform inplace scaling and avoid a copy (if the input
    is already a numpy array).

Returns
-------
X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The transformed data.

.. warning:: Risk of data leak

    Do not use :func:`~sklearn.preprocessing.maxabs_scale` unless you know
    what you are doing. A common mistake is to apply it to the entire data
    *before* splitting into training and test sets. This will bias the
    model evaluation because information would have leaked from the test
    set to the training set.
    In general, we recommend using
    :class:`~sklearn.preprocessing.MaxAbsScaler` within a
    :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data
    leaking: `pipe = make_pipeline(MaxAbsScaler(), LogisticRegression())`.

See Also
--------
MaxAbsScaler : Performs scaling to the [-1, 1] range using
    the Transformer API (e.g. as part of a preprocessing
    :class:`~sklearn.pipeline.Pipeline`).

Notes
-----
NaNs are treated as missing values: disregarded to compute the statistics,
and maintained during the data transformation.

For a comparison of the different scalers, transformers, and normalizers,
see :ref:`examples/preprocessing/plot_all_scaling.py
&lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1280</span><span class="keyword">def </span>maxabs_scale(X, *, axis=0, copy=True):</div>
<div class="line"><span class="lineno"> 1281</span>    <span class="stringliteral">&quot;&quot;&quot;Scale each feature to the [-1, 1] range without breaking the sparsity.</span></div>
<div class="line"><span class="lineno"> 1282</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1283</span><span class="stringliteral">    This estimator scales each feature individually such</span></div>
<div class="line"><span class="lineno"> 1284</span><span class="stringliteral">    that the maximal absolute value of each feature in the</span></div>
<div class="line"><span class="lineno"> 1285</span><span class="stringliteral">    training set will be 1.0.</span></div>
<div class="line"><span class="lineno"> 1286</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1287</span><span class="stringliteral">    This scaler can also be applied to sparse CSR or CSC matrices.</span></div>
<div class="line"><span class="lineno"> 1288</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1289</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 1290</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 1291</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 1292</span><span class="stringliteral">        The data.</span></div>
<div class="line"><span class="lineno"> 1293</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1294</span><span class="stringliteral">    axis : int, default=0</span></div>
<div class="line"><span class="lineno"> 1295</span><span class="stringliteral">        Axis used to scale along. If 0, independently scale each feature,</span></div>
<div class="line"><span class="lineno"> 1296</span><span class="stringliteral">        otherwise (if 1) scale each sample.</span></div>
<div class="line"><span class="lineno"> 1297</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1298</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno"> 1299</span><span class="stringliteral">        Set to False to perform inplace scaling and avoid a copy (if the input</span></div>
<div class="line"><span class="lineno"> 1300</span><span class="stringliteral">        is already a numpy array).</span></div>
<div class="line"><span class="lineno"> 1301</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1302</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 1303</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 1304</span><span class="stringliteral">    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 1305</span><span class="stringliteral">        The transformed data.</span></div>
<div class="line"><span class="lineno"> 1306</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1307</span><span class="stringliteral">    .. warning:: Risk of data leak</span></div>
<div class="line"><span class="lineno"> 1308</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1309</span><span class="stringliteral">        Do not use :func:`~sklearn.preprocessing.maxabs_scale` unless you know</span></div>
<div class="line"><span class="lineno"> 1310</span><span class="stringliteral">        what you are doing. A common mistake is to apply it to the entire data</span></div>
<div class="line"><span class="lineno"> 1311</span><span class="stringliteral">        *before* splitting into training and test sets. This will bias the</span></div>
<div class="line"><span class="lineno"> 1312</span><span class="stringliteral">        model evaluation because information would have leaked from the test</span></div>
<div class="line"><span class="lineno"> 1313</span><span class="stringliteral">        set to the training set.</span></div>
<div class="line"><span class="lineno"> 1314</span><span class="stringliteral">        In general, we recommend using</span></div>
<div class="line"><span class="lineno"> 1315</span><span class="stringliteral">        :class:`~sklearn.preprocessing.MaxAbsScaler` within a</span></div>
<div class="line"><span class="lineno"> 1316</span><span class="stringliteral">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span></div>
<div class="line"><span class="lineno"> 1317</span><span class="stringliteral">        leaking: `pipe = make_pipeline(MaxAbsScaler(), LogisticRegression())`.</span></div>
<div class="line"><span class="lineno"> 1318</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1319</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno"> 1320</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 1321</span><span class="stringliteral">    MaxAbsScaler : Performs scaling to the [-1, 1] range using</span></div>
<div class="line"><span class="lineno"> 1322</span><span class="stringliteral">        the Transformer API (e.g. as part of a preprocessing</span></div>
<div class="line"><span class="lineno"> 1323</span><span class="stringliteral">        :class:`~sklearn.pipeline.Pipeline`).</span></div>
<div class="line"><span class="lineno"> 1324</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1325</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno"> 1326</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno"> 1327</span><span class="stringliteral">    NaNs are treated as missing values: disregarded to compute the statistics,</span></div>
<div class="line"><span class="lineno"> 1328</span><span class="stringliteral">    and maintained during the data transformation.</span></div>
<div class="line"><span class="lineno"> 1329</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1330</span><span class="stringliteral">    For a comparison of the different scalers, transformers, and normalizers,</span></div>
<div class="line"><span class="lineno"> 1331</span><span class="stringliteral">    see :ref:`examples/preprocessing/plot_all_scaling.py</span></div>
<div class="line"><span class="lineno"> 1332</span><span class="stringliteral">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span></div>
<div class="line"><span class="lineno"> 1333</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1334</span>    <span class="comment"># Unlike the scaler object, this function allows 1d input.</span></div>
<div class="line"><span class="lineno"> 1335</span> </div>
<div class="line"><span class="lineno"> 1336</span>    <span class="comment"># If copy is required, it will be done inside the scaler object.</span></div>
<div class="line"><span class="lineno"> 1337</span>    X = check_array(</div>
<div class="line"><span class="lineno"> 1338</span>        X,</div>
<div class="line"><span class="lineno"> 1339</span>        accept_sparse=(<span class="stringliteral">&quot;csr&quot;</span>, <span class="stringliteral">&quot;csc&quot;</span>),</div>
<div class="line"><span class="lineno"> 1340</span>        copy=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1341</span>        ensure_2d=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1342</span>        dtype=FLOAT_DTYPES,</div>
<div class="line"><span class="lineno"> 1343</span>        force_all_finite=<span class="stringliteral">&quot;allow-nan&quot;</span>,</div>
<div class="line"><span class="lineno"> 1344</span>    )</div>
<div class="line"><span class="lineno"> 1345</span>    original_ndim = X.ndim</div>
<div class="line"><span class="lineno"> 1346</span> </div>
<div class="line"><span class="lineno"> 1347</span>    <span class="keywordflow">if</span> original_ndim == 1:</div>
<div class="line"><span class="lineno"> 1348</span>        X = X.reshape(X.shape[0], 1)</div>
<div class="line"><span class="lineno"> 1349</span> </div>
<div class="line"><span class="lineno"> 1350</span>    s = MaxAbsScaler(copy=copy)</div>
<div class="line"><span class="lineno"> 1351</span>    <span class="keywordflow">if</span> axis == 0:</div>
<div class="line"><span class="lineno"> 1352</span>        X = s.fit_transform(X)</div>
<div class="line"><span class="lineno"> 1353</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1354</span>        X = s.fit_transform(X.T).T</div>
<div class="line"><span class="lineno"> 1355</span> </div>
<div class="line"><span class="lineno"> 1356</span>    <span class="keywordflow">if</span> original_ndim == 1:</div>
<div class="line"><span class="lineno"> 1357</span>        X = X.ravel()</div>
<div class="line"><span class="lineno"> 1358</span> </div>
<div class="line"><span class="lineno"> 1359</span>    <span class="keywordflow">return</span> X</div>
<div class="line"><span class="lineno"> 1360</span> </div>
<div class="line"><span class="lineno"> 1361</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ace15f3fc63a7ec0af22fb230ab23cc30" name="ace15f3fc63a7ec0af22fb230ab23cc30"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ace15f3fc63a7ec0af22fb230ab23cc30">&#9670;&#160;</a></span>minmax_scale()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data.minmax_scale </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>feature_range</em> = <code>(0,&#160;1)</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>axis</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Transform features by scaling each feature to a given range.

This estimator scales and translates each feature individually such
that it is in the given range on the training set, i.e. between
zero and one.

The transformation is given by (when ``axis=0``)::

    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
    X_scaled = X_std * (max - min) + min

where min, max = feature_range.

The transformation is calculated as (when ``axis=0``)::

   X_scaled = scale * X + min - X.min(axis=0) * scale
   where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))

This transformation is often used as an alternative to zero mean,
unit variance scaling.

Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.

.. versionadded:: 0.17
   *minmax_scale* function interface
   to :class:`~sklearn.preprocessing.MinMaxScaler`.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    The data.

feature_range : tuple (min, max), default=(0, 1)
    Desired range of transformed data.

axis : int, default=0
    Axis used to scale along. If 0, independently scale each feature,
    otherwise (if 1) scale each sample.

copy : bool, default=True
    Set to False to perform inplace scaling and avoid a copy (if the input
    is already a numpy array).

Returns
-------
X_tr : ndarray of shape (n_samples, n_features)
    The transformed data.

.. warning:: Risk of data leak

    Do not use :func:`~sklearn.preprocessing.minmax_scale` unless you know
    what you are doing. A common mistake is to apply it to the entire data
    *before* splitting into training and test sets. This will bias the
    model evaluation because information would have leaked from the test
    set to the training set.
    In general, we recommend using
    :class:`~sklearn.preprocessing.MinMaxScaler` within a
    :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data
    leaking: `pipe = make_pipeline(MinMaxScaler(), LogisticRegression())`.

See Also
--------
MinMaxScaler : Performs scaling to a given range using the Transformer
    API (e.g. as part of a preprocessing
    :class:`~sklearn.pipeline.Pipeline`).

Notes
-----
For a comparison of the different scalers, transformers, and normalizers,
see :ref:`examples/preprocessing/plot_all_scaling.py
&lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.
</pre> <div class="fragment"><div class="line"><span class="lineno">  549</span><span class="keyword">def </span>minmax_scale(X, feature_range=(0, 1), *, axis=0, copy=<span class="keyword">True</span>):</div>
<div class="line"><span class="lineno">  550</span>    <span class="stringliteral">&quot;&quot;&quot;Transform features by scaling each feature to a given range.</span></div>
<div class="line"><span class="lineno">  551</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  552</span><span class="stringliteral">    This estimator scales and translates each feature individually such</span></div>
<div class="line"><span class="lineno">  553</span><span class="stringliteral">    that it is in the given range on the training set, i.e. between</span></div>
<div class="line"><span class="lineno">  554</span><span class="stringliteral">    zero and one.</span></div>
<div class="line"><span class="lineno">  555</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  556</span><span class="stringliteral">    The transformation is given by (when ``axis=0``)::</span></div>
<div class="line"><span class="lineno">  557</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  558</span><span class="stringliteral">        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))</span></div>
<div class="line"><span class="lineno">  559</span><span class="stringliteral">        X_scaled = X_std * (max - min) + min</span></div>
<div class="line"><span class="lineno">  560</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  561</span><span class="stringliteral">    where min, max = feature_range.</span></div>
<div class="line"><span class="lineno">  562</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  563</span><span class="stringliteral">    The transformation is calculated as (when ``axis=0``)::</span></div>
<div class="line"><span class="lineno">  564</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  565</span><span class="stringliteral">       X_scaled = scale * X + min - X.min(axis=0) * scale</span></div>
<div class="line"><span class="lineno">  566</span><span class="stringliteral">       where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))</span></div>
<div class="line"><span class="lineno">  567</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  568</span><span class="stringliteral">    This transformation is often used as an alternative to zero mean,</span></div>
<div class="line"><span class="lineno">  569</span><span class="stringliteral">    unit variance scaling.</span></div>
<div class="line"><span class="lineno">  570</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  571</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.</span></div>
<div class="line"><span class="lineno">  572</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  573</span><span class="stringliteral">    .. versionadded:: 0.17</span></div>
<div class="line"><span class="lineno">  574</span><span class="stringliteral">       *minmax_scale* function interface</span></div>
<div class="line"><span class="lineno">  575</span><span class="stringliteral">       to :class:`~sklearn.preprocessing.MinMaxScaler`.</span></div>
<div class="line"><span class="lineno">  576</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  577</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  578</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  579</span><span class="stringliteral">    X : array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  580</span><span class="stringliteral">        The data.</span></div>
<div class="line"><span class="lineno">  581</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  582</span><span class="stringliteral">    feature_range : tuple (min, max), default=(0, 1)</span></div>
<div class="line"><span class="lineno">  583</span><span class="stringliteral">        Desired range of transformed data.</span></div>
<div class="line"><span class="lineno">  584</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  585</span><span class="stringliteral">    axis : int, default=0</span></div>
<div class="line"><span class="lineno">  586</span><span class="stringliteral">        Axis used to scale along. If 0, independently scale each feature,</span></div>
<div class="line"><span class="lineno">  587</span><span class="stringliteral">        otherwise (if 1) scale each sample.</span></div>
<div class="line"><span class="lineno">  588</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  589</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno">  590</span><span class="stringliteral">        Set to False to perform inplace scaling and avoid a copy (if the input</span></div>
<div class="line"><span class="lineno">  591</span><span class="stringliteral">        is already a numpy array).</span></div>
<div class="line"><span class="lineno">  592</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  593</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  594</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  595</span><span class="stringliteral">    X_tr : ndarray of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  596</span><span class="stringliteral">        The transformed data.</span></div>
<div class="line"><span class="lineno">  597</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  598</span><span class="stringliteral">    .. warning:: Risk of data leak</span></div>
<div class="line"><span class="lineno">  599</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  600</span><span class="stringliteral">        Do not use :func:`~sklearn.preprocessing.minmax_scale` unless you know</span></div>
<div class="line"><span class="lineno">  601</span><span class="stringliteral">        what you are doing. A common mistake is to apply it to the entire data</span></div>
<div class="line"><span class="lineno">  602</span><span class="stringliteral">        *before* splitting into training and test sets. This will bias the</span></div>
<div class="line"><span class="lineno">  603</span><span class="stringliteral">        model evaluation because information would have leaked from the test</span></div>
<div class="line"><span class="lineno">  604</span><span class="stringliteral">        set to the training set.</span></div>
<div class="line"><span class="lineno">  605</span><span class="stringliteral">        In general, we recommend using</span></div>
<div class="line"><span class="lineno">  606</span><span class="stringliteral">        :class:`~sklearn.preprocessing.MinMaxScaler` within a</span></div>
<div class="line"><span class="lineno">  607</span><span class="stringliteral">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span></div>
<div class="line"><span class="lineno">  608</span><span class="stringliteral">        leaking: `pipe = make_pipeline(MinMaxScaler(), LogisticRegression())`.</span></div>
<div class="line"><span class="lineno">  609</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  610</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  611</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  612</span><span class="stringliteral">    MinMaxScaler : Performs scaling to a given range using the Transformer</span></div>
<div class="line"><span class="lineno">  613</span><span class="stringliteral">        API (e.g. as part of a preprocessing</span></div>
<div class="line"><span class="lineno">  614</span><span class="stringliteral">        :class:`~sklearn.pipeline.Pipeline`).</span></div>
<div class="line"><span class="lineno">  615</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  616</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  617</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  618</span><span class="stringliteral">    For a comparison of the different scalers, transformers, and normalizers,</span></div>
<div class="line"><span class="lineno">  619</span><span class="stringliteral">    see :ref:`examples/preprocessing/plot_all_scaling.py</span></div>
<div class="line"><span class="lineno">  620</span><span class="stringliteral">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span></div>
<div class="line"><span class="lineno">  621</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  622</span>    <span class="comment"># Unlike the scaler object, this function allows 1d input.</span></div>
<div class="line"><span class="lineno">  623</span>    <span class="comment"># If copy is required, it will be done inside the scaler object.</span></div>
<div class="line"><span class="lineno">  624</span>    X = check_array(</div>
<div class="line"><span class="lineno">  625</span>        X, copy=<span class="keyword">False</span>, ensure_2d=<span class="keyword">False</span>, dtype=FLOAT_DTYPES, force_all_finite=<span class="stringliteral">&quot;allow-nan&quot;</span></div>
<div class="line"><span class="lineno">  626</span>    )</div>
<div class="line"><span class="lineno">  627</span>    original_ndim = X.ndim</div>
<div class="line"><span class="lineno">  628</span> </div>
<div class="line"><span class="lineno">  629</span>    <span class="keywordflow">if</span> original_ndim == 1:</div>
<div class="line"><span class="lineno">  630</span>        X = X.reshape(X.shape[0], 1)</div>
<div class="line"><span class="lineno">  631</span> </div>
<div class="line"><span class="lineno">  632</span>    s = MinMaxScaler(feature_range=feature_range, copy=copy)</div>
<div class="line"><span class="lineno">  633</span>    <span class="keywordflow">if</span> axis == 0:</div>
<div class="line"><span class="lineno">  634</span>        X = s.fit_transform(X)</div>
<div class="line"><span class="lineno">  635</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  636</span>        X = s.fit_transform(X.T).T</div>
<div class="line"><span class="lineno">  637</span> </div>
<div class="line"><span class="lineno">  638</span>    <span class="keywordflow">if</span> original_ndim == 1:</div>
<div class="line"><span class="lineno">  639</span>        X = X.ravel()</div>
<div class="line"><span class="lineno">  640</span> </div>
<div class="line"><span class="lineno">  641</span>    <span class="keywordflow">return</span> X</div>
<div class="line"><span class="lineno">  642</span> </div>
<div class="line"><span class="lineno">  643</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a7bacbfb60137f112c90028a3cc917767" name="a7bacbfb60137f112c90028a3cc917767"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7bacbfb60137f112c90028a3cc917767">&#9670;&#160;</a></span>normalize()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data.normalize </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>norm</em> = <code>&quot;l2&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>axis</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>return_norm</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Scale input vectors individually to unit norm (vector length).

Read more in the :ref:`User Guide &lt;preprocessing_normalization&gt;`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data to normalize, element by element.
    scipy.sparse matrices should be in CSR format to avoid an
    un-necessary copy.

norm : {'l1', 'l2', 'max'}, default='l2'
    The norm to use to normalize each non zero sample (or each non-zero
    feature if axis is 0).

axis : {0, 1}, default=1
    Define axis used to normalize the data along. If 1, independently
    normalize each sample, otherwise (if 0) normalize each feature.

copy : bool, default=True
    Set to False to perform inplace row normalization and avoid a
    copy (if the input is already a numpy array or a scipy.sparse
    CSR matrix and if axis is 1).

return_norm : bool, default=False
    Whether to return the computed norms.

Returns
-------
X : {ndarray, sparse matrix} of shape (n_samples, n_features)
    Normalized input X.

norms : ndarray of shape (n_samples, ) if axis=1 else (n_features, )
    An array of norms along given axis for X.
    When X is sparse, a NotImplementedError will be raised
    for norm 'l1' or 'l2'.

See Also
--------
Normalizer : Performs normalization using the Transformer API
    (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).

Notes
-----
For a comparison of the different scalers, transformers, and normalizers,
see :ref:`examples/preprocessing/plot_all_scaling.py
&lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1758</span><span class="keyword">def </span>normalize(X, norm=&quot;l2&quot;, *, axis=1, copy=True, return_norm=False):</div>
<div class="line"><span class="lineno"> 1759</span>    <span class="stringliteral">&quot;&quot;&quot;Scale input vectors individually to unit norm (vector length).</span></div>
<div class="line"><span class="lineno"> 1760</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1761</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;preprocessing_normalization&gt;`.</span></div>
<div class="line"><span class="lineno"> 1762</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1763</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 1764</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 1765</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 1766</span><span class="stringliteral">        The data to normalize, element by element.</span></div>
<div class="line"><span class="lineno"> 1767</span><span class="stringliteral">        scipy.sparse matrices should be in CSR format to avoid an</span></div>
<div class="line"><span class="lineno"> 1768</span><span class="stringliteral">        un-necessary copy.</span></div>
<div class="line"><span class="lineno"> 1769</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1770</span><span class="stringliteral">    norm : {&#39;l1&#39;, &#39;l2&#39;, &#39;max&#39;}, default=&#39;l2&#39;</span></div>
<div class="line"><span class="lineno"> 1771</span><span class="stringliteral">        The norm to use to normalize each non zero sample (or each non-zero</span></div>
<div class="line"><span class="lineno"> 1772</span><span class="stringliteral">        feature if axis is 0).</span></div>
<div class="line"><span class="lineno"> 1773</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1774</span><span class="stringliteral">    axis : {0, 1}, default=1</span></div>
<div class="line"><span class="lineno"> 1775</span><span class="stringliteral">        Define axis used to normalize the data along. If 1, independently</span></div>
<div class="line"><span class="lineno"> 1776</span><span class="stringliteral">        normalize each sample, otherwise (if 0) normalize each feature.</span></div>
<div class="line"><span class="lineno"> 1777</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1778</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno"> 1779</span><span class="stringliteral">        Set to False to perform inplace row normalization and avoid a</span></div>
<div class="line"><span class="lineno"> 1780</span><span class="stringliteral">        copy (if the input is already a numpy array or a scipy.sparse</span></div>
<div class="line"><span class="lineno"> 1781</span><span class="stringliteral">        CSR matrix and if axis is 1).</span></div>
<div class="line"><span class="lineno"> 1782</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1783</span><span class="stringliteral">    return_norm : bool, default=False</span></div>
<div class="line"><span class="lineno"> 1784</span><span class="stringliteral">        Whether to return the computed norms.</span></div>
<div class="line"><span class="lineno"> 1785</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1786</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 1787</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 1788</span><span class="stringliteral">    X : {ndarray, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 1789</span><span class="stringliteral">        Normalized input X.</span></div>
<div class="line"><span class="lineno"> 1790</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1791</span><span class="stringliteral">    norms : ndarray of shape (n_samples, ) if axis=1 else (n_features, )</span></div>
<div class="line"><span class="lineno"> 1792</span><span class="stringliteral">        An array of norms along given axis for X.</span></div>
<div class="line"><span class="lineno"> 1793</span><span class="stringliteral">        When X is sparse, a NotImplementedError will be raised</span></div>
<div class="line"><span class="lineno"> 1794</span><span class="stringliteral">        for norm &#39;l1&#39; or &#39;l2&#39;.</span></div>
<div class="line"><span class="lineno"> 1795</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1796</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno"> 1797</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 1798</span><span class="stringliteral">    Normalizer : Performs normalization using the Transformer API</span></div>
<div class="line"><span class="lineno"> 1799</span><span class="stringliteral">        (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).</span></div>
<div class="line"><span class="lineno"> 1800</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1801</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno"> 1802</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno"> 1803</span><span class="stringliteral">    For a comparison of the different scalers, transformers, and normalizers,</span></div>
<div class="line"><span class="lineno"> 1804</span><span class="stringliteral">    see :ref:`examples/preprocessing/plot_all_scaling.py</span></div>
<div class="line"><span class="lineno"> 1805</span><span class="stringliteral">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span></div>
<div class="line"><span class="lineno"> 1806</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1807</span>    <span class="keywordflow">if</span> norm <span class="keywordflow">not</span> <span class="keywordflow">in</span> (<span class="stringliteral">&quot;l1&quot;</span>, <span class="stringliteral">&quot;l2&quot;</span>, <span class="stringliteral">&quot;max&quot;</span>):</div>
<div class="line"><span class="lineno"> 1808</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;&#39;%s&#39; is not a supported norm&quot;</span> % norm)</div>
<div class="line"><span class="lineno"> 1809</span> </div>
<div class="line"><span class="lineno"> 1810</span>    <span class="keywordflow">if</span> axis == 0:</div>
<div class="line"><span class="lineno"> 1811</span>        sparse_format = <span class="stringliteral">&quot;csc&quot;</span></div>
<div class="line"><span class="lineno"> 1812</span>    <span class="keywordflow">elif</span> axis == 1:</div>
<div class="line"><span class="lineno"> 1813</span>        sparse_format = <span class="stringliteral">&quot;csr&quot;</span></div>
<div class="line"><span class="lineno"> 1814</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1815</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;&#39;%d&#39; is not a supported axis&quot;</span> % axis)</div>
<div class="line"><span class="lineno"> 1816</span> </div>
<div class="line"><span class="lineno"> 1817</span>    X = check_array(</div>
<div class="line"><span class="lineno"> 1818</span>        X,</div>
<div class="line"><span class="lineno"> 1819</span>        accept_sparse=sparse_format,</div>
<div class="line"><span class="lineno"> 1820</span>        copy=copy,</div>
<div class="line"><span class="lineno"> 1821</span>        estimator=<span class="stringliteral">&quot;the normalize function&quot;</span>,</div>
<div class="line"><span class="lineno"> 1822</span>        dtype=FLOAT_DTYPES,</div>
<div class="line"><span class="lineno"> 1823</span>    )</div>
<div class="line"><span class="lineno"> 1824</span>    <span class="keywordflow">if</span> axis == 0:</div>
<div class="line"><span class="lineno"> 1825</span>        X = X.T</div>
<div class="line"><span class="lineno"> 1826</span> </div>
<div class="line"><span class="lineno"> 1827</span>    <span class="keywordflow">if</span> sparse.issparse(X):</div>
<div class="line"><span class="lineno"> 1828</span>        <span class="keywordflow">if</span> return_norm <span class="keywordflow">and</span> norm <span class="keywordflow">in</span> (<span class="stringliteral">&quot;l1&quot;</span>, <span class="stringliteral">&quot;l2&quot;</span>):</div>
<div class="line"><span class="lineno"> 1829</span>            <span class="keywordflow">raise</span> NotImplementedError(</div>
<div class="line"><span class="lineno"> 1830</span>                <span class="stringliteral">&quot;return_norm=True is not implemented &quot;</span></div>
<div class="line"><span class="lineno"> 1831</span>                <span class="stringliteral">&quot;for sparse matrices with norm &#39;l1&#39; &quot;</span></div>
<div class="line"><span class="lineno"> 1832</span>                <span class="stringliteral">&quot;or norm &#39;l2&#39;&quot;</span></div>
<div class="line"><span class="lineno"> 1833</span>            )</div>
<div class="line"><span class="lineno"> 1834</span>        <span class="keywordflow">if</span> norm == <span class="stringliteral">&quot;l1&quot;</span>:</div>
<div class="line"><span class="lineno"> 1835</span>            inplace_csr_row_normalize_l1(X)</div>
<div class="line"><span class="lineno"> 1836</span>        <span class="keywordflow">elif</span> norm == <span class="stringliteral">&quot;l2&quot;</span>:</div>
<div class="line"><span class="lineno"> 1837</span>            inplace_csr_row_normalize_l2(X)</div>
<div class="line"><span class="lineno"> 1838</span>        <span class="keywordflow">elif</span> norm == <span class="stringliteral">&quot;max&quot;</span>:</div>
<div class="line"><span class="lineno"> 1839</span>            mins, maxes = min_max_axis(X, 1)</div>
<div class="line"><span class="lineno"> 1840</span>            norms = np.maximum(abs(mins), maxes)</div>
<div class="line"><span class="lineno"> 1841</span>            norms_elementwise = norms.repeat(np.diff(X.indptr))</div>
<div class="line"><span class="lineno"> 1842</span>            mask = norms_elementwise != 0</div>
<div class="line"><span class="lineno"> 1843</span>            X.data[mask] /= norms_elementwise[mask]</div>
<div class="line"><span class="lineno"> 1844</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1845</span>        <span class="keywordflow">if</span> norm == <span class="stringliteral">&quot;l1&quot;</span>:</div>
<div class="line"><span class="lineno"> 1846</span>            norms = np.abs(X).sum(axis=1)</div>
<div class="line"><span class="lineno"> 1847</span>        <span class="keywordflow">elif</span> norm == <span class="stringliteral">&quot;l2&quot;</span>:</div>
<div class="line"><span class="lineno"> 1848</span>            norms = row_norms(X)</div>
<div class="line"><span class="lineno"> 1849</span>        <span class="keywordflow">elif</span> norm == <span class="stringliteral">&quot;max&quot;</span>:</div>
<div class="line"><span class="lineno"> 1850</span>            norms = np.max(abs(X), axis=1)</div>
<div class="line"><span class="lineno"> 1851</span>        norms = _handle_zeros_in_scale(norms, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1852</span>        X /= norms[:, np.newaxis]</div>
<div class="line"><span class="lineno"> 1853</span> </div>
<div class="line"><span class="lineno"> 1854</span>    <span class="keywordflow">if</span> axis == 0:</div>
<div class="line"><span class="lineno"> 1855</span>        X = X.T</div>
<div class="line"><span class="lineno"> 1856</span> </div>
<div class="line"><span class="lineno"> 1857</span>    <span class="keywordflow">if</span> return_norm:</div>
<div class="line"><span class="lineno"> 1858</span>        <span class="keywordflow">return</span> X, norms</div>
<div class="line"><span class="lineno"> 1859</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1860</span>        <span class="keywordflow">return</span> X</div>
<div class="line"><span class="lineno"> 1861</span> </div>
<div class="line"><span class="lineno"> 1862</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a3b2d76087caf444ecc19dbf34960fbd7" name="a3b2d76087caf444ecc19dbf34960fbd7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3b2d76087caf444ecc19dbf34960fbd7">&#9670;&#160;</a></span>power_transform()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data.power_transform </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>method</em> = <code>&quot;yeo-johnson&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>standardize</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Parametric, monotonic transformation to make data more Gaussian-like.

Power transforms are a family of parametric, monotonic transformations
that are applied to make data more Gaussian-like. This is useful for
modeling issues related to heteroscedasticity (non-constant variance),
or other situations where normality is desired.

Currently, power_transform supports the Box-Cox transform and the
Yeo-Johnson transform. The optimal parameter for stabilizing variance and
minimizing skewness is estimated through maximum likelihood.

Box-Cox requires input data to be strictly positive, while Yeo-Johnson
supports both positive or negative data.

By default, zero-mean, unit-variance normalization is applied to the
transformed data.

Read more in the :ref:`User Guide &lt;preprocessing_transformer&gt;`.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    The data to be transformed using a power transformation.

method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'
    The power transform method. Available methods are:

    - 'yeo-johnson' [1]_, works with positive and negative values
    - 'box-cox' [2]_, only works with strictly positive values

    .. versionchanged:: 0.23
        The default value of the `method` parameter changed from
        'box-cox' to 'yeo-johnson' in 0.23.

standardize : bool, default=True
    Set to True to apply zero-mean, unit-variance normalization to the
    transformed output.

copy : bool, default=True
    Set to False to perform inplace computation during transformation.

Returns
-------
X_trans : ndarray of shape (n_samples, n_features)
    The transformed data.

See Also
--------
PowerTransformer : Equivalent transformation with the
    Transformer API (e.g. as part of a preprocessing
    :class:`~sklearn.pipeline.Pipeline`).

quantile_transform : Maps data to a standard normal distribution with
    the parameter `output_distribution='normal'`.

Notes
-----
NaNs are treated as missing values: disregarded in ``fit``, and maintained
in ``transform``.

For a comparison of the different scalers, transformers, and normalizers,
see :ref:`examples/preprocessing/plot_all_scaling.py
&lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.

References
----------

.. [1] I.K. Yeo and R.A. Johnson, "A new family of power transformations to
       improve normality or symmetry." Biometrika, 87(4), pp.954-959,
       (2000).

.. [2] G.E.P. Box and D.R. Cox, "An Analysis of Transformations", Journal
       of the Royal Statistical Society B, 26, 211-252 (1964).

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.preprocessing import power_transform
&gt;&gt;&gt; data = [[1, 2], [3, 2], [4, 5]]
&gt;&gt;&gt; print(power_transform(data, method='box-cox'))
[[-1.332... -0.707...]
 [ 0.256... -0.707...]
 [ 1.076...  1.414...]]

.. warning:: Risk of data leak.
    Do not use :func:`~sklearn.preprocessing.power_transform` unless you
    know what you are doing. A common mistake is to apply it to the entire
    data *before* splitting into training and test sets. This will bias the
    model evaluation because information would have leaked from the test
    set to the training set.
    In general, we recommend using
    :class:`~sklearn.preprocessing.PowerTransformer` within a
    :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data
    leaking, e.g.: `pipe = make_pipeline(PowerTransformer(),
    LogisticRegression())`.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 3354</span><span class="keyword">def </span>power_transform(X, method=&quot;yeo-johnson&quot;, *, standardize=True, copy=True):</div>
<div class="line"><span class="lineno"> 3355</span>    <span class="stringliteral">&quot;&quot;&quot;Parametric, monotonic transformation to make data more Gaussian-like.</span></div>
<div class="line"><span class="lineno"> 3356</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3357</span><span class="stringliteral">    Power transforms are a family of parametric, monotonic transformations</span></div>
<div class="line"><span class="lineno"> 3358</span><span class="stringliteral">    that are applied to make data more Gaussian-like. This is useful for</span></div>
<div class="line"><span class="lineno"> 3359</span><span class="stringliteral">    modeling issues related to heteroscedasticity (non-constant variance),</span></div>
<div class="line"><span class="lineno"> 3360</span><span class="stringliteral">    or other situations where normality is desired.</span></div>
<div class="line"><span class="lineno"> 3361</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3362</span><span class="stringliteral">    Currently, power_transform supports the Box-Cox transform and the</span></div>
<div class="line"><span class="lineno"> 3363</span><span class="stringliteral">    Yeo-Johnson transform. The optimal parameter for stabilizing variance and</span></div>
<div class="line"><span class="lineno"> 3364</span><span class="stringliteral">    minimizing skewness is estimated through maximum likelihood.</span></div>
<div class="line"><span class="lineno"> 3365</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3366</span><span class="stringliteral">    Box-Cox requires input data to be strictly positive, while Yeo-Johnson</span></div>
<div class="line"><span class="lineno"> 3367</span><span class="stringliteral">    supports both positive or negative data.</span></div>
<div class="line"><span class="lineno"> 3368</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3369</span><span class="stringliteral">    By default, zero-mean, unit-variance normalization is applied to the</span></div>
<div class="line"><span class="lineno"> 3370</span><span class="stringliteral">    transformed data.</span></div>
<div class="line"><span class="lineno"> 3371</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3372</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;preprocessing_transformer&gt;`.</span></div>
<div class="line"><span class="lineno"> 3373</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3374</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 3375</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 3376</span><span class="stringliteral">    X : array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 3377</span><span class="stringliteral">        The data to be transformed using a power transformation.</span></div>
<div class="line"><span class="lineno"> 3378</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3379</span><span class="stringliteral">    method : {&#39;yeo-johnson&#39;, &#39;box-cox&#39;}, default=&#39;yeo-johnson&#39;</span></div>
<div class="line"><span class="lineno"> 3380</span><span class="stringliteral">        The power transform method. Available methods are:</span></div>
<div class="line"><span class="lineno"> 3381</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3382</span><span class="stringliteral">        - &#39;yeo-johnson&#39; [1]_, works with positive and negative values</span></div>
<div class="line"><span class="lineno"> 3383</span><span class="stringliteral">        - &#39;box-cox&#39; [2]_, only works with strictly positive values</span></div>
<div class="line"><span class="lineno"> 3384</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3385</span><span class="stringliteral">        .. versionchanged:: 0.23</span></div>
<div class="line"><span class="lineno"> 3386</span><span class="stringliteral">            The default value of the `method` parameter changed from</span></div>
<div class="line"><span class="lineno"> 3387</span><span class="stringliteral">            &#39;box-cox&#39; to &#39;yeo-johnson&#39; in 0.23.</span></div>
<div class="line"><span class="lineno"> 3388</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3389</span><span class="stringliteral">    standardize : bool, default=True</span></div>
<div class="line"><span class="lineno"> 3390</span><span class="stringliteral">        Set to True to apply zero-mean, unit-variance normalization to the</span></div>
<div class="line"><span class="lineno"> 3391</span><span class="stringliteral">        transformed output.</span></div>
<div class="line"><span class="lineno"> 3392</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3393</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno"> 3394</span><span class="stringliteral">        Set to False to perform inplace computation during transformation.</span></div>
<div class="line"><span class="lineno"> 3395</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3396</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 3397</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 3398</span><span class="stringliteral">    X_trans : ndarray of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 3399</span><span class="stringliteral">        The transformed data.</span></div>
<div class="line"><span class="lineno"> 3400</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3401</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno"> 3402</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 3403</span><span class="stringliteral">    PowerTransformer : Equivalent transformation with the</span></div>
<div class="line"><span class="lineno"> 3404</span><span class="stringliteral">        Transformer API (e.g. as part of a preprocessing</span></div>
<div class="line"><span class="lineno"> 3405</span><span class="stringliteral">        :class:`~sklearn.pipeline.Pipeline`).</span></div>
<div class="line"><span class="lineno"> 3406</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3407</span><span class="stringliteral">    quantile_transform : Maps data to a standard normal distribution with</span></div>
<div class="line"><span class="lineno"> 3408</span><span class="stringliteral">        the parameter `output_distribution=&#39;normal&#39;`.</span></div>
<div class="line"><span class="lineno"> 3409</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3410</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno"> 3411</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno"> 3412</span><span class="stringliteral">    NaNs are treated as missing values: disregarded in ``fit``, and maintained</span></div>
<div class="line"><span class="lineno"> 3413</span><span class="stringliteral">    in ``transform``.</span></div>
<div class="line"><span class="lineno"> 3414</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3415</span><span class="stringliteral">    For a comparison of the different scalers, transformers, and normalizers,</span></div>
<div class="line"><span class="lineno"> 3416</span><span class="stringliteral">    see :ref:`examples/preprocessing/plot_all_scaling.py</span></div>
<div class="line"><span class="lineno"> 3417</span><span class="stringliteral">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span></div>
<div class="line"><span class="lineno"> 3418</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3419</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno"> 3420</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 3421</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3422</span><span class="stringliteral">    .. [1] I.K. Yeo and R.A. Johnson, &quot;A new family of power transformations to</span></div>
<div class="line"><span class="lineno"> 3423</span><span class="stringliteral">           improve normality or symmetry.&quot; Biometrika, 87(4), pp.954-959,</span></div>
<div class="line"><span class="lineno"> 3424</span><span class="stringliteral">           (2000).</span></div>
<div class="line"><span class="lineno"> 3425</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3426</span><span class="stringliteral">    .. [2] G.E.P. Box and D.R. Cox, &quot;An Analysis of Transformations&quot;, Journal</span></div>
<div class="line"><span class="lineno"> 3427</span><span class="stringliteral">           of the Royal Statistical Society B, 26, 211-252 (1964).</span></div>
<div class="line"><span class="lineno"> 3428</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3429</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno"> 3430</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 3431</span><span class="stringliteral">    &gt;&gt;&gt; import numpy as np</span></div>
<div class="line"><span class="lineno"> 3432</span><span class="stringliteral">    &gt;&gt;&gt; from sklearn.preprocessing import power_transform</span></div>
<div class="line"><span class="lineno"> 3433</span><span class="stringliteral">    &gt;&gt;&gt; data = [[1, 2], [3, 2], [4, 5]]</span></div>
<div class="line"><span class="lineno"> 3434</span><span class="stringliteral">    &gt;&gt;&gt; print(power_transform(data, method=&#39;box-cox&#39;))</span></div>
<div class="line"><span class="lineno"> 3435</span><span class="stringliteral">    [[-1.332... -0.707...]</span></div>
<div class="line"><span class="lineno"> 3436</span><span class="stringliteral">     [ 0.256... -0.707...]</span></div>
<div class="line"><span class="lineno"> 3437</span><span class="stringliteral">     [ 1.076...  1.414...]]</span></div>
<div class="line"><span class="lineno"> 3438</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 3439</span><span class="stringliteral">    .. warning:: Risk of data leak.</span></div>
<div class="line"><span class="lineno"> 3440</span><span class="stringliteral">        Do not use :func:`~sklearn.preprocessing.power_transform` unless you</span></div>
<div class="line"><span class="lineno"> 3441</span><span class="stringliteral">        know what you are doing. A common mistake is to apply it to the entire</span></div>
<div class="line"><span class="lineno"> 3442</span><span class="stringliteral">        data *before* splitting into training and test sets. This will bias the</span></div>
<div class="line"><span class="lineno"> 3443</span><span class="stringliteral">        model evaluation because information would have leaked from the test</span></div>
<div class="line"><span class="lineno"> 3444</span><span class="stringliteral">        set to the training set.</span></div>
<div class="line"><span class="lineno"> 3445</span><span class="stringliteral">        In general, we recommend using</span></div>
<div class="line"><span class="lineno"> 3446</span><span class="stringliteral">        :class:`~sklearn.preprocessing.PowerTransformer` within a</span></div>
<div class="line"><span class="lineno"> 3447</span><span class="stringliteral">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span></div>
<div class="line"><span class="lineno"> 3448</span><span class="stringliteral">        leaking, e.g.: `pipe = make_pipeline(PowerTransformer(),</span></div>
<div class="line"><span class="lineno"> 3449</span><span class="stringliteral">        LogisticRegression())`.</span></div>
<div class="line"><span class="lineno"> 3450</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 3451</span>    pt = PowerTransformer(method=method, standardize=standardize, copy=copy)</div>
<div class="line"><span class="lineno"> 3452</span>    <span class="keywordflow">return</span> pt.fit_transform(X)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a6839a4a003c881de83d7b1794b8399a4" name="a6839a4a003c881de83d7b1794b8399a4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6839a4a003c881de83d7b1794b8399a4">&#9670;&#160;</a></span>quantile_transform()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data.quantile_transform </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>axis</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_quantiles</em> = <code>1000</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>output_distribution</em> = <code>&quot;uniform&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ignore_implicit_zeros</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>subsample</em> = <code>int(1e5)</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Transform features using quantiles information.

This method transforms the features to follow a uniform or a normal
distribution. Therefore, for a given feature, this transformation tends
to spread out the most frequent values. It also reduces the impact of
(marginal) outliers: this is therefore a robust preprocessing scheme.

The transformation is applied on each feature independently. First an
estimate of the cumulative distribution function of a feature is
used to map the original values to a uniform distribution. The obtained
values are then mapped to the desired output distribution using the
associated quantile function. Features values of new/unseen data that fall
below or above the fitted range will be mapped to the bounds of the output
distribution. Note that this transform is non-linear. It may distort linear
correlations between variables measured at the same scale but renders
variables measured at different scales more directly comparable.

Read more in the :ref:`User Guide &lt;preprocessing_transformer&gt;`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data to transform.

axis : int, default=0
    Axis used to compute the means and standard deviations along. If 0,
    transform each feature, otherwise (if 1) transform each sample.

n_quantiles : int, default=1000 or n_samples
    Number of quantiles to be computed. It corresponds to the number
    of landmarks used to discretize the cumulative distribution function.
    If n_quantiles is larger than the number of samples, n_quantiles is set
    to the number of samples as a larger number of quantiles does not give
    a better approximation of the cumulative distribution function
    estimator.

output_distribution : {'uniform', 'normal'}, default='uniform'
    Marginal distribution for the transformed data. The choices are
    'uniform' (default) or 'normal'.

ignore_implicit_zeros : bool, default=False
    Only applies to sparse matrices. If True, the sparse entries of the
    matrix are discarded to compute the quantile statistics. If False,
    these entries are treated as zeros.

subsample : int, default=1e5
    Maximum number of samples used to estimate the quantiles for
    computational efficiency. Note that the subsampling procedure may
    differ for value-identical sparse and dense matrices.

random_state : int, RandomState instance or None, default=None
    Determines random number generation for subsampling and smoothing
    noise.
    Please see ``subsample`` for more details.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

copy : bool, default=True
    Set to False to perform inplace transformation and avoid a copy (if the
    input is already a numpy array). If True, a copy of `X` is transformed,
    leaving the original `X` unchanged.

    .. versionchanged:: 0.23
        The default value of `copy` changed from False to True in 0.23.

Returns
-------
Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The transformed data.

See Also
--------
QuantileTransformer : Performs quantile-based scaling using the
    Transformer API (e.g. as part of a preprocessing
    :class:`~sklearn.pipeline.Pipeline`).
power_transform : Maps data to a normal distribution using a
    power transformation.
scale : Performs standardization that is faster, but less robust
    to outliers.
robust_scale : Performs robust standardization that removes the influence
    of outliers but does not put outliers and inliers on the same scale.

Notes
-----
NaNs are treated as missing values: disregarded in fit, and maintained in
transform.

.. warning:: Risk of data leak

    Do not use :func:`~sklearn.preprocessing.quantile_transform` unless
    you know what you are doing. A common mistake is to apply it
    to the entire data *before* splitting into training and
    test sets. This will bias the model evaluation because
    information would have leaked from the test set to the
    training set.
    In general, we recommend using
    :class:`~sklearn.preprocessing.QuantileTransformer` within a
    :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data
    leaking:`pipe = make_pipeline(QuantileTransformer(),
    LogisticRegression())`.

For a comparison of the different scalers, transformers, and normalizers,
see :ref:`examples/preprocessing/plot_all_scaling.py
&lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.preprocessing import quantile_transform
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)
&gt;&gt;&gt; quantile_transform(X, n_quantiles=10, random_state=0, copy=True)
array([...])
</pre> <div class="fragment"><div class="line"><span class="lineno"> 2819</span>):</div>
<div class="line"><span class="lineno"> 2820</span>    <span class="stringliteral">&quot;&quot;&quot;Transform features using quantiles information.</span></div>
<div class="line"><span class="lineno"> 2821</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2822</span><span class="stringliteral">    This method transforms the features to follow a uniform or a normal</span></div>
<div class="line"><span class="lineno"> 2823</span><span class="stringliteral">    distribution. Therefore, for a given feature, this transformation tends</span></div>
<div class="line"><span class="lineno"> 2824</span><span class="stringliteral">    to spread out the most frequent values. It also reduces the impact of</span></div>
<div class="line"><span class="lineno"> 2825</span><span class="stringliteral">    (marginal) outliers: this is therefore a robust preprocessing scheme.</span></div>
<div class="line"><span class="lineno"> 2826</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2827</span><span class="stringliteral">    The transformation is applied on each feature independently. First an</span></div>
<div class="line"><span class="lineno"> 2828</span><span class="stringliteral">    estimate of the cumulative distribution function of a feature is</span></div>
<div class="line"><span class="lineno"> 2829</span><span class="stringliteral">    used to map the original values to a uniform distribution. The obtained</span></div>
<div class="line"><span class="lineno"> 2830</span><span class="stringliteral">    values are then mapped to the desired output distribution using the</span></div>
<div class="line"><span class="lineno"> 2831</span><span class="stringliteral">    associated quantile function. Features values of new/unseen data that fall</span></div>
<div class="line"><span class="lineno"> 2832</span><span class="stringliteral">    below or above the fitted range will be mapped to the bounds of the output</span></div>
<div class="line"><span class="lineno"> 2833</span><span class="stringliteral">    distribution. Note that this transform is non-linear. It may distort linear</span></div>
<div class="line"><span class="lineno"> 2834</span><span class="stringliteral">    correlations between variables measured at the same scale but renders</span></div>
<div class="line"><span class="lineno"> 2835</span><span class="stringliteral">    variables measured at different scales more directly comparable.</span></div>
<div class="line"><span class="lineno"> 2836</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2837</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;preprocessing_transformer&gt;`.</span></div>
<div class="line"><span class="lineno"> 2838</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2839</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 2840</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 2841</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 2842</span><span class="stringliteral">        The data to transform.</span></div>
<div class="line"><span class="lineno"> 2843</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2844</span><span class="stringliteral">    axis : int, default=0</span></div>
<div class="line"><span class="lineno"> 2845</span><span class="stringliteral">        Axis used to compute the means and standard deviations along. If 0,</span></div>
<div class="line"><span class="lineno"> 2846</span><span class="stringliteral">        transform each feature, otherwise (if 1) transform each sample.</span></div>
<div class="line"><span class="lineno"> 2847</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2848</span><span class="stringliteral">    n_quantiles : int, default=1000 or n_samples</span></div>
<div class="line"><span class="lineno"> 2849</span><span class="stringliteral">        Number of quantiles to be computed. It corresponds to the number</span></div>
<div class="line"><span class="lineno"> 2850</span><span class="stringliteral">        of landmarks used to discretize the cumulative distribution function.</span></div>
<div class="line"><span class="lineno"> 2851</span><span class="stringliteral">        If n_quantiles is larger than the number of samples, n_quantiles is set</span></div>
<div class="line"><span class="lineno"> 2852</span><span class="stringliteral">        to the number of samples as a larger number of quantiles does not give</span></div>
<div class="line"><span class="lineno"> 2853</span><span class="stringliteral">        a better approximation of the cumulative distribution function</span></div>
<div class="line"><span class="lineno"> 2854</span><span class="stringliteral">        estimator.</span></div>
<div class="line"><span class="lineno"> 2855</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2856</span><span class="stringliteral">    output_distribution : {&#39;uniform&#39;, &#39;normal&#39;}, default=&#39;uniform&#39;</span></div>
<div class="line"><span class="lineno"> 2857</span><span class="stringliteral">        Marginal distribution for the transformed data. The choices are</span></div>
<div class="line"><span class="lineno"> 2858</span><span class="stringliteral">        &#39;uniform&#39; (default) or &#39;normal&#39;.</span></div>
<div class="line"><span class="lineno"> 2859</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2860</span><span class="stringliteral">    ignore_implicit_zeros : bool, default=False</span></div>
<div class="line"><span class="lineno"> 2861</span><span class="stringliteral">        Only applies to sparse matrices. If True, the sparse entries of the</span></div>
<div class="line"><span class="lineno"> 2862</span><span class="stringliteral">        matrix are discarded to compute the quantile statistics. If False,</span></div>
<div class="line"><span class="lineno"> 2863</span><span class="stringliteral">        these entries are treated as zeros.</span></div>
<div class="line"><span class="lineno"> 2864</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2865</span><span class="stringliteral">    subsample : int, default=1e5</span></div>
<div class="line"><span class="lineno"> 2866</span><span class="stringliteral">        Maximum number of samples used to estimate the quantiles for</span></div>
<div class="line"><span class="lineno"> 2867</span><span class="stringliteral">        computational efficiency. Note that the subsampling procedure may</span></div>
<div class="line"><span class="lineno"> 2868</span><span class="stringliteral">        differ for value-identical sparse and dense matrices.</span></div>
<div class="line"><span class="lineno"> 2869</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2870</span><span class="stringliteral">    random_state : int, RandomState instance or None, default=None</span></div>
<div class="line"><span class="lineno"> 2871</span><span class="stringliteral">        Determines random number generation for subsampling and smoothing</span></div>
<div class="line"><span class="lineno"> 2872</span><span class="stringliteral">        noise.</span></div>
<div class="line"><span class="lineno"> 2873</span><span class="stringliteral">        Please see ``subsample`` for more details.</span></div>
<div class="line"><span class="lineno"> 2874</span><span class="stringliteral">        Pass an int for reproducible results across multiple function calls.</span></div>
<div class="line"><span class="lineno"> 2875</span><span class="stringliteral">        See :term:`Glossary &lt;random_state&gt;`.</span></div>
<div class="line"><span class="lineno"> 2876</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2877</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno"> 2878</span><span class="stringliteral">        Set to False to perform inplace transformation and avoid a copy (if the</span></div>
<div class="line"><span class="lineno"> 2879</span><span class="stringliteral">        input is already a numpy array). If True, a copy of `X` is transformed,</span></div>
<div class="line"><span class="lineno"> 2880</span><span class="stringliteral">        leaving the original `X` unchanged.</span></div>
<div class="line"><span class="lineno"> 2881</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2882</span><span class="stringliteral">        .. versionchanged:: 0.23</span></div>
<div class="line"><span class="lineno"> 2883</span><span class="stringliteral">            The default value of `copy` changed from False to True in 0.23.</span></div>
<div class="line"><span class="lineno"> 2884</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2885</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 2886</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 2887</span><span class="stringliteral">    Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 2888</span><span class="stringliteral">        The transformed data.</span></div>
<div class="line"><span class="lineno"> 2889</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2890</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno"> 2891</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 2892</span><span class="stringliteral">    QuantileTransformer : Performs quantile-based scaling using the</span></div>
<div class="line"><span class="lineno"> 2893</span><span class="stringliteral">        Transformer API (e.g. as part of a preprocessing</span></div>
<div class="line"><span class="lineno"> 2894</span><span class="stringliteral">        :class:`~sklearn.pipeline.Pipeline`).</span></div>
<div class="line"><span class="lineno"> 2895</span><span class="stringliteral">    power_transform : Maps data to a normal distribution using a</span></div>
<div class="line"><span class="lineno"> 2896</span><span class="stringliteral">        power transformation.</span></div>
<div class="line"><span class="lineno"> 2897</span><span class="stringliteral">    scale : Performs standardization that is faster, but less robust</span></div>
<div class="line"><span class="lineno"> 2898</span><span class="stringliteral">        to outliers.</span></div>
<div class="line"><span class="lineno"> 2899</span><span class="stringliteral">    robust_scale : Performs robust standardization that removes the influence</span></div>
<div class="line"><span class="lineno"> 2900</span><span class="stringliteral">        of outliers but does not put outliers and inliers on the same scale.</span></div>
<div class="line"><span class="lineno"> 2901</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2902</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno"> 2903</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno"> 2904</span><span class="stringliteral">    NaNs are treated as missing values: disregarded in fit, and maintained in</span></div>
<div class="line"><span class="lineno"> 2905</span><span class="stringliteral">    transform.</span></div>
<div class="line"><span class="lineno"> 2906</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2907</span><span class="stringliteral">    .. warning:: Risk of data leak</span></div>
<div class="line"><span class="lineno"> 2908</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2909</span><span class="stringliteral">        Do not use :func:`~sklearn.preprocessing.quantile_transform` unless</span></div>
<div class="line"><span class="lineno"> 2910</span><span class="stringliteral">        you know what you are doing. A common mistake is to apply it</span></div>
<div class="line"><span class="lineno"> 2911</span><span class="stringliteral">        to the entire data *before* splitting into training and</span></div>
<div class="line"><span class="lineno"> 2912</span><span class="stringliteral">        test sets. This will bias the model evaluation because</span></div>
<div class="line"><span class="lineno"> 2913</span><span class="stringliteral">        information would have leaked from the test set to the</span></div>
<div class="line"><span class="lineno"> 2914</span><span class="stringliteral">        training set.</span></div>
<div class="line"><span class="lineno"> 2915</span><span class="stringliteral">        In general, we recommend using</span></div>
<div class="line"><span class="lineno"> 2916</span><span class="stringliteral">        :class:`~sklearn.preprocessing.QuantileTransformer` within a</span></div>
<div class="line"><span class="lineno"> 2917</span><span class="stringliteral">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span></div>
<div class="line"><span class="lineno"> 2918</span><span class="stringliteral">        leaking:`pipe = make_pipeline(QuantileTransformer(),</span></div>
<div class="line"><span class="lineno"> 2919</span><span class="stringliteral">        LogisticRegression())`.</span></div>
<div class="line"><span class="lineno"> 2920</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2921</span><span class="stringliteral">    For a comparison of the different scalers, transformers, and normalizers,</span></div>
<div class="line"><span class="lineno"> 2922</span><span class="stringliteral">    see :ref:`examples/preprocessing/plot_all_scaling.py</span></div>
<div class="line"><span class="lineno"> 2923</span><span class="stringliteral">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span></div>
<div class="line"><span class="lineno"> 2924</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 2925</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno"> 2926</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 2927</span><span class="stringliteral">    &gt;&gt;&gt; import numpy as np</span></div>
<div class="line"><span class="lineno"> 2928</span><span class="stringliteral">    &gt;&gt;&gt; from sklearn.preprocessing import quantile_transform</span></div>
<div class="line"><span class="lineno"> 2929</span><span class="stringliteral">    &gt;&gt;&gt; rng = np.random.RandomState(0)</span></div>
<div class="line"><span class="lineno"> 2930</span><span class="stringliteral">    &gt;&gt;&gt; X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)</span></div>
<div class="line"><span class="lineno"> 2931</span><span class="stringliteral">    &gt;&gt;&gt; quantile_transform(X, n_quantiles=10, random_state=0, copy=True)</span></div>
<div class="line"><span class="lineno"> 2932</span><span class="stringliteral">    array([...])</span></div>
<div class="line"><span class="lineno"> 2933</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 2934</span>    n = QuantileTransformer(</div>
<div class="line"><span class="lineno"> 2935</span>        n_quantiles=n_quantiles,</div>
<div class="line"><span class="lineno"> 2936</span>        output_distribution=output_distribution,</div>
<div class="line"><span class="lineno"> 2937</span>        subsample=subsample,</div>
<div class="line"><span class="lineno"> 2938</span>        ignore_implicit_zeros=ignore_implicit_zeros,</div>
<div class="line"><span class="lineno"> 2939</span>        random_state=random_state,</div>
<div class="line"><span class="lineno"> 2940</span>        copy=copy,</div>
<div class="line"><span class="lineno"> 2941</span>    )</div>
<div class="line"><span class="lineno"> 2942</span>    <span class="keywordflow">if</span> axis == 0:</div>
<div class="line"><span class="lineno"> 2943</span>        <span class="keywordflow">return</span> n.fit_transform(X)</div>
<div class="line"><span class="lineno"> 2944</span>    <span class="keywordflow">elif</span> axis == 1:</div>
<div class="line"><span class="lineno"> 2945</span>        <span class="keywordflow">return</span> n.fit_transform(X.T).T</div>
<div class="line"><span class="lineno"> 2946</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 2947</span>        <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno"> 2948</span>            <span class="stringliteral">&quot;axis should be either equal to 0 or 1. Got axis={}&quot;</span>.format(axis)</div>
<div class="line"><span class="lineno"> 2949</span>        )</div>
<div class="line"><span class="lineno"> 2950</span> </div>
<div class="line"><span class="lineno"> 2951</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a8707cbc7bec0ada5d7def37adc313ded" name="a8707cbc7bec0ada5d7def37adc313ded"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8707cbc7bec0ada5d7def37adc313ded">&#9670;&#160;</a></span>robust_scale()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data.robust_scale </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>axis</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>with_centering</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>with_scaling</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>quantile_range</em> = <code>(25.0,&#160;75.0)</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>unit_variance</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Standardize a dataset along any axis.

Center to the median and component wise scale
according to the interquartile range.

Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_sample, n_features)
    The data to center and scale.

axis : int, default=0
    Axis used to compute the medians and IQR along. If 0,
    independently scale each feature, otherwise (if 1) scale
    each sample.

with_centering : bool, default=True
    If `True`, center the data before scaling.

with_scaling : bool, default=True
    If `True`, scale the data to unit variance (or equivalently,
    unit standard deviation).

quantile_range : tuple (q_min, q_max), 0.0 &lt; q_min &lt; q_max &lt; 100.0,\
    default=(25.0, 75.0)
    Quantile range used to calculate `scale_`. By default this is equal to
    the IQR, i.e., `q_min` is the first quantile and `q_max` is the third
    quantile.

    .. versionadded:: 0.18

copy : bool, default=True
    Set to `False` to perform inplace row normalization and avoid a
    copy (if the input is already a numpy array or a scipy.sparse
    CSR matrix and if axis is 1).

unit_variance : bool, default=False
    If `True`, scale data so that normally distributed features have a
    variance of 1. In general, if the difference between the x-values of
    `q_max` and `q_min` for a standard normal distribution is greater
    than 1, the dataset will be scaled down. If less than 1, the dataset
    will be scaled up.

    .. versionadded:: 0.24

Returns
-------
X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The transformed data.

See Also
--------
RobustScaler : Performs centering and scaling using the Transformer API
    (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).

Notes
-----
This implementation will refuse to center scipy.sparse matrices
since it would make them non-sparse and would potentially crash the
program with memory exhaustion problems.

Instead the caller is expected to either set explicitly
`with_centering=False` (in that case, only variance scaling will be
performed on the features of the CSR matrix) or to call `X.toarray()`
if he/she expects the materialized dense array to fit in memory.

To avoid memory copy the caller should pass a CSR matrix.

For a comparison of the different scalers, transformers, and normalizers,
see :ref:`examples/preprocessing/plot_all_scaling.py
&lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.

.. warning:: Risk of data leak

    Do not use :func:`~sklearn.preprocessing.robust_scale` unless you know
    what you are doing. A common mistake is to apply it to the entire data
    *before* splitting into training and test sets. This will bias the
    model evaluation because information would have leaked from the test
    set to the training set.
    In general, we recommend using
    :class:`~sklearn.preprocessing.RobustScaler` within a
    :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data
    leaking: `pipe = make_pipeline(RobustScaler(), LogisticRegression())`.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1641</span>):</div>
<div class="line"><span class="lineno"> 1642</span>    <span class="stringliteral">&quot;&quot;&quot;Standardize a dataset along any axis.</span></div>
<div class="line"><span class="lineno"> 1643</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1644</span><span class="stringliteral">    Center to the median and component wise scale</span></div>
<div class="line"><span class="lineno"> 1645</span><span class="stringliteral">    according to the interquartile range.</span></div>
<div class="line"><span class="lineno"> 1646</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1647</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.</span></div>
<div class="line"><span class="lineno"> 1648</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1649</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 1650</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 1651</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_sample, n_features)</span></div>
<div class="line"><span class="lineno"> 1652</span><span class="stringliteral">        The data to center and scale.</span></div>
<div class="line"><span class="lineno"> 1653</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1654</span><span class="stringliteral">    axis : int, default=0</span></div>
<div class="line"><span class="lineno"> 1655</span><span class="stringliteral">        Axis used to compute the medians and IQR along. If 0,</span></div>
<div class="line"><span class="lineno"> 1656</span><span class="stringliteral">        independently scale each feature, otherwise (if 1) scale</span></div>
<div class="line"><span class="lineno"> 1657</span><span class="stringliteral">        each sample.</span></div>
<div class="line"><span class="lineno"> 1658</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1659</span><span class="stringliteral">    with_centering : bool, default=True</span></div>
<div class="line"><span class="lineno"> 1660</span><span class="stringliteral">        If `True`, center the data before scaling.</span></div>
<div class="line"><span class="lineno"> 1661</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1662</span><span class="stringliteral">    with_scaling : bool, default=True</span></div>
<div class="line"><span class="lineno"> 1663</span><span class="stringliteral">        If `True`, scale the data to unit variance (or equivalently,</span></div>
<div class="line"><span class="lineno"> 1664</span><span class="stringliteral">        unit standard deviation).</span></div>
<div class="line"><span class="lineno"> 1665</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1666</span><span class="stringliteral">    quantile_range : tuple (q_min, q_max), 0.0 &lt; q_min &lt; q_max &lt; 100.0,\</span></div>
<div class="line"><span class="lineno"> 1667</span><span class="stringliteral">        default=(25.0, 75.0)</span></div>
<div class="line"><span class="lineno"> 1668</span><span class="stringliteral">        Quantile range used to calculate `scale_`. By default this is equal to</span></div>
<div class="line"><span class="lineno"> 1669</span><span class="stringliteral">        the IQR, i.e., `q_min` is the first quantile and `q_max` is the third</span></div>
<div class="line"><span class="lineno"> 1670</span><span class="stringliteral">        quantile.</span></div>
<div class="line"><span class="lineno"> 1671</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1672</span><span class="stringliteral">        .. versionadded:: 0.18</span></div>
<div class="line"><span class="lineno"> 1673</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1674</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno"> 1675</span><span class="stringliteral">        Set to `False` to perform inplace row normalization and avoid a</span></div>
<div class="line"><span class="lineno"> 1676</span><span class="stringliteral">        copy (if the input is already a numpy array or a scipy.sparse</span></div>
<div class="line"><span class="lineno"> 1677</span><span class="stringliteral">        CSR matrix and if axis is 1).</span></div>
<div class="line"><span class="lineno"> 1678</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1679</span><span class="stringliteral">    unit_variance : bool, default=False</span></div>
<div class="line"><span class="lineno"> 1680</span><span class="stringliteral">        If `True`, scale data so that normally distributed features have a</span></div>
<div class="line"><span class="lineno"> 1681</span><span class="stringliteral">        variance of 1. In general, if the difference between the x-values of</span></div>
<div class="line"><span class="lineno"> 1682</span><span class="stringliteral">        `q_max` and `q_min` for a standard normal distribution is greater</span></div>
<div class="line"><span class="lineno"> 1683</span><span class="stringliteral">        than 1, the dataset will be scaled down. If less than 1, the dataset</span></div>
<div class="line"><span class="lineno"> 1684</span><span class="stringliteral">        will be scaled up.</span></div>
<div class="line"><span class="lineno"> 1685</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1686</span><span class="stringliteral">        .. versionadded:: 0.24</span></div>
<div class="line"><span class="lineno"> 1687</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1688</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 1689</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 1690</span><span class="stringliteral">    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 1691</span><span class="stringliteral">        The transformed data.</span></div>
<div class="line"><span class="lineno"> 1692</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1693</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno"> 1694</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 1695</span><span class="stringliteral">    RobustScaler : Performs centering and scaling using the Transformer API</span></div>
<div class="line"><span class="lineno"> 1696</span><span class="stringliteral">        (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`).</span></div>
<div class="line"><span class="lineno"> 1697</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1698</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno"> 1699</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno"> 1700</span><span class="stringliteral">    This implementation will refuse to center scipy.sparse matrices</span></div>
<div class="line"><span class="lineno"> 1701</span><span class="stringliteral">    since it would make them non-sparse and would potentially crash the</span></div>
<div class="line"><span class="lineno"> 1702</span><span class="stringliteral">    program with memory exhaustion problems.</span></div>
<div class="line"><span class="lineno"> 1703</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1704</span><span class="stringliteral">    Instead the caller is expected to either set explicitly</span></div>
<div class="line"><span class="lineno"> 1705</span><span class="stringliteral">    `with_centering=False` (in that case, only variance scaling will be</span></div>
<div class="line"><span class="lineno"> 1706</span><span class="stringliteral">    performed on the features of the CSR matrix) or to call `X.toarray()`</span></div>
<div class="line"><span class="lineno"> 1707</span><span class="stringliteral">    if he/she expects the materialized dense array to fit in memory.</span></div>
<div class="line"><span class="lineno"> 1708</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1709</span><span class="stringliteral">    To avoid memory copy the caller should pass a CSR matrix.</span></div>
<div class="line"><span class="lineno"> 1710</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1711</span><span class="stringliteral">    For a comparison of the different scalers, transformers, and normalizers,</span></div>
<div class="line"><span class="lineno"> 1712</span><span class="stringliteral">    see :ref:`examples/preprocessing/plot_all_scaling.py</span></div>
<div class="line"><span class="lineno"> 1713</span><span class="stringliteral">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span></div>
<div class="line"><span class="lineno"> 1714</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1715</span><span class="stringliteral">    .. warning:: Risk of data leak</span></div>
<div class="line"><span class="lineno"> 1716</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1717</span><span class="stringliteral">        Do not use :func:`~sklearn.preprocessing.robust_scale` unless you know</span></div>
<div class="line"><span class="lineno"> 1718</span><span class="stringliteral">        what you are doing. A common mistake is to apply it to the entire data</span></div>
<div class="line"><span class="lineno"> 1719</span><span class="stringliteral">        *before* splitting into training and test sets. This will bias the</span></div>
<div class="line"><span class="lineno"> 1720</span><span class="stringliteral">        model evaluation because information would have leaked from the test</span></div>
<div class="line"><span class="lineno"> 1721</span><span class="stringliteral">        set to the training set.</span></div>
<div class="line"><span class="lineno"> 1722</span><span class="stringliteral">        In general, we recommend using</span></div>
<div class="line"><span class="lineno"> 1723</span><span class="stringliteral">        :class:`~sklearn.preprocessing.RobustScaler` within a</span></div>
<div class="line"><span class="lineno"> 1724</span><span class="stringliteral">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span></div>
<div class="line"><span class="lineno"> 1725</span><span class="stringliteral">        leaking: `pipe = make_pipeline(RobustScaler(), LogisticRegression())`.</span></div>
<div class="line"><span class="lineno"> 1726</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1727</span>    X = check_array(</div>
<div class="line"><span class="lineno"> 1728</span>        X,</div>
<div class="line"><span class="lineno"> 1729</span>        accept_sparse=(<span class="stringliteral">&quot;csr&quot;</span>, <span class="stringliteral">&quot;csc&quot;</span>),</div>
<div class="line"><span class="lineno"> 1730</span>        copy=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1731</span>        ensure_2d=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1732</span>        dtype=FLOAT_DTYPES,</div>
<div class="line"><span class="lineno"> 1733</span>        force_all_finite=<span class="stringliteral">&quot;allow-nan&quot;</span>,</div>
<div class="line"><span class="lineno"> 1734</span>    )</div>
<div class="line"><span class="lineno"> 1735</span>    original_ndim = X.ndim</div>
<div class="line"><span class="lineno"> 1736</span> </div>
<div class="line"><span class="lineno"> 1737</span>    <span class="keywordflow">if</span> original_ndim == 1:</div>
<div class="line"><span class="lineno"> 1738</span>        X = X.reshape(X.shape[0], 1)</div>
<div class="line"><span class="lineno"> 1739</span> </div>
<div class="line"><span class="lineno"> 1740</span>    s = RobustScaler(</div>
<div class="line"><span class="lineno"> 1741</span>        with_centering=with_centering,</div>
<div class="line"><span class="lineno"> 1742</span>        with_scaling=with_scaling,</div>
<div class="line"><span class="lineno"> 1743</span>        quantile_range=quantile_range,</div>
<div class="line"><span class="lineno"> 1744</span>        unit_variance=unit_variance,</div>
<div class="line"><span class="lineno"> 1745</span>        copy=copy,</div>
<div class="line"><span class="lineno"> 1746</span>    )</div>
<div class="line"><span class="lineno"> 1747</span>    <span class="keywordflow">if</span> axis == 0:</div>
<div class="line"><span class="lineno"> 1748</span>        X = s.fit_transform(X)</div>
<div class="line"><span class="lineno"> 1749</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1750</span>        X = s.fit_transform(X.T).T</div>
<div class="line"><span class="lineno"> 1751</span> </div>
<div class="line"><span class="lineno"> 1752</span>    <span class="keywordflow">if</span> original_ndim == 1:</div>
<div class="line"><span class="lineno"> 1753</span>        X = X.ravel()</div>
<div class="line"><span class="lineno"> 1754</span> </div>
<div class="line"><span class="lineno"> 1755</span>    <span class="keywordflow">return</span> X</div>
<div class="line"><span class="lineno"> 1756</span> </div>
<div class="line"><span class="lineno"> 1757</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="acdc48832c8ced035403f3ee58b8f0b01" name="acdc48832c8ced035403f3ee58b8f0b01"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acdc48832c8ced035403f3ee58b8f0b01">&#9670;&#160;</a></span>scale()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.preprocessing._data.scale </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>axis</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>with_mean</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>with_std</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Standardize a dataset along any axis.

Center to the mean and component wise scale to unit variance.

Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data to center and scale.

axis : int, default=0
    Axis used to compute the means and standard deviations along. If 0,
    independently standardize each feature, otherwise (if 1) standardize
    each sample.

with_mean : bool, default=True
    If True, center the data before scaling.

with_std : bool, default=True
    If True, scale the data to unit variance (or equivalently,
    unit standard deviation).

copy : bool, default=True
    Set to False to perform inplace row normalization and avoid a
    copy (if the input is already a numpy array or a scipy.sparse
    CSC matrix and if axis is 1).

Returns
-------
X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)
    The transformed data.

See Also
--------
StandardScaler : Performs scaling to unit variance using the Transformer
    API (e.g. as part of a preprocessing
    :class:`~sklearn.pipeline.Pipeline`).

Notes
-----
This implementation will refuse to center scipy.sparse matrices
since it would make them non-sparse and would potentially crash the
program with memory exhaustion problems.

Instead the caller is expected to either set explicitly
`with_mean=False` (in that case, only variance scaling will be
performed on the features of the CSC matrix) or to call `X.toarray()`
if he/she expects the materialized dense array to fit in memory.

To avoid memory copy the caller should pass a CSC matrix.

NaNs are treated as missing values: disregarded to compute the statistics,
and maintained during the data transformation.

We use a biased estimator for the standard deviation, equivalent to
`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to
affect model performance.

For a comparison of the different scalers, transformers, and normalizers,
see :ref:`examples/preprocessing/plot_all_scaling.py
&lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.

.. warning:: Risk of data leak

    Do not use :func:`~sklearn.preprocessing.scale` unless you know
    what you are doing. A common mistake is to apply it to the entire data
    *before* splitting into training and test sets. This will bias the
    model evaluation because information would have leaked from the test
    set to the training set.
    In general, we recommend using
    :class:`~sklearn.preprocessing.StandardScaler` within a
    :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data
    leaking: `pipe = make_pipeline(StandardScaler(), LogisticRegression())`.
</pre> <div class="fragment"><div class="line"><span class="lineno">  123</span><span class="keyword">def </span>scale(X, *, axis=0, with_mean=True, with_std=True, copy=True):</div>
<div class="line"><span class="lineno">  124</span>    <span class="stringliteral">&quot;&quot;&quot;Standardize a dataset along any axis.</span></div>
<div class="line"><span class="lineno">  125</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  126</span><span class="stringliteral">    Center to the mean and component wise scale to unit variance.</span></div>
<div class="line"><span class="lineno">  127</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  128</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;preprocessing_scaler&gt;`.</span></div>
<div class="line"><span class="lineno">  129</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  130</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  131</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  132</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  133</span><span class="stringliteral">        The data to center and scale.</span></div>
<div class="line"><span class="lineno">  134</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  135</span><span class="stringliteral">    axis : int, default=0</span></div>
<div class="line"><span class="lineno">  136</span><span class="stringliteral">        Axis used to compute the means and standard deviations along. If 0,</span></div>
<div class="line"><span class="lineno">  137</span><span class="stringliteral">        independently standardize each feature, otherwise (if 1) standardize</span></div>
<div class="line"><span class="lineno">  138</span><span class="stringliteral">        each sample.</span></div>
<div class="line"><span class="lineno">  139</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  140</span><span class="stringliteral">    with_mean : bool, default=True</span></div>
<div class="line"><span class="lineno">  141</span><span class="stringliteral">        If True, center the data before scaling.</span></div>
<div class="line"><span class="lineno">  142</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  143</span><span class="stringliteral">    with_std : bool, default=True</span></div>
<div class="line"><span class="lineno">  144</span><span class="stringliteral">        If True, scale the data to unit variance (or equivalently,</span></div>
<div class="line"><span class="lineno">  145</span><span class="stringliteral">        unit standard deviation).</span></div>
<div class="line"><span class="lineno">  146</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  147</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno">  148</span><span class="stringliteral">        Set to False to perform inplace row normalization and avoid a</span></div>
<div class="line"><span class="lineno">  149</span><span class="stringliteral">        copy (if the input is already a numpy array or a scipy.sparse</span></div>
<div class="line"><span class="lineno">  150</span><span class="stringliteral">        CSC matrix and if axis is 1).</span></div>
<div class="line"><span class="lineno">  151</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  152</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  153</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  154</span><span class="stringliteral">    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  155</span><span class="stringliteral">        The transformed data.</span></div>
<div class="line"><span class="lineno">  156</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  157</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  158</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  159</span><span class="stringliteral">    StandardScaler : Performs scaling to unit variance using the Transformer</span></div>
<div class="line"><span class="lineno">  160</span><span class="stringliteral">        API (e.g. as part of a preprocessing</span></div>
<div class="line"><span class="lineno">  161</span><span class="stringliteral">        :class:`~sklearn.pipeline.Pipeline`).</span></div>
<div class="line"><span class="lineno">  162</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  163</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  164</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  165</span><span class="stringliteral">    This implementation will refuse to center scipy.sparse matrices</span></div>
<div class="line"><span class="lineno">  166</span><span class="stringliteral">    since it would make them non-sparse and would potentially crash the</span></div>
<div class="line"><span class="lineno">  167</span><span class="stringliteral">    program with memory exhaustion problems.</span></div>
<div class="line"><span class="lineno">  168</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  169</span><span class="stringliteral">    Instead the caller is expected to either set explicitly</span></div>
<div class="line"><span class="lineno">  170</span><span class="stringliteral">    `with_mean=False` (in that case, only variance scaling will be</span></div>
<div class="line"><span class="lineno">  171</span><span class="stringliteral">    performed on the features of the CSC matrix) or to call `X.toarray()`</span></div>
<div class="line"><span class="lineno">  172</span><span class="stringliteral">    if he/she expects the materialized dense array to fit in memory.</span></div>
<div class="line"><span class="lineno">  173</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  174</span><span class="stringliteral">    To avoid memory copy the caller should pass a CSC matrix.</span></div>
<div class="line"><span class="lineno">  175</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  176</span><span class="stringliteral">    NaNs are treated as missing values: disregarded to compute the statistics,</span></div>
<div class="line"><span class="lineno">  177</span><span class="stringliteral">    and maintained during the data transformation.</span></div>
<div class="line"><span class="lineno">  178</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  179</span><span class="stringliteral">    We use a biased estimator for the standard deviation, equivalent to</span></div>
<div class="line"><span class="lineno">  180</span><span class="stringliteral">    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to</span></div>
<div class="line"><span class="lineno">  181</span><span class="stringliteral">    affect model performance.</span></div>
<div class="line"><span class="lineno">  182</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  183</span><span class="stringliteral">    For a comparison of the different scalers, transformers, and normalizers,</span></div>
<div class="line"><span class="lineno">  184</span><span class="stringliteral">    see :ref:`examples/preprocessing/plot_all_scaling.py</span></div>
<div class="line"><span class="lineno">  185</span><span class="stringliteral">    &lt;sphx_glr_auto_examples_preprocessing_plot_all_scaling.py&gt;`.</span></div>
<div class="line"><span class="lineno">  186</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  187</span><span class="stringliteral">    .. warning:: Risk of data leak</span></div>
<div class="line"><span class="lineno">  188</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  189</span><span class="stringliteral">        Do not use :func:`~sklearn.preprocessing.scale` unless you know</span></div>
<div class="line"><span class="lineno">  190</span><span class="stringliteral">        what you are doing. A common mistake is to apply it to the entire data</span></div>
<div class="line"><span class="lineno">  191</span><span class="stringliteral">        *before* splitting into training and test sets. This will bias the</span></div>
<div class="line"><span class="lineno">  192</span><span class="stringliteral">        model evaluation because information would have leaked from the test</span></div>
<div class="line"><span class="lineno">  193</span><span class="stringliteral">        set to the training set.</span></div>
<div class="line"><span class="lineno">  194</span><span class="stringliteral">        In general, we recommend using</span></div>
<div class="line"><span class="lineno">  195</span><span class="stringliteral">        :class:`~sklearn.preprocessing.StandardScaler` within a</span></div>
<div class="line"><span class="lineno">  196</span><span class="stringliteral">        :ref:`Pipeline &lt;pipeline&gt;` in order to prevent most risks of data</span></div>
<div class="line"><span class="lineno">  197</span><span class="stringliteral">        leaking: `pipe = make_pipeline(StandardScaler(), LogisticRegression())`.</span></div>
<div class="line"><span class="lineno">  198</span><span class="stringliteral">    &quot;&quot;&quot;</span>  <span class="comment"># noqa</span></div>
<div class="line"><span class="lineno">  199</span>    X = check_array(</div>
<div class="line"><span class="lineno">  200</span>        X,</div>
<div class="line"><span class="lineno">  201</span>        accept_sparse=<span class="stringliteral">&quot;csc&quot;</span>,</div>
<div class="line"><span class="lineno">  202</span>        copy=copy,</div>
<div class="line"><span class="lineno">  203</span>        ensure_2d=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno">  204</span>        estimator=<span class="stringliteral">&quot;the scale function&quot;</span>,</div>
<div class="line"><span class="lineno">  205</span>        dtype=FLOAT_DTYPES,</div>
<div class="line"><span class="lineno">  206</span>        force_all_finite=<span class="stringliteral">&quot;allow-nan&quot;</span>,</div>
<div class="line"><span class="lineno">  207</span>    )</div>
<div class="line"><span class="lineno">  208</span>    <span class="keywordflow">if</span> sparse.issparse(X):</div>
<div class="line"><span class="lineno">  209</span>        <span class="keywordflow">if</span> with_mean:</div>
<div class="line"><span class="lineno">  210</span>            <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">  211</span>                <span class="stringliteral">&quot;Cannot center sparse matrices: pass `with_mean=False` instead&quot;</span></div>
<div class="line"><span class="lineno">  212</span>                <span class="stringliteral">&quot; See docstring for motivation and alternatives.&quot;</span></div>
<div class="line"><span class="lineno">  213</span>            )</div>
<div class="line"><span class="lineno">  214</span>        <span class="keywordflow">if</span> axis != 0:</div>
<div class="line"><span class="lineno">  215</span>            <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">  216</span>                <span class="stringliteral">&quot;Can only scale sparse matrix on axis=0,  got axis=%d&quot;</span> % axis</div>
<div class="line"><span class="lineno">  217</span>            )</div>
<div class="line"><span class="lineno">  218</span>        <span class="keywordflow">if</span> with_std:</div>
<div class="line"><span class="lineno">  219</span>            _, var = mean_variance_axis(X, axis=0)</div>
<div class="line"><span class="lineno">  220</span>            var = _handle_zeros_in_scale(var, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  221</span>            inplace_column_scale(X, 1 / np.sqrt(var))</div>
<div class="line"><span class="lineno">  222</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  223</span>        X = np.asarray(X)</div>
<div class="line"><span class="lineno">  224</span>        <span class="keywordflow">if</span> with_mean:</div>
<div class="line"><span class="lineno">  225</span>            mean_ = np.nanmean(X, axis)</div>
<div class="line"><span class="lineno">  226</span>        <span class="keywordflow">if</span> with_std:</div>
<div class="line"><span class="lineno">  227</span>            scale_ = np.nanstd(X, axis)</div>
<div class="line"><span class="lineno">  228</span>        <span class="comment"># Xr is a view on the original array that enables easy use of</span></div>
<div class="line"><span class="lineno">  229</span>        <span class="comment"># broadcasting on the axis in which we are interested in</span></div>
<div class="line"><span class="lineno">  230</span>        Xr = np.rollaxis(X, axis)</div>
<div class="line"><span class="lineno">  231</span>        <span class="keywordflow">if</span> with_mean:</div>
<div class="line"><span class="lineno">  232</span>            Xr -= mean_</div>
<div class="line"><span class="lineno">  233</span>            mean_1 = np.nanmean(Xr, axis=0)</div>
<div class="line"><span class="lineno">  234</span>            <span class="comment"># Verify that mean_1 is &#39;close to zero&#39;. If X contains very</span></div>
<div class="line"><span class="lineno">  235</span>            <span class="comment"># large values, mean_1 can also be very large, due to a lack of</span></div>
<div class="line"><span class="lineno">  236</span>            <span class="comment"># precision of mean_. In this case, a pre-scaling of the</span></div>
<div class="line"><span class="lineno">  237</span>            <span class="comment"># concerned feature is efficient, for instance by its mean or</span></div>
<div class="line"><span class="lineno">  238</span>            <span class="comment"># maximum.</span></div>
<div class="line"><span class="lineno">  239</span>            <span class="keywordflow">if</span> <span class="keywordflow">not</span> np.allclose(mean_1, 0):</div>
<div class="line"><span class="lineno">  240</span>                warnings.warn(</div>
<div class="line"><span class="lineno">  241</span>                    <span class="stringliteral">&quot;Numerical issues were encountered &quot;</span></div>
<div class="line"><span class="lineno">  242</span>                    <span class="stringliteral">&quot;when centering the data &quot;</span></div>
<div class="line"><span class="lineno">  243</span>                    <span class="stringliteral">&quot;and might not be solved. Dataset may &quot;</span></div>
<div class="line"><span class="lineno">  244</span>                    <span class="stringliteral">&quot;contain too large values. You may need &quot;</span></div>
<div class="line"><span class="lineno">  245</span>                    <span class="stringliteral">&quot;to prescale your features.&quot;</span></div>
<div class="line"><span class="lineno">  246</span>                )</div>
<div class="line"><span class="lineno">  247</span>                Xr -= mean_1</div>
<div class="line"><span class="lineno">  248</span>        <span class="keywordflow">if</span> with_std:</div>
<div class="line"><span class="lineno">  249</span>            scale_ = _handle_zeros_in_scale(scale_, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  250</span>            Xr /= scale_</div>
<div class="line"><span class="lineno">  251</span>            <span class="keywordflow">if</span> with_mean:</div>
<div class="line"><span class="lineno">  252</span>                mean_2 = np.nanmean(Xr, axis=0)</div>
<div class="line"><span class="lineno">  253</span>                <span class="comment"># If mean_2 is not &#39;close to zero&#39;, it comes from the fact that</span></div>
<div class="line"><span class="lineno">  254</span>                <span class="comment"># scale_ is very small so that mean_2 = mean_1/scale_ &gt; 0, even</span></div>
<div class="line"><span class="lineno">  255</span>                <span class="comment"># if mean_1 was close to zero. The problem is thus essentially</span></div>
<div class="line"><span class="lineno">  256</span>                <span class="comment"># due to the lack of precision of mean_. A solution is then to</span></div>
<div class="line"><span class="lineno">  257</span>                <span class="comment"># subtract the mean again:</span></div>
<div class="line"><span class="lineno">  258</span>                <span class="keywordflow">if</span> <span class="keywordflow">not</span> np.allclose(mean_2, 0):</div>
<div class="line"><span class="lineno">  259</span>                    warnings.warn(</div>
<div class="line"><span class="lineno">  260</span>                        <span class="stringliteral">&quot;Numerical issues were encountered &quot;</span></div>
<div class="line"><span class="lineno">  261</span>                        <span class="stringliteral">&quot;when scaling the data &quot;</span></div>
<div class="line"><span class="lineno">  262</span>                        <span class="stringliteral">&quot;and might not be solved. The standard &quot;</span></div>
<div class="line"><span class="lineno">  263</span>                        <span class="stringliteral">&quot;deviation of the data is probably &quot;</span></div>
<div class="line"><span class="lineno">  264</span>                        <span class="stringliteral">&quot;very close to 0. &quot;</span></div>
<div class="line"><span class="lineno">  265</span>                    )</div>
<div class="line"><span class="lineno">  266</span>                    Xr -= mean_2</div>
<div class="line"><span class="lineno">  267</span>    <span class="keywordflow">return</span> X</div>
<div class="line"><span class="lineno">  268</span> </div>
<div class="line"><span class="lineno">  269</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a1ed07fa0a5dd1bd2f5d841da47681820" name="a1ed07fa0a5dd1bd2f5d841da47681820"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1ed07fa0a5dd1bd2f5d841da47681820">&#9670;&#160;</a></span>BOUNDS_THRESHOLD</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int sklearn.preprocessing._data.BOUNDS_THRESHOLD = 1e-7</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
