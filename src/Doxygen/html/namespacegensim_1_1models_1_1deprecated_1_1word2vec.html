<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: gensim.models.deprecated.word2vec Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacegensim.html">gensim</a></li><li class="navelem"><a class="el" href="namespacegensim_1_1models.html">models</a></li><li class="navelem"><a class="el" href="namespacegensim_1_1models_1_1deprecated.html">deprecated</a></li><li class="navelem"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html">word2vec</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">gensim.models.deprecated.word2vec Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1deprecated_1_1word2vec_1_1_brown_corpus.html">BrownCorpus</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1deprecated_1_1word2vec_1_1_line_sentence.html">LineSentence</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1deprecated_1_1word2vec_1_1_path_line_sentences.html">PathLineSentences</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1deprecated_1_1word2vec_1_1_text8_corpus.html">Text8Corpus</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1deprecated_1_1word2vec_1_1_word2_vec.html">Word2Vec</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a0aaa5fe8a8e34eabf5cf0d9cae7c7708" id="r_a0aaa5fe8a8e34eabf5cf0d9cae7c7708"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a0aaa5fe8a8e34eabf5cf0d9cae7c7708">load_old_word2vec</a> (*<a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a18eabe1e89d13a9a285a7ebc46197302">args</a>, **kwargs)</td></tr>
<tr class="separator:a0aaa5fe8a8e34eabf5cf0d9cae7c7708"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a222f8a3a71c52337bad6109ec840ec30" id="r_a222f8a3a71c52337bad6109ec840ec30"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a222f8a3a71c52337bad6109ec840ec30">train_batch_sg</a> (<a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a6ac4f31ea646dc416e034430eb9bd41e">model</a>, sentences, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>, <a class="el" href="__lapack__subroutines_8h.html#ad2e2a6e930388ddee2f5afb58c5bf8ad">work</a>=None, compute_loss=False)</td></tr>
<tr class="separator:a222f8a3a71c52337bad6109ec840ec30"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab7bb718e3a63f9b2f03df265dfb0b938" id="r_ab7bb718e3a63f9b2f03df265dfb0b938"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ab7bb718e3a63f9b2f03df265dfb0b938">train_batch_cbow</a> (<a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a6ac4f31ea646dc416e034430eb9bd41e">model</a>, sentences, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>, <a class="el" href="__lapack__subroutines_8h.html#ad2e2a6e930388ddee2f5afb58c5bf8ad">work</a>=None, neu1=None, compute_loss=False)</td></tr>
<tr class="separator:ab7bb718e3a63f9b2f03df265dfb0b938"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aab6898fcb39cd838bdcaef6bdbf1a0b2" id="r_aab6898fcb39cd838bdcaef6bdbf1a0b2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#aab6898fcb39cd838bdcaef6bdbf1a0b2">score_sentence_sg</a> (<a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a6ac4f31ea646dc416e034430eb9bd41e">model</a>, sentence, <a class="el" href="__lapack__subroutines_8h.html#ad2e2a6e930388ddee2f5afb58c5bf8ad">work</a>=None)</td></tr>
<tr class="separator:aab6898fcb39cd838bdcaef6bdbf1a0b2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af27aebb545e81d4f99160532e3fd9577" id="r_af27aebb545e81d4f99160532e3fd9577"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#af27aebb545e81d4f99160532e3fd9577">score_sentence_cbow</a> (<a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a6ac4f31ea646dc416e034430eb9bd41e">model</a>, sentence, <a class="el" href="__lapack__subroutines_8h.html#ad2e2a6e930388ddee2f5afb58c5bf8ad">work</a>=None, neu1=None)</td></tr>
<tr class="separator:af27aebb545e81d4f99160532e3fd9577"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a32eb23a1e9ef7d58f62640580f3ad3d8" id="r_a32eb23a1e9ef7d58f62640580f3ad3d8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a32eb23a1e9ef7d58f62640580f3ad3d8">train_sg_pair</a> (<a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a6ac4f31ea646dc416e034430eb9bd41e">model</a>, word, context_index, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>, learn_vectors=True, learn_hidden=True, context_vectors=None, context_locks=None, compute_loss=False, is_ft=False)</td></tr>
<tr class="separator:a32eb23a1e9ef7d58f62640580f3ad3d8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a49eabffc774c15890363898ffecf94ed" id="r_a49eabffc774c15890363898ffecf94ed"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a49eabffc774c15890363898ffecf94ed">train_cbow_pair</a> (<a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a6ac4f31ea646dc416e034430eb9bd41e">model</a>, word, input_word_indices, l1, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>, learn_vectors=True, learn_hidden=True, compute_loss=False, context_vectors=None, context_locks=None, is_ft=False)</td></tr>
<tr class="separator:a49eabffc774c15890363898ffecf94ed"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4c9d207fcf526f5f17759f142a46aaf" id="r_ab4c9d207fcf526f5f17759f142a46aaf"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ab4c9d207fcf526f5f17759f142a46aaf">score_sg_pair</a> (<a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a6ac4f31ea646dc416e034430eb9bd41e">model</a>, word, word2)</td></tr>
<tr class="separator:ab4c9d207fcf526f5f17759f142a46aaf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aefcce5b1288c7b0d29270b8a687883c4" id="r_aefcce5b1288c7b0d29270b8a687883c4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#aefcce5b1288c7b0d29270b8a687883c4">score_cbow_pair</a> (<a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a6ac4f31ea646dc416e034430eb9bd41e">model</a>, word, l1)</td></tr>
<tr class="separator:aefcce5b1288c7b0d29270b8a687883c4"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a2f949c8f14ee680b075543db1c3ff34b" id="r_a2f949c8f14ee680b075543db1c3ff34b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a2f949c8f14ee680b075543db1c3ff34b">logger</a> = logging.getLogger(__name__)</td></tr>
<tr class="separator:a2f949c8f14ee680b075543db1c3ff34b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acb33d2c07d8e0fad27410b3afe6df02d" id="r_acb33d2c07d8e0fad27410b3afe6df02d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ad226751260da61bc5831d7c01f788543">int</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#acb33d2c07d8e0fad27410b3afe6df02d">MAX_WORDS_IN_BATCH</a> = 10000</td></tr>
<tr class="separator:acb33d2c07d8e0fad27410b3afe6df02d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a29ae5c108732b5a6d0d351f94a36877d" id="r_a29ae5c108732b5a6d0d351f94a36877d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a29ae5c108732b5a6d0d351f94a36877d">format</a></td></tr>
<tr class="separator:a29ae5c108732b5a6d0d351f94a36877d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2d624122e2eed5779b878361593e4522" id="r_a2d624122e2eed5779b878361593e4522"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a2d624122e2eed5779b878361593e4522">level</a></td></tr>
<tr class="separator:a2d624122e2eed5779b878361593e4522"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a297d9ec1de9dd6ad011dbc8c211eae97" id="r_a297d9ec1de9dd6ad011dbc8c211eae97"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a297d9ec1de9dd6ad011dbc8c211eae97">program</a> = os.path.basename(sys.argv[0])</td></tr>
<tr class="separator:a297d9ec1de9dd6ad011dbc8c211eae97"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5f1ec4812ebc8ae60e0130cbe9e93da9" id="r_a5f1ec4812ebc8ae60e0130cbe9e93da9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a5f1ec4812ebc8ae60e0130cbe9e93da9">all</a></td></tr>
<tr class="separator:a5f1ec4812ebc8ae60e0130cbe9e93da9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2067b6abb4e2ae037204800c8b9d2377" id="r_a2067b6abb4e2ae037204800c8b9d2377"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a2067b6abb4e2ae037204800c8b9d2377">parser</a> = argparse.ArgumentParser()</td></tr>
<tr class="separator:a2067b6abb4e2ae037204800c8b9d2377"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a89104efbfbb61e297b4632e421a3338d" id="r_a89104efbfbb61e297b4632e421a3338d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a89104efbfbb61e297b4632e421a3338d">help</a></td></tr>
<tr class="separator:a89104efbfbb61e297b4632e421a3338d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa1d90aadef047816163f2e835cac62d6" id="r_aa1d90aadef047816163f2e835cac62d6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#aa1d90aadef047816163f2e835cac62d6">required</a></td></tr>
<tr class="separator:aa1d90aadef047816163f2e835cac62d6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9c85e45324cd38c19cb630fe617a046a" id="r_a9c85e45324cd38c19cb630fe617a046a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a9c85e45324cd38c19cb630fe617a046a">type</a></td></tr>
<tr class="separator:a9c85e45324cd38c19cb630fe617a046a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad226751260da61bc5831d7c01f788543" id="r_ad226751260da61bc5831d7c01f788543"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ad226751260da61bc5831d7c01f788543">int</a></td></tr>
<tr class="separator:ad226751260da61bc5831d7c01f788543"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad0c637f5a071da1a3622024b053b54dc" id="r_ad0c637f5a071da1a3622024b053b54dc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ad0c637f5a071da1a3622024b053b54dc">default</a></td></tr>
<tr class="separator:ad0c637f5a071da1a3622024b053b54dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a35340b1720f20682fca8879712246152" id="r_a35340b1720f20682fca8879712246152"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a35340b1720f20682fca8879712246152">float</a></td></tr>
<tr class="separator:a35340b1720f20682fca8879712246152"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2a500249682adb9260029bd3104d726e" id="r_a2a500249682adb9260029bd3104d726e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a2a500249682adb9260029bd3104d726e">choices</a></td></tr>
<tr class="separator:a2a500249682adb9260029bd3104d726e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18eabe1e89d13a9a285a7ebc46197302" id="r_a18eabe1e89d13a9a285a7ebc46197302"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a18eabe1e89d13a9a285a7ebc46197302">args</a> = parser.parse_args()</td></tr>
<tr class="separator:a18eabe1e89d13a9a285a7ebc46197302"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac36eaf2216a519f9c329f1a8d98f30e5" id="r_ac36eaf2216a519f9c329f1a8d98f30e5"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ad226751260da61bc5831d7c01f788543">int</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ac36eaf2216a519f9c329f1a8d98f30e5">skipgram</a> = 1</td></tr>
<tr class="separator:ac36eaf2216a519f9c329f1a8d98f30e5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a86408c0fb3acb38af0015d92012cedba" id="r_a86408c0fb3acb38af0015d92012cedba"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a86408c0fb3acb38af0015d92012cedba">corpus</a> = <a class="el" href="classgensim_1_1models_1_1deprecated_1_1word2vec_1_1_line_sentence.html">LineSentence</a>(args.train)</td></tr>
<tr class="separator:a86408c0fb3acb38af0015d92012cedba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6ac4f31ea646dc416e034430eb9bd41e" id="r_a6ac4f31ea646dc416e034430eb9bd41e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a6ac4f31ea646dc416e034430eb9bd41e">model</a></td></tr>
<tr class="separator:a6ac4f31ea646dc416e034430eb9bd41e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9e31cf70044843e7ff356975b8c57b19" id="r_a9e31cf70044843e7ff356975b8c57b19"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#a9e31cf70044843e7ff356975b8c57b19">outfile</a> = args.output</td></tr>
<tr class="separator:a9e31cf70044843e7ff356975b8c57b19"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad0b897897ce2f11d5d259f33f227f7a0" id="r_ad0b897897ce2f11d5d259f33f227f7a0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ad0b897897ce2f11d5d259f33f227f7a0">binary</a></td></tr>
<tr class="separator:ad0b897897ce2f11d5d259f33f227f7a0"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Warnings
--------
.. deprecated:: 3.3.0
   Use :mod:`gensim.models.word2vec` instead.


Produce word vectors with deep learning via word2vec's "skip-gram and CBOW models", using either
hierarchical softmax or negative sampling [1]_ [2]_.

NOTE: There are more ways to get word vectors in Gensim than just Word2Vec.
See wrappers for FastText, VarEmbed and WordRank.

The training algorithms were originally ported from the C package https://code.google.com/p/word2vec/
and extended with additional functionality.

For a blog tutorial on gensim word2vec, with an interactive web app trained on GoogleNews,
visit http://radimrehurek.com/2014/02/word2vec-tutorial/

**Make sure you have a C compiler before installing gensim, to use optimized (compiled) word2vec training**
(70x speedup compared to plain NumPy implementation [3]_).

Initialize a model with e.g.:

.. sourcecode:: pycon

    &gt;&gt;&gt; model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)

Persist a model to disk with:

.. sourcecode:: pycon

    &gt;&gt;&gt; model.save(fname)
    &gt;&gt;&gt; model = Word2Vec.load(fname)  # you can continue training with the loaded model!

The word vectors are stored in a KeyedVectors instance in model.wv.
This separates the read-only word vector lookup operations in KeyedVectors from the training code in Word2Vec:

.. sourcecode:: pycon

    &gt;&gt;&gt; model.wv['computer']  # numpy vector of a word
    array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)

The word vectors can also be instantiated from an existing file on disk in the word2vec C format
as a KeyedVectors instance::

    NOTE: It is impossible to continue training the vectors loaded from the C format because hidden weights,
    vocabulary frequency and the binary tree is missing:

    .. sourcecode:: pycon

        &gt;&gt;&gt; from gensim.models.keyedvectors import KeyedVectors
        &gt;&gt;&gt; word_vectors = KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)  # C text format
        &gt;&gt;&gt; word_vectors = KeyedVectors.load_word2vec_format('/tmp/vectors.bin', binary=True)  # C binary format


You can perform various NLP word tasks with the model. Some of them
are already built-in:

.. sourcecode:: pycon

    &gt;&gt;&gt; model.wv.most_similar(positive=['woman', 'king'], negative=['man'])
    [('queen', 0.50882536), ...]

    &gt;&gt;&gt; model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])
    [('queen', 0.71382287), ...]

    &gt;&gt;&gt; model.wv.doesnt_match("breakfast cereal dinner lunch".split())
    'cereal'

    &gt;&gt;&gt; model.wv.similarity('woman', 'man')
    0.73723527

Probability of a text under the model:

.. sourcecode:: pycon

    &gt;&gt;&gt; model.score(["The fox jumped over a lazy dog".split()])
    0.2158356

Correlation with human opinion on word similarity:

.. sourcecode:: pycon

    &gt;&gt;&gt; model.wv.evaluate_word_pairs(os.path.join(module_path, 'test_data','wordsim353.tsv'))
    0.51, 0.62, 0.13

And on analogies:

.. sourcecode:: pycon

    &gt;&gt;&gt; model.wv.accuracy(os.path.join(module_path, 'test_data', 'questions-words.txt'))

and so on.

If you're finished training a model (i.e. no more updates, only querying),
then switch to the :mod:`gensim.models.KeyedVectors` instance in wv

.. sourcecode:: pycon

    &gt;&gt;&gt; word_vectors = model.wv
    &gt;&gt;&gt; del model

to trim unneeded model memory = use much less RAM.

Note that there is a :mod:`gensim.models.phrases` module which lets you automatically
detect phrases longer than one word. Using phrases, you can learn a word2vec model
where "words" are actually multiword expressions, such as `new_york_times` or `financial_crisis`:

.. sourcecode:: pycon

    &gt;&gt;&gt; bigram_transformer = gensim.models.Phrases(sentences)
    &gt;&gt;&gt; model = Word2Vec(bigram_transformer[sentences], size=100, ...)

.. [1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
       Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.
.. [2] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
       Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.
.. [3] Optimizing word2vec in gensim, http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="a0aaa5fe8a8e34eabf5cf0d9cae7c7708" name="a0aaa5fe8a8e34eabf5cf0d9cae7c7708"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0aaa5fe8a8e34eabf5cf0d9cae7c7708">&#9670;&#160;</a></span>load_old_word2vec()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.load_old_word2vec </td>
          <td>(</td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>args</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  168</span><span class="keyword">def </span>load_old_word2vec(*args, **kwargs):</div>
<div class="line"><span class="lineno">  169</span>    old_model = Word2Vec.load(*args, **kwargs)</div>
<div class="line"><span class="lineno">  170</span>    vector_size = getattr(old_model, <span class="stringliteral">&#39;vector_size&#39;</span>, old_model.layer1_size)</div>
<div class="line"><span class="lineno">  171</span>    params = {</div>
<div class="line"><span class="lineno">  172</span>        <span class="stringliteral">&#39;size&#39;</span>: vector_size,</div>
<div class="line"><span class="lineno">  173</span>        <span class="stringliteral">&#39;alpha&#39;</span>: old_model.alpha,</div>
<div class="line"><span class="lineno">  174</span>        <span class="stringliteral">&#39;window&#39;</span>: old_model.window,</div>
<div class="line"><span class="lineno">  175</span>        <span class="stringliteral">&#39;min_count&#39;</span>: old_model.min_count,</div>
<div class="line"><span class="lineno">  176</span>        <span class="stringliteral">&#39;max_vocab_size&#39;</span>: old_model.__dict__.get(<span class="stringliteral">&#39;max_vocab_size&#39;</span>, <span class="keywordtype">None</span>),</div>
<div class="line"><span class="lineno">  177</span>        <span class="stringliteral">&#39;sample&#39;</span>: old_model.__dict__.get(<span class="stringliteral">&#39;sample&#39;</span>, 1e-3),</div>
<div class="line"><span class="lineno">  178</span>        <span class="stringliteral">&#39;seed&#39;</span>: old_model.seed,</div>
<div class="line"><span class="lineno">  179</span>        <span class="stringliteral">&#39;workers&#39;</span>: old_model.workers,</div>
<div class="line"><span class="lineno">  180</span>        <span class="stringliteral">&#39;min_alpha&#39;</span>: old_model.min_alpha,</div>
<div class="line"><span class="lineno">  181</span>        <span class="stringliteral">&#39;sg&#39;</span>: old_model.sg,</div>
<div class="line"><span class="lineno">  182</span>        <span class="stringliteral">&#39;hs&#39;</span>: old_model.hs,</div>
<div class="line"><span class="lineno">  183</span>        <span class="stringliteral">&#39;negative&#39;</span>: old_model.negative,</div>
<div class="line"><span class="lineno">  184</span>        <span class="stringliteral">&#39;cbow_mean&#39;</span>: old_model.cbow_mean,</div>
<div class="line"><span class="lineno">  185</span>        <span class="stringliteral">&#39;hashfxn&#39;</span>: old_model.__dict__.get(<span class="stringliteral">&#39;hashfxn&#39;</span>, hash),</div>
<div class="line"><span class="lineno">  186</span>        <span class="stringliteral">&#39;iter&#39;</span>: old_model.__dict__.get(<span class="stringliteral">&#39;iter&#39;</span>, 5),</div>
<div class="line"><span class="lineno">  187</span>        <span class="stringliteral">&#39;null_word&#39;</span>: old_model.__dict__.get(<span class="stringliteral">&#39;null_word&#39;</span>, 0),</div>
<div class="line"><span class="lineno">  188</span>        <span class="stringliteral">&#39;sorted_vocab&#39;</span>: old_model.__dict__.get(<span class="stringliteral">&#39;sorted_vocab&#39;</span>, 1),</div>
<div class="line"><span class="lineno">  189</span>        <span class="stringliteral">&#39;batch_words&#39;</span>: old_model.__dict__.get(<span class="stringliteral">&#39;batch_words&#39;</span>, MAX_WORDS_IN_BATCH),</div>
<div class="line"><span class="lineno">  190</span>        <span class="stringliteral">&#39;compute_loss&#39;</span>: old_model.__dict__.get(<span class="stringliteral">&#39;compute_loss&#39;</span>, <span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">  191</span>    }</div>
<div class="line"><span class="lineno">  192</span>    new_model = NewWord2Vec(**params)</div>
<div class="line"><span class="lineno">  193</span>    <span class="comment"># set trainables attributes</span></div>
<div class="line"><span class="lineno">  194</span>    new_model.wv.vectors = old_model.wv.syn0</div>
<div class="line"><span class="lineno">  195</span>    <span class="keywordflow">if</span> hasattr(old_model.wv, <span class="stringliteral">&#39;syn0norm&#39;</span>):</div>
<div class="line"><span class="lineno">  196</span>        new_model.wv.vectors_norm = old_model.wv.syn0norm</div>
<div class="line"><span class="lineno">  197</span>    <span class="keywordflow">if</span> hasattr(old_model, <span class="stringliteral">&#39;syn1&#39;</span>):</div>
<div class="line"><span class="lineno">  198</span>        new_model.trainables.syn1 = old_model.syn1</div>
<div class="line"><span class="lineno">  199</span>    <span class="keywordflow">if</span> hasattr(old_model, <span class="stringliteral">&#39;syn1neg&#39;</span>):</div>
<div class="line"><span class="lineno">  200</span>        new_model.trainables.syn1neg = old_model.syn1neg</div>
<div class="line"><span class="lineno">  201</span>    <span class="keywordflow">if</span> hasattr(old_model, <span class="stringliteral">&#39;syn0_lockf&#39;</span>):</div>
<div class="line"><span class="lineno">  202</span>        new_model.trainables.vectors_lockf = old_model.syn0_lockf</div>
<div class="line"><span class="lineno">  203</span>    <span class="comment"># set vocabulary attributes</span></div>
<div class="line"><span class="lineno">  204</span>    new_model.wv.vocab = old_model.wv.vocab</div>
<div class="line"><span class="lineno">  205</span>    new_model.wv.index2word = old_model.wv.index2word</div>
<div class="line"><span class="lineno">  206</span>    new_model.vocabulary.cum_table = old_model.__dict__.get(<span class="stringliteral">&#39;cum_table&#39;</span>, <span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">  207</span> </div>
<div class="line"><span class="lineno">  208</span>    new_model.train_count = old_model.__dict__.get(<span class="stringliteral">&#39;train_count&#39;</span>, <span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">  209</span>    new_model.corpus_count = old_model.__dict__.get(<span class="stringliteral">&#39;corpus_count&#39;</span>, <span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">  210</span>    new_model.corpus_total_words = old_model.__dict__.get(<span class="stringliteral">&#39;corpus_total_words&#39;</span>, <span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">  211</span>    new_model.running_training_loss = old_model.__dict__.get(<span class="stringliteral">&#39;running_training_loss&#39;</span>, 0)</div>
<div class="line"><span class="lineno">  212</span>    new_model.total_train_time = old_model.__dict__.get(<span class="stringliteral">&#39;total_train_time&#39;</span>, <span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">  213</span>    new_model.min_alpha_yet_reached = old_model.__dict__.get(<span class="stringliteral">&#39;min_alpha_yet_reached&#39;</span>, old_model.alpha)</div>
<div class="line"><span class="lineno">  214</span>    new_model.model_trimmed_post_training = old_model.__dict__.get(<span class="stringliteral">&#39;model_trimmed_post_training&#39;</span>, <span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">  215</span> </div>
<div class="line"><span class="lineno">  216</span>    <span class="keywordflow">return</span> new_model</div>
<div class="line"><span class="lineno">  217</span> </div>
<div class="line"><span class="lineno">  218</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aefcce5b1288c7b0d29270b8a687883c4" name="aefcce5b1288c7b0d29270b8a687883c4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aefcce5b1288c7b0d29270b8a687883c4">&#9670;&#160;</a></span>score_cbow_pair()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.score_cbow_pair </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  492</span><span class="keyword">def </span>score_cbow_pair(model, word, l1):</div>
<div class="line"><span class="lineno">  493</span>    l2a = model.syn1[word.point]  <span class="comment"># 2d matrix, codelen x layer1_size</span></div>
<div class="line"><span class="lineno">  494</span>    sgn = (-1.0)**word.code  <span class="comment"># ch function, 0-&gt; 1, 1 -&gt; -1</span></div>
<div class="line"><span class="lineno">  495</span>    lprob = -logaddexp(0, -sgn * dot(l1, l2a.T))</div>
<div class="line"><span class="lineno">  496</span>    <span class="keywordflow">return</span> sum(lprob)</div>
<div class="line"><span class="lineno">  497</span> </div>
<div class="line"><span class="lineno">  498</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="af27aebb545e81d4f99160532e3fd9577" name="af27aebb545e81d4f99160532e3fd9577"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af27aebb545e81d4f99160532e3fd9577">&#9670;&#160;</a></span>score_sentence_cbow()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.score_sentence_cbow </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sentence</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>work</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>neu1</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Obtain likelihood score for a single sentence in a fitted CBOW representaion.

The sentence is a list of Vocab objects (or None, where the corresponding
word is not in the vocabulary. Called internally from `Word2Vec.score()`.

This is the non-optimized, Python version. If you have cython installed, gensim
will use the optimized version from word2vec_inner instead.</pre> <div class="fragment"><div class="line"><span class="lineno">  308</span><span class="keyword">def </span>score_sentence_cbow(model, sentence, work=None, neu1=None):</div>
<div class="line"><span class="lineno">  309</span>    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  310</span><span class="stringliteral">    Obtain likelihood score for a single sentence in a fitted CBOW representaion.</span></div>
<div class="line"><span class="lineno">  311</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  312</span><span class="stringliteral">    The sentence is a list of Vocab objects (or None, where the corresponding</span></div>
<div class="line"><span class="lineno">  313</span><span class="stringliteral">    word is not in the vocabulary. Called internally from `Word2Vec.score()`.</span></div>
<div class="line"><span class="lineno">  314</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  315</span><span class="stringliteral">    This is the non-optimized, Python version. If you have cython installed, gensim</span></div>
<div class="line"><span class="lineno">  316</span><span class="stringliteral">    will use the optimized version from word2vec_inner instead.</span></div>
<div class="line"><span class="lineno">  317</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  318</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  319</span>    log_prob_sentence = 0.0</div>
<div class="line"><span class="lineno">  320</span>    <span class="keywordflow">if</span> model.negative:</div>
<div class="line"><span class="lineno">  321</span>        <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&quot;scoring is only available for HS=True&quot;</span>)</div>
<div class="line"><span class="lineno">  322</span> </div>
<div class="line"><span class="lineno">  323</span>    word_vocabs = [model.wv.vocab[w] <span class="keywordflow">for</span> w <span class="keywordflow">in</span> sentence <span class="keywordflow">if</span> w <span class="keywordflow">in</span> model.wv.vocab]</div>
<div class="line"><span class="lineno">  324</span>    <span class="keywordflow">for</span> pos, word <span class="keywordflow">in</span> enumerate(word_vocabs):</div>
<div class="line"><span class="lineno">  325</span>        <span class="keywordflow">if</span> word <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  326</span>            <span class="keywordflow">continue</span>  <span class="comment"># OOV word in the input sentence =&gt; skip</span></div>
<div class="line"><span class="lineno">  327</span> </div>
<div class="line"><span class="lineno">  328</span>        start = max(0, pos - model.window)</div>
<div class="line"><span class="lineno">  329</span>        window_pos = enumerate(word_vocabs[start:(pos + model.window + 1)], start)</div>
<div class="line"><span class="lineno">  330</span>        word2_indices = [word2.index <span class="keywordflow">for</span> pos2, word2 <span class="keywordflow">in</span> window_pos <span class="keywordflow">if</span> (word2 <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> pos2 != pos)]</div>
<div class="line"><span class="lineno">  331</span>        l1 = np_sum(model.wv.syn0[word2_indices], axis=0)  <span class="comment"># 1 x layer1_size</span></div>
<div class="line"><span class="lineno">  332</span>        <span class="keywordflow">if</span> word2_indices <span class="keywordflow">and</span> model.cbow_mean:</div>
<div class="line"><span class="lineno">  333</span>            l1 /= len(word2_indices)</div>
<div class="line"><span class="lineno">  334</span>        log_prob_sentence += score_cbow_pair(model, word, l1)</div>
<div class="line"><span class="lineno">  335</span> </div>
<div class="line"><span class="lineno">  336</span>    <span class="keywordflow">return</span> log_prob_sentence</div>
<div class="line"><span class="lineno">  337</span> </div>
<div class="line"><span class="lineno">  338</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aab6898fcb39cd838bdcaef6bdbf1a0b2" name="aab6898fcb39cd838bdcaef6bdbf1a0b2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aab6898fcb39cd838bdcaef6bdbf1a0b2">&#9670;&#160;</a></span>score_sentence_sg()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.score_sentence_sg </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sentence</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>work</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Obtain likelihood score for a single sentence in a fitted skip-gram representaion.

The sentence is a list of Vocab objects (or None, when the corresponding
word is not in the vocabulary). Called internally from `Word2Vec.score()`.

This is the non-optimized, Python version. If you have cython installed, gensim
will use the optimized version from word2vec_inner instead.</pre> <div class="fragment"><div class="line"><span class="lineno">  278</span><span class="keyword">def </span>score_sentence_sg(model, sentence, work=None):</div>
<div class="line"><span class="lineno">  279</span>    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  280</span><span class="stringliteral">    Obtain likelihood score for a single sentence in a fitted skip-gram representaion.</span></div>
<div class="line"><span class="lineno">  281</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  282</span><span class="stringliteral">    The sentence is a list of Vocab objects (or None, when the corresponding</span></div>
<div class="line"><span class="lineno">  283</span><span class="stringliteral">    word is not in the vocabulary). Called internally from `Word2Vec.score()`.</span></div>
<div class="line"><span class="lineno">  284</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  285</span><span class="stringliteral">    This is the non-optimized, Python version. If you have cython installed, gensim</span></div>
<div class="line"><span class="lineno">  286</span><span class="stringliteral">    will use the optimized version from word2vec_inner instead.</span></div>
<div class="line"><span class="lineno">  287</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  288</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  289</span>    log_prob_sentence = 0.0</div>
<div class="line"><span class="lineno">  290</span>    <span class="keywordflow">if</span> model.negative:</div>
<div class="line"><span class="lineno">  291</span>        <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&quot;scoring is only available for HS=True&quot;</span>)</div>
<div class="line"><span class="lineno">  292</span> </div>
<div class="line"><span class="lineno">  293</span>    word_vocabs = [model.wv.vocab[w] <span class="keywordflow">for</span> w <span class="keywordflow">in</span> sentence <span class="keywordflow">if</span> w <span class="keywordflow">in</span> model.wv.vocab]</div>
<div class="line"><span class="lineno">  294</span>    <span class="keywordflow">for</span> pos, word <span class="keywordflow">in</span> enumerate(word_vocabs):</div>
<div class="line"><span class="lineno">  295</span>        <span class="keywordflow">if</span> word <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  296</span>            <span class="keywordflow">continue</span>  <span class="comment"># OOV word in the input sentence =&gt; skip</span></div>
<div class="line"><span class="lineno">  297</span> </div>
<div class="line"><span class="lineno">  298</span>        <span class="comment"># now go over all words from the window, predicting each one in turn</span></div>
<div class="line"><span class="lineno">  299</span>        start = max(0, pos - model.window)</div>
<div class="line"><span class="lineno">  300</span>        <span class="keywordflow">for</span> pos2, word2 <span class="keywordflow">in</span> enumerate(word_vocabs[start: pos + model.window + 1], start):</div>
<div class="line"><span class="lineno">  301</span>            <span class="comment"># don&#39;t train on OOV words and on the `word` itself</span></div>
<div class="line"><span class="lineno">  302</span>            <span class="keywordflow">if</span> word2 <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> pos2 != pos:</div>
<div class="line"><span class="lineno">  303</span>                log_prob_sentence += score_sg_pair(model, word, word2)</div>
<div class="line"><span class="lineno">  304</span> </div>
<div class="line"><span class="lineno">  305</span>    <span class="keywordflow">return</span> log_prob_sentence</div>
<div class="line"><span class="lineno">  306</span> </div>
<div class="line"><span class="lineno">  307</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ab4c9d207fcf526f5f17759f142a46aaf" name="ab4c9d207fcf526f5f17759f142a46aaf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab4c9d207fcf526f5f17759f142a46aaf">&#9670;&#160;</a></span>score_sg_pair()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.score_sg_pair </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word2</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  484</span><span class="keyword">def </span>score_sg_pair(model, word, word2):</div>
<div class="line"><span class="lineno">  485</span>    l1 = model.wv.syn0[word2.index]</div>
<div class="line"><span class="lineno">  486</span>    l2a = deepcopy(model.syn1[word.point])  <span class="comment"># 2d matrix, codelen x layer1_size</span></div>
<div class="line"><span class="lineno">  487</span>    sgn = (-1.0)**word.code  <span class="comment"># ch function, 0-&gt; 1, 1 -&gt; -1</span></div>
<div class="line"><span class="lineno">  488</span>    lprob = -logaddexp(0, -sgn * dot(l1, l2a.T))</div>
<div class="line"><span class="lineno">  489</span>    <span class="keywordflow">return</span> sum(lprob)</div>
<div class="line"><span class="lineno">  490</span> </div>
<div class="line"><span class="lineno">  491</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ab7bb718e3a63f9b2f03df265dfb0b938" name="ab7bb718e3a63f9b2f03df265dfb0b938"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab7bb718e3a63f9b2f03df265dfb0b938">&#9670;&#160;</a></span>train_batch_cbow()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.train_batch_cbow </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sentences</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>work</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>neu1</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_loss</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Update CBOW model by training on a sequence of sentences.

Each sentence is a list of string tokens, which are looked up in the model's
vocab dictionary. Called internally from `Word2Vec.train()`.

This is the non-optimized, Python version. If you have cython installed, gensim
will use the optimized version from word2vec_inner instead.</pre> <div class="fragment"><div class="line"><span class="lineno">  250</span><span class="keyword">def </span>train_batch_cbow(model, sentences, alpha, work=None, neu1=None, compute_loss=False):</div>
<div class="line"><span class="lineno">  251</span>    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  252</span><span class="stringliteral">    Update CBOW model by training on a sequence of sentences.</span></div>
<div class="line"><span class="lineno">  253</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  254</span><span class="stringliteral">    Each sentence is a list of string tokens, which are looked up in the model&#39;s</span></div>
<div class="line"><span class="lineno">  255</span><span class="stringliteral">    vocab dictionary. Called internally from `Word2Vec.train()`.</span></div>
<div class="line"><span class="lineno">  256</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  257</span><span class="stringliteral">    This is the non-optimized, Python version. If you have cython installed, gensim</span></div>
<div class="line"><span class="lineno">  258</span><span class="stringliteral">    will use the optimized version from word2vec_inner instead.</span></div>
<div class="line"><span class="lineno">  259</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  260</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  261</span>    result = 0</div>
<div class="line"><span class="lineno">  262</span>    <span class="keywordflow">for</span> sentence <span class="keywordflow">in</span> sentences:</div>
<div class="line"><span class="lineno">  263</span>        word_vocabs = [model.wv.vocab[w] <span class="keywordflow">for</span> w <span class="keywordflow">in</span> sentence <span class="keywordflow">if</span> w <span class="keywordflow">in</span> model.wv.vocab</div>
<div class="line"><span class="lineno">  264</span>                       <span class="keywordflow">and</span> model.wv.vocab[w].sample_int &gt; model.random.rand() * 2**32]</div>
<div class="line"><span class="lineno">  265</span>        <span class="keywordflow">for</span> pos, word <span class="keywordflow">in</span> enumerate(word_vocabs):</div>
<div class="line"><span class="lineno">  266</span>            reduced_window = model.random.randint(model.window)  <span class="comment"># `b` in the original word2vec code</span></div>
<div class="line"><span class="lineno">  267</span>            start = max(0, pos - model.window + reduced_window)</div>
<div class="line"><span class="lineno">  268</span>            window_pos = enumerate(word_vocabs[start:(pos + model.window + 1 - reduced_window)], start)</div>
<div class="line"><span class="lineno">  269</span>            word2_indices = [word2.index <span class="keywordflow">for</span> pos2, word2 <span class="keywordflow">in</span> window_pos <span class="keywordflow">if</span> (word2 <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> pos2 != pos)]</div>
<div class="line"><span class="lineno">  270</span>            l1 = np_sum(model.wv.syn0[word2_indices], axis=0)  <span class="comment"># 1 x vector_size</span></div>
<div class="line"><span class="lineno">  271</span>            <span class="keywordflow">if</span> word2_indices <span class="keywordflow">and</span> model.cbow_mean:</div>
<div class="line"><span class="lineno">  272</span>                l1 /= len(word2_indices)</div>
<div class="line"><span class="lineno">  273</span>            train_cbow_pair(model, word, word2_indices, l1, alpha, compute_loss=compute_loss)</div>
<div class="line"><span class="lineno">  274</span>        result += len(word_vocabs)</div>
<div class="line"><span class="lineno">  275</span>    <span class="keywordflow">return</span> result</div>
<div class="line"><span class="lineno">  276</span> </div>
<div class="line"><span class="lineno">  277</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a222f8a3a71c52337bad6109ec840ec30" name="a222f8a3a71c52337bad6109ec840ec30"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a222f8a3a71c52337bad6109ec840ec30">&#9670;&#160;</a></span>train_batch_sg()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.train_batch_sg </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sentences</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>work</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_loss</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Update skip-gram model by training on a sequence of sentences.

Each sentence is a list of string tokens, which are looked up in the model's
vocab dictionary. Called internally from `Word2Vec.train()`.

This is the non-optimized, Python version. If you have cython installed, gensim
will use the optimized version from word2vec_inner instead.</pre> <div class="fragment"><div class="line"><span class="lineno">  219</span><span class="keyword">def </span>train_batch_sg(model, sentences, alpha, work=None, compute_loss=False):</div>
<div class="line"><span class="lineno">  220</span>    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  221</span><span class="stringliteral">    Update skip-gram model by training on a sequence of sentences.</span></div>
<div class="line"><span class="lineno">  222</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  223</span><span class="stringliteral">    Each sentence is a list of string tokens, which are looked up in the model&#39;s</span></div>
<div class="line"><span class="lineno">  224</span><span class="stringliteral">    vocab dictionary. Called internally from `Word2Vec.train()`.</span></div>
<div class="line"><span class="lineno">  225</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  226</span><span class="stringliteral">    This is the non-optimized, Python version. If you have cython installed, gensim</span></div>
<div class="line"><span class="lineno">  227</span><span class="stringliteral">    will use the optimized version from word2vec_inner instead.</span></div>
<div class="line"><span class="lineno">  228</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  229</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  230</span>    result = 0</div>
<div class="line"><span class="lineno">  231</span>    <span class="keywordflow">for</span> sentence <span class="keywordflow">in</span> sentences:</div>
<div class="line"><span class="lineno">  232</span>        word_vocabs = [model.wv.vocab[w] <span class="keywordflow">for</span> w <span class="keywordflow">in</span> sentence <span class="keywordflow">if</span> w <span class="keywordflow">in</span> model.wv.vocab</div>
<div class="line"><span class="lineno">  233</span>                       <span class="keywordflow">and</span> model.wv.vocab[w].sample_int &gt; model.random.rand() * 2**32]</div>
<div class="line"><span class="lineno">  234</span>        <span class="keywordflow">for</span> pos, word <span class="keywordflow">in</span> enumerate(word_vocabs):</div>
<div class="line"><span class="lineno">  235</span>            reduced_window = model.random.randint(model.window)  <span class="comment"># `b` in the original word2vec code</span></div>
<div class="line"><span class="lineno">  236</span> </div>
<div class="line"><span class="lineno">  237</span>            <span class="comment"># now go over all words from the (reduced) window, predicting each one in turn</span></div>
<div class="line"><span class="lineno">  238</span>            start = max(0, pos - model.window + reduced_window)</div>
<div class="line"><span class="lineno">  239</span>            <span class="keywordflow">for</span> pos2, word2 <span class="keywordflow">in</span> enumerate(word_vocabs[start:(pos + model.window + 1 - reduced_window)], start):</div>
<div class="line"><span class="lineno">  240</span>                <span class="comment"># don&#39;t train on the `word` itself</span></div>
<div class="line"><span class="lineno">  241</span>                <span class="keywordflow">if</span> pos2 != pos:</div>
<div class="line"><span class="lineno">  242</span>                    train_sg_pair(</div>
<div class="line"><span class="lineno">  243</span>                        model, model.wv.index2word[word.index], word2.index, alpha, compute_loss=compute_loss</div>
<div class="line"><span class="lineno">  244</span>                    )</div>
<div class="line"><span class="lineno">  245</span> </div>
<div class="line"><span class="lineno">  246</span>        result += len(word_vocabs)</div>
<div class="line"><span class="lineno">  247</span>    <span class="keywordflow">return</span> result</div>
<div class="line"><span class="lineno">  248</span> </div>
<div class="line"><span class="lineno">  249</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a49eabffc774c15890363898ffecf94ed" name="a49eabffc774c15890363898ffecf94ed"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a49eabffc774c15890363898ffecf94ed">&#9670;&#160;</a></span>train_cbow_pair()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.train_cbow_pair </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>input_word_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learn_vectors</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learn_hidden</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_loss</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_vectors</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_locks</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>is_ft</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  416</span>                    compute_loss=<span class="keyword">False</span>, context_vectors=<span class="keywordtype">None</span>, context_locks=<span class="keywordtype">None</span>, is_ft=<span class="keyword">False</span>):</div>
<div class="line"><span class="lineno">  417</span>    <span class="keywordflow">if</span> context_vectors <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  418</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  419</span>            context_vectors_vocab = model.wv.syn0_vocab</div>
<div class="line"><span class="lineno">  420</span>            context_vectors_ngrams = model.wv.syn0_ngrams</div>
<div class="line"><span class="lineno">  421</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  422</span>            context_vectors = model.wv.syn0</div>
<div class="line"><span class="lineno">  423</span>    <span class="keywordflow">if</span> context_locks <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  424</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  425</span>            context_locks_vocab = model.syn0_vocab_lockf</div>
<div class="line"><span class="lineno">  426</span>            context_locks_ngrams = model.syn0_ngrams_lockf</div>
<div class="line"><span class="lineno">  427</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  428</span>            context_locks = model.syn0_lockf</div>
<div class="line"><span class="lineno">  429</span> </div>
<div class="line"><span class="lineno">  430</span>    neu1e = zeros(l1.shape)</div>
<div class="line"><span class="lineno">  431</span> </div>
<div class="line"><span class="lineno">  432</span>    <span class="keywordflow">if</span> model.hs:</div>
<div class="line"><span class="lineno">  433</span>        l2a = model.syn1[word.point]  <span class="comment"># 2d matrix, codelen x layer1_size</span></div>
<div class="line"><span class="lineno">  434</span>        prod_term = dot(l1, l2a.T)</div>
<div class="line"><span class="lineno">  435</span>        fa = expit(prod_term)  <span class="comment"># propagate hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  436</span>        ga = (1. - word.code - fa) * alpha  <span class="comment"># vector of error gradients multiplied by the learning rate</span></div>
<div class="line"><span class="lineno">  437</span>        <span class="keywordflow">if</span> learn_hidden:</div>
<div class="line"><span class="lineno">  438</span>            model.syn1[word.point] += outer(ga, l1)  <span class="comment"># learn hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  439</span>        neu1e += dot(ga, l2a)  <span class="comment"># save error</span></div>
<div class="line"><span class="lineno">  440</span> </div>
<div class="line"><span class="lineno">  441</span>        <span class="comment"># loss component corresponding to hierarchical softmax</span></div>
<div class="line"><span class="lineno">  442</span>        <span class="keywordflow">if</span> compute_loss:</div>
<div class="line"><span class="lineno">  443</span>            sgn = (-1.0)**word.code  <span class="comment"># ch function, 0-&gt; 1, 1 -&gt; -1</span></div>
<div class="line"><span class="lineno">  444</span>            model.running_training_loss += sum(-log(expit(-sgn * prod_term)))</div>
<div class="line"><span class="lineno">  445</span> </div>
<div class="line"><span class="lineno">  446</span>    <span class="keywordflow">if</span> model.negative:</div>
<div class="line"><span class="lineno">  447</span>        <span class="comment"># use this word (label = 1) + `negative` other random words not from this sentence (label = 0)</span></div>
<div class="line"><span class="lineno">  448</span>        word_indices = [word.index]</div>
<div class="line"><span class="lineno">  449</span>        <span class="keywordflow">while</span> len(word_indices) &lt; model.negative + 1:</div>
<div class="line"><span class="lineno">  450</span>            w = model.cum_table.searchsorted(model.random.randint(model.cum_table[-1]))</div>
<div class="line"><span class="lineno">  451</span>            <span class="keywordflow">if</span> w != word.index:</div>
<div class="line"><span class="lineno">  452</span>                word_indices.append(w)</div>
<div class="line"><span class="lineno">  453</span>        l2b = model.syn1neg[word_indices]  <span class="comment"># 2d matrix, k+1 x layer1_size</span></div>
<div class="line"><span class="lineno">  454</span>        prod_term = dot(l1, l2b.T)</div>
<div class="line"><span class="lineno">  455</span>        fb = expit(prod_term)  <span class="comment"># propagate hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  456</span>        gb = (model.neg_labels - fb) * alpha  <span class="comment"># vector of error gradients multiplied by the learning rate</span></div>
<div class="line"><span class="lineno">  457</span>        <span class="keywordflow">if</span> learn_hidden:</div>
<div class="line"><span class="lineno">  458</span>            model.syn1neg[word_indices] += outer(gb, l1)  <span class="comment"># learn hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  459</span>        neu1e += dot(gb, l2b)  <span class="comment"># save error</span></div>
<div class="line"><span class="lineno">  460</span> </div>
<div class="line"><span class="lineno">  461</span>        <span class="comment"># loss component corresponding to negative sampling</span></div>
<div class="line"><span class="lineno">  462</span>        <span class="keywordflow">if</span> compute_loss:</div>
<div class="line"><span class="lineno">  463</span>            model.running_training_loss -= sum(log(expit(-1 * prod_term[1:])))  <span class="comment"># for the sampled words</span></div>
<div class="line"><span class="lineno">  464</span>            model.running_training_loss -= log(expit(prod_term[0]))  <span class="comment"># for the output word</span></div>
<div class="line"><span class="lineno">  465</span> </div>
<div class="line"><span class="lineno">  466</span>    <span class="keywordflow">if</span> learn_vectors:</div>
<div class="line"><span class="lineno">  467</span>        <span class="comment"># learn input -&gt; hidden, here for all words in the window separately</span></div>
<div class="line"><span class="lineno">  468</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  469</span>            <span class="keywordflow">if</span> <span class="keywordflow">not</span> model.cbow_mean <span class="keywordflow">and</span> input_word_indices:</div>
<div class="line"><span class="lineno">  470</span>                neu1e /= (len(input_word_indices[0]) + len(input_word_indices[1]))</div>
<div class="line"><span class="lineno">  471</span>            <span class="keywordflow">for</span> i <span class="keywordflow">in</span> input_word_indices[0]:</div>
<div class="line"><span class="lineno">  472</span>                context_vectors_vocab[i] += neu1e * context_locks_vocab[i]</div>
<div class="line"><span class="lineno">  473</span>            <span class="keywordflow">for</span> i <span class="keywordflow">in</span> input_word_indices[1]:</div>
<div class="line"><span class="lineno">  474</span>                context_vectors_ngrams[i] += neu1e * context_locks_ngrams[i]</div>
<div class="line"><span class="lineno">  475</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  476</span>            <span class="keywordflow">if</span> <span class="keywordflow">not</span> model.cbow_mean <span class="keywordflow">and</span> input_word_indices:</div>
<div class="line"><span class="lineno">  477</span>                neu1e /= len(input_word_indices)</div>
<div class="line"><span class="lineno">  478</span>            <span class="keywordflow">for</span> i <span class="keywordflow">in</span> input_word_indices:</div>
<div class="line"><span class="lineno">  479</span>                context_vectors[i] += neu1e * context_locks[i]</div>
<div class="line"><span class="lineno">  480</span> </div>
<div class="line"><span class="lineno">  481</span>    <span class="keywordflow">return</span> neu1e</div>
<div class="line"><span class="lineno">  482</span> </div>
<div class="line"><span class="lineno">  483</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a32eb23a1e9ef7d58f62640580f3ad3d8" name="a32eb23a1e9ef7d58f62640580f3ad3d8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a32eb23a1e9ef7d58f62640580f3ad3d8">&#9670;&#160;</a></span>train_sg_pair()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.train_sg_pair </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learn_vectors</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learn_hidden</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_vectors</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_locks</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_loss</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>is_ft</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  340</span>                  context_vectors=<span class="keywordtype">None</span>, context_locks=<span class="keywordtype">None</span>, compute_loss=<span class="keyword">False</span>, is_ft=<span class="keyword">False</span>):</div>
<div class="line"><span class="lineno">  341</span>    <span class="keywordflow">if</span> context_vectors <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  342</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  343</span>            context_vectors_vocab = model.wv.syn0_vocab</div>
<div class="line"><span class="lineno">  344</span>            context_vectors_ngrams = model.wv.syn0_ngrams</div>
<div class="line"><span class="lineno">  345</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  346</span>            context_vectors = model.wv.syn0</div>
<div class="line"><span class="lineno">  347</span>    <span class="keywordflow">if</span> context_locks <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  348</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  349</span>            context_locks_vocab = model.syn0_vocab_lockf</div>
<div class="line"><span class="lineno">  350</span>            context_locks_ngrams = model.syn0_ngrams_lockf</div>
<div class="line"><span class="lineno">  351</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  352</span>            context_locks = model.syn0_lockf</div>
<div class="line"><span class="lineno">  353</span> </div>
<div class="line"><span class="lineno">  354</span>    <span class="keywordflow">if</span> word <span class="keywordflow">not</span> <span class="keywordflow">in</span> model.wv.vocab:</div>
<div class="line"><span class="lineno">  355</span>        <span class="keywordflow">return</span></div>
<div class="line"><span class="lineno">  356</span>    predict_word = model.wv.vocab[word]  <span class="comment"># target word (NN output)</span></div>
<div class="line"><span class="lineno">  357</span> </div>
<div class="line"><span class="lineno">  358</span>    <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  359</span>        l1_vocab = context_vectors_vocab[context_index[0]]</div>
<div class="line"><span class="lineno">  360</span>        l1_ngrams = np_sum(context_vectors_ngrams[context_index[1:]], axis=0)</div>
<div class="line"><span class="lineno">  361</span>        <span class="keywordflow">if</span> context_index:</div>
<div class="line"><span class="lineno">  362</span>            l1 = np_sum([l1_vocab, l1_ngrams], axis=0) / len(context_index)</div>
<div class="line"><span class="lineno">  363</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  364</span>        l1 = context_vectors[context_index]  <span class="comment"># input word (NN input/projection layer)</span></div>
<div class="line"><span class="lineno">  365</span>        lock_factor = context_locks[context_index]</div>
<div class="line"><span class="lineno">  366</span> </div>
<div class="line"><span class="lineno">  367</span>    neu1e = zeros(l1.shape)</div>
<div class="line"><span class="lineno">  368</span> </div>
<div class="line"><span class="lineno">  369</span>    <span class="keywordflow">if</span> model.hs:</div>
<div class="line"><span class="lineno">  370</span>        <span class="comment"># work on the entire tree at once, to push as much work into numpy&#39;s C routines as possible (performance)</span></div>
<div class="line"><span class="lineno">  371</span>        l2a = deepcopy(model.syn1[predict_word.point])  <span class="comment"># 2d matrix, codelen x layer1_size</span></div>
<div class="line"><span class="lineno">  372</span>        prod_term = dot(l1, l2a.T)</div>
<div class="line"><span class="lineno">  373</span>        fa = expit(prod_term)  <span class="comment"># propagate hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  374</span>        ga = (1 - predict_word.code - fa) * alpha  <span class="comment"># vector of error gradients multiplied by the learning rate</span></div>
<div class="line"><span class="lineno">  375</span>        <span class="keywordflow">if</span> learn_hidden:</div>
<div class="line"><span class="lineno">  376</span>            model.syn1[predict_word.point] += outer(ga, l1)  <span class="comment"># learn hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  377</span>        neu1e += dot(ga, l2a)  <span class="comment"># save error</span></div>
<div class="line"><span class="lineno">  378</span> </div>
<div class="line"><span class="lineno">  379</span>        <span class="comment"># loss component corresponding to hierarchical softmax</span></div>
<div class="line"><span class="lineno">  380</span>        <span class="keywordflow">if</span> compute_loss:</div>
<div class="line"><span class="lineno">  381</span>            sgn = (-1.0)**predict_word.code  <span class="comment"># `ch` function, 0 -&gt; 1, 1 -&gt; -1</span></div>
<div class="line"><span class="lineno">  382</span>            lprob = -log(expit(-sgn * prod_term))</div>
<div class="line"><span class="lineno">  383</span>            model.running_training_loss += sum(lprob)</div>
<div class="line"><span class="lineno">  384</span> </div>
<div class="line"><span class="lineno">  385</span>    <span class="keywordflow">if</span> model.negative:</div>
<div class="line"><span class="lineno">  386</span>        <span class="comment"># use this word (label = 1) + `negative` other random words not from this sentence (label = 0)</span></div>
<div class="line"><span class="lineno">  387</span>        word_indices = [predict_word.index]</div>
<div class="line"><span class="lineno">  388</span>        <span class="keywordflow">while</span> len(word_indices) &lt; model.negative + 1:</div>
<div class="line"><span class="lineno">  389</span>            w = model.cum_table.searchsorted(model.random.randint(model.cum_table[-1]))</div>
<div class="line"><span class="lineno">  390</span>            <span class="keywordflow">if</span> w != predict_word.index:</div>
<div class="line"><span class="lineno">  391</span>                word_indices.append(w)</div>
<div class="line"><span class="lineno">  392</span>        l2b = model.syn1neg[word_indices]  <span class="comment"># 2d matrix, k+1 x layer1_size</span></div>
<div class="line"><span class="lineno">  393</span>        prod_term = dot(l1, l2b.T)</div>
<div class="line"><span class="lineno">  394</span>        fb = expit(prod_term)  <span class="comment"># propagate hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  395</span>        gb = (model.neg_labels - fb) * alpha  <span class="comment"># vector of error gradients multiplied by the learning rate</span></div>
<div class="line"><span class="lineno">  396</span>        <span class="keywordflow">if</span> learn_hidden:</div>
<div class="line"><span class="lineno">  397</span>            model.syn1neg[word_indices] += outer(gb, l1)  <span class="comment"># learn hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  398</span>        neu1e += dot(gb, l2b)  <span class="comment"># save error</span></div>
<div class="line"><span class="lineno">  399</span> </div>
<div class="line"><span class="lineno">  400</span>        <span class="comment"># loss component corresponding to negative sampling</span></div>
<div class="line"><span class="lineno">  401</span>        <span class="keywordflow">if</span> compute_loss:</div>
<div class="line"><span class="lineno">  402</span>            model.running_training_loss -= sum(log(expit(-1 * prod_term[1:])))  <span class="comment"># for the sampled words</span></div>
<div class="line"><span class="lineno">  403</span>            model.running_training_loss -= log(expit(prod_term[0]))  <span class="comment"># for the output word</span></div>
<div class="line"><span class="lineno">  404</span> </div>
<div class="line"><span class="lineno">  405</span>    <span class="keywordflow">if</span> learn_vectors:</div>
<div class="line"><span class="lineno">  406</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  407</span>            model.wv.syn0_vocab[context_index[0]] += neu1e * context_locks_vocab[context_index[0]]</div>
<div class="line"><span class="lineno">  408</span>            <span class="keywordflow">for</span> i <span class="keywordflow">in</span> context_index[1:]:</div>
<div class="line"><span class="lineno">  409</span>                model.wv.syn0_ngrams[i] += neu1e * context_locks_ngrams[i]</div>
<div class="line"><span class="lineno">  410</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  411</span>            l1 += neu1e * lock_factor  <span class="comment"># learn input -&gt; hidden (mutates model.wv.syn0[word2.index], if that is l1)</span></div>
<div class="line"><span class="lineno">  412</span>    <span class="keywordflow">return</span> neu1e</div>
<div class="line"><span class="lineno">  413</span> </div>
<div class="line"><span class="lineno">  414</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a5f1ec4812ebc8ae60e0130cbe9e93da9" name="a5f1ec4812ebc8ae60e0130cbe9e93da9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5f1ec4812ebc8ae60e0130cbe9e93da9">&#9670;&#160;</a></span>all</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.all</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a18eabe1e89d13a9a285a7ebc46197302" name="a18eabe1e89d13a9a285a7ebc46197302"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a18eabe1e89d13a9a285a7ebc46197302">&#9670;&#160;</a></span>args</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.args = parser.parse_args()</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ad0b897897ce2f11d5d259f33f227f7a0" name="ad0b897897ce2f11d5d259f33f227f7a0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad0b897897ce2f11d5d259f33f227f7a0">&#9670;&#160;</a></span>binary</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.binary</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2a500249682adb9260029bd3104d726e" name="a2a500249682adb9260029bd3104d726e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2a500249682adb9260029bd3104d726e">&#9670;&#160;</a></span>choices</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.choices</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a86408c0fb3acb38af0015d92012cedba" name="a86408c0fb3acb38af0015d92012cedba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a86408c0fb3acb38af0015d92012cedba">&#9670;&#160;</a></span>corpus</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.corpus = <a class="el" href="classgensim_1_1models_1_1deprecated_1_1word2vec_1_1_line_sentence.html">LineSentence</a>(args.train)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ad0c637f5a071da1a3622024b053b54dc" name="ad0c637f5a071da1a3622024b053b54dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad0c637f5a071da1a3622024b053b54dc">&#9670;&#160;</a></span>default</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.default</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a35340b1720f20682fca8879712246152" name="a35340b1720f20682fca8879712246152"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a35340b1720f20682fca8879712246152">&#9670;&#160;</a></span>float</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.float</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a29ae5c108732b5a6d0d351f94a36877d" name="a29ae5c108732b5a6d0d351f94a36877d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a29ae5c108732b5a6d0d351f94a36877d">&#9670;&#160;</a></span>format</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.format</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a89104efbfbb61e297b4632e421a3338d" name="a89104efbfbb61e297b4632e421a3338d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a89104efbfbb61e297b4632e421a3338d">&#9670;&#160;</a></span>help</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.help</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ad226751260da61bc5831d7c01f788543" name="ad226751260da61bc5831d7c01f788543"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad226751260da61bc5831d7c01f788543">&#9670;&#160;</a></span>int</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.int</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2d624122e2eed5779b878361593e4522" name="a2d624122e2eed5779b878361593e4522"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2d624122e2eed5779b878361593e4522">&#9670;&#160;</a></span>level</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.level</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2f949c8f14ee680b075543db1c3ff34b" name="a2f949c8f14ee680b075543db1c3ff34b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2f949c8f14ee680b075543db1c3ff34b">&#9670;&#160;</a></span>logger</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.logger = logging.getLogger(__name__)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="acb33d2c07d8e0fad27410b3afe6df02d" name="acb33d2c07d8e0fad27410b3afe6df02d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acb33d2c07d8e0fad27410b3afe6df02d">&#9670;&#160;</a></span>MAX_WORDS_IN_BATCH</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ad226751260da61bc5831d7c01f788543">int</a> gensim.models.deprecated.word2vec.MAX_WORDS_IN_BATCH = 10000</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a6ac4f31ea646dc416e034430eb9bd41e" name="a6ac4f31ea646dc416e034430eb9bd41e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6ac4f31ea646dc416e034430eb9bd41e">&#9670;&#160;</a></span>model</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.model</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><span class="lineno">    1</span>=  Word2Vec(</div>
<div class="line"><span class="lineno">    2</span>        corpus, size=args.size, min_count=args.min_count, workers=args.threads,</div>
<div class="line"><span class="lineno">    3</span>        window=args.window, sample=args.sample, sg=skipgram, hs=args.hs,</div>
<div class="line"><span class="lineno">    4</span>        negative=args.negative, cbow_mean=1, iter=args.iter</div>
<div class="line"><span class="lineno">    5</span>    )</div>
</div><!-- fragment -->
</div>
</div>
<a id="a9e31cf70044843e7ff356975b8c57b19" name="a9e31cf70044843e7ff356975b8c57b19"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9e31cf70044843e7ff356975b8c57b19">&#9670;&#160;</a></span>outfile</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.outfile = args.output</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2067b6abb4e2ae037204800c8b9d2377" name="a2067b6abb4e2ae037204800c8b9d2377"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2067b6abb4e2ae037204800c8b9d2377">&#9670;&#160;</a></span>parser</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.parser = argparse.ArgumentParser()</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a297d9ec1de9dd6ad011dbc8c211eae97" name="a297d9ec1de9dd6ad011dbc8c211eae97"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a297d9ec1de9dd6ad011dbc8c211eae97">&#9670;&#160;</a></span>program</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.program = os.path.basename(sys.argv[0])</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aa1d90aadef047816163f2e835cac62d6" name="aa1d90aadef047816163f2e835cac62d6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa1d90aadef047816163f2e835cac62d6">&#9670;&#160;</a></span>required</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.required</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ac36eaf2216a519f9c329f1a8d98f30e5" name="ac36eaf2216a519f9c329f1a8d98f30e5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac36eaf2216a519f9c329f1a8d98f30e5">&#9670;&#160;</a></span>skipgram</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacegensim_1_1models_1_1deprecated_1_1word2vec.html#ad226751260da61bc5831d7c01f788543">int</a> gensim.models.deprecated.word2vec.skipgram = 1</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a9c85e45324cd38c19cb630fe617a046a" name="a9c85e45324cd38c19cb630fe617a046a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9c85e45324cd38c19cb630fe617a046a">&#9670;&#160;</a></span>type</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.deprecated.word2vec.type</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
