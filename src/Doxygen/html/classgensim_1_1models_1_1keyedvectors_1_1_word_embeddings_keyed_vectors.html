<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: gensim.models.keyedvectors.WordEmbeddingsKeyedVectors Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacegensim.html">gensim</a></li><li class="navelem"><a class="el" href="namespacegensim_1_1models.html">models</a></li><li class="navelem"><a class="el" href="namespacegensim_1_1models_1_1keyedvectors.html">keyedvectors</a></li><li class="navelem"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html">WordEmbeddingsKeyedVectors</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="#pro-static-methods">Static Protected Member Functions</a> &#124;
<a href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors Class Reference</div></div>
</div><!--header-->
<div class="contents">
<div class="dynheader">
Inheritance diagram for gensim.models.keyedvectors.WordEmbeddingsKeyedVectors:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.png" usemap="#gensim.models.keyedvectors.WordEmbeddingsKeyedVectors_map" alt=""/>
  <map id="gensim.models.keyedvectors.WordEmbeddingsKeyedVectors_map" name="gensim.models.keyedvectors.WordEmbeddingsKeyedVectors_map">
<area href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html" alt="gensim.models.keyedvectors.BaseKeyedVectors" shape="rect" coords="183,112,539,136"/>
<area href="classgensim_1_1utils_1_1_save_load.html" alt="gensim.utils.SaveLoad" shape="rect" coords="183,56,539,80"/>
<area href="classgensim_1_1models_1_1keyedvectors_1_1_fast_text_keyed_vectors.html" alt="gensim.models.keyedvectors.FastTextKeyedVectors" shape="rect" coords="0,224,356,248"/>
<area href="classgensim_1_1models_1_1keyedvectors_1_1_word2_vec_keyed_vectors.html" alt="gensim.models.keyedvectors.Word2VecKeyedVectors" shape="rect" coords="366,224,722,248"/>
  </map>
</div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:af0e694b54a730bb889689e78ea6b722b" id="r_af0e694b54a730bb889689e78ea6b722b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#af0e694b54a730bb889689e78ea6b722b">__init__</a> (self, <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a49a52265342e9fb43bd64ac2cef5d9cf">vector_size</a>)</td></tr>
<tr class="separator:af0e694b54a730bb889689e78ea6b722b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0a1303a60ea0c17948f70883aa6b6ef9" id="r_a0a1303a60ea0c17948f70883aa6b6ef9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a0a1303a60ea0c17948f70883aa6b6ef9">wv</a> (self)</td></tr>
<tr class="separator:a0a1303a60ea0c17948f70883aa6b6ef9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a67f6e4a62aa70ef4a4a7054070241fc1" id="r_a67f6e4a62aa70ef4a4a7054070241fc1"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a67f6e4a62aa70ef4a4a7054070241fc1">index2entity</a> (self)</td></tr>
<tr class="separator:a67f6e4a62aa70ef4a4a7054070241fc1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8855570ab9d3aab786061b57aa41ad37" id="r_a8855570ab9d3aab786061b57aa41ad37"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a8855570ab9d3aab786061b57aa41ad37">index2entity</a> (self, value)</td></tr>
<tr class="separator:a8855570ab9d3aab786061b57aa41ad37"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac5b8c3198b1542de529c0bcbbd447bfd" id="r_ac5b8c3198b1542de529c0bcbbd447bfd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#ac5b8c3198b1542de529c0bcbbd447bfd">syn0</a> (self)</td></tr>
<tr class="separator:ac5b8c3198b1542de529c0bcbbd447bfd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1727f4601c2dd0128ca2f0f00642b7c8" id="r_a1727f4601c2dd0128ca2f0f00642b7c8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a1727f4601c2dd0128ca2f0f00642b7c8">syn0</a> (self, value)</td></tr>
<tr class="separator:a1727f4601c2dd0128ca2f0f00642b7c8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5bb9fc7a5b3c7a7a2decbfd37c5be131" id="r_a5bb9fc7a5b3c7a7a2decbfd37c5be131"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a5bb9fc7a5b3c7a7a2decbfd37c5be131">syn0norm</a> (self)</td></tr>
<tr class="separator:a5bb9fc7a5b3c7a7a2decbfd37c5be131"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a07ec262bb97cc0ad077621a589df56bf" id="r_a07ec262bb97cc0ad077621a589df56bf"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a07ec262bb97cc0ad077621a589df56bf">syn0norm</a> (self, value)</td></tr>
<tr class="separator:a07ec262bb97cc0ad077621a589df56bf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ad624f94233904b2c370edaab75215a" id="r_a2ad624f94233904b2c370edaab75215a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a2ad624f94233904b2c370edaab75215a">__contains__</a> (self, word)</td></tr>
<tr class="separator:a2ad624f94233904b2c370edaab75215a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adbb98824bcdd5ea4cebea513726fa185" id="r_adbb98824bcdd5ea4cebea513726fa185"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#adbb98824bcdd5ea4cebea513726fa185">save</a> (self, *args, **kwargs)</td></tr>
<tr class="separator:adbb98824bcdd5ea4cebea513726fa185"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a69ed7f63fefee33351613579c16bdc89" id="r_a69ed7f63fefee33351613579c16bdc89"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a69ed7f63fefee33351613579c16bdc89">word_vec</a> (self, word, use_norm=False)</td></tr>
<tr class="separator:a69ed7f63fefee33351613579c16bdc89"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80a53a13d32d853d9268450dd329c045" id="r_a80a53a13d32d853d9268450dd329c045"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a80a53a13d32d853d9268450dd329c045">get_vector</a> (self, word)</td></tr>
<tr class="separator:a80a53a13d32d853d9268450dd329c045"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7115e7f05bb3b7ca3410f86c9b0e67eb" id="r_a7115e7f05bb3b7ca3410f86c9b0e67eb"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a7115e7f05bb3b7ca3410f86c9b0e67eb">words_closer_than</a> (self, w1, w2)</td></tr>
<tr class="separator:a7115e7f05bb3b7ca3410f86c9b0e67eb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aff7e95dfc65042877e01641a59a66044" id="r_aff7e95dfc65042877e01641a59a66044"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#aff7e95dfc65042877e01641a59a66044">most_similar</a> (self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)</td></tr>
<tr class="separator:aff7e95dfc65042877e01641a59a66044"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4adeb26aad7590eaacd3e6f69d9384b4" id="r_a4adeb26aad7590eaacd3e6f69d9384b4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a4adeb26aad7590eaacd3e6f69d9384b4">similar_by_word</a> (self, word, topn=10, restrict_vocab=None)</td></tr>
<tr class="separator:a4adeb26aad7590eaacd3e6f69d9384b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af7c562f07431491e0cf373ef2fef18f3" id="r_af7c562f07431491e0cf373ef2fef18f3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#af7c562f07431491e0cf373ef2fef18f3">similar_by_vector</a> (self, vector, topn=10, restrict_vocab=None)</td></tr>
<tr class="separator:af7c562f07431491e0cf373ef2fef18f3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a15557ed25f0f0161e36042da3ea9c085" id="r_a15557ed25f0f0161e36042da3ea9c085"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a15557ed25f0f0161e36042da3ea9c085">similarity_matrix</a> (self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=REAL)</td></tr>
<tr class="separator:a15557ed25f0f0161e36042da3ea9c085"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afef84b4d9bcfb25802fee7a8e944d065" id="r_afef84b4d9bcfb25802fee7a8e944d065"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#afef84b4d9bcfb25802fee7a8e944d065">wmdistance</a> (self, document1, document2)</td></tr>
<tr class="separator:afef84b4d9bcfb25802fee7a8e944d065"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac84151f7ed286587a0230067afcb0b30" id="r_ac84151f7ed286587a0230067afcb0b30"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#ac84151f7ed286587a0230067afcb0b30">most_similar_cosmul</a> (self, positive=None, negative=None, topn=10)</td></tr>
<tr class="separator:ac84151f7ed286587a0230067afcb0b30"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa8f151aaf0f9081d3371c64c6b956da0" id="r_aa8f151aaf0f9081d3371c64c6b956da0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#aa8f151aaf0f9081d3371c64c6b956da0">doesnt_match</a> (self, words)</td></tr>
<tr class="separator:aa8f151aaf0f9081d3371c64c6b956da0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a03b946de671190c6337139ca8f49ca6e" id="r_a03b946de671190c6337139ca8f49ca6e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a03b946de671190c6337139ca8f49ca6e">distances</a> (self, word_or_vector, other_words=())</td></tr>
<tr class="separator:a03b946de671190c6337139ca8f49ca6e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a09eca9510d0d8be32cc86f3a8f2f6564" id="r_a09eca9510d0d8be32cc86f3a8f2f6564"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a09eca9510d0d8be32cc86f3a8f2f6564">distance</a> (self, w1, w2)</td></tr>
<tr class="separator:a09eca9510d0d8be32cc86f3a8f2f6564"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0478860f175cd86807696b3c80506798" id="r_a0478860f175cd86807696b3c80506798"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a0478860f175cd86807696b3c80506798">similarity</a> (self, w1, w2)</td></tr>
<tr class="separator:a0478860f175cd86807696b3c80506798"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a90f521c82652badb88a6f3e103a6388c" id="r_a90f521c82652badb88a6f3e103a6388c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a90f521c82652badb88a6f3e103a6388c">n_similarity</a> (self, ws1, ws2)</td></tr>
<tr class="separator:a90f521c82652badb88a6f3e103a6388c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:add7da9382fdafa447e5f126047f10ad9" id="r_add7da9382fdafa447e5f126047f10ad9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#add7da9382fdafa447e5f126047f10ad9">evaluate_word_analogies</a> (self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)</td></tr>
<tr class="separator:add7da9382fdafa447e5f126047f10ad9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a24838d511a5431e920d45ecd9ef427" id="r_a4a24838d511a5431e920d45ecd9ef427"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a4a24838d511a5431e920d45ecd9ef427">accuracy</a> (self, questions, restrict_vocab=30000, <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#aff7e95dfc65042877e01641a59a66044">most_similar</a>=<a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#aff7e95dfc65042877e01641a59a66044">most_similar</a>, case_insensitive=True)</td></tr>
<tr class="separator:a4a24838d511a5431e920d45ecd9ef427"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a705c7309716e14e12d7259d6b0146fe9" id="r_a705c7309716e14e12d7259d6b0146fe9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a705c7309716e14e12d7259d6b0146fe9">evaluate_word_pairs</a> (self, pairs, delimiter='\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)</td></tr>
<tr class="separator:a705c7309716e14e12d7259d6b0146fe9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a450c10092da0f707d70186035f6a84e2" id="r_a450c10092da0f707d70186035f6a84e2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a450c10092da0f707d70186035f6a84e2">init_sims</a> (self, replace=False)</td></tr>
<tr class="separator:a450c10092da0f707d70186035f6a84e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a73703bf01d377e1b2e39d2b18516c8ed" id="r_a73703bf01d377e1b2e39d2b18516c8ed"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a73703bf01d377e1b2e39d2b18516c8ed">relative_cosine_similarity</a> (self, wa, wb, topn=10)</td></tr>
<tr class="separator:a73703bf01d377e1b2e39d2b18516c8ed"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a156c7e36afd9150c8b7199f92c71c92f" id="r_a156c7e36afd9150c8b7199f92c71c92f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a156c7e36afd9150c8b7199f92c71c92f">get_keras_embedding</a> (self, train_embeddings=False, word_index=None)</td></tr>
<tr class="separator:a156c7e36afd9150c8b7199f92c71c92f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html">gensim.models.keyedvectors.BaseKeyedVectors</a></td></tr>
<tr class="memitem:a9f65d506a3472449c80ff239dae67de3 inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_a9f65d506a3472449c80ff239dae67de3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a9f65d506a3472449c80ff239dae67de3">load</a> (cls, fname_or_handle, **kwargs)</td></tr>
<tr class="separator:a9f65d506a3472449c80ff239dae67de3 inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4c76759a002cfc6297363b26b310b27a inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_a4c76759a002cfc6297363b26b310b27a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a4c76759a002cfc6297363b26b310b27a">add</a> (self, entities, weights, replace=False)</td></tr>
<tr class="separator:a4c76759a002cfc6297363b26b310b27a inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af6523e5c473c4b6b9835412bf8eaa07c inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_af6523e5c473c4b6b9835412bf8eaa07c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#af6523e5c473c4b6b9835412bf8eaa07c">__setitem__</a> (self, entities, weights)</td></tr>
<tr class="separator:af6523e5c473c4b6b9835412bf8eaa07c inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1e4102165263afe6f45a5ce0ff7d2f80 inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_a1e4102165263afe6f45a5ce0ff7d2f80"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a1e4102165263afe6f45a5ce0ff7d2f80">__getitem__</a> (self, entities)</td></tr>
<tr class="separator:a1e4102165263afe6f45a5ce0ff7d2f80 inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2b23b20e4a04b55b8a8728793f8309e8 inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_a2b23b20e4a04b55b8a8728793f8309e8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a2b23b20e4a04b55b8a8728793f8309e8">most_similar_to_given</a> (self, entity1, entities_list)</td></tr>
<tr class="separator:a2b23b20e4a04b55b8a8728793f8309e8 inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace72a2f7d92479b7d6e6126724172158 inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_ace72a2f7d92479b7d6e6126724172158"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#ace72a2f7d92479b7d6e6126724172158">closer_than</a> (self, entity1, entity2)</td></tr>
<tr class="separator:ace72a2f7d92479b7d6e6126724172158 inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0c6968e6974474f95ccb2e063c016a9c inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_a0c6968e6974474f95ccb2e063c016a9c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a0c6968e6974474f95ccb2e063c016a9c">rank</a> (self, entity1, entity2)</td></tr>
<tr class="separator:a0c6968e6974474f95ccb2e063c016a9c inherit pub_methods_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-static-methods" name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr class="memitem:ac69712c08e9ba6d7d050d491a1462ad6" id="r_ac69712c08e9ba6d7d050d491a1462ad6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#ac69712c08e9ba6d7d050d491a1462ad6">cosine_similarities</a> (vector_1, vectors_all)</td></tr>
<tr class="separator:ac69712c08e9ba6d7d050d491a1462ad6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad47210e9e4cfcc625474e7c4f4a0b97d" id="r_ad47210e9e4cfcc625474e7c4f4a0b97d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#ad47210e9e4cfcc625474e7c4f4a0b97d">log_accuracy</a> (section)</td></tr>
<tr class="separator:ad47210e9e4cfcc625474e7c4f4a0b97d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6440ae6dc4ac72bed5b25d1f3f5da8a5" id="r_a6440ae6dc4ac72bed5b25d1f3f5da8a5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a6440ae6dc4ac72bed5b25d1f3f5da8a5">log_evaluate_word_pairs</a> (pearson, spearman, oov, pairs)</td></tr>
<tr class="separator:a6440ae6dc4ac72bed5b25d1f3f5da8a5"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:a9fdf5db4b2ebea45838cb4df96cf1fe9" id="r_a9fdf5db4b2ebea45838cb4df96cf1fe9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a9fdf5db4b2ebea45838cb4df96cf1fe9">vectors_norm</a></td></tr>
<tr class="separator:a9fdf5db4b2ebea45838cb4df96cf1fe9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18d2c5d7f34d26e57c65670f837be0f5" id="r_a18d2c5d7f34d26e57c65670f837be0f5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a18d2c5d7f34d26e57c65670f837be0f5">index2word</a></td></tr>
<tr class="separator:a18d2c5d7f34d26e57c65670f837be0f5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2e4dbd0e2afe642fa32ca0152232e029" id="r_a2e4dbd0e2afe642fa32ca0152232e029"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#a2e4dbd0e2afe642fa32ca0152232e029">vectors</a></td></tr>
<tr class="separator:a2e4dbd0e2afe642fa32ca0152232e029"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac96fcca29f06f59a54d65d3f395bac6f" id="r_ac96fcca29f06f59a54d65d3f395bac6f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#ac96fcca29f06f59a54d65d3f395bac6f">vocab</a></td></tr>
<tr class="separator:ac96fcca29f06f59a54d65d3f395bac6f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_attribs_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td colspan="2" onclick="javascript:toggleInherit('pub_attribs_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors')"><img src="closed.png" alt="-"/>&#160;Public Attributes inherited from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html">gensim.models.keyedvectors.BaseKeyedVectors</a></td></tr>
<tr class="memitem:ad6751bc78fe7f1c958f91e1969eda787 inherit pub_attribs_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_ad6751bc78fe7f1c958f91e1969eda787"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#ad6751bc78fe7f1c958f91e1969eda787">vectors</a></td></tr>
<tr class="separator:ad6751bc78fe7f1c958f91e1969eda787 inherit pub_attribs_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6dbce4c24b8d36c535376be06b5a3cc8 inherit pub_attribs_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_a6dbce4c24b8d36c535376be06b5a3cc8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a6dbce4c24b8d36c535376be06b5a3cc8">vocab</a></td></tr>
<tr class="separator:a6dbce4c24b8d36c535376be06b5a3cc8 inherit pub_attribs_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a49a52265342e9fb43bd64ac2cef5d9cf inherit pub_attribs_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors" id="r_a49a52265342e9fb43bd64ac2cef5d9cf"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a49a52265342e9fb43bd64ac2cef5d9cf">vector_size</a></td></tr>
<tr class="separator:a49a52265342e9fb43bd64ac2cef5d9cf inherit pub_attribs_classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pro-static-methods" name="pro-static-methods"></a>
Static Protected Member Functions</h2></td></tr>
<tr class="memitem:ad3972d99fb4c9e4bdc037a10d0e1c3ac" id="r_ad3972d99fb4c9e4bdc037a10d0e1c3ac"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#ad3972d99fb4c9e4bdc037a10d0e1c3ac">_log_evaluate_word_analogies</a> (section)</td></tr>
<tr class="separator:ad3972d99fb4c9e4bdc037a10d0e1c3ac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_static_methods_classgensim_1_1utils_1_1_save_load"><td colspan="2" onclick="javascript:toggleInherit('pro_static_methods_classgensim_1_1utils_1_1_save_load')"><img src="closed.png" alt="-"/>&#160;Static Protected Member Functions inherited from <a class="el" href="classgensim_1_1utils_1_1_save_load.html">gensim.utils.SaveLoad</a></td></tr>
<tr class="memitem:a6cf051b348407267110444260535d143 inherit pro_static_methods_classgensim_1_1utils_1_1_save_load" id="r_a6cf051b348407267110444260535d143"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1utils_1_1_save_load.html#a6cf051b348407267110444260535d143">_adapt_by_suffix</a> (fname)</td></tr>
<tr class="separator:a6cf051b348407267110444260535d143 inherit pro_static_methods_classgensim_1_1utils_1_1_save_load"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="inherited" name="inherited"></a>
Additional Inherited Members</h2></td></tr>
<tr class="inherit_header pro_methods_classgensim_1_1utils_1_1_save_load"><td colspan="2" onclick="javascript:toggleInherit('pro_methods_classgensim_1_1utils_1_1_save_load')"><img src="closed.png" alt="-"/>&#160;Protected Member Functions inherited from <a class="el" href="classgensim_1_1utils_1_1_save_load.html">gensim.utils.SaveLoad</a></td></tr>
<tr class="memitem:ace7b79d8870c44c2bab0d590a5aca91e inherit pro_methods_classgensim_1_1utils_1_1_save_load" id="r_ace7b79d8870c44c2bab0d590a5aca91e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1utils_1_1_save_load.html#ace7b79d8870c44c2bab0d590a5aca91e">_load_specials</a> (self, fname, mmap, compress, subname)</td></tr>
<tr class="separator:ace7b79d8870c44c2bab0d590a5aca91e inherit pro_methods_classgensim_1_1utils_1_1_save_load"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acd9faa42ba3aae8f0a175418889d425c inherit pro_methods_classgensim_1_1utils_1_1_save_load" id="r_acd9faa42ba3aae8f0a175418889d425c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1utils_1_1_save_load.html#acd9faa42ba3aae8f0a175418889d425c">_smart_save</a> (self, fname, separately=None, sep_limit=10 *1024 **2, ignore=frozenset(), pickle_protocol=2)</td></tr>
<tr class="separator:acd9faa42ba3aae8f0a175418889d425c inherit pro_methods_classgensim_1_1utils_1_1_save_load"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4bae38a76fa8264d2c8759b9d40a6f5c inherit pro_methods_classgensim_1_1utils_1_1_save_load" id="r_a4bae38a76fa8264d2c8759b9d40a6f5c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1utils_1_1_save_load.html#a4bae38a76fa8264d2c8759b9d40a6f5c">_save_specials</a> (self, fname, separately, sep_limit, ignore, pickle_protocol, compress, subname)</td></tr>
<tr class="separator:a4bae38a76fa8264d2c8759b9d40a6f5c inherit pro_methods_classgensim_1_1utils_1_1_save_load"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Class containing common methods for operations over word vectors.</pre> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="af0e694b54a730bb889689e78ea6b722b" name="af0e694b54a730bb889689e78ea6b722b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af0e694b54a730bb889689e78ea6b722b">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>vector_size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a5f9b18799271f0fa8638b5e921880aa0">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>

<p>Reimplemented in <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_fast_text_keyed_vectors.html#a385f4af1c1ae04bc77655ba578734d7f">gensim.models.keyedvectors.FastTextKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  379</span>    <span class="keyword">def </span>__init__(self, vector_size):</div>
<div class="line"><span class="lineno">  380</span>        super(WordEmbeddingsKeyedVectors, self).__init__(vector_size=vector_size)</div>
<div class="line"><span class="lineno">  381</span>        self.vectors_norm = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  382</span>        self.index2word = []</div>
<div class="line"><span class="lineno">  383</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a2ad624f94233904b2c370edaab75215a" name="a2ad624f94233904b2c370edaab75215a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2ad624f94233904b2c370edaab75215a">&#9670;&#160;</a></span>__contains__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.__contains__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a0df54a6974b5ecf3c45560c68c516e86">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>

<p>Reimplemented in <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_fast_text_keyed_vectors.html#aed30999a59bfb8ca1de18439479711f0">gensim.models.keyedvectors.FastTextKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  417</span>    <span class="keyword">def </span>__contains__(self, word):</div>
<div class="line"><span class="lineno">  418</span>        <span class="keywordflow">return</span> word <span class="keywordflow">in</span> self.vocab</div>
<div class="line"><span class="lineno">  419</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad3972d99fb4c9e4bdc037a10d0e1c3ac" name="ad3972d99fb4c9e4bdc037a10d0e1c3ac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad3972d99fb4c9e4bdc037a10d0e1c3ac">&#9670;&#160;</a></span>_log_evaluate_word_analogies()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors._log_evaluate_word_analogies </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>section</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Calculate score by section, helper for
:meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_analogies`.

Parameters
----------
section : dict of (str, (str, str, str, str))
    Section given from evaluation.

Returns
-------
float
    Accuracy score.</pre> <div class="fragment"><div class="line"><span class="lineno">  999</span>    <span class="keyword">def </span>_log_evaluate_word_analogies(section):</div>
<div class="line"><span class="lineno"> 1000</span>        <span class="stringliteral">&quot;&quot;&quot;Calculate score by section, helper for</span></div>
<div class="line"><span class="lineno"> 1001</span><span class="stringliteral">        :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_analogies`.</span></div>
<div class="line"><span class="lineno"> 1002</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1003</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno"> 1004</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno"> 1005</span><span class="stringliteral">        section : dict of (str, (str, str, str, str))</span></div>
<div class="line"><span class="lineno"> 1006</span><span class="stringliteral">            Section given from evaluation.</span></div>
<div class="line"><span class="lineno"> 1007</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1008</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno"> 1009</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno"> 1010</span><span class="stringliteral">        float</span></div>
<div class="line"><span class="lineno"> 1011</span><span class="stringliteral">            Accuracy score.</span></div>
<div class="line"><span class="lineno"> 1012</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1013</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1014</span>        correct, incorrect = len(section[<span class="stringliteral">&#39;correct&#39;</span>]), len(section[<span class="stringliteral">&#39;incorrect&#39;</span>])</div>
<div class="line"><span class="lineno"> 1015</span>        <span class="keywordflow">if</span> correct + incorrect &gt; 0:</div>
<div class="line"><span class="lineno"> 1016</span>            score = correct / (correct + incorrect)</div>
<div class="line"><span class="lineno"> 1017</span>            logger.info(<span class="stringliteral">&quot;%s: %.1f%% (%i/%i)&quot;</span>, section[<span class="stringliteral">&#39;section&#39;</span>], 100.0 * score, correct, correct + incorrect)</div>
<div class="line"><span class="lineno"> 1018</span>            <span class="keywordflow">return</span> score</div>
<div class="line"><span class="lineno"> 1019</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a4a24838d511a5431e920d45ecd9ef427" name="a4a24838d511a5431e920d45ecd9ef427"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4a24838d511a5431e920d45ecd9ef427">&#9670;&#160;</a></span>accuracy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>questions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>restrict_vocab</em> = <code>30000</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>most_similar</em> = <code><a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_word_embeddings_keyed_vectors.html#aff7e95dfc65042877e01641a59a66044">most_similar</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>case_insensitive</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute accuracy of the model.

The accuracy is reported (=printed to log and returned as a list) for each
section separately, plus there's one aggregate summary at the end.

Parameters
----------
questions : str
    Path to file, where lines are 4-tuples of words, split into sections by ": SECTION NAME" lines.
    See `gensim/test/test_data/questions-words.txt` as example.
restrict_vocab : int, optional
    Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.
    This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard
    in modern word embedding models).
most_similar : function, optional
    Function used for similarity calculation.
case_insensitive : bool, optional
    If True - convert all words to their uppercase form before evaluating the performance.
    Useful to handle case-mismatch between training tokens and words in the test set.
    In case of multiple case variants of a single word, the vector for the first occurrence
    (also the most frequent if vocabulary is sorted) is taken.

Returns
-------
list of dict of (str, (str, str, str)
    Full lists of correct and incorrect predictions divided by sections.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1146</span>    <span class="keyword">def </span>accuracy(self, questions, restrict_vocab=30000, most_similar=most_similar, case_insensitive=True):</div>
<div class="line"><span class="lineno"> 1147</span>        <span class="stringliteral">&quot;&quot;&quot;Compute accuracy of the model.</span></div>
<div class="line"><span class="lineno"> 1148</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1149</span><span class="stringliteral">        The accuracy is reported (=printed to log and returned as a list) for each</span></div>
<div class="line"><span class="lineno"> 1150</span><span class="stringliteral">        section separately, plus there&#39;s one aggregate summary at the end.</span></div>
<div class="line"><span class="lineno"> 1151</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1152</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno"> 1153</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno"> 1154</span><span class="stringliteral">        questions : str</span></div>
<div class="line"><span class="lineno"> 1155</span><span class="stringliteral">            Path to file, where lines are 4-tuples of words, split into sections by &quot;: SECTION NAME&quot; lines.</span></div>
<div class="line"><span class="lineno"> 1156</span><span class="stringliteral">            See `gensim/test/test_data/questions-words.txt` as example.</span></div>
<div class="line"><span class="lineno"> 1157</span><span class="stringliteral">        restrict_vocab : int, optional</span></div>
<div class="line"><span class="lineno"> 1158</span><span class="stringliteral">            Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.</span></div>
<div class="line"><span class="lineno"> 1159</span><span class="stringliteral">            This may be meaningful if you&#39;ve sorted the model vocabulary by descending frequency (which is standard</span></div>
<div class="line"><span class="lineno"> 1160</span><span class="stringliteral">            in modern word embedding models).</span></div>
<div class="line"><span class="lineno"> 1161</span><span class="stringliteral">        most_similar : function, optional</span></div>
<div class="line"><span class="lineno"> 1162</span><span class="stringliteral">            Function used for similarity calculation.</span></div>
<div class="line"><span class="lineno"> 1163</span><span class="stringliteral">        case_insensitive : bool, optional</span></div>
<div class="line"><span class="lineno"> 1164</span><span class="stringliteral">            If True - convert all words to their uppercase form before evaluating the performance.</span></div>
<div class="line"><span class="lineno"> 1165</span><span class="stringliteral">            Useful to handle case-mismatch between training tokens and words in the test set.</span></div>
<div class="line"><span class="lineno"> 1166</span><span class="stringliteral">            In case of multiple case variants of a single word, the vector for the first occurrence</span></div>
<div class="line"><span class="lineno"> 1167</span><span class="stringliteral">            (also the most frequent if vocabulary is sorted) is taken.</span></div>
<div class="line"><span class="lineno"> 1168</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1169</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno"> 1170</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno"> 1171</span><span class="stringliteral">        list of dict of (str, (str, str, str)</span></div>
<div class="line"><span class="lineno"> 1172</span><span class="stringliteral">            Full lists of correct and incorrect predictions divided by sections.</span></div>
<div class="line"><span class="lineno"> 1173</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1174</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1175</span>        ok_vocab = [(w, self.vocab[w]) <span class="keywordflow">for</span> w <span class="keywordflow">in</span> self.index2word[:restrict_vocab]]</div>
<div class="line"><span class="lineno"> 1176</span>        ok_vocab = {w.upper(): v <span class="keywordflow">for</span> w, v <span class="keywordflow">in</span> reversed(ok_vocab)} <span class="keywordflow">if</span> case_insensitive <span class="keywordflow">else</span> dict(ok_vocab)</div>
<div class="line"><span class="lineno"> 1177</span> </div>
<div class="line"><span class="lineno"> 1178</span>        sections, section = [], <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1179</span>        <span class="keyword">with</span> utils.open(questions, <span class="stringliteral">&#39;rb&#39;</span>) <span class="keyword">as</span> fin:</div>
<div class="line"><span class="lineno"> 1180</span>            <span class="keywordflow">for</span> line_no, line <span class="keywordflow">in</span> enumerate(fin):</div>
<div class="line"><span class="lineno"> 1181</span>                <span class="comment"># TODO: use level3 BLAS (=evaluate multiple questions at once), for speed</span></div>
<div class="line"><span class="lineno"> 1182</span>                line = utils.to_unicode(line)</div>
<div class="line"><span class="lineno"> 1183</span>                <span class="keywordflow">if</span> line.startswith(<span class="stringliteral">&#39;: &#39;</span>):</div>
<div class="line"><span class="lineno"> 1184</span>                    <span class="comment"># a new section starts =&gt; store the old section</span></div>
<div class="line"><span class="lineno"> 1185</span>                    <span class="keywordflow">if</span> section:</div>
<div class="line"><span class="lineno"> 1186</span>                        sections.append(section)</div>
<div class="line"><span class="lineno"> 1187</span>                        self.log_accuracy(section)</div>
<div class="line"><span class="lineno"> 1188</span>                    section = {<span class="stringliteral">&#39;section&#39;</span>: line.lstrip(<span class="stringliteral">&#39;: &#39;</span>).strip(), <span class="stringliteral">&#39;correct&#39;</span>: [], <span class="stringliteral">&#39;incorrect&#39;</span>: []}</div>
<div class="line"><span class="lineno"> 1189</span>                <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1190</span>                    <span class="keywordflow">if</span> <span class="keywordflow">not</span> section:</div>
<div class="line"><span class="lineno"> 1191</span>                        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Missing section header before line #%i in %s&quot;</span> % (line_no, questions))</div>
<div class="line"><span class="lineno"> 1192</span>                    <span class="keywordflow">try</span>:</div>
<div class="line"><span class="lineno"> 1193</span>                        <span class="keywordflow">if</span> case_insensitive:</div>
<div class="line"><span class="lineno"> 1194</span>                            a, b, c, expected = [word.upper() <span class="keywordflow">for</span> word <span class="keywordflow">in</span> line.split()]</div>
<div class="line"><span class="lineno"> 1195</span>                        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1196</span>                            a, b, c, expected = [word <span class="keywordflow">for</span> word <span class="keywordflow">in</span> line.split()]</div>
<div class="line"><span class="lineno"> 1197</span>                    <span class="keywordflow">except</span> ValueError:</div>
<div class="line"><span class="lineno"> 1198</span>                        logger.info(<span class="stringliteral">&quot;Skipping invalid line #%i in %s&quot;</span>, line_no, questions)</div>
<div class="line"><span class="lineno"> 1199</span>                        <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno"> 1200</span>                    <span class="keywordflow">if</span> a <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab <span class="keywordflow">or</span> b <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab <span class="keywordflow">or</span> c <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab <span class="keywordflow">or</span> expected <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab:</div>
<div class="line"><span class="lineno"> 1201</span>                        logger.debug(<span class="stringliteral">&quot;Skipping line #%i with OOV words: %s&quot;</span>, line_no, line.strip())</div>
<div class="line"><span class="lineno"> 1202</span>                        <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno"> 1203</span>                    original_vocab = self.vocab</div>
<div class="line"><span class="lineno"> 1204</span>                    self.vocab = ok_vocab</div>
<div class="line"><span class="lineno"> 1205</span>                    ignore = {a, b, c}  <span class="comment"># input words to be ignored</span></div>
<div class="line"><span class="lineno"> 1206</span>                    predicted = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1207</span>                    <span class="comment"># find the most likely prediction, ignoring OOV words and input words</span></div>
<div class="line"><span class="lineno"> 1208</span>                    sims = most_similar(self, positive=[b, c], negative=[a], topn=<span class="keywordtype">None</span>, restrict_vocab=restrict_vocab)</div>
<div class="line"><span class="lineno"> 1209</span>                    self.vocab = original_vocab</div>
<div class="line"><span class="lineno"> 1210</span>                    <span class="keywordflow">for</span> index <span class="keywordflow">in</span> matutils.argsort(sims, reverse=<span class="keyword">True</span>):</div>
<div class="line"><span class="lineno"> 1211</span>                        predicted = self.index2word[index].upper() <span class="keywordflow">if</span> case_insensitive <span class="keywordflow">else</span> self.index2word[index]</div>
<div class="line"><span class="lineno"> 1212</span>                        <span class="keywordflow">if</span> predicted <span class="keywordflow">in</span> ok_vocab <span class="keywordflow">and</span> predicted <span class="keywordflow">not</span> <span class="keywordflow">in</span> ignore:</div>
<div class="line"><span class="lineno"> 1213</span>                            <span class="keywordflow">if</span> predicted != expected:</div>
<div class="line"><span class="lineno"> 1214</span>                                logger.debug(<span class="stringliteral">&quot;%s: expected %s, predicted %s&quot;</span>, line.strip(), expected, predicted)</div>
<div class="line"><span class="lineno"> 1215</span>                            <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno"> 1216</span>                    <span class="keywordflow">if</span> predicted == expected:</div>
<div class="line"><span class="lineno"> 1217</span>                        section[<span class="stringliteral">&#39;correct&#39;</span>].append((a, b, c, expected))</div>
<div class="line"><span class="lineno"> 1218</span>                    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1219</span>                        section[<span class="stringliteral">&#39;incorrect&#39;</span>].append((a, b, c, expected))</div>
<div class="line"><span class="lineno"> 1220</span>        <span class="keywordflow">if</span> section:</div>
<div class="line"><span class="lineno"> 1221</span>            <span class="comment"># store the last section, too</span></div>
<div class="line"><span class="lineno"> 1222</span>            sections.append(section)</div>
<div class="line"><span class="lineno"> 1223</span>            self.log_accuracy(section)</div>
<div class="line"><span class="lineno"> 1224</span> </div>
<div class="line"><span class="lineno"> 1225</span>        total = {</div>
<div class="line"><span class="lineno"> 1226</span>            <span class="stringliteral">&#39;section&#39;</span>: <span class="stringliteral">&#39;total&#39;</span>,</div>
<div class="line"><span class="lineno"> 1227</span>            <span class="stringliteral">&#39;correct&#39;</span>: list(chain.from_iterable(s[<span class="stringliteral">&#39;correct&#39;</span>] <span class="keywordflow">for</span> s <span class="keywordflow">in</span> sections)),</div>
<div class="line"><span class="lineno"> 1228</span>            <span class="stringliteral">&#39;incorrect&#39;</span>: list(chain.from_iterable(s[<span class="stringliteral">&#39;incorrect&#39;</span>] <span class="keywordflow">for</span> s <span class="keywordflow">in</span> sections)),</div>
<div class="line"><span class="lineno"> 1229</span>        }</div>
<div class="line"><span class="lineno"> 1230</span>        self.log_accuracy(total)</div>
<div class="line"><span class="lineno"> 1231</span>        sections.append(total)</div>
<div class="line"><span class="lineno"> 1232</span>        <span class="keywordflow">return</span> sections</div>
<div class="line"><span class="lineno"> 1233</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ac69712c08e9ba6d7d050d491a1462ad6" name="ac69712c08e9ba6d7d050d491a1462ad6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac69712c08e9ba6d7d050d491a1462ad6">&#9670;&#160;</a></span>cosine_similarities()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.cosine_similarities </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>vector_1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>vectors_all</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute cosine similarities between one vector and a set of other vectors.

Parameters
----------
vector_1 : numpy.ndarray
    Vector from which similarities are to be computed, expected shape (dim,).
vectors_all : numpy.ndarray
    For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).

Returns
-------
numpy.ndarray
    Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).</pre> <div class="fragment"><div class="line"><span class="lineno">  883</span>    <span class="keyword">def </span>cosine_similarities(vector_1, vectors_all):</div>
<div class="line"><span class="lineno">  884</span>        <span class="stringliteral">&quot;&quot;&quot;Compute cosine similarities between one vector and a set of other vectors.</span></div>
<div class="line"><span class="lineno">  885</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  886</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  887</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  888</span><span class="stringliteral">        vector_1 : numpy.ndarray</span></div>
<div class="line"><span class="lineno">  889</span><span class="stringliteral">            Vector from which similarities are to be computed, expected shape (dim,).</span></div>
<div class="line"><span class="lineno">  890</span><span class="stringliteral">        vectors_all : numpy.ndarray</span></div>
<div class="line"><span class="lineno">  891</span><span class="stringliteral">            For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).</span></div>
<div class="line"><span class="lineno">  892</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  893</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  894</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  895</span><span class="stringliteral">        numpy.ndarray</span></div>
<div class="line"><span class="lineno">  896</span><span class="stringliteral">            Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).</span></div>
<div class="line"><span class="lineno">  897</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  898</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  899</span>        norm = np.linalg.norm(vector_1)</div>
<div class="line"><span class="lineno">  900</span>        all_norms = np.linalg.norm(vectors_all, axis=1)</div>
<div class="line"><span class="lineno">  901</span>        dot_products = dot(vectors_all, vector_1)</div>
<div class="line"><span class="lineno">  902</span>        similarities = dot_products / (norm * all_norms)</div>
<div class="line"><span class="lineno">  903</span>        <span class="keywordflow">return</span> similarities</div>
<div class="line"><span class="lineno">  904</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a09eca9510d0d8be32cc86f3a8f2f6564" name="a09eca9510d0d8be32cc86f3a8f2f6564"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a09eca9510d0d8be32cc86f3a8f2f6564">&#9670;&#160;</a></span>distance()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.distance </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>w1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>w2</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute cosine distance between two words.
Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.

Parameters
----------
w1 : str
    Input word.
w2 : str
    Input word.

Returns
-------
float
    Distance between `w1` and `w2`.</pre> 
<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a5ef012983dbc5673ea67a8630fff2b20">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  939</span>    <span class="keyword">def </span>distance(self, w1, w2):</div>
<div class="line"><span class="lineno">  940</span>        <span class="stringliteral">&quot;&quot;&quot;Compute cosine distance between two words.</span></div>
<div class="line"><span class="lineno">  941</span><span class="stringliteral">        Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.</span></div>
<div class="line"><span class="lineno">  942</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  943</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  944</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  945</span><span class="stringliteral">        w1 : str</span></div>
<div class="line"><span class="lineno">  946</span><span class="stringliteral">            Input word.</span></div>
<div class="line"><span class="lineno">  947</span><span class="stringliteral">        w2 : str</span></div>
<div class="line"><span class="lineno">  948</span><span class="stringliteral">            Input word.</span></div>
<div class="line"><span class="lineno">  949</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  950</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  951</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  952</span><span class="stringliteral">        float</span></div>
<div class="line"><span class="lineno">  953</span><span class="stringliteral">            Distance between `w1` and `w2`.</span></div>
<div class="line"><span class="lineno">  954</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  955</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  956</span>        <span class="keywordflow">return</span> 1 - self.similarity(w1, w2)</div>
<div class="line"><span class="lineno">  957</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a03b946de671190c6337139ca8f49ca6e" name="a03b946de671190c6337139ca8f49ca6e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a03b946de671190c6337139ca8f49ca6e">&#9670;&#160;</a></span>distances()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.distances </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word_or_vector</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>other_words</em> = <code>()</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute cosine distances from given word or vector to all words in `other_words`.
If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.

Parameters
----------
word_or_vector : {str, numpy.ndarray}
    Word or vector from which distances are to be computed.
other_words : iterable of str
    For each word in `other_words` distance from `word_or_vector` is computed.
    If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).

Returns
-------
numpy.array
    Array containing distances to all words in `other_words` from input `word_or_vector`.

Raises
-----
KeyError
    If either `word_or_vector` or any word in `other_words` is absent from vocab.</pre> 
<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a7d10a12a184301faec2dcd9f2aec0dbd">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  905</span>    <span class="keyword">def </span>distances(self, word_or_vector, other_words=()):</div>
<div class="line"><span class="lineno">  906</span>        <span class="stringliteral">&quot;&quot;&quot;Compute cosine distances from given word or vector to all words in `other_words`.</span></div>
<div class="line"><span class="lineno">  907</span><span class="stringliteral">        If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.</span></div>
<div class="line"><span class="lineno">  908</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  909</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  910</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  911</span><span class="stringliteral">        word_or_vector : {str, numpy.ndarray}</span></div>
<div class="line"><span class="lineno">  912</span><span class="stringliteral">            Word or vector from which distances are to be computed.</span></div>
<div class="line"><span class="lineno">  913</span><span class="stringliteral">        other_words : iterable of str</span></div>
<div class="line"><span class="lineno">  914</span><span class="stringliteral">            For each word in `other_words` distance from `word_or_vector` is computed.</span></div>
<div class="line"><span class="lineno">  915</span><span class="stringliteral">            If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).</span></div>
<div class="line"><span class="lineno">  916</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  917</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  918</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  919</span><span class="stringliteral">        numpy.array</span></div>
<div class="line"><span class="lineno">  920</span><span class="stringliteral">            Array containing distances to all words in `other_words` from input `word_or_vector`.</span></div>
<div class="line"><span class="lineno">  921</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  922</span><span class="stringliteral">        Raises</span></div>
<div class="line"><span class="lineno">  923</span><span class="stringliteral">        -----</span></div>
<div class="line"><span class="lineno">  924</span><span class="stringliteral">        KeyError</span></div>
<div class="line"><span class="lineno">  925</span><span class="stringliteral">            If either `word_or_vector` or any word in `other_words` is absent from vocab.</span></div>
<div class="line"><span class="lineno">  926</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  927</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  928</span>        <span class="keywordflow">if</span> isinstance(word_or_vector, string_types):</div>
<div class="line"><span class="lineno">  929</span>            input_vector = self.word_vec(word_or_vector)</div>
<div class="line"><span class="lineno">  930</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  931</span>            input_vector = word_or_vector</div>
<div class="line"><span class="lineno">  932</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> other_words:</div>
<div class="line"><span class="lineno">  933</span>            other_vectors = self.vectors</div>
<div class="line"><span class="lineno">  934</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  935</span>            other_indices = [self.vocab[word].index <span class="keywordflow">for</span> word <span class="keywordflow">in</span> other_words]</div>
<div class="line"><span class="lineno">  936</span>            other_vectors = self.vectors[other_indices]</div>
<div class="line"><span class="lineno">  937</span>        <span class="keywordflow">return</span> 1 - self.cosine_similarities(input_vector, other_vectors)</div>
<div class="line"><span class="lineno">  938</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aa8f151aaf0f9081d3371c64c6b956da0" name="aa8f151aaf0f9081d3371c64c6b956da0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa8f151aaf0f9081d3371c64c6b956da0">&#9670;&#160;</a></span>doesnt_match()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>words</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Which word from the given list doesn't go with the others?

Parameters
----------
words : list of str
    List of words.

Returns
-------
str
    The word further away from the mean of all words.</pre> <div class="fragment"><div class="line"><span class="lineno">  855</span>    <span class="keyword">def </span>doesnt_match(self, words):</div>
<div class="line"><span class="lineno">  856</span>        <span class="stringliteral">&quot;&quot;&quot;Which word from the given list doesn&#39;t go with the others?</span></div>
<div class="line"><span class="lineno">  857</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  858</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  859</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  860</span><span class="stringliteral">        words : list of str</span></div>
<div class="line"><span class="lineno">  861</span><span class="stringliteral">            List of words.</span></div>
<div class="line"><span class="lineno">  862</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  863</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  864</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  865</span><span class="stringliteral">        str</span></div>
<div class="line"><span class="lineno">  866</span><span class="stringliteral">            The word further away from the mean of all words.</span></div>
<div class="line"><span class="lineno">  867</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  868</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  869</span>        self.init_sims()</div>
<div class="line"><span class="lineno">  870</span> </div>
<div class="line"><span class="lineno">  871</span>        used_words = [word <span class="keywordflow">for</span> word <span class="keywordflow">in</span> words <span class="keywordflow">if</span> word <span class="keywordflow">in</span> self]</div>
<div class="line"><span class="lineno">  872</span>        <span class="keywordflow">if</span> len(used_words) != len(words):</div>
<div class="line"><span class="lineno">  873</span>            ignored_words = set(words) - set(used_words)</div>
<div class="line"><span class="lineno">  874</span>            logger.warning(<span class="stringliteral">&quot;vectors for words %s are not present in the model, ignoring these words&quot;</span>, ignored_words)</div>
<div class="line"><span class="lineno">  875</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> used_words:</div>
<div class="line"><span class="lineno">  876</span>            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;cannot select a word from an empty list&quot;</span>)</div>
<div class="line"><span class="lineno">  877</span>        vectors = vstack(self.word_vec(word, use_norm=<span class="keyword">True</span>) <span class="keywordflow">for</span> word <span class="keywordflow">in</span> used_words).astype(REAL)</div>
<div class="line"><span class="lineno">  878</span>        mean = matutils.unitvec(vectors.mean(axis=0)).astype(REAL)</div>
<div class="line"><span class="lineno">  879</span>        dists = dot(vectors, mean)</div>
<div class="line"><span class="lineno">  880</span>        <span class="keywordflow">return</span> sorted(zip(dists, used_words))[0][1]</div>
<div class="line"><span class="lineno">  881</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="add7da9382fdafa447e5f126047f10ad9" name="add7da9382fdafa447e5f126047f10ad9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#add7da9382fdafa447e5f126047f10ad9">&#9670;&#160;</a></span>evaluate_word_analogies()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_analogies </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>analogies</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>restrict_vocab</em> = <code>300000</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>case_insensitive</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dummy4unknown</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute performance of the model on an analogy test set.

This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see
`discussion on GitHub #1935 &lt;https://github.com/RaRe-Technologies/gensim/pull/1935&gt;`_.

The accuracy is reported (printed to log and returned as a score) for each section separately,
plus there's one aggregate summary at the end.

This method corresponds to the `compute-accuracy` script of the original C word2vec.
See also `Analogy (State of the art) &lt;https://aclweb.org/aclwiki/Analogy_(State_of_the_art)&gt;`_.

Parameters
----------
analogies : str
    Path to file, where lines are 4-tuples of words, split into sections by ": SECTION NAME" lines.
    See `gensim/test/test_data/questions-words.txt` as example.
restrict_vocab : int, optional
    Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.
    This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard
    in modern word embedding models).
case_insensitive : bool, optional
    If True - convert all words to their uppercase form before evaluating the performance.
    Useful to handle case-mismatch between training tokens and words in the test set.
    In case of multiple case variants of a single word, the vector for the first occurrence
    (also the most frequent if vocabulary is sorted) is taken.
dummy4unknown : bool, optional
    If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.
    Otherwise, these tuples are skipped entirely and not used in the evaluation.

Returns
-------
score : float
    The overall evaluation score on the entire evaluation set
sections : list of dict of {str : str or list of tuple of (str, str, str, str)}
    Results broken down by each section of the evaluation set. Each dict contains the name of the section
    under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the
    keys 'correct' and 'incorrect'.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1020</span>    <span class="keyword">def </span>evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False):</div>
<div class="line"><span class="lineno"> 1021</span>        <span class="stringliteral">&quot;&quot;&quot;Compute performance of the model on an analogy test set.</span></div>
<div class="line"><span class="lineno"> 1022</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1023</span><span class="stringliteral">        This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see</span></div>
<div class="line"><span class="lineno"> 1024</span><span class="stringliteral">        `discussion on GitHub #1935 &lt;https://github.com/RaRe-Technologies/gensim/pull/1935&gt;`_.</span></div>
<div class="line"><span class="lineno"> 1025</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1026</span><span class="stringliteral">        The accuracy is reported (printed to log and returned as a score) for each section separately,</span></div>
<div class="line"><span class="lineno"> 1027</span><span class="stringliteral">        plus there&#39;s one aggregate summary at the end.</span></div>
<div class="line"><span class="lineno"> 1028</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1029</span><span class="stringliteral">        This method corresponds to the `compute-accuracy` script of the original C word2vec.</span></div>
<div class="line"><span class="lineno"> 1030</span><span class="stringliteral">        See also `Analogy (State of the art) &lt;https://aclweb.org/aclwiki/Analogy_(State_of_the_art)&gt;`_.</span></div>
<div class="line"><span class="lineno"> 1031</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1032</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno"> 1033</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno"> 1034</span><span class="stringliteral">        analogies : str</span></div>
<div class="line"><span class="lineno"> 1035</span><span class="stringliteral">            Path to file, where lines are 4-tuples of words, split into sections by &quot;: SECTION NAME&quot; lines.</span></div>
<div class="line"><span class="lineno"> 1036</span><span class="stringliteral">            See `gensim/test/test_data/questions-words.txt` as example.</span></div>
<div class="line"><span class="lineno"> 1037</span><span class="stringliteral">        restrict_vocab : int, optional</span></div>
<div class="line"><span class="lineno"> 1038</span><span class="stringliteral">            Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.</span></div>
<div class="line"><span class="lineno"> 1039</span><span class="stringliteral">            This may be meaningful if you&#39;ve sorted the model vocabulary by descending frequency (which is standard</span></div>
<div class="line"><span class="lineno"> 1040</span><span class="stringliteral">            in modern word embedding models).</span></div>
<div class="line"><span class="lineno"> 1041</span><span class="stringliteral">        case_insensitive : bool, optional</span></div>
<div class="line"><span class="lineno"> 1042</span><span class="stringliteral">            If True - convert all words to their uppercase form before evaluating the performance.</span></div>
<div class="line"><span class="lineno"> 1043</span><span class="stringliteral">            Useful to handle case-mismatch between training tokens and words in the test set.</span></div>
<div class="line"><span class="lineno"> 1044</span><span class="stringliteral">            In case of multiple case variants of a single word, the vector for the first occurrence</span></div>
<div class="line"><span class="lineno"> 1045</span><span class="stringliteral">            (also the most frequent if vocabulary is sorted) is taken.</span></div>
<div class="line"><span class="lineno"> 1046</span><span class="stringliteral">        dummy4unknown : bool, optional</span></div>
<div class="line"><span class="lineno"> 1047</span><span class="stringliteral">            If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.</span></div>
<div class="line"><span class="lineno"> 1048</span><span class="stringliteral">            Otherwise, these tuples are skipped entirely and not used in the evaluation.</span></div>
<div class="line"><span class="lineno"> 1049</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1050</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno"> 1051</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno"> 1052</span><span class="stringliteral">        score : float</span></div>
<div class="line"><span class="lineno"> 1053</span><span class="stringliteral">            The overall evaluation score on the entire evaluation set</span></div>
<div class="line"><span class="lineno"> 1054</span><span class="stringliteral">        sections : list of dict of {str : str or list of tuple of (str, str, str, str)}</span></div>
<div class="line"><span class="lineno"> 1055</span><span class="stringliteral">            Results broken down by each section of the evaluation set. Each dict contains the name of the section</span></div>
<div class="line"><span class="lineno"> 1056</span><span class="stringliteral">            under the key &#39;section&#39;, and lists of correctly and incorrectly predicted 4-tuples of words under the</span></div>
<div class="line"><span class="lineno"> 1057</span><span class="stringliteral">            keys &#39;correct&#39; and &#39;incorrect&#39;.</span></div>
<div class="line"><span class="lineno"> 1058</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1059</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1060</span>        ok_vocab = [(w, self.vocab[w]) <span class="keywordflow">for</span> w <span class="keywordflow">in</span> self.index2word[:restrict_vocab]]</div>
<div class="line"><span class="lineno"> 1061</span>        ok_vocab = {w.upper(): v <span class="keywordflow">for</span> w, v <span class="keywordflow">in</span> reversed(ok_vocab)} <span class="keywordflow">if</span> case_insensitive <span class="keywordflow">else</span> dict(ok_vocab)</div>
<div class="line"><span class="lineno"> 1062</span>        oov = 0</div>
<div class="line"><span class="lineno"> 1063</span>        logger.info(<span class="stringliteral">&quot;Evaluating word analogies for top %i words in the model on %s&quot;</span>, restrict_vocab, analogies)</div>
<div class="line"><span class="lineno"> 1064</span>        sections, section = [], <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1065</span>        quadruplets_no = 0</div>
<div class="line"><span class="lineno"> 1066</span>        <span class="keyword">with</span> utils.open(analogies, <span class="stringliteral">&#39;rb&#39;</span>) <span class="keyword">as</span> fin:</div>
<div class="line"><span class="lineno"> 1067</span>            <span class="keywordflow">for</span> line_no, line <span class="keywordflow">in</span> enumerate(fin):</div>
<div class="line"><span class="lineno"> 1068</span>                line = utils.to_unicode(line)</div>
<div class="line"><span class="lineno"> 1069</span>                <span class="keywordflow">if</span> line.startswith(<span class="stringliteral">&#39;: &#39;</span>):</div>
<div class="line"><span class="lineno"> 1070</span>                    <span class="comment"># a new section starts =&gt; store the old section</span></div>
<div class="line"><span class="lineno"> 1071</span>                    <span class="keywordflow">if</span> section:</div>
<div class="line"><span class="lineno"> 1072</span>                        sections.append(section)</div>
<div class="line"><span class="lineno"> 1073</span>                        self._log_evaluate_word_analogies(section)</div>
<div class="line"><span class="lineno"> 1074</span>                    section = {<span class="stringliteral">&#39;section&#39;</span>: line.lstrip(<span class="stringliteral">&#39;: &#39;</span>).strip(), <span class="stringliteral">&#39;correct&#39;</span>: [], <span class="stringliteral">&#39;incorrect&#39;</span>: []}</div>
<div class="line"><span class="lineno"> 1075</span>                <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1076</span>                    <span class="keywordflow">if</span> <span class="keywordflow">not</span> section:</div>
<div class="line"><span class="lineno"> 1077</span>                        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Missing section header before line #%i in %s&quot;</span> % (line_no, analogies))</div>
<div class="line"><span class="lineno"> 1078</span>                    <span class="keywordflow">try</span>:</div>
<div class="line"><span class="lineno"> 1079</span>                        <span class="keywordflow">if</span> case_insensitive:</div>
<div class="line"><span class="lineno"> 1080</span>                            a, b, c, expected = [word.upper() <span class="keywordflow">for</span> word <span class="keywordflow">in</span> line.split()]</div>
<div class="line"><span class="lineno"> 1081</span>                        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1082</span>                            a, b, c, expected = [word <span class="keywordflow">for</span> word <span class="keywordflow">in</span> line.split()]</div>
<div class="line"><span class="lineno"> 1083</span>                    <span class="keywordflow">except</span> ValueError:</div>
<div class="line"><span class="lineno"> 1084</span>                        logger.info(<span class="stringliteral">&quot;Skipping invalid line #%i in %s&quot;</span>, line_no, analogies)</div>
<div class="line"><span class="lineno"> 1085</span>                        <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno"> 1086</span>                    quadruplets_no += 1</div>
<div class="line"><span class="lineno"> 1087</span>                    <span class="keywordflow">if</span> a <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab <span class="keywordflow">or</span> b <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab <span class="keywordflow">or</span> c <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab <span class="keywordflow">or</span> expected <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab:</div>
<div class="line"><span class="lineno"> 1088</span>                        oov += 1</div>
<div class="line"><span class="lineno"> 1089</span>                        <span class="keywordflow">if</span> dummy4unknown:</div>
<div class="line"><span class="lineno"> 1090</span>                            logger.debug(<span class="stringliteral">&#39;Zero accuracy for line #%d with OOV words: %s&#39;</span>, line_no, line.strip())</div>
<div class="line"><span class="lineno"> 1091</span>                            section[<span class="stringliteral">&#39;incorrect&#39;</span>].append((a, b, c, expected))</div>
<div class="line"><span class="lineno"> 1092</span>                        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1093</span>                            logger.debug(<span class="stringliteral">&quot;Skipping line #%i with OOV words: %s&quot;</span>, line_no, line.strip())</div>
<div class="line"><span class="lineno"> 1094</span>                        <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno"> 1095</span>                    original_vocab = self.vocab</div>
<div class="line"><span class="lineno"> 1096</span>                    self.vocab = ok_vocab</div>
<div class="line"><span class="lineno"> 1097</span>                    ignore = {a, b, c}  <span class="comment"># input words to be ignored</span></div>
<div class="line"><span class="lineno"> 1098</span>                    predicted = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1099</span>                    <span class="comment"># find the most likely prediction using 3CosAdd (vector offset) method</span></div>
<div class="line"><span class="lineno"> 1100</span>                    <span class="comment"># TODO: implement 3CosMul and set-based methods for solving analogies</span></div>
<div class="line"><span class="lineno"> 1101</span>                    sims = self.most_similar(positive=[b, c], negative=[a], topn=5, restrict_vocab=restrict_vocab)</div>
<div class="line"><span class="lineno"> 1102</span>                    self.vocab = original_vocab</div>
<div class="line"><span class="lineno"> 1103</span>                    <span class="keywordflow">for</span> element <span class="keywordflow">in</span> sims:</div>
<div class="line"><span class="lineno"> 1104</span>                        predicted = element[0].upper() <span class="keywordflow">if</span> case_insensitive <span class="keywordflow">else</span> element[0]</div>
<div class="line"><span class="lineno"> 1105</span>                        <span class="keywordflow">if</span> predicted <span class="keywordflow">in</span> ok_vocab <span class="keywordflow">and</span> predicted <span class="keywordflow">not</span> <span class="keywordflow">in</span> ignore:</div>
<div class="line"><span class="lineno"> 1106</span>                            <span class="keywordflow">if</span> predicted != expected:</div>
<div class="line"><span class="lineno"> 1107</span>                                logger.debug(<span class="stringliteral">&quot;%s: expected %s, predicted %s&quot;</span>, line.strip(), expected, predicted)</div>
<div class="line"><span class="lineno"> 1108</span>                            <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno"> 1109</span>                    <span class="keywordflow">if</span> predicted == expected:</div>
<div class="line"><span class="lineno"> 1110</span>                        section[<span class="stringliteral">&#39;correct&#39;</span>].append((a, b, c, expected))</div>
<div class="line"><span class="lineno"> 1111</span>                    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1112</span>                        section[<span class="stringliteral">&#39;incorrect&#39;</span>].append((a, b, c, expected))</div>
<div class="line"><span class="lineno"> 1113</span>        <span class="keywordflow">if</span> section:</div>
<div class="line"><span class="lineno"> 1114</span>            <span class="comment"># store the last section, too</span></div>
<div class="line"><span class="lineno"> 1115</span>            sections.append(section)</div>
<div class="line"><span class="lineno"> 1116</span>            self._log_evaluate_word_analogies(section)</div>
<div class="line"><span class="lineno"> 1117</span> </div>
<div class="line"><span class="lineno"> 1118</span>        total = {</div>
<div class="line"><span class="lineno"> 1119</span>            <span class="stringliteral">&#39;section&#39;</span>: <span class="stringliteral">&#39;Total accuracy&#39;</span>,</div>
<div class="line"><span class="lineno"> 1120</span>            <span class="stringliteral">&#39;correct&#39;</span>: list(chain.from_iterable(s[<span class="stringliteral">&#39;correct&#39;</span>] <span class="keywordflow">for</span> s <span class="keywordflow">in</span> sections)),</div>
<div class="line"><span class="lineno"> 1121</span>            <span class="stringliteral">&#39;incorrect&#39;</span>: list(chain.from_iterable(s[<span class="stringliteral">&#39;incorrect&#39;</span>] <span class="keywordflow">for</span> s <span class="keywordflow">in</span> sections)),</div>
<div class="line"><span class="lineno"> 1122</span>        }</div>
<div class="line"><span class="lineno"> 1123</span> </div>
<div class="line"><span class="lineno"> 1124</span>        oov_ratio = float(oov) / quadruplets_no * 100</div>
<div class="line"><span class="lineno"> 1125</span>        logger.info(<span class="stringliteral">&#39;Quadruplets with out-of-vocabulary words: %.1f%%&#39;</span>, oov_ratio)</div>
<div class="line"><span class="lineno"> 1126</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> dummy4unknown:</div>
<div class="line"><span class="lineno"> 1127</span>            logger.info(</div>
<div class="line"><span class="lineno"> 1128</span>                <span class="stringliteral">&#39;NB: analogies containing OOV words were skipped from evaluation! &#39;</span></div>
<div class="line"><span class="lineno"> 1129</span>                <span class="stringliteral">&#39;To change this behavior, use &quot;dummy4unknown=True&quot;&#39;</span></div>
<div class="line"><span class="lineno"> 1130</span>            )</div>
<div class="line"><span class="lineno"> 1131</span>        analogies_score = self._log_evaluate_word_analogies(total)</div>
<div class="line"><span class="lineno"> 1132</span>        sections.append(total)</div>
<div class="line"><span class="lineno"> 1133</span>        <span class="comment"># Return the overall score and the full lists of correct and incorrect analogies</span></div>
<div class="line"><span class="lineno"> 1134</span>        <span class="keywordflow">return</span> analogies_score, sections</div>
<div class="line"><span class="lineno"> 1135</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a705c7309716e14e12d7259d6b0146fe9" name="a705c7309716e14e12d7259d6b0146fe9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a705c7309716e14e12d7259d6b0146fe9">&#9670;&#160;</a></span>evaluate_word_pairs()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>pairs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>delimiter</em> = <code>'\t'</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>restrict_vocab</em> = <code>300000</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>case_insensitive</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dummy4unknown</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute correlation of the model with human similarity judgments.

Notes
-----
More datasets can be found at
* http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html
* https://www.cl.cam.ac.uk/~fh295/simlex.html.

Parameters
----------
pairs : str
    Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.
    See `test/test_data/wordsim353.tsv` as example.
delimiter : str, optional
    Separator in `pairs` file.
restrict_vocab : int, optional
    Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.
    This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard
    in modern word embedding models).
case_insensitive : bool, optional
    If True - convert all words to their uppercase form before evaluating the performance.
    Useful to handle case-mismatch between training tokens and words in the test set.
    In case of multiple case variants of a single word, the vector for the first occurrence
    (also the most frequent if vocabulary is sorted) is taken.
dummy4unknown : bool, optional
    If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.
    Otherwise, these tuples are skipped entirely and not used in the evaluation.

Returns
-------
pearson : tuple of (float, float)
    Pearson correlation coefficient with 2-tailed p-value.
spearman : tuple of (float, float)
    Spearman rank-order correlation coefficient between the similarities from the dataset and the
    similarities produced by the model itself, with 2-tailed p-value.
oov_ratio : float
    The ratio of pairs with unknown words.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1241</span>                            case_insensitive=<span class="keyword">True</span>, dummy4unknown=<span class="keyword">False</span>):</div>
<div class="line"><span class="lineno"> 1242</span>        <span class="stringliteral">&quot;&quot;&quot;Compute correlation of the model with human similarity judgments.</span></div>
<div class="line"><span class="lineno"> 1243</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1244</span><span class="stringliteral">        Notes</span></div>
<div class="line"><span class="lineno"> 1245</span><span class="stringliteral">        -----</span></div>
<div class="line"><span class="lineno"> 1246</span><span class="stringliteral">        More datasets can be found at</span></div>
<div class="line"><span class="lineno"> 1247</span><span class="stringliteral">        * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html</span></div>
<div class="line"><span class="lineno"> 1248</span><span class="stringliteral">        * https://www.cl.cam.ac.uk/~fh295/simlex.html.</span></div>
<div class="line"><span class="lineno"> 1249</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1250</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno"> 1251</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno"> 1252</span><span class="stringliteral">        pairs : str</span></div>
<div class="line"><span class="lineno"> 1253</span><span class="stringliteral">            Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.</span></div>
<div class="line"><span class="lineno"> 1254</span><span class="stringliteral">            See `test/test_data/wordsim353.tsv` as example.</span></div>
<div class="line"><span class="lineno"> 1255</span><span class="stringliteral">        delimiter : str, optional</span></div>
<div class="line"><span class="lineno"> 1256</span><span class="stringliteral">            Separator in `pairs` file.</span></div>
<div class="line"><span class="lineno"> 1257</span><span class="stringliteral">        restrict_vocab : int, optional</span></div>
<div class="line"><span class="lineno"> 1258</span><span class="stringliteral">            Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.</span></div>
<div class="line"><span class="lineno"> 1259</span><span class="stringliteral">            This may be meaningful if you&#39;ve sorted the model vocabulary by descending frequency (which is standard</span></div>
<div class="line"><span class="lineno"> 1260</span><span class="stringliteral">            in modern word embedding models).</span></div>
<div class="line"><span class="lineno"> 1261</span><span class="stringliteral">        case_insensitive : bool, optional</span></div>
<div class="line"><span class="lineno"> 1262</span><span class="stringliteral">            If True - convert all words to their uppercase form before evaluating the performance.</span></div>
<div class="line"><span class="lineno"> 1263</span><span class="stringliteral">            Useful to handle case-mismatch between training tokens and words in the test set.</span></div>
<div class="line"><span class="lineno"> 1264</span><span class="stringliteral">            In case of multiple case variants of a single word, the vector for the first occurrence</span></div>
<div class="line"><span class="lineno"> 1265</span><span class="stringliteral">            (also the most frequent if vocabulary is sorted) is taken.</span></div>
<div class="line"><span class="lineno"> 1266</span><span class="stringliteral">        dummy4unknown : bool, optional</span></div>
<div class="line"><span class="lineno"> 1267</span><span class="stringliteral">            If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.</span></div>
<div class="line"><span class="lineno"> 1268</span><span class="stringliteral">            Otherwise, these tuples are skipped entirely and not used in the evaluation.</span></div>
<div class="line"><span class="lineno"> 1269</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1270</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno"> 1271</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno"> 1272</span><span class="stringliteral">        pearson : tuple of (float, float)</span></div>
<div class="line"><span class="lineno"> 1273</span><span class="stringliteral">            Pearson correlation coefficient with 2-tailed p-value.</span></div>
<div class="line"><span class="lineno"> 1274</span><span class="stringliteral">        spearman : tuple of (float, float)</span></div>
<div class="line"><span class="lineno"> 1275</span><span class="stringliteral">            Spearman rank-order correlation coefficient between the similarities from the dataset and the</span></div>
<div class="line"><span class="lineno"> 1276</span><span class="stringliteral">            similarities produced by the model itself, with 2-tailed p-value.</span></div>
<div class="line"><span class="lineno"> 1277</span><span class="stringliteral">        oov_ratio : float</span></div>
<div class="line"><span class="lineno"> 1278</span><span class="stringliteral">            The ratio of pairs with unknown words.</span></div>
<div class="line"><span class="lineno"> 1279</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1280</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1281</span>        ok_vocab = [(w, self.vocab[w]) <span class="keywordflow">for</span> w <span class="keywordflow">in</span> self.index2word[:restrict_vocab]]</div>
<div class="line"><span class="lineno"> 1282</span>        ok_vocab = {w.upper(): v <span class="keywordflow">for</span> w, v <span class="keywordflow">in</span> reversed(ok_vocab)} <span class="keywordflow">if</span> case_insensitive <span class="keywordflow">else</span> dict(ok_vocab)</div>
<div class="line"><span class="lineno"> 1283</span> </div>
<div class="line"><span class="lineno"> 1284</span>        similarity_gold = []</div>
<div class="line"><span class="lineno"> 1285</span>        similarity_model = []</div>
<div class="line"><span class="lineno"> 1286</span>        oov = 0</div>
<div class="line"><span class="lineno"> 1287</span> </div>
<div class="line"><span class="lineno"> 1288</span>        original_vocab = self.vocab</div>
<div class="line"><span class="lineno"> 1289</span>        self.vocab = ok_vocab</div>
<div class="line"><span class="lineno"> 1290</span> </div>
<div class="line"><span class="lineno"> 1291</span>        <span class="keyword">with</span> utils.open(pairs, <span class="stringliteral">&#39;rb&#39;</span>) <span class="keyword">as</span> fin:</div>
<div class="line"><span class="lineno"> 1292</span>            <span class="keywordflow">for</span> line_no, line <span class="keywordflow">in</span> enumerate(fin):</div>
<div class="line"><span class="lineno"> 1293</span>                line = utils.to_unicode(line)</div>
<div class="line"><span class="lineno"> 1294</span>                <span class="keywordflow">if</span> line.startswith(<span class="stringliteral">&#39;#&#39;</span>):</div>
<div class="line"><span class="lineno"> 1295</span>                    <span class="comment"># May be a comment</span></div>
<div class="line"><span class="lineno"> 1296</span>                    <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno"> 1297</span>                <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1298</span>                    <span class="keywordflow">try</span>:</div>
<div class="line"><span class="lineno"> 1299</span>                        <span class="keywordflow">if</span> case_insensitive:</div>
<div class="line"><span class="lineno"> 1300</span>                            a, b, sim = [word.upper() <span class="keywordflow">for</span> word <span class="keywordflow">in</span> line.split(delimiter)]</div>
<div class="line"><span class="lineno"> 1301</span>                        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1302</span>                            a, b, sim = [word <span class="keywordflow">for</span> word <span class="keywordflow">in</span> line.split(delimiter)]</div>
<div class="line"><span class="lineno"> 1303</span>                        sim = float(sim)</div>
<div class="line"><span class="lineno"> 1304</span>                    <span class="keywordflow">except</span> (ValueError, TypeError):</div>
<div class="line"><span class="lineno"> 1305</span>                        logger.info(<span class="stringliteral">&#39;Skipping invalid line #%d in %s&#39;</span>, line_no, pairs)</div>
<div class="line"><span class="lineno"> 1306</span>                        <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno"> 1307</span>                    <span class="keywordflow">if</span> a <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab <span class="keywordflow">or</span> b <span class="keywordflow">not</span> <span class="keywordflow">in</span> ok_vocab:</div>
<div class="line"><span class="lineno"> 1308</span>                        oov += 1</div>
<div class="line"><span class="lineno"> 1309</span>                        <span class="keywordflow">if</span> dummy4unknown:</div>
<div class="line"><span class="lineno"> 1310</span>                            logger.debug(<span class="stringliteral">&#39;Zero similarity for line #%d with OOV words: %s&#39;</span>, line_no, line.strip())</div>
<div class="line"><span class="lineno"> 1311</span>                            similarity_model.append(0.0)</div>
<div class="line"><span class="lineno"> 1312</span>                            similarity_gold.append(sim)</div>
<div class="line"><span class="lineno"> 1313</span>                            <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno"> 1314</span>                        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1315</span>                            logger.debug(<span class="stringliteral">&#39;Skipping line #%d with OOV words: %s&#39;</span>, line_no, line.strip())</div>
<div class="line"><span class="lineno"> 1316</span>                            <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno"> 1317</span>                    similarity_gold.append(sim)  <span class="comment"># Similarity from the dataset</span></div>
<div class="line"><span class="lineno"> 1318</span>                    similarity_model.append(self.similarity(a, b))  <span class="comment"># Similarity from the model</span></div>
<div class="line"><span class="lineno"> 1319</span>        self.vocab = original_vocab</div>
<div class="line"><span class="lineno"> 1320</span>        spearman = stats.spearmanr(similarity_gold, similarity_model)</div>
<div class="line"><span class="lineno"> 1321</span>        pearson = stats.pearsonr(similarity_gold, similarity_model)</div>
<div class="line"><span class="lineno"> 1322</span>        <span class="keywordflow">if</span> dummy4unknown:</div>
<div class="line"><span class="lineno"> 1323</span>            oov_ratio = float(oov) / len(similarity_gold) * 100</div>
<div class="line"><span class="lineno"> 1324</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1325</span>            oov_ratio = float(oov) / (len(similarity_gold) + oov) * 100</div>
<div class="line"><span class="lineno"> 1326</span> </div>
<div class="line"><span class="lineno"> 1327</span>        logger.debug(<span class="stringliteral">&#39;Pearson correlation coefficient against %s: %f with p-value %f&#39;</span>, pairs, pearson[0], pearson[1])</div>
<div class="line"><span class="lineno"> 1328</span>        logger.debug(</div>
<div class="line"><span class="lineno"> 1329</span>            <span class="stringliteral">&#39;Spearman rank-order correlation coefficient against %s: %f with p-value %f&#39;</span>,</div>
<div class="line"><span class="lineno"> 1330</span>            pairs, spearman[0], spearman[1]</div>
<div class="line"><span class="lineno"> 1331</span>        )</div>
<div class="line"><span class="lineno"> 1332</span>        logger.debug(<span class="stringliteral">&#39;Pairs with unknown words: %d&#39;</span>, oov)</div>
<div class="line"><span class="lineno"> 1333</span>        self.log_evaluate_word_pairs(pearson, spearman, oov_ratio, pairs)</div>
<div class="line"><span class="lineno"> 1334</span>        <span class="keywordflow">return</span> pearson, spearman, oov_ratio</div>
<div class="line"><span class="lineno"> 1335</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a156c7e36afd9150c8b7199f92c71c92f" name="a156c7e36afd9150c8b7199f92c71c92f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a156c7e36afd9150c8b7199f92c71c92f">&#9670;&#160;</a></span>get_keras_embedding()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.get_keras_embedding </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>train_embeddings</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word_index</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.

Parameters
----------
train_embeddings : bool
    If False, the weights are frozen and stopped from being updated.
    If True, the weights can/will be further trained/updated.

word_index : {str : int}
    A mapping from tokens to their indices the way they will be provided in the input to the embedding layer.
    The embedding of each token will be placed at the corresponding index in the returned matrix.
    Tokens not in the index are ignored.
    This is useful when the token indices are produced by a process that is not coupled with the embedding
    model, e.x. an Keras Tokenizer object.
    If None, the embedding matrix in the embedding layer will be indexed according to self.vocab

Returns
-------
`keras.layers.Embedding`
    Embedding layer.

Raises
------
ImportError
    If `Keras &lt;https://pypi.org/project/Keras/&gt;`_ not installed.

Warnings
--------
Current method works only if `Keras &lt;https://pypi.org/project/Keras/&gt;`_ installed.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1386</span>    <span class="keyword">def </span>get_keras_embedding(self, train_embeddings=False, word_index=None):</div>
<div class="line"><span class="lineno"> 1387</span>        <span class="stringliteral">&quot;&quot;&quot;Get a Keras &#39;Embedding&#39; layer with weights set as the Word2Vec model&#39;s learned word embeddings.</span></div>
<div class="line"><span class="lineno"> 1388</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1389</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno"> 1390</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno"> 1391</span><span class="stringliteral">        train_embeddings : bool</span></div>
<div class="line"><span class="lineno"> 1392</span><span class="stringliteral">            If False, the weights are frozen and stopped from being updated.</span></div>
<div class="line"><span class="lineno"> 1393</span><span class="stringliteral">            If True, the weights can/will be further trained/updated.</span></div>
<div class="line"><span class="lineno"> 1394</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1395</span><span class="stringliteral">        word_index : {str : int}</span></div>
<div class="line"><span class="lineno"> 1396</span><span class="stringliteral">            A mapping from tokens to their indices the way they will be provided in the input to the embedding layer.</span></div>
<div class="line"><span class="lineno"> 1397</span><span class="stringliteral">            The embedding of each token will be placed at the corresponding index in the returned matrix.</span></div>
<div class="line"><span class="lineno"> 1398</span><span class="stringliteral">            Tokens not in the index are ignored.</span></div>
<div class="line"><span class="lineno"> 1399</span><span class="stringliteral">            This is useful when the token indices are produced by a process that is not coupled with the embedding</span></div>
<div class="line"><span class="lineno"> 1400</span><span class="stringliteral">            model, e.x. an Keras Tokenizer object.</span></div>
<div class="line"><span class="lineno"> 1401</span><span class="stringliteral">            If None, the embedding matrix in the embedding layer will be indexed according to self.vocab</span></div>
<div class="line"><span class="lineno"> 1402</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1403</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno"> 1404</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno"> 1405</span><span class="stringliteral">        `keras.layers.Embedding`</span></div>
<div class="line"><span class="lineno"> 1406</span><span class="stringliteral">            Embedding layer.</span></div>
<div class="line"><span class="lineno"> 1407</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1408</span><span class="stringliteral">        Raises</span></div>
<div class="line"><span class="lineno"> 1409</span><span class="stringliteral">        ------</span></div>
<div class="line"><span class="lineno"> 1410</span><span class="stringliteral">        ImportError</span></div>
<div class="line"><span class="lineno"> 1411</span><span class="stringliteral">            If `Keras &lt;https://pypi.org/project/Keras/&gt;`_ not installed.</span></div>
<div class="line"><span class="lineno"> 1412</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1413</span><span class="stringliteral">        Warnings</span></div>
<div class="line"><span class="lineno"> 1414</span><span class="stringliteral">        --------</span></div>
<div class="line"><span class="lineno"> 1415</span><span class="stringliteral">        Current method works only if `Keras &lt;https://pypi.org/project/Keras/&gt;`_ installed.</span></div>
<div class="line"><span class="lineno"> 1416</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1417</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1418</span>        <span class="keywordflow">try</span>:</div>
<div class="line"><span class="lineno"> 1419</span>            <span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding</div>
<div class="line"><span class="lineno"> 1420</span>        <span class="keywordflow">except</span> ImportError:</div>
<div class="line"><span class="lineno"> 1421</span>            <span class="keywordflow">raise</span> ImportError(<span class="stringliteral">&quot;Please install Keras to use this function&quot;</span>)</div>
<div class="line"><span class="lineno"> 1422</span>        <span class="keywordflow">if</span> word_index <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno"> 1423</span>            weights = self.vectors</div>
<div class="line"><span class="lineno"> 1424</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1425</span>            max_index = max(word_index.values())</div>
<div class="line"><span class="lineno"> 1426</span>            weights = np.random.normal(size=(max_index + 1, self.vectors.shape[1]))</div>
<div class="line"><span class="lineno"> 1427</span>            <span class="keywordflow">for</span> word, index <span class="keywordflow">in</span> word_index.items():</div>
<div class="line"><span class="lineno"> 1428</span>                <span class="keywordflow">if</span> word <span class="keywordflow">in</span> self.vocab:</div>
<div class="line"><span class="lineno"> 1429</span>                    weights[index] = self.get_vector(word)</div>
<div class="line"><span class="lineno"> 1430</span> </div>
<div class="line"><span class="lineno"> 1431</span>        layer = Embedding(</div>
<div class="line"><span class="lineno"> 1432</span>            input_dim=weights.shape[0], output_dim=weights.shape[1],</div>
<div class="line"><span class="lineno"> 1433</span>            weights=[weights], trainable=train_embeddings</div>
<div class="line"><span class="lineno"> 1434</span>        )</div>
<div class="line"><span class="lineno"> 1435</span>        <span class="keywordflow">return</span> layer</div>
<div class="line"><span class="lineno"> 1436</span> </div>
<div class="line"><span class="lineno"> 1437</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a80a53a13d32d853d9268450dd329c045" name="a80a53a13d32d853d9268450dd329c045"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80a53a13d32d853d9268450dd329c045">&#9670;&#160;</a></span>get_vector()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.get_vector </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>entity</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Get the entity's representations in vector space, as a 1D numpy array.

Parameters
----------
entity : str
    Identifier of the entity to return the vector for.

Returns
-------
numpy.ndarray
    Vector for the specified entity.

Raises
------
KeyError
    If the given entity identifier doesn't exist.</pre> 
<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a1e86262c90e69b4b2189a79db7ac4ae6">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  470</span>    <span class="keyword">def </span>get_vector(self, word):</div>
<div class="line"><span class="lineno">  471</span>        <span class="keywordflow">return</span> self.word_vec(word)</div>
<div class="line"><span class="lineno">  472</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a67f6e4a62aa70ef4a4a7054070241fc1" name="a67f6e4a62aa70ef4a4a7054070241fc1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a67f6e4a62aa70ef4a4a7054070241fc1">&#9670;&#160;</a></span>index2entity() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.index2entity </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a5e3cde2164b4a9e8d1d76605b8cf766f">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  390</span>    <span class="keyword">def </span>index2entity(self):</div>
<div class="line"><span class="lineno">  391</span>        <span class="keywordflow">return</span> self.index2word</div>
<div class="line"><span class="lineno">  392</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a8855570ab9d3aab786061b57aa41ad37" name="a8855570ab9d3aab786061b57aa41ad37"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8855570ab9d3aab786061b57aa41ad37">&#9670;&#160;</a></span>index2entity() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.index2entity </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>value</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a5e3cde2164b4a9e8d1d76605b8cf766f">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  394</span>    <span class="keyword">def </span>index2entity(self, value):</div>
<div class="line"><span class="lineno">  395</span>        self.index2word = value</div>
<div class="line"><span class="lineno">  396</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a450c10092da0f707d70186035f6a84e2" name="a450c10092da0f707d70186035f6a84e2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a450c10092da0f707d70186035f6a84e2">&#9670;&#160;</a></span>init_sims()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.init_sims </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>replace</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Precompute L2-normalized vectors.

Parameters
----------
replace : bool, optional
    If True - forget the original vectors and only keep the normalized ones = saves lots of memory!

Warnings
--------
You **cannot continue training** after doing a replace.
The model becomes effectively read-only: you can call
:meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,
:meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.</pre> 
<p>Reimplemented in <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_fast_text_keyed_vectors.html#aebcdc8ab3680b38f72d1fd1a9e0a11b8">gensim.models.keyedvectors.FastTextKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno"> 1336</span>    <span class="keyword">def </span>init_sims(self, replace=False):</div>
<div class="line"><span class="lineno"> 1337</span>        <span class="stringliteral">&quot;&quot;&quot;Precompute L2-normalized vectors.</span></div>
<div class="line"><span class="lineno"> 1338</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1339</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno"> 1340</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno"> 1341</span><span class="stringliteral">        replace : bool, optional</span></div>
<div class="line"><span class="lineno"> 1342</span><span class="stringliteral">            If True - forget the original vectors and only keep the normalized ones = saves lots of memory!</span></div>
<div class="line"><span class="lineno"> 1343</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1344</span><span class="stringliteral">        Warnings</span></div>
<div class="line"><span class="lineno"> 1345</span><span class="stringliteral">        --------</span></div>
<div class="line"><span class="lineno"> 1346</span><span class="stringliteral">        You **cannot continue training** after doing a replace.</span></div>
<div class="line"><span class="lineno"> 1347</span><span class="stringliteral">        The model becomes effectively read-only: you can call</span></div>
<div class="line"><span class="lineno"> 1348</span><span class="stringliteral">        :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,</span></div>
<div class="line"><span class="lineno"> 1349</span><span class="stringliteral">        :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.</span></div>
<div class="line"><span class="lineno"> 1350</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1351</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1352</span>        <span class="keywordflow">if</span> getattr(self, <span class="stringliteral">&#39;vectors_norm&#39;</span>, <span class="keywordtype">None</span>) <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">or</span> replace:</div>
<div class="line"><span class="lineno"> 1353</span>            logger.info(<span class="stringliteral">&quot;precomputing L2-norms of word weight vectors&quot;</span>)</div>
<div class="line"><span class="lineno"> 1354</span>            self.vectors_norm = _l2_norm(self.vectors, replace=replace)</div>
<div class="line"><span class="lineno"> 1355</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad47210e9e4cfcc625474e7c4f4a0b97d" name="ad47210e9e4cfcc625474e7c4f4a0b97d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad47210e9e4cfcc625474e7c4f4a0b97d">&#9670;&#160;</a></span>log_accuracy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.log_accuracy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>section</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1137</span>    <span class="keyword">def </span>log_accuracy(section):</div>
<div class="line"><span class="lineno"> 1138</span>        correct, incorrect = len(section[<span class="stringliteral">&#39;correct&#39;</span>]), len(section[<span class="stringliteral">&#39;incorrect&#39;</span>])</div>
<div class="line"><span class="lineno"> 1139</span>        <span class="keywordflow">if</span> correct + incorrect &gt; 0:</div>
<div class="line"><span class="lineno"> 1140</span>            logger.info(</div>
<div class="line"><span class="lineno"> 1141</span>                <span class="stringliteral">&quot;%s: %.1f%% (%i/%i)&quot;</span>,</div>
<div class="line"><span class="lineno"> 1142</span>                section[<span class="stringliteral">&#39;section&#39;</span>], 100.0 * correct / (correct + incorrect), correct, correct + incorrect</div>
<div class="line"><span class="lineno"> 1143</span>            )</div>
<div class="line"><span class="lineno"> 1144</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a6440ae6dc4ac72bed5b25d1f3f5da8a5" name="a6440ae6dc4ac72bed5b25d1f3f5da8a5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6440ae6dc4ac72bed5b25d1f3f5da8a5">&#9670;&#160;</a></span>log_evaluate_word_pairs()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.log_evaluate_word_pairs </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>pearson</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>spearman</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>oov</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>pairs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1235</span>    <span class="keyword">def </span>log_evaluate_word_pairs(pearson, spearman, oov, pairs):</div>
<div class="line"><span class="lineno"> 1236</span>        logger.info(<span class="stringliteral">&#39;Pearson correlation coefficient against %s: %.4f&#39;</span>, pairs, pearson[0])</div>
<div class="line"><span class="lineno"> 1237</span>        logger.info(<span class="stringliteral">&#39;Spearman rank-order correlation coefficient against %s: %.4f&#39;</span>, pairs, spearman[0])</div>
<div class="line"><span class="lineno"> 1238</span>        logger.info(<span class="stringliteral">&#39;Pairs with unknown words ratio: %.1f%%&#39;</span>, oov)</div>
<div class="line"><span class="lineno"> 1239</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aff7e95dfc65042877e01641a59a66044" name="aff7e95dfc65042877e01641a59a66044"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aff7e95dfc65042877e01641a59a66044">&#9670;&#160;</a></span>most_similar()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>positive</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>negative</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>topn</em> = <code>10</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>restrict_vocab</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>indexer</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Find the top-N most similar words.
Positive words contribute positively towards the similarity, negative words negatively.

This method computes cosine similarity between a simple mean of the projection
weight vectors of the given words and the vectors for each word in the model.
The method corresponds to the `word-analogy` and `distance` scripts in the original
word2vec implementation.

Parameters
----------
positive : list of str, optional
    List of words that contribute positively.
negative : list of str, optional
    List of words that contribute negatively.
topn : int or None, optional
    Number of top-N similar words to return, when `topn` is int. When `topn` is None,
    then similarities for all words are returned.
restrict_vocab : int, optional
    Optional integer which limits the range of vectors which
    are searched for most-similar values. For example, restrict_vocab=10000 would
    only check the first 10000 word vectors in the vocabulary order. (This may be
    meaningful if you've sorted the vocabulary by descending frequency.)

Returns
-------
list of (str, float) or numpy.array
    When `topn` is int, a sequence of (word, similarity) is returned.
    When `topn` is None, then similarities for all words are returned as a
    one-dimensional numpy array with the size of the vocabulary.</pre> 
<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a6e15035276e6c2cf209b979ff4de9093">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  491</span>    <span class="keyword">def </span>most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None):</div>
<div class="line"><span class="lineno">  492</span>        <span class="stringliteral">&quot;&quot;&quot;Find the top-N most similar words.</span></div>
<div class="line"><span class="lineno">  493</span><span class="stringliteral">        Positive words contribute positively towards the similarity, negative words negatively.</span></div>
<div class="line"><span class="lineno">  494</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  495</span><span class="stringliteral">        This method computes cosine similarity between a simple mean of the projection</span></div>
<div class="line"><span class="lineno">  496</span><span class="stringliteral">        weight vectors of the given words and the vectors for each word in the model.</span></div>
<div class="line"><span class="lineno">  497</span><span class="stringliteral">        The method corresponds to the `word-analogy` and `distance` scripts in the original</span></div>
<div class="line"><span class="lineno">  498</span><span class="stringliteral">        word2vec implementation.</span></div>
<div class="line"><span class="lineno">  499</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  500</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  501</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  502</span><span class="stringliteral">        positive : list of str, optional</span></div>
<div class="line"><span class="lineno">  503</span><span class="stringliteral">            List of words that contribute positively.</span></div>
<div class="line"><span class="lineno">  504</span><span class="stringliteral">        negative : list of str, optional</span></div>
<div class="line"><span class="lineno">  505</span><span class="stringliteral">            List of words that contribute negatively.</span></div>
<div class="line"><span class="lineno">  506</span><span class="stringliteral">        topn : int or None, optional</span></div>
<div class="line"><span class="lineno">  507</span><span class="stringliteral">            Number of top-N similar words to return, when `topn` is int. When `topn` is None,</span></div>
<div class="line"><span class="lineno">  508</span><span class="stringliteral">            then similarities for all words are returned.</span></div>
<div class="line"><span class="lineno">  509</span><span class="stringliteral">        restrict_vocab : int, optional</span></div>
<div class="line"><span class="lineno">  510</span><span class="stringliteral">            Optional integer which limits the range of vectors which</span></div>
<div class="line"><span class="lineno">  511</span><span class="stringliteral">            are searched for most-similar values. For example, restrict_vocab=10000 would</span></div>
<div class="line"><span class="lineno">  512</span><span class="stringliteral">            only check the first 10000 word vectors in the vocabulary order. (This may be</span></div>
<div class="line"><span class="lineno">  513</span><span class="stringliteral">            meaningful if you&#39;ve sorted the vocabulary by descending frequency.)</span></div>
<div class="line"><span class="lineno">  514</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  515</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  516</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  517</span><span class="stringliteral">        list of (str, float) or numpy.array</span></div>
<div class="line"><span class="lineno">  518</span><span class="stringliteral">            When `topn` is int, a sequence of (word, similarity) is returned.</span></div>
<div class="line"><span class="lineno">  519</span><span class="stringliteral">            When `topn` is None, then similarities for all words are returned as a</span></div>
<div class="line"><span class="lineno">  520</span><span class="stringliteral">            one-dimensional numpy array with the size of the vocabulary.</span></div>
<div class="line"><span class="lineno">  521</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  522</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  523</span>        <span class="keywordflow">if</span> isinstance(topn, Integral) <span class="keywordflow">and</span> topn &lt; 1:</div>
<div class="line"><span class="lineno">  524</span>            <span class="keywordflow">return</span> []</div>
<div class="line"><span class="lineno">  525</span> </div>
<div class="line"><span class="lineno">  526</span>        <span class="keywordflow">if</span> positive <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  527</span>            positive = []</div>
<div class="line"><span class="lineno">  528</span>        <span class="keywordflow">if</span> negative <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  529</span>            negative = []</div>
<div class="line"><span class="lineno">  530</span> </div>
<div class="line"><span class="lineno">  531</span>        self.init_sims()</div>
<div class="line"><span class="lineno">  532</span> </div>
<div class="line"><span class="lineno">  533</span>        <span class="keywordflow">if</span> isinstance(positive, string_types) <span class="keywordflow">and</span> <span class="keywordflow">not</span> negative:</div>
<div class="line"><span class="lineno">  534</span>            <span class="comment"># allow calls like most_similar(&#39;dog&#39;), as a shorthand for most_similar([&#39;dog&#39;])</span></div>
<div class="line"><span class="lineno">  535</span>            positive = [positive]</div>
<div class="line"><span class="lineno">  536</span> </div>
<div class="line"><span class="lineno">  537</span>        <span class="comment"># add weights for each word, if not already present; default to 1.0 for positive and -1.0 for negative words</span></div>
<div class="line"><span class="lineno">  538</span>        positive = [</div>
<div class="line"><span class="lineno">  539</span>            (word, 1.0) <span class="keywordflow">if</span> isinstance(word, string_types + (ndarray,)) <span class="keywordflow">else</span> word</div>
<div class="line"><span class="lineno">  540</span>            <span class="keywordflow">for</span> word <span class="keywordflow">in</span> positive</div>
<div class="line"><span class="lineno">  541</span>        ]</div>
<div class="line"><span class="lineno">  542</span>        negative = [</div>
<div class="line"><span class="lineno">  543</span>            (word, -1.0) <span class="keywordflow">if</span> isinstance(word, string_types + (ndarray,)) <span class="keywordflow">else</span> word</div>
<div class="line"><span class="lineno">  544</span>            <span class="keywordflow">for</span> word <span class="keywordflow">in</span> negative</div>
<div class="line"><span class="lineno">  545</span>        ]</div>
<div class="line"><span class="lineno">  546</span> </div>
<div class="line"><span class="lineno">  547</span>        <span class="comment"># compute the weighted average of all words</span></div>
<div class="line"><span class="lineno">  548</span>        all_words, mean = set(), []</div>
<div class="line"><span class="lineno">  549</span>        <span class="keywordflow">for</span> word, weight <span class="keywordflow">in</span> positive + negative:</div>
<div class="line"><span class="lineno">  550</span>            <span class="keywordflow">if</span> isinstance(word, ndarray):</div>
<div class="line"><span class="lineno">  551</span>                mean.append(weight * word)</div>
<div class="line"><span class="lineno">  552</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  553</span>                mean.append(weight * self.word_vec(word, use_norm=<span class="keyword">True</span>))</div>
<div class="line"><span class="lineno">  554</span>                <span class="keywordflow">if</span> word <span class="keywordflow">in</span> self.vocab:</div>
<div class="line"><span class="lineno">  555</span>                    all_words.add(self.vocab[word].index)</div>
<div class="line"><span class="lineno">  556</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> mean:</div>
<div class="line"><span class="lineno">  557</span>            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;cannot compute similarity with no input&quot;</span>)</div>
<div class="line"><span class="lineno">  558</span>        mean = matutils.unitvec(array(mean).mean(axis=0)).astype(REAL)</div>
<div class="line"><span class="lineno">  559</span> </div>
<div class="line"><span class="lineno">  560</span>        <span class="keywordflow">if</span> indexer <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> isinstance(topn, int):</div>
<div class="line"><span class="lineno">  561</span>            <span class="keywordflow">return</span> indexer.most_similar(mean, topn)</div>
<div class="line"><span class="lineno">  562</span> </div>
<div class="line"><span class="lineno">  563</span>        limited = self.vectors_norm <span class="keywordflow">if</span> restrict_vocab <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">else</span> self.vectors_norm[:restrict_vocab]</div>
<div class="line"><span class="lineno">  564</span>        dists = dot(limited, mean)</div>
<div class="line"><span class="lineno">  565</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> topn:</div>
<div class="line"><span class="lineno">  566</span>            <span class="keywordflow">return</span> dists</div>
<div class="line"><span class="lineno">  567</span>        best = matutils.argsort(dists, topn=topn + len(all_words), reverse=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  568</span>        <span class="comment"># ignore (don&#39;t return) words from the input</span></div>
<div class="line"><span class="lineno">  569</span>        result = [(self.index2word[sim], float(dists[sim])) <span class="keywordflow">for</span> sim <span class="keywordflow">in</span> best <span class="keywordflow">if</span> sim <span class="keywordflow">not</span> <span class="keywordflow">in</span> all_words]</div>
<div class="line"><span class="lineno">  570</span>        <span class="keywordflow">return</span> result[:topn]</div>
<div class="line"><span class="lineno">  571</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ac84151f7ed286587a0230067afcb0b30" name="ac84151f7ed286587a0230067afcb0b30"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac84151f7ed286587a0230067afcb0b30">&#9670;&#160;</a></span>most_similar_cosmul()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>positive</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>negative</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>topn</em> = <code>10</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Find the top-N most similar words, using the multiplicative combination objective,
proposed by `Omer Levy and Yoav Goldberg "Linguistic Regularities in Sparse and Explicit Word Representations"
&lt;http://www.aclweb.org/anthology/W14-1618&gt;`_. Positive words still contribute positively towards the similarity,
negative words negatively, but with less susceptibility to one large distance dominating the calculation.
In the common analogy-solving case, of two positive and one negative examples,
this method is equivalent to the "3CosMul" objective (equation (4)) of Levy and Goldberg.

Additional positive or negative examples contribute to the numerator or denominator,
respectively - a potentially sensible but untested extension of the method.
With a single positive example, rankings will be the same as in the default
:meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.

Parameters
----------
positive : list of str, optional
    List of words that contribute positively.
negative : list of str, optional
    List of words that contribute negatively.
topn : int or None, optional
    Number of top-N similar words to return, when `topn` is int. When `topn` is None,
    then similarities for all words are returned.

Returns
-------
list of (str, float) or numpy.array
    When `topn` is int, a sequence of (word, similarity) is returned.
    When `topn` is None, then similarities for all words are returned as a
    one-dimensional numpy array with the size of the vocabulary.</pre> <div class="fragment"><div class="line"><span class="lineno">  780</span>    <span class="keyword">def </span>most_similar_cosmul(self, positive=None, negative=None, topn=10):</div>
<div class="line"><span class="lineno">  781</span>        <span class="stringliteral">&quot;&quot;&quot;Find the top-N most similar words, using the multiplicative combination objective,</span></div>
<div class="line"><span class="lineno">  782</span><span class="stringliteral">        proposed by `Omer Levy and Yoav Goldberg &quot;Linguistic Regularities in Sparse and Explicit Word Representations&quot;</span></div>
<div class="line"><span class="lineno">  783</span><span class="stringliteral">        &lt;http://www.aclweb.org/anthology/W14-1618&gt;`_. Positive words still contribute positively towards the similarity,</span></div>
<div class="line"><span class="lineno">  784</span><span class="stringliteral">        negative words negatively, but with less susceptibility to one large distance dominating the calculation.</span></div>
<div class="line"><span class="lineno">  785</span><span class="stringliteral">        In the common analogy-solving case, of two positive and one negative examples,</span></div>
<div class="line"><span class="lineno">  786</span><span class="stringliteral">        this method is equivalent to the &quot;3CosMul&quot; objective (equation (4)) of Levy and Goldberg.</span></div>
<div class="line"><span class="lineno">  787</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  788</span><span class="stringliteral">        Additional positive or negative examples contribute to the numerator or denominator,</span></div>
<div class="line"><span class="lineno">  789</span><span class="stringliteral">        respectively - a potentially sensible but untested extension of the method.</span></div>
<div class="line"><span class="lineno">  790</span><span class="stringliteral">        With a single positive example, rankings will be the same as in the default</span></div>
<div class="line"><span class="lineno">  791</span><span class="stringliteral">        :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.</span></div>
<div class="line"><span class="lineno">  792</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  793</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  794</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  795</span><span class="stringliteral">        positive : list of str, optional</span></div>
<div class="line"><span class="lineno">  796</span><span class="stringliteral">            List of words that contribute positively.</span></div>
<div class="line"><span class="lineno">  797</span><span class="stringliteral">        negative : list of str, optional</span></div>
<div class="line"><span class="lineno">  798</span><span class="stringliteral">            List of words that contribute negatively.</span></div>
<div class="line"><span class="lineno">  799</span><span class="stringliteral">        topn : int or None, optional</span></div>
<div class="line"><span class="lineno">  800</span><span class="stringliteral">            Number of top-N similar words to return, when `topn` is int. When `topn` is None,</span></div>
<div class="line"><span class="lineno">  801</span><span class="stringliteral">            then similarities for all words are returned.</span></div>
<div class="line"><span class="lineno">  802</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  803</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  804</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  805</span><span class="stringliteral">        list of (str, float) or numpy.array</span></div>
<div class="line"><span class="lineno">  806</span><span class="stringliteral">            When `topn` is int, a sequence of (word, similarity) is returned.</span></div>
<div class="line"><span class="lineno">  807</span><span class="stringliteral">            When `topn` is None, then similarities for all words are returned as a</span></div>
<div class="line"><span class="lineno">  808</span><span class="stringliteral">            one-dimensional numpy array with the size of the vocabulary.</span></div>
<div class="line"><span class="lineno">  809</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  810</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  811</span>        <span class="keywordflow">if</span> isinstance(topn, Integral) <span class="keywordflow">and</span> topn &lt; 1:</div>
<div class="line"><span class="lineno">  812</span>            <span class="keywordflow">return</span> []</div>
<div class="line"><span class="lineno">  813</span> </div>
<div class="line"><span class="lineno">  814</span>        <span class="keywordflow">if</span> positive <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  815</span>            positive = []</div>
<div class="line"><span class="lineno">  816</span>        <span class="keywordflow">if</span> negative <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  817</span>            negative = []</div>
<div class="line"><span class="lineno">  818</span> </div>
<div class="line"><span class="lineno">  819</span>        self.init_sims()</div>
<div class="line"><span class="lineno">  820</span> </div>
<div class="line"><span class="lineno">  821</span>        <span class="keywordflow">if</span> isinstance(positive, string_types) <span class="keywordflow">and</span> <span class="keywordflow">not</span> negative:</div>
<div class="line"><span class="lineno">  822</span>            <span class="comment"># allow calls like most_similar_cosmul(&#39;dog&#39;), as a shorthand for most_similar_cosmul([&#39;dog&#39;])</span></div>
<div class="line"><span class="lineno">  823</span>            positive = [positive]</div>
<div class="line"><span class="lineno">  824</span> </div>
<div class="line"><span class="lineno">  825</span>        all_words = {</div>
<div class="line"><span class="lineno">  826</span>            self.vocab[word].index <span class="keywordflow">for</span> word <span class="keywordflow">in</span> positive + negative</div>
<div class="line"><span class="lineno">  827</span>            <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(word, ndarray) <span class="keywordflow">and</span> word <span class="keywordflow">in</span> self.vocab</div>
<div class="line"><span class="lineno">  828</span>            }</div>
<div class="line"><span class="lineno">  829</span> </div>
<div class="line"><span class="lineno">  830</span>        positive = [</div>
<div class="line"><span class="lineno">  831</span>            self.word_vec(word, use_norm=<span class="keyword">True</span>) <span class="keywordflow">if</span> isinstance(word, string_types) <span class="keywordflow">else</span> word</div>
<div class="line"><span class="lineno">  832</span>            <span class="keywordflow">for</span> word <span class="keywordflow">in</span> positive</div>
<div class="line"><span class="lineno">  833</span>        ]</div>
<div class="line"><span class="lineno">  834</span>        negative = [</div>
<div class="line"><span class="lineno">  835</span>            self.word_vec(word, use_norm=<span class="keyword">True</span>) <span class="keywordflow">if</span> isinstance(word, string_types) <span class="keywordflow">else</span> word</div>
<div class="line"><span class="lineno">  836</span>            <span class="keywordflow">for</span> word <span class="keywordflow">in</span> negative</div>
<div class="line"><span class="lineno">  837</span>        ]</div>
<div class="line"><span class="lineno">  838</span> </div>
<div class="line"><span class="lineno">  839</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> positive:</div>
<div class="line"><span class="lineno">  840</span>            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;cannot compute similarity with no input&quot;</span>)</div>
<div class="line"><span class="lineno">  841</span> </div>
<div class="line"><span class="lineno">  842</span>        <span class="comment"># equation (4) of Levy &amp; Goldberg &quot;Linguistic Regularities...&quot;,</span></div>
<div class="line"><span class="lineno">  843</span>        <span class="comment"># with distances shifted to [0,1] per footnote (7)</span></div>
<div class="line"><span class="lineno">  844</span>        pos_dists = [((1 + dot(self.vectors_norm, term)) / 2) <span class="keywordflow">for</span> term <span class="keywordflow">in</span> positive]</div>
<div class="line"><span class="lineno">  845</span>        neg_dists = [((1 + dot(self.vectors_norm, term)) / 2) <span class="keywordflow">for</span> term <span class="keywordflow">in</span> negative]</div>
<div class="line"><span class="lineno">  846</span>        dists = prod(pos_dists, axis=0) / (prod(neg_dists, axis=0) + 0.000001)</div>
<div class="line"><span class="lineno">  847</span> </div>
<div class="line"><span class="lineno">  848</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> topn:</div>
<div class="line"><span class="lineno">  849</span>            <span class="keywordflow">return</span> dists</div>
<div class="line"><span class="lineno">  850</span>        best = matutils.argsort(dists, topn=topn + len(all_words), reverse=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  851</span>        <span class="comment"># ignore (don&#39;t return) words from the input</span></div>
<div class="line"><span class="lineno">  852</span>        result = [(self.index2word[sim], float(dists[sim])) <span class="keywordflow">for</span> sim <span class="keywordflow">in</span> best <span class="keywordflow">if</span> sim <span class="keywordflow">not</span> <span class="keywordflow">in</span> all_words]</div>
<div class="line"><span class="lineno">  853</span>        <span class="keywordflow">return</span> result[:topn]</div>
<div class="line"><span class="lineno">  854</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a90f521c82652badb88a6f3e103a6388c" name="a90f521c82652badb88a6f3e103a6388c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a90f521c82652badb88a6f3e103a6388c">&#9670;&#160;</a></span>n_similarity()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ws1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>ws2</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute cosine similarity between two sets of words.

Parameters
----------
ws1 : list of str
    Sequence of words.
ws2: list of str
    Sequence of words.

Returns
-------
numpy.ndarray
    Similarities between `ws1` and `ws2`.</pre> <div class="fragment"><div class="line"><span class="lineno">  976</span>    <span class="keyword">def </span>n_similarity(self, ws1, ws2):</div>
<div class="line"><span class="lineno">  977</span>        <span class="stringliteral">&quot;&quot;&quot;Compute cosine similarity between two sets of words.</span></div>
<div class="line"><span class="lineno">  978</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  979</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  980</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  981</span><span class="stringliteral">        ws1 : list of str</span></div>
<div class="line"><span class="lineno">  982</span><span class="stringliteral">            Sequence of words.</span></div>
<div class="line"><span class="lineno">  983</span><span class="stringliteral">        ws2: list of str</span></div>
<div class="line"><span class="lineno">  984</span><span class="stringliteral">            Sequence of words.</span></div>
<div class="line"><span class="lineno">  985</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  986</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  987</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  988</span><span class="stringliteral">        numpy.ndarray</span></div>
<div class="line"><span class="lineno">  989</span><span class="stringliteral">            Similarities between `ws1` and `ws2`.</span></div>
<div class="line"><span class="lineno">  990</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  991</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  992</span>        <span class="keywordflow">if</span> not(len(ws1) <span class="keywordflow">and</span> len(ws2)):</div>
<div class="line"><span class="lineno">  993</span>            <span class="keywordflow">raise</span> ZeroDivisionError(<span class="stringliteral">&#39;At least one of the passed list is empty.&#39;</span>)</div>
<div class="line"><span class="lineno">  994</span>        v1 = [self[word] <span class="keywordflow">for</span> word <span class="keywordflow">in</span> ws1]</div>
<div class="line"><span class="lineno">  995</span>        v2 = [self[word] <span class="keywordflow">for</span> word <span class="keywordflow">in</span> ws2]</div>
<div class="line"><span class="lineno">  996</span>        <span class="keywordflow">return</span> dot(matutils.unitvec(array(v1).mean(axis=0)), matutils.unitvec(array(v2).mean(axis=0)))</div>
<div class="line"><span class="lineno">  997</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a73703bf01d377e1b2e39d2b18516c8ed" name="a73703bf01d377e1b2e39d2b18516c8ed"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a73703bf01d377e1b2e39d2b18516c8ed">&#9670;&#160;</a></span>relative_cosine_similarity()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.relative_cosine_similarity </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>wa</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>wb</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>topn</em> = <code>10</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute the relative cosine similarity between two words given top-n similar words,
by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc "A Minimally Supervised Approach
for Synonym Extraction with Word Embeddings" &lt;https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf&gt;`_.

To calculate relative cosine similarity between two words, equation (1) of the paper is used.
For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than
any arbitrary word pairs.

Parameters
----------
wa: str
    Word for which we have to look top-n similar word.
wb: str
    Word for which we evaluating relative cosine similarity with wa.
topn: int, optional
    Number of top-n similar words to look with respect to wa.

Returns
-------
numpy.float64
    Relative cosine similarity between wa and wb.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1356</span>    <span class="keyword">def </span>relative_cosine_similarity(self, wa, wb, topn=10):</div>
<div class="line"><span class="lineno"> 1357</span>        <span class="stringliteral">&quot;&quot;&quot;Compute the relative cosine similarity between two words given top-n similar words,</span></div>
<div class="line"><span class="lineno"> 1358</span><span class="stringliteral">        by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc &quot;A Minimally Supervised Approach</span></div>
<div class="line"><span class="lineno"> 1359</span><span class="stringliteral">        for Synonym Extraction with Word Embeddings&quot; &lt;https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf&gt;`_.</span></div>
<div class="line"><span class="lineno"> 1360</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1361</span><span class="stringliteral">        To calculate relative cosine similarity between two words, equation (1) of the paper is used.</span></div>
<div class="line"><span class="lineno"> 1362</span><span class="stringliteral">        For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than</span></div>
<div class="line"><span class="lineno"> 1363</span><span class="stringliteral">        any arbitrary word pairs.</span></div>
<div class="line"><span class="lineno"> 1364</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1365</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno"> 1366</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno"> 1367</span><span class="stringliteral">        wa: str</span></div>
<div class="line"><span class="lineno"> 1368</span><span class="stringliteral">            Word for which we have to look top-n similar word.</span></div>
<div class="line"><span class="lineno"> 1369</span><span class="stringliteral">        wb: str</span></div>
<div class="line"><span class="lineno"> 1370</span><span class="stringliteral">            Word for which we evaluating relative cosine similarity with wa.</span></div>
<div class="line"><span class="lineno"> 1371</span><span class="stringliteral">        topn: int, optional</span></div>
<div class="line"><span class="lineno"> 1372</span><span class="stringliteral">            Number of top-n similar words to look with respect to wa.</span></div>
<div class="line"><span class="lineno"> 1373</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1374</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno"> 1375</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno"> 1376</span><span class="stringliteral">        numpy.float64</span></div>
<div class="line"><span class="lineno"> 1377</span><span class="stringliteral">            Relative cosine similarity between wa and wb.</span></div>
<div class="line"><span class="lineno"> 1378</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1379</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1380</span>        sims = self.similar_by_word(wa, topn)</div>
<div class="line"><span class="lineno"> 1381</span>        <span class="keyword">assert</span> sims, <span class="stringliteral">&quot;Failed code invariant: list of similar words must never be empty.&quot;</span></div>
<div class="line"><span class="lineno"> 1382</span>        rcs = float(self.similarity(wa, wb)) / (sum(sim <span class="keywordflow">for</span> _, sim <span class="keywordflow">in</span> sims))</div>
<div class="line"><span class="lineno"> 1383</span> </div>
<div class="line"><span class="lineno"> 1384</span>        <span class="keywordflow">return</span> rcs</div>
<div class="line"><span class="lineno"> 1385</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="adbb98824bcdd5ea4cebea513726fa185" name="adbb98824bcdd5ea4cebea513726fa185"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adbb98824bcdd5ea4cebea513726fa185">&#9670;&#160;</a></span>save()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.save </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>args</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Save KeyedVectors.

Parameters
----------
fname : str
    Path to the output file.

See Also
--------
:meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`
    Load saved model.</pre> 
<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#ae62d5015e2e1d6ecde71878f025ece0d">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>

<p>Reimplemented in <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_fast_text_keyed_vectors.html#a73c6c949dd914cde987032c249d3341d">gensim.models.keyedvectors.FastTextKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  420</span>    <span class="keyword">def </span>save(self, *args, **kwargs):</div>
<div class="line"><span class="lineno">  421</span>        <span class="stringliteral">&quot;&quot;&quot;Save KeyedVectors.</span></div>
<div class="line"><span class="lineno">  422</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  423</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  424</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  425</span><span class="stringliteral">        fname : str</span></div>
<div class="line"><span class="lineno">  426</span><span class="stringliteral">            Path to the output file.</span></div>
<div class="line"><span class="lineno">  427</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  428</span><span class="stringliteral">        See Also</span></div>
<div class="line"><span class="lineno">  429</span><span class="stringliteral">        --------</span></div>
<div class="line"><span class="lineno">  430</span><span class="stringliteral">        :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`</span></div>
<div class="line"><span class="lineno">  431</span><span class="stringliteral">            Load saved model.</span></div>
<div class="line"><span class="lineno">  432</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  433</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  434</span>        <span class="comment"># don&#39;t bother storing the cached normalized vectors</span></div>
<div class="line"><span class="lineno">  435</span>        kwargs[<span class="stringliteral">&#39;ignore&#39;</span>] = kwargs.get(<span class="stringliteral">&#39;ignore&#39;</span>, [<span class="stringliteral">&#39;vectors_norm&#39;</span>])</div>
<div class="line"><span class="lineno">  436</span>        super(WordEmbeddingsKeyedVectors, self).save(*args, **kwargs)</div>
<div class="line"><span class="lineno">  437</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="af7c562f07431491e0cf373ef2fef18f3" name="af7c562f07431491e0cf373ef2fef18f3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af7c562f07431491e0cf373ef2fef18f3">&#9670;&#160;</a></span>similar_by_vector()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>vector</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>topn</em> = <code>10</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>restrict_vocab</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Find the top-N most similar words by vector.

Parameters
----------
vector : numpy.array
    Vector from which similarities are to be computed.
topn : int or None, optional
    Number of top-N similar words to return, when `topn` is int. When `topn` is None,
    then similarities for all words are returned.
restrict_vocab : int, optional
    Optional integer which limits the range of vectors which
    are searched for most-similar values. For example, restrict_vocab=10000 would
    only check the first 10000 word vectors in the vocabulary order. (This may be
    meaningful if you've sorted the vocabulary by descending frequency.)

Returns
-------
list of (str, float) or numpy.array
    When `topn` is int, a sequence of (word, similarity) is returned.
    When `topn` is None, then similarities for all words are returned as a
    one-dimensional numpy array with the size of the vocabulary.</pre> <div class="fragment"><div class="line"><span class="lineno">  598</span>    <span class="keyword">def </span>similar_by_vector(self, vector, topn=10, restrict_vocab=None):</div>
<div class="line"><span class="lineno">  599</span>        <span class="stringliteral">&quot;&quot;&quot;Find the top-N most similar words by vector.</span></div>
<div class="line"><span class="lineno">  600</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  601</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  602</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  603</span><span class="stringliteral">        vector : numpy.array</span></div>
<div class="line"><span class="lineno">  604</span><span class="stringliteral">            Vector from which similarities are to be computed.</span></div>
<div class="line"><span class="lineno">  605</span><span class="stringliteral">        topn : int or None, optional</span></div>
<div class="line"><span class="lineno">  606</span><span class="stringliteral">            Number of top-N similar words to return, when `topn` is int. When `topn` is None,</span></div>
<div class="line"><span class="lineno">  607</span><span class="stringliteral">            then similarities for all words are returned.</span></div>
<div class="line"><span class="lineno">  608</span><span class="stringliteral">        restrict_vocab : int, optional</span></div>
<div class="line"><span class="lineno">  609</span><span class="stringliteral">            Optional integer which limits the range of vectors which</span></div>
<div class="line"><span class="lineno">  610</span><span class="stringliteral">            are searched for most-similar values. For example, restrict_vocab=10000 would</span></div>
<div class="line"><span class="lineno">  611</span><span class="stringliteral">            only check the first 10000 word vectors in the vocabulary order. (This may be</span></div>
<div class="line"><span class="lineno">  612</span><span class="stringliteral">            meaningful if you&#39;ve sorted the vocabulary by descending frequency.)</span></div>
<div class="line"><span class="lineno">  613</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  614</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  615</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  616</span><span class="stringliteral">        list of (str, float) or numpy.array</span></div>
<div class="line"><span class="lineno">  617</span><span class="stringliteral">            When `topn` is int, a sequence of (word, similarity) is returned.</span></div>
<div class="line"><span class="lineno">  618</span><span class="stringliteral">            When `topn` is None, then similarities for all words are returned as a</span></div>
<div class="line"><span class="lineno">  619</span><span class="stringliteral">            one-dimensional numpy array with the size of the vocabulary.</span></div>
<div class="line"><span class="lineno">  620</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  621</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  622</span>        <span class="keywordflow">return</span> self.most_similar(positive=[vector], topn=topn, restrict_vocab=restrict_vocab)</div>
<div class="line"><span class="lineno">  623</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a4adeb26aad7590eaacd3e6f69d9384b4" name="a4adeb26aad7590eaacd3e6f69d9384b4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4adeb26aad7590eaacd3e6f69d9384b4">&#9670;&#160;</a></span>similar_by_word()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>topn</em> = <code>10</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>restrict_vocab</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Find the top-N most similar words.

Parameters
----------
word : str
    Word
topn : int or None, optional
    Number of top-N similar words to return. If topn is None, similar_by_word returns
    the vector of similarity scores.
restrict_vocab : int, optional
    Optional integer which limits the range of vectors which
    are searched for most-similar values. For example, restrict_vocab=10000 would
    only check the first 10000 word vectors in the vocabulary order. (This may be
    meaningful if you've sorted the vocabulary by descending frequency.)

Returns
-------
list of (str, float) or numpy.array
    When `topn` is int, a sequence of (word, similarity) is returned.
    When `topn` is None, then similarities for all words are returned as a
    one-dimensional numpy array with the size of the vocabulary.</pre> <div class="fragment"><div class="line"><span class="lineno">  572</span>    <span class="keyword">def </span>similar_by_word(self, word, topn=10, restrict_vocab=None):</div>
<div class="line"><span class="lineno">  573</span>        <span class="stringliteral">&quot;&quot;&quot;Find the top-N most similar words.</span></div>
<div class="line"><span class="lineno">  574</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  575</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  576</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  577</span><span class="stringliteral">        word : str</span></div>
<div class="line"><span class="lineno">  578</span><span class="stringliteral">            Word</span></div>
<div class="line"><span class="lineno">  579</span><span class="stringliteral">        topn : int or None, optional</span></div>
<div class="line"><span class="lineno">  580</span><span class="stringliteral">            Number of top-N similar words to return. If topn is None, similar_by_word returns</span></div>
<div class="line"><span class="lineno">  581</span><span class="stringliteral">            the vector of similarity scores.</span></div>
<div class="line"><span class="lineno">  582</span><span class="stringliteral">        restrict_vocab : int, optional</span></div>
<div class="line"><span class="lineno">  583</span><span class="stringliteral">            Optional integer which limits the range of vectors which</span></div>
<div class="line"><span class="lineno">  584</span><span class="stringliteral">            are searched for most-similar values. For example, restrict_vocab=10000 would</span></div>
<div class="line"><span class="lineno">  585</span><span class="stringliteral">            only check the first 10000 word vectors in the vocabulary order. (This may be</span></div>
<div class="line"><span class="lineno">  586</span><span class="stringliteral">            meaningful if you&#39;ve sorted the vocabulary by descending frequency.)</span></div>
<div class="line"><span class="lineno">  587</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  588</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  589</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  590</span><span class="stringliteral">        list of (str, float) or numpy.array</span></div>
<div class="line"><span class="lineno">  591</span><span class="stringliteral">            When `topn` is int, a sequence of (word, similarity) is returned.</span></div>
<div class="line"><span class="lineno">  592</span><span class="stringliteral">            When `topn` is None, then similarities for all words are returned as a</span></div>
<div class="line"><span class="lineno">  593</span><span class="stringliteral">            one-dimensional numpy array with the size of the vocabulary.</span></div>
<div class="line"><span class="lineno">  594</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  595</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  596</span>        <span class="keywordflow">return</span> self.most_similar(positive=[word], topn=topn, restrict_vocab=restrict_vocab)</div>
<div class="line"><span class="lineno">  597</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a0478860f175cd86807696b3c80506798" name="a0478860f175cd86807696b3c80506798"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0478860f175cd86807696b3c80506798">&#9670;&#160;</a></span>similarity()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>w1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>w2</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute cosine similarity between two words.

Parameters
----------
w1 : str
    Input word.
w2 : str
    Input word.

Returns
-------
float
    Cosine similarity between `w1` and `w2`.</pre> 
<p>Reimplemented from <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_base_keyed_vectors.html#a5a0ff0b9bf234a503f980cbed951561f">gensim.models.keyedvectors.BaseKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  958</span>    <span class="keyword">def </span>similarity(self, w1, w2):</div>
<div class="line"><span class="lineno">  959</span>        <span class="stringliteral">&quot;&quot;&quot;Compute cosine similarity between two words.</span></div>
<div class="line"><span class="lineno">  960</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  961</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  962</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  963</span><span class="stringliteral">        w1 : str</span></div>
<div class="line"><span class="lineno">  964</span><span class="stringliteral">            Input word.</span></div>
<div class="line"><span class="lineno">  965</span><span class="stringliteral">        w2 : str</span></div>
<div class="line"><span class="lineno">  966</span><span class="stringliteral">            Input word.</span></div>
<div class="line"><span class="lineno">  967</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  968</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  969</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  970</span><span class="stringliteral">        float</span></div>
<div class="line"><span class="lineno">  971</span><span class="stringliteral">            Cosine similarity between `w1` and `w2`.</span></div>
<div class="line"><span class="lineno">  972</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  973</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  974</span>        <span class="keywordflow">return</span> dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))</div>
<div class="line"><span class="lineno">  975</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a15557ed25f0f0161e36042da3ea9c085" name="a15557ed25f0f0161e36042da3ea9c085"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a15557ed25f0f0161e36042da3ea9c085">&#9670;&#160;</a></span>similarity_matrix()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity_matrix </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dictionary</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tfidf</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>threshold</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>exponent</em> = <code>2.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>nonzero_limit</em> = <code>100</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dtype</em> = <code>REAL</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Construct a term similarity matrix for computing Soft Cosine Measure.

This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing
Soft Cosine Measure between documents.

Parameters
----------
dictionary : :class:`~gensim.corpora.dictionary.Dictionary`
    A dictionary that specifies the considered terms.
tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional
    A model that specifies the relative importance of the terms in the dictionary. The
    columns of the term similarity matrix will be build in a decreasing order of importance
    of terms, or in the order of term identifiers if None.
threshold : float, optional
    Only embeddings more similar than `threshold` are considered when retrieving word
    embeddings closest to a given word embedding.
exponent : float, optional
    Take the word embedding similarities larger than `threshold` to the power of `exponent`.
nonzero_limit : int, optional
    The maximum number of non-zero elements outside the diagonal in a single column of the
    sparse term similarity matrix.
dtype : numpy.dtype, optional
    Data-type of the sparse term similarity matrix.

Returns
-------
:class:`scipy.sparse.csc_matrix`
    Term similarity matrix.

See Also
--------
:func:`gensim.matutils.softcossim`
    The Soft Cosine Measure.
:class:`~gensim.similarities.docsim.SoftCosineSimilarity`
    A class for performing corpus-based similarity queries with Soft Cosine Measure.

Notes
-----
The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of
`Delphine Charlet and Geraldine Damnati, "SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity
between Questions for Community Question Answering", 2017
&lt;http://www.aclweb.org/anthology/S/S17/S17-2051.pdf&gt;`_.</pre> <div class="fragment"><div class="line"><span class="lineno">  627</span>    <span class="keyword">def </span>similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=REAL):</div>
<div class="line"><span class="lineno">  628</span>        <span class="stringliteral">&quot;&quot;&quot;Construct a term similarity matrix for computing Soft Cosine Measure.</span></div>
<div class="line"><span class="lineno">  629</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  630</span><span class="stringliteral">        This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing</span></div>
<div class="line"><span class="lineno">  631</span><span class="stringliteral">        Soft Cosine Measure between documents.</span></div>
<div class="line"><span class="lineno">  632</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  633</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  634</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  635</span><span class="stringliteral">        dictionary : :class:`~gensim.corpora.dictionary.Dictionary`</span></div>
<div class="line"><span class="lineno">  636</span><span class="stringliteral">            A dictionary that specifies the considered terms.</span></div>
<div class="line"><span class="lineno">  637</span><span class="stringliteral">        tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional</span></div>
<div class="line"><span class="lineno">  638</span><span class="stringliteral">            A model that specifies the relative importance of the terms in the dictionary. The</span></div>
<div class="line"><span class="lineno">  639</span><span class="stringliteral">            columns of the term similarity matrix will be build in a decreasing order of importance</span></div>
<div class="line"><span class="lineno">  640</span><span class="stringliteral">            of terms, or in the order of term identifiers if None.</span></div>
<div class="line"><span class="lineno">  641</span><span class="stringliteral">        threshold : float, optional</span></div>
<div class="line"><span class="lineno">  642</span><span class="stringliteral">            Only embeddings more similar than `threshold` are considered when retrieving word</span></div>
<div class="line"><span class="lineno">  643</span><span class="stringliteral">            embeddings closest to a given word embedding.</span></div>
<div class="line"><span class="lineno">  644</span><span class="stringliteral">        exponent : float, optional</span></div>
<div class="line"><span class="lineno">  645</span><span class="stringliteral">            Take the word embedding similarities larger than `threshold` to the power of `exponent`.</span></div>
<div class="line"><span class="lineno">  646</span><span class="stringliteral">        nonzero_limit : int, optional</span></div>
<div class="line"><span class="lineno">  647</span><span class="stringliteral">            The maximum number of non-zero elements outside the diagonal in a single column of the</span></div>
<div class="line"><span class="lineno">  648</span><span class="stringliteral">            sparse term similarity matrix.</span></div>
<div class="line"><span class="lineno">  649</span><span class="stringliteral">        dtype : numpy.dtype, optional</span></div>
<div class="line"><span class="lineno">  650</span><span class="stringliteral">            Data-type of the sparse term similarity matrix.</span></div>
<div class="line"><span class="lineno">  651</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  652</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  653</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  654</span><span class="stringliteral">        :class:`scipy.sparse.csc_matrix`</span></div>
<div class="line"><span class="lineno">  655</span><span class="stringliteral">            Term similarity matrix.</span></div>
<div class="line"><span class="lineno">  656</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  657</span><span class="stringliteral">        See Also</span></div>
<div class="line"><span class="lineno">  658</span><span class="stringliteral">        --------</span></div>
<div class="line"><span class="lineno">  659</span><span class="stringliteral">        :func:`gensim.matutils.softcossim`</span></div>
<div class="line"><span class="lineno">  660</span><span class="stringliteral">            The Soft Cosine Measure.</span></div>
<div class="line"><span class="lineno">  661</span><span class="stringliteral">        :class:`~gensim.similarities.docsim.SoftCosineSimilarity`</span></div>
<div class="line"><span class="lineno">  662</span><span class="stringliteral">            A class for performing corpus-based similarity queries with Soft Cosine Measure.</span></div>
<div class="line"><span class="lineno">  663</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  664</span><span class="stringliteral">        Notes</span></div>
<div class="line"><span class="lineno">  665</span><span class="stringliteral">        -----</span></div>
<div class="line"><span class="lineno">  666</span><span class="stringliteral">        The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of</span></div>
<div class="line"><span class="lineno">  667</span><span class="stringliteral">        `Delphine Charlet and Geraldine Damnati, &quot;SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity</span></div>
<div class="line"><span class="lineno">  668</span><span class="stringliteral">        between Questions for Community Question Answering&quot;, 2017</span></div>
<div class="line"><span class="lineno">  669</span><span class="stringliteral">        &lt;http://www.aclweb.org/anthology/S/S17/S17-2051.pdf&gt;`_.</span></div>
<div class="line"><span class="lineno">  670</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  671</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  672</span>        index = WordEmbeddingSimilarityIndex(self, threshold=threshold, exponent=exponent)</div>
<div class="line"><span class="lineno">  673</span>        similarity_matrix = SparseTermSimilarityMatrix(</div>
<div class="line"><span class="lineno">  674</span>            index, dictionary, tfidf=tfidf, nonzero_limit=nonzero_limit, dtype=dtype)</div>
<div class="line"><span class="lineno">  675</span>        <span class="keywordflow">return</span> similarity_matrix.matrix</div>
<div class="line"><span class="lineno">  676</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ac5b8c3198b1542de529c0bcbbd447bfd" name="ac5b8c3198b1542de529c0bcbbd447bfd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac5b8c3198b1542de529c0bcbbd447bfd">&#9670;&#160;</a></span>syn0() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.syn0 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  399</span>    <span class="keyword">def </span>syn0(self):</div>
<div class="line"><span class="lineno">  400</span>        <span class="keywordflow">return</span> self.vectors</div>
<div class="line"><span class="lineno">  401</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a1727f4601c2dd0128ca2f0f00642b7c8" name="a1727f4601c2dd0128ca2f0f00642b7c8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1727f4601c2dd0128ca2f0f00642b7c8">&#9670;&#160;</a></span>syn0() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.syn0 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>value</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  404</span>    <span class="keyword">def </span>syn0(self, value):</div>
<div class="line"><span class="lineno">  405</span>        self.vectors = value</div>
<div class="line"><span class="lineno">  406</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a5bb9fc7a5b3c7a7a2decbfd37c5be131" name="a5bb9fc7a5b3c7a7a2decbfd37c5be131"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5bb9fc7a5b3c7a7a2decbfd37c5be131">&#9670;&#160;</a></span>syn0norm() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.syn0norm </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  409</span>    <span class="keyword">def </span>syn0norm(self):</div>
<div class="line"><span class="lineno">  410</span>        <span class="keywordflow">return</span> self.vectors_norm</div>
<div class="line"><span class="lineno">  411</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a07ec262bb97cc0ad077621a589df56bf" name="a07ec262bb97cc0ad077621a589df56bf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a07ec262bb97cc0ad077621a589df56bf">&#9670;&#160;</a></span>syn0norm() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.syn0norm </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>value</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  414</span>    <span class="keyword">def </span>syn0norm(self, value):</div>
<div class="line"><span class="lineno">  415</span>        self.vectors_norm = value</div>
<div class="line"><span class="lineno">  416</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="afef84b4d9bcfb25802fee7a8e944d065" name="afef84b4d9bcfb25802fee7a8e944d065"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afef84b4d9bcfb25802fee7a8e944d065">&#9670;&#160;</a></span>wmdistance()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>document1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>document2</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute the Word Mover's Distance between two documents.

When using this code, please consider citing the following papers:

* `Ofir Pele and Michael Werman "A linear time histogram metric for improved SIFT matching"
  &lt;http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf&gt;`_
* `Ofir Pele and Michael Werman "Fast and robust earth mover's distances"
  &lt;https://ieeexplore.ieee.org/document/5459199/&gt;`_
* `Matt Kusner et al. "From Word Embeddings To Document Distances"
  &lt;http://proceedings.mlr.press/v37/kusnerb15.pdf&gt;`_.

Parameters
----------
document1 : list of str
    Input document.
document2 : list of str
    Input document.

Returns
-------
float
    Word Mover's distance between `document1` and `document2`.

Warnings
--------
This method only works if `pyemd &lt;https://pypi.org/project/pyemd/&gt;`_ is installed.

If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)
will be returned.

Raises
------
ImportError
    If `pyemd &lt;https://pypi.org/project/pyemd/&gt;`_  isn't installed.</pre> <div class="fragment"><div class="line"><span class="lineno">  677</span>    <span class="keyword">def </span>wmdistance(self, document1, document2):</div>
<div class="line"><span class="lineno">  678</span>        <span class="stringliteral">&quot;&quot;&quot;Compute the Word Mover&#39;s Distance between two documents.</span></div>
<div class="line"><span class="lineno">  679</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  680</span><span class="stringliteral">        When using this code, please consider citing the following papers:</span></div>
<div class="line"><span class="lineno">  681</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  682</span><span class="stringliteral">        * `Ofir Pele and Michael Werman &quot;A linear time histogram metric for improved SIFT matching&quot;</span></div>
<div class="line"><span class="lineno">  683</span><span class="stringliteral">          &lt;http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf&gt;`_</span></div>
<div class="line"><span class="lineno">  684</span><span class="stringliteral">        * `Ofir Pele and Michael Werman &quot;Fast and robust earth mover&#39;s distances&quot;</span></div>
<div class="line"><span class="lineno">  685</span><span class="stringliteral">          &lt;https://ieeexplore.ieee.org/document/5459199/&gt;`_</span></div>
<div class="line"><span class="lineno">  686</span><span class="stringliteral">        * `Matt Kusner et al. &quot;From Word Embeddings To Document Distances&quot;</span></div>
<div class="line"><span class="lineno">  687</span><span class="stringliteral">          &lt;http://proceedings.mlr.press/v37/kusnerb15.pdf&gt;`_.</span></div>
<div class="line"><span class="lineno">  688</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  689</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  690</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  691</span><span class="stringliteral">        document1 : list of str</span></div>
<div class="line"><span class="lineno">  692</span><span class="stringliteral">            Input document.</span></div>
<div class="line"><span class="lineno">  693</span><span class="stringliteral">        document2 : list of str</span></div>
<div class="line"><span class="lineno">  694</span><span class="stringliteral">            Input document.</span></div>
<div class="line"><span class="lineno">  695</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  696</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  697</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  698</span><span class="stringliteral">        float</span></div>
<div class="line"><span class="lineno">  699</span><span class="stringliteral">            Word Mover&#39;s distance between `document1` and `document2`.</span></div>
<div class="line"><span class="lineno">  700</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  701</span><span class="stringliteral">        Warnings</span></div>
<div class="line"><span class="lineno">  702</span><span class="stringliteral">        --------</span></div>
<div class="line"><span class="lineno">  703</span><span class="stringliteral">        This method only works if `pyemd &lt;https://pypi.org/project/pyemd/&gt;`_ is installed.</span></div>
<div class="line"><span class="lineno">  704</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  705</span><span class="stringliteral">        If one of the documents have no words that exist in the vocab, `float(&#39;inf&#39;)` (i.e. infinity)</span></div>
<div class="line"><span class="lineno">  706</span><span class="stringliteral">        will be returned.</span></div>
<div class="line"><span class="lineno">  707</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  708</span><span class="stringliteral">        Raises</span></div>
<div class="line"><span class="lineno">  709</span><span class="stringliteral">        ------</span></div>
<div class="line"><span class="lineno">  710</span><span class="stringliteral">        ImportError</span></div>
<div class="line"><span class="lineno">  711</span><span class="stringliteral">            If `pyemd &lt;https://pypi.org/project/pyemd/&gt;`_  isn&#39;t installed.</span></div>
<div class="line"><span class="lineno">  712</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  713</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  714</span> </div>
<div class="line"><span class="lineno">  715</span>        <span class="comment"># If pyemd C extension is available, import it.</span></div>
<div class="line"><span class="lineno">  716</span>        <span class="comment"># If pyemd is attempted to be used, but isn&#39;t installed, ImportError will be raised in wmdistance</span></div>
<div class="line"><span class="lineno">  717</span>        <span class="keyword">from</span> pyemd <span class="keyword">import</span> emd</div>
<div class="line"><span class="lineno">  718</span> </div>
<div class="line"><span class="lineno">  719</span>        <span class="comment"># Remove out-of-vocabulary words.</span></div>
<div class="line"><span class="lineno">  720</span>        len_pre_oov1 = len(document1)</div>
<div class="line"><span class="lineno">  721</span>        len_pre_oov2 = len(document2)</div>
<div class="line"><span class="lineno">  722</span>        document1 = [token <span class="keywordflow">for</span> token <span class="keywordflow">in</span> document1 <span class="keywordflow">if</span> token <span class="keywordflow">in</span> self]</div>
<div class="line"><span class="lineno">  723</span>        document2 = [token <span class="keywordflow">for</span> token <span class="keywordflow">in</span> document2 <span class="keywordflow">if</span> token <span class="keywordflow">in</span> self]</div>
<div class="line"><span class="lineno">  724</span>        diff1 = len_pre_oov1 - len(document1)</div>
<div class="line"><span class="lineno">  725</span>        diff2 = len_pre_oov2 - len(document2)</div>
<div class="line"><span class="lineno">  726</span>        <span class="keywordflow">if</span> diff1 &gt; 0 <span class="keywordflow">or</span> diff2 &gt; 0:</div>
<div class="line"><span class="lineno">  727</span>            logger.info(<span class="stringliteral">&#39;Removed %d and %d OOV words from document 1 and 2 (respectively).&#39;</span>, diff1, diff2)</div>
<div class="line"><span class="lineno">  728</span> </div>
<div class="line"><span class="lineno">  729</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> document1 <span class="keywordflow">or</span> <span class="keywordflow">not</span> document2:</div>
<div class="line"><span class="lineno">  730</span>            logger.info(</div>
<div class="line"><span class="lineno">  731</span>                <span class="stringliteral">&quot;At least one of the documents had no words that were in the vocabulary. &quot;</span></div>
<div class="line"><span class="lineno">  732</span>                <span class="stringliteral">&quot;Aborting (returning inf).&quot;</span></div>
<div class="line"><span class="lineno">  733</span>            )</div>
<div class="line"><span class="lineno">  734</span>            <span class="keywordflow">return</span> float(<span class="stringliteral">&#39;inf&#39;</span>)</div>
<div class="line"><span class="lineno">  735</span> </div>
<div class="line"><span class="lineno">  736</span>        dictionary = Dictionary(documents=[document1, document2])</div>
<div class="line"><span class="lineno">  737</span>        vocab_len = len(dictionary)</div>
<div class="line"><span class="lineno">  738</span> </div>
<div class="line"><span class="lineno">  739</span>        <span class="keywordflow">if</span> vocab_len == 1:</div>
<div class="line"><span class="lineno">  740</span>            <span class="comment"># Both documents are composed by a single unique token</span></div>
<div class="line"><span class="lineno">  741</span>            <span class="keywordflow">return</span> 0.0</div>
<div class="line"><span class="lineno">  742</span> </div>
<div class="line"><span class="lineno">  743</span>        <span class="comment"># Sets for faster look-up.</span></div>
<div class="line"><span class="lineno">  744</span>        docset1 = set(document1)</div>
<div class="line"><span class="lineno">  745</span>        docset2 = set(document2)</div>
<div class="line"><span class="lineno">  746</span> </div>
<div class="line"><span class="lineno">  747</span>        <span class="comment"># Compute distance matrix.</span></div>
<div class="line"><span class="lineno">  748</span>        distance_matrix = zeros((vocab_len, vocab_len), dtype=double)</div>
<div class="line"><span class="lineno">  749</span>        <span class="keywordflow">for</span> i, t1 <span class="keywordflow">in</span> dictionary.items():</div>
<div class="line"><span class="lineno">  750</span>            <span class="keywordflow">if</span> t1 <span class="keywordflow">not</span> <span class="keywordflow">in</span> docset1:</div>
<div class="line"><span class="lineno">  751</span>                <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno">  752</span> </div>
<div class="line"><span class="lineno">  753</span>            <span class="keywordflow">for</span> j, t2 <span class="keywordflow">in</span> dictionary.items():</div>
<div class="line"><span class="lineno">  754</span>                <span class="keywordflow">if</span> t2 <span class="keywordflow">not</span> <span class="keywordflow">in</span> docset2 <span class="keywordflow">or</span> distance_matrix[i, j] != 0.0:</div>
<div class="line"><span class="lineno">  755</span>                    <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno">  756</span> </div>
<div class="line"><span class="lineno">  757</span>                <span class="comment"># Compute Euclidean distance between word vectors.</span></div>
<div class="line"><span class="lineno">  758</span>                distance_matrix[i, j] = distance_matrix[j, i] = sqrt(np_sum((self[t1] - self[t2])**2))</div>
<div class="line"><span class="lineno">  759</span> </div>
<div class="line"><span class="lineno">  760</span>        <span class="keywordflow">if</span> np_sum(distance_matrix) == 0.0:</div>
<div class="line"><span class="lineno">  761</span>            <span class="comment"># `emd` gets stuck if the distance matrix contains only zeros.</span></div>
<div class="line"><span class="lineno">  762</span>            logger.info(<span class="stringliteral">&#39;The distance matrix is all zeros. Aborting (returning inf).&#39;</span>)</div>
<div class="line"><span class="lineno">  763</span>            <span class="keywordflow">return</span> float(<span class="stringliteral">&#39;inf&#39;</span>)</div>
<div class="line"><span class="lineno">  764</span> </div>
<div class="line"><span class="lineno">  765</span>        <span class="keyword">def </span>nbow(document):</div>
<div class="line"><span class="lineno">  766</span>            d = zeros(vocab_len, dtype=double)</div>
<div class="line"><span class="lineno">  767</span>            nbow = dictionary.doc2bow(document)  <span class="comment"># Word frequencies.</span></div>
<div class="line"><span class="lineno">  768</span>            doc_len = len(document)</div>
<div class="line"><span class="lineno">  769</span>            <span class="keywordflow">for</span> idx, freq <span class="keywordflow">in</span> nbow:</div>
<div class="line"><span class="lineno">  770</span>                d[idx] = freq / float(doc_len)  <span class="comment"># Normalized word frequencies.</span></div>
<div class="line"><span class="lineno">  771</span>            <span class="keywordflow">return</span> d</div>
<div class="line"><span class="lineno">  772</span> </div>
<div class="line"><span class="lineno">  773</span>        <span class="comment"># Compute nBOW representation of documents.</span></div>
<div class="line"><span class="lineno">  774</span>        d1 = nbow(document1)</div>
<div class="line"><span class="lineno">  775</span>        d2 = nbow(document2)</div>
<div class="line"><span class="lineno">  776</span> </div>
<div class="line"><span class="lineno">  777</span>        <span class="comment"># Compute WMD.</span></div>
<div class="line"><span class="lineno">  778</span>        <span class="keywordflow">return</span> emd(d1, d2, distance_matrix)</div>
<div class="line"><span class="lineno">  779</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a69ed7f63fefee33351613579c16bdc89" name="a69ed7f63fefee33351613579c16bdc89"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a69ed7f63fefee33351613579c16bdc89">&#9670;&#160;</a></span>word_vec()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.word_vec </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>use_norm</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Get `word` representations in vector space, as a 1D numpy array.

Parameters
----------
word : str
    Input word
use_norm : bool, optional
    If True - resulting vector will be L2-normalized (unit euclidean length).

Returns
-------
numpy.ndarray
    Vector representation of `word`.

Raises
------
KeyError
    If word not in vocabulary.</pre> 
<p>Reimplemented in <a class="el" href="classgensim_1_1models_1_1keyedvectors_1_1_fast_text_keyed_vectors.html#a53a2807ce0cae7b1641934b5061dbf7e">gensim.models.keyedvectors.FastTextKeyedVectors</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  438</span>    <span class="keyword">def </span>word_vec(self, word, use_norm=False):</div>
<div class="line"><span class="lineno">  439</span>        <span class="stringliteral">&quot;&quot;&quot;Get `word` representations in vector space, as a 1D numpy array.</span></div>
<div class="line"><span class="lineno">  440</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  441</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  442</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  443</span><span class="stringliteral">        word : str</span></div>
<div class="line"><span class="lineno">  444</span><span class="stringliteral">            Input word</span></div>
<div class="line"><span class="lineno">  445</span><span class="stringliteral">        use_norm : bool, optional</span></div>
<div class="line"><span class="lineno">  446</span><span class="stringliteral">            If True - resulting vector will be L2-normalized (unit euclidean length).</span></div>
<div class="line"><span class="lineno">  447</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  448</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  449</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  450</span><span class="stringliteral">        numpy.ndarray</span></div>
<div class="line"><span class="lineno">  451</span><span class="stringliteral">            Vector representation of `word`.</span></div>
<div class="line"><span class="lineno">  452</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  453</span><span class="stringliteral">        Raises</span></div>
<div class="line"><span class="lineno">  454</span><span class="stringliteral">        ------</span></div>
<div class="line"><span class="lineno">  455</span><span class="stringliteral">        KeyError</span></div>
<div class="line"><span class="lineno">  456</span><span class="stringliteral">            If word not in vocabulary.</span></div>
<div class="line"><span class="lineno">  457</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  458</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  459</span>        <span class="keywordflow">if</span> word <span class="keywordflow">in</span> self.vocab:</div>
<div class="line"><span class="lineno">  460</span>            <span class="keywordflow">if</span> use_norm:</div>
<div class="line"><span class="lineno">  461</span>                result = self.vectors_norm[self.vocab[word].index]</div>
<div class="line"><span class="lineno">  462</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  463</span>                result = self.vectors[self.vocab[word].index]</div>
<div class="line"><span class="lineno">  464</span> </div>
<div class="line"><span class="lineno">  465</span>            result.setflags(write=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  466</span>            <span class="keywordflow">return</span> result</div>
<div class="line"><span class="lineno">  467</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  468</span>            <span class="keywordflow">raise</span> KeyError(<span class="stringliteral">&quot;word &#39;%s&#39; not in vocabulary&quot;</span> % word)</div>
<div class="line"><span class="lineno">  469</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a7115e7f05bb3b7ca3410f86c9b0e67eb" name="a7115e7f05bb3b7ca3410f86c9b0e67eb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7115e7f05bb3b7ca3410f86c9b0e67eb">&#9670;&#160;</a></span>words_closer_than()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.words_closer_than </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>w1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>w2</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Get all words that are closer to `w1` than `w2` is to `w1`.

Parameters
----------
w1 : str
    Input word.
w2 : str
    Input word.

Returns
-------
list (str)
    List of words that are closer to `w1` than `w2` is to `w1`.</pre> <div class="fragment"><div class="line"><span class="lineno">  473</span>    <span class="keyword">def </span>words_closer_than(self, w1, w2):</div>
<div class="line"><span class="lineno">  474</span>        <span class="stringliteral">&quot;&quot;&quot;Get all words that are closer to `w1` than `w2` is to `w1`.</span></div>
<div class="line"><span class="lineno">  475</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  476</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  477</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  478</span><span class="stringliteral">        w1 : str</span></div>
<div class="line"><span class="lineno">  479</span><span class="stringliteral">            Input word.</span></div>
<div class="line"><span class="lineno">  480</span><span class="stringliteral">        w2 : str</span></div>
<div class="line"><span class="lineno">  481</span><span class="stringliteral">            Input word.</span></div>
<div class="line"><span class="lineno">  482</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  483</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  484</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  485</span><span class="stringliteral">        list (str)</span></div>
<div class="line"><span class="lineno">  486</span><span class="stringliteral">            List of words that are closer to `w1` than `w2` is to `w1`.</span></div>
<div class="line"><span class="lineno">  487</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  488</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  489</span>        <span class="keywordflow">return</span> super(WordEmbeddingsKeyedVectors, self).closer_than(w1, w2)</div>
<div class="line"><span class="lineno">  490</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a0a1303a60ea0c17948f70883aa6b6ef9" name="a0a1303a60ea0c17948f70883aa6b6ef9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0a1303a60ea0c17948f70883aa6b6ef9">&#9670;&#160;</a></span>wv()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wv </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  386</span>    <span class="keyword">def </span><a class="code hl_variable" href="__lapack__subroutines_8h.html#a74495572856ad48dce0855c1bd9f38a4">wv</a>(self):</div>
<div class="line"><span class="lineno">  387</span>        <span class="keywordflow">return</span> self</div>
<div class="line"><span class="lineno">  388</span> </div>
<div class="ttc" id="a__lapack__subroutines_8h_html_a74495572856ad48dce0855c1bd9f38a4"><div class="ttname"><a href="__lapack__subroutines_8h.html#a74495572856ad48dce0855c1bd9f38a4">wv</a></div><div class="ttdeci">void int int int int int npy_complex64 int int int npy_complex64 int int int npy_complex64 npy_complex64 int int npy_complex64 int int npy_complex64 * wv</div><div class="ttdef"><b>Definition</b> _lapack_subroutines.h:265</div></div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="a18d2c5d7f34d26e57c65670f837be0f5" name="a18d2c5d7f34d26e57c65670f837be0f5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a18d2c5d7f34d26e57c65670f837be0f5">&#9670;&#160;</a></span>index2word</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.index2word</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2e4dbd0e2afe642fa32ca0152232e029" name="a2e4dbd0e2afe642fa32ca0152232e029"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2e4dbd0e2afe642fa32ca0152232e029">&#9670;&#160;</a></span>vectors</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.vectors</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a9fdf5db4b2ebea45838cb4df96cf1fe9" name="a9fdf5db4b2ebea45838cb4df96cf1fe9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9fdf5db4b2ebea45838cb4df96cf1fe9">&#9670;&#160;</a></span>vectors_norm</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.vectors_norm</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ac96fcca29f06f59a54d65d3f395bac6f" name="ac96fcca29f06f59a54d65d3f395bac6f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac96fcca29f06f59a54d65d3f395bac6f">&#9670;&#160;</a></span>vocab</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.vocab</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>/Users/rafael/Documents/GitHub/PUC-GCES-PY/Tp-GCS-Rafael-Augusto/venv/lib/python3.9/site-packages/gensim/models/<a class="el" href="keyedvectors_8py.html">keyedvectors.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
