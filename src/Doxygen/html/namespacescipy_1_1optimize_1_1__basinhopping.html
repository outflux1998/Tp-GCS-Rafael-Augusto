<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: scipy.optimize._basinhopping Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacescipy.html">scipy</a></li><li class="navelem"><a class="el" href="namespacescipy_1_1optimize.html">optimize</a></li><li class="navelem"><a class="el" href="namespacescipy_1_1optimize_1_1__basinhopping.html">_basinhopping</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">scipy.optimize._basinhopping Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classscipy_1_1optimize_1_1__basinhopping_1_1_adaptive_stepsize.html">AdaptiveStepsize</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classscipy_1_1optimize_1_1__basinhopping_1_1_basin_hopping_runner.html">BasinHoppingRunner</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classscipy_1_1optimize_1_1__basinhopping_1_1_metropolis.html">Metropolis</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classscipy_1_1optimize_1_1__basinhopping_1_1_minimizer_wrapper.html">MinimizerWrapper</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classscipy_1_1optimize_1_1__basinhopping_1_1_random_displacement.html">RandomDisplacement</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classscipy_1_1optimize_1_1__basinhopping_1_1_storage.html">Storage</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:af9397510f8928f1b766f1323511bb6e6" id="r_af9397510f8928f1b766f1323511bb6e6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__basinhopping.html#af9397510f8928f1b766f1323511bb6e6">basinhopping</a> (<a class="el" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>, <a class="el" href="namespacescipy_1_1optimize_1_1__basinhopping.html#acaac440cada1a865aa697e918189e3ab">x0</a>, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None, seed=None, *target_accept_rate=0.5, stepwise_factor=0.9)</td></tr>
<tr class="separator:af9397510f8928f1b766f1323511bb6e6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a303bb59973990045c5ecb640c8c6bc29" id="r_a303bb59973990045c5ecb640c8c6bc29"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__basinhopping.html#a303bb59973990045c5ecb640c8c6bc29">_test_func2d_nograd</a> (<a class="el" href="namespacescipy_1_1optimize_1_1__nonlin.html#a9f63beec99dbf869db2b02f71b2dd31e">x</a>)</td></tr>
<tr class="separator:a303bb59973990045c5ecb640c8c6bc29"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a48f5af617800358fd0e57ec58cf75c61" id="r_a48f5af617800358fd0e57ec58cf75c61"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__basinhopping.html#a48f5af617800358fd0e57ec58cf75c61">_test_func2d</a> (<a class="el" href="namespacescipy_1_1optimize_1_1__nonlin.html#a9f63beec99dbf869db2b02f71b2dd31e">x</a>)</td></tr>
<tr class="separator:a48f5af617800358fd0e57ec58cf75c61"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a7880aca351e6c23c94195c1d38e8d6ed" id="r_a7880aca351e6c23c94195c1d38e8d6ed"><td class="memItemLeft" align="right" valign="top">dict&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__basinhopping.html#a7880aca351e6c23c94195c1d38e8d6ed">kwargs</a> = {&quot;method&quot;: &quot;L-<a class="el" href="classscipy_1_1optimize_1_1__hessian__update__strategy_1_1_b_f_g_s.html">BFGS</a>-B&quot;}</td></tr>
<tr class="separator:a7880aca351e6c23c94195c1d38e8d6ed"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acaac440cada1a865aa697e918189e3ab" id="r_acaac440cada1a865aa697e918189e3ab"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__basinhopping.html#acaac440cada1a865aa697e918189e3ab">x0</a> = np.array([1.0, 1.])</td></tr>
<tr class="separator:acaac440cada1a865aa697e918189e3ab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8da73ccef779576742d10fa46da14880" id="r_a8da73ccef779576742d10fa46da14880"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1optimize_1_1__basinhopping.html#a8da73ccef779576742d10fa46da14880">ret</a></td></tr>
<tr class="separator:a8da73ccef779576742d10fa46da14880"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">basinhopping: The basinhopping global optimization algorithm
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="a48f5af617800358fd0e57ec58cf75c61" name="a48f5af617800358fd0e57ec58cf75c61"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a48f5af617800358fd0e57ec58cf75c61">&#9670;&#160;</a></span>_test_func2d()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._basinhopping._test_func2d </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  770</span><span class="keyword">def </span>_test_func2d(x):</div>
<div class="line"><span class="lineno">  771</span>    f = (cos(14.5 * x[0] - 0.3) + (x[0] + 0.2) * x[0] + cos(14.5 * x[1] -</div>
<div class="line"><span class="lineno">  772</span>         0.3) + (x[1] + 0.2) * x[1] + x[0] * x[1] + 1.963879482144252)</div>
<div class="line"><span class="lineno">  773</span>    df = np.zeros(2)</div>
<div class="line"><span class="lineno">  774</span>    df[0] = -14.5 * sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2 + x[1]</div>
<div class="line"><span class="lineno">  775</span>    df[1] = -14.5 * sin(14.5 * x[1] - 0.3) + 2. * x[1] + 0.2 + x[0]</div>
<div class="line"><span class="lineno">  776</span>    <span class="keywordflow">return</span> f, df</div>
<div class="line"><span class="lineno">  777</span> </div>
<div class="line"><span class="lineno">  778</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a303bb59973990045c5ecb640c8c6bc29" name="a303bb59973990045c5ecb640c8c6bc29"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a303bb59973990045c5ecb640c8c6bc29">&#9670;&#160;</a></span>_test_func2d_nograd()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._basinhopping._test_func2d_nograd </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  764</span><span class="keyword">def </span>_test_func2d_nograd(x):</div>
<div class="line"><span class="lineno">  765</span>    f = (cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] + 0.2) * x[0]</div>
<div class="line"><span class="lineno">  766</span>         + 1.010876184442655)</div>
<div class="line"><span class="lineno">  767</span>    <span class="keywordflow">return</span> f</div>
<div class="line"><span class="lineno">  768</span> </div>
<div class="line"><span class="lineno">  769</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="af9397510f8928f1b766f1323511bb6e6" name="af9397510f8928f1b766f1323511bb6e6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af9397510f8928f1b766f1323511bb6e6">&#9670;&#160;</a></span>basinhopping()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._basinhopping.basinhopping </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>func</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x0</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>niter</em> = <code>100</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>T</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>stepsize</em> = <code>0.5</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>minimizer_kwargs</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>take_step</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>accept_test</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>callback</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>interval</em> = <code>50</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>disp</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>niter_success</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>seed</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>target_accept_rate</em> = <code>0.5</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>stepwise_factor</em> = <code>0.9</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Find the global minimum of a function using the basin-hopping algorithm.

Basin-hopping is a two-phase method that combines a global stepping
algorithm with local minimization at each step. Designed to mimic
the natural process of energy minimization of clusters of atoms, it works
well for similar problems with "funnel-like, but rugged" energy landscapes
[5]_.

As the step-taking, step acceptance, and minimization methods are all
customizable, this function can also be used to implement other two-phase
methods.

Parameters
----------
func : callable ``f(x, *args)``
    Function to be optimized.  ``args`` can be passed as an optional item
    in the dict ``minimizer_kwargs``
x0 : array_like
    Initial guess.
niter : integer, optional
    The number of basin-hopping iterations. There will be a total of
    ``niter + 1`` runs of the local minimizer.
T : float, optional
    The "temperature" parameter for the accept or reject criterion. Higher
    "temperatures" mean that larger jumps in function value will be
    accepted.  For best results ``T`` should be comparable to the
    separation (in function value) between local minima.
stepsize : float, optional
    Maximum step size for use in the random displacement.
minimizer_kwargs : dict, optional
    Extra keyword arguments to be passed to the local minimizer
    ``scipy.optimize.minimize()`` Some important options could be:

        method : str
            The minimization method (e.g. ``"L-BFGS-B"``)
        args : tuple
            Extra arguments passed to the objective function (``func``) and
            its derivatives (Jacobian, Hessian).

take_step : callable ``take_step(x)``, optional
    Replace the default step-taking routine with this routine. The default
    step-taking routine is a random displacement of the coordinates, but
    other step-taking algorithms may be better for some systems.
    ``take_step`` can optionally have the attribute ``take_step.stepsize``.
    If this attribute exists, then ``basinhopping`` will adjust
    ``take_step.stepsize`` in order to try to optimize the global minimum
    search.
accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional
    Define a test which will be used to judge whether or not to accept the
    step.  This will be used in addition to the Metropolis test based on
    "temperature" ``T``.  The acceptable return values are True,
    False, or ``"force accept"``. If any of the tests return False
    then the step is rejected. If the latter, then this will override any
    other tests in order to accept the step. This can be used, for example,
    to forcefully escape from a local minimum that ``basinhopping`` is
    trapped in.
callback : callable, ``callback(x, f, accept)``, optional
    A callback function which will be called for all minima found. ``x``
    and ``f`` are the coordinates and function value of the trial minimum,
    and ``accept`` is whether or not that minimum was accepted. This can
    be used, for example, to save the lowest N minima found. Also,
    ``callback`` can be used to specify a user defined stop criterion by
    optionally returning True to stop the ``basinhopping`` routine.
interval : integer, optional
    interval for how often to update the ``stepsize``
disp : bool, optional
    Set to True to print status messages
niter_success : integer, optional
    Stop the run if the global minimum candidate remains the same for this
    number of iterations.
seed : {None, int, `numpy.random.Generator`,
        `numpy.random.RandomState`}, optional

    If `seed` is None (or `np.random`), the `numpy.random.RandomState`
    singleton is used.
    If `seed` is an int, a new ``RandomState`` instance is used,
    seeded with `seed`.
    If `seed` is already a ``Generator`` or ``RandomState`` instance then
    that instance is used.
    Specify `seed` for repeatable minimizations. The random numbers
    generated with this seed only affect the default Metropolis
    `accept_test` and the default `take_step`. If you supply your own
    `take_step` and `accept_test`, and these functions use random
    number generation, then those functions are responsible for the state
    of their random number generator.
target_accept_rate : float, optional
    The target acceptance rate that is used to adjust the `stepsize`.
    If the current acceptance rate is greater than the target,
    then the `stepsize` is increased. Otherwise, it is decreased.
    Range is (0, 1). Default is 0.5.

    .. versionadded:: 1.8.0

stepwise_factor : float, optional
    The `stepsize` is multiplied or divided by this stepwise factor upon
    each update. Range is (0, 1). Default is 0.9.

    .. versionadded:: 1.8.0

Returns
-------
res : OptimizeResult
    The optimization result represented as a ``OptimizeResult`` object.
    Important attributes are: ``x`` the solution array, ``fun`` the value
    of the function at the solution, and ``message`` which describes the
    cause of the termination. The ``OptimizeResult`` object returned by the
    selected minimizer at the lowest minimum is also contained within this
    object and can be accessed through the ``lowest_optimization_result``
    attribute.  See `OptimizeResult` for a description of other attributes.

See Also
--------
minimize :
    The local minimization function called once for each basinhopping step.
    ``minimizer_kwargs`` is passed to this routine.

Notes
-----
Basin-hopping is a stochastic algorithm which attempts to find the global
minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_
[4]_. The algorithm in its current form was described by David Wales and
Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.

The algorithm is iterative with each cycle composed of the following
features

1) random perturbation of the coordinates

2) local minimization

3) accept or reject the new coordinates based on the minimized function
   value

The acceptance test used here is the Metropolis criterion of standard Monte
Carlo algorithms, although there are many other possibilities [3]_.

This global minimization method has been shown to be extremely efficient
for a wide variety of problems in physics and chemistry. It is
particularly useful when the function has many minima separated by large
barriers. See the Cambridge Cluster Database
http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems
that have been optimized primarily using basin-hopping. This database
includes minimization problems exceeding 300 degrees of freedom.

See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for
a Fortran implementation of basin-hopping. This implementation has many
different variations of the procedure described above, including more
advanced step taking algorithms and alternate acceptance criterion.

For stochastic global optimization there is no way to determine if the true
global minimum has actually been found. Instead, as a consistency check,
the algorithm can be run from a number of different random starting points
to ensure the lowest minimum found in each example has converged to the
global minimum. For this reason, ``basinhopping`` will by default simply
run for the number of iterations ``niter`` and return the lowest minimum
found. It is left to the user to ensure that this is in fact the global
minimum.

Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and
depends on the problem being solved. The step is chosen uniformly in the
region from x0-stepsize to x0+stepsize, in each dimension. Ideally, it
should be comparable to the typical separation (in argument values) between
local minima of the function being optimized. ``basinhopping`` will, by
default, adjust ``stepsize`` to find an optimal value, but this may take
many iterations. You will get quicker results if you set a sensible
initial value for ``stepsize``.

Choosing ``T``: The parameter ``T`` is the "temperature" used in the
Metropolis criterion. Basinhopping steps are always accepted if
``func(xnew) &lt; func(xold)``. Otherwise, they are accepted with
probability::

    exp( -(func(xnew) - func(xold)) / T )

So, for best results, ``T`` should to be comparable to the typical
difference (in function values) between local minima. (The height of
"walls" between local minima is irrelevant.)

If ``T`` is 0, the algorithm becomes Monotonic Basin-Hopping, in which all
steps that increase energy are rejected.

.. versionadded:: 0.12.0

References
----------
.. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,
    Cambridge, UK.
.. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and
    the Lowest Energy Structures of Lennard-Jones Clusters Containing up to
    110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.
.. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the
    multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,
    1987, 84, 6611.
.. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,
    crystals, and biomolecules, Science, 1999, 285, 1368.
.. [5] Olson, B., Hashmi, I., Molloy, K., and Shehu1, A., Basin Hopping as
    a General and Versatile Optimization Framework for the Characterization
    of Biological Macromolecules, Advances in Artificial Intelligence,
    Volume 2012 (2012), Article ID 674832, :doi:`10.1155/2012/674832`

Examples
--------
The following example is a 1-D minimization problem, with many
local minima superimposed on a parabola.

&gt;&gt;&gt; from scipy.optimize import basinhopping
&gt;&gt;&gt; func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x
&gt;&gt;&gt; x0=[1.]

Basinhopping, internally, uses a local minimization algorithm. We will use
the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to
use and how to set up that minimizer. This parameter will be passed to
``scipy.optimize.minimize()``.

&gt;&gt;&gt; minimizer_kwargs = {"method": "BFGS"}
&gt;&gt;&gt; ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,
...                    niter=200)
&gt;&gt;&gt; print("global minimum: x = %.4f, f(x) = %.4f" % (ret.x, ret.fun))
global minimum: x = -0.1951, f(x) = -1.0009

Next consider a 2-D minimization problem. Also, this time, we
will use gradient information to significantly speed up the search.

&gt;&gt;&gt; def func2d(x):
...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +
...                                                            0.2) * x[0]
...     df = np.zeros(2)
...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2
...     df[1] = 2. * x[1] + 0.2
...     return f, df

We'll also use a different local minimization algorithm. Also, we must tell
the minimizer that our function returns both energy and gradient (Jacobian).

&gt;&gt;&gt; minimizer_kwargs = {"method":"L-BFGS-B", "jac":True}
&gt;&gt;&gt; x0 = [1.0, 1.0]
&gt;&gt;&gt; ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,
...                    niter=200)
&gt;&gt;&gt; print("global minimum: x = [%.4f, %.4f], f(x) = %.4f" % (ret.x[0],
...                                                           ret.x[1],
...                                                           ret.fun))
global minimum: x = [-0.1951, -0.1000], f(x) = -1.0109


Here is an example using a custom step-taking routine. Imagine you want
the first coordinate to take larger steps than the rest of the coordinates.
This can be implemented like so:

&gt;&gt;&gt; class MyTakeStep:
...    def __init__(self, stepsize=0.5):
...        self.stepsize = stepsize
...        self.rng = np.random.default_rng()
...    def __call__(self, x):
...        s = self.stepsize
...        x[0] += self.rng.uniform(-2.*s, 2.*s)
...        x[1:] += self.rng.uniform(-s, s, x[1:].shape)
...        return x

Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude
of ``stepsize`` to optimize the search. We'll use the same 2-D function as
before

&gt;&gt;&gt; mytakestep = MyTakeStep()
&gt;&gt;&gt; ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,
...                    niter=200, take_step=mytakestep)
&gt;&gt;&gt; print("global minimum: x = [%.4f, %.4f], f(x) = %.4f" % (ret.x[0],
...                                                           ret.x[1],
...                                                           ret.fun))
global minimum: x = [-0.1951, -0.1000], f(x) = -1.0109


Now, let's do an example using a custom callback function which prints the
value of every minimum found

&gt;&gt;&gt; def print_fun(x, f, accepted):
...         print("at minimum %.4f accepted %d" % (f, int(accepted)))

We'll run it for only 10 basinhopping steps this time.

&gt;&gt;&gt; rng = np.random.default_rng()
&gt;&gt;&gt; ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,
...                    niter=10, callback=print_fun, seed=rng)
at minimum 0.4159 accepted 1
at minimum -0.4317 accepted 1
at minimum -1.0109 accepted 1
at minimum -0.9073 accepted 1
at minimum -0.4317 accepted 0
at minimum -0.1021 accepted 1
at minimum -0.7425 accepted 1
at minimum -0.9073 accepted 1
at minimum -0.4317 accepted 0
at minimum -0.7425 accepted 1
at minimum -0.9073 accepted 1


The minimum at -1.0109 is actually the global minimum, found already on the
8th iteration.

Now let's implement bounds on the problem using a custom ``accept_test``:

&gt;&gt;&gt; class MyBounds:
...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):
...         self.xmax = np.array(xmax)
...         self.xmin = np.array(xmin)
...     def __call__(self, **kwargs):
...         x = kwargs["x_new"]
...         tmax = bool(np.all(x &lt;= self.xmax))
...         tmin = bool(np.all(x &gt;= self.xmin))
...         return tmax and tmin

&gt;&gt;&gt; mybounds = MyBounds()
&gt;&gt;&gt; ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,
...                    niter=10, accept_test=mybounds)</pre> <div class="fragment"><div class="line"><span class="lineno">  354</span>                 seed=<span class="keywordtype">None</span>, *, target_accept_rate=0.5, stepwise_factor=0.9):</div>
<div class="line"><span class="lineno">  355</span>    <span class="stringliteral">&quot;&quot;&quot;Find the global minimum of a function using the basin-hopping algorithm.</span></div>
<div class="line"><span class="lineno">  356</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  357</span><span class="stringliteral">    Basin-hopping is a two-phase method that combines a global stepping</span></div>
<div class="line"><span class="lineno">  358</span><span class="stringliteral">    algorithm with local minimization at each step. Designed to mimic</span></div>
<div class="line"><span class="lineno">  359</span><span class="stringliteral">    the natural process of energy minimization of clusters of atoms, it works</span></div>
<div class="line"><span class="lineno">  360</span><span class="stringliteral">    well for similar problems with &quot;funnel-like, but rugged&quot; energy landscapes</span></div>
<div class="line"><span class="lineno">  361</span><span class="stringliteral">    [5]_.</span></div>
<div class="line"><span class="lineno">  362</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  363</span><span class="stringliteral">    As the step-taking, step acceptance, and minimization methods are all</span></div>
<div class="line"><span class="lineno">  364</span><span class="stringliteral">    customizable, this function can also be used to implement other two-phase</span></div>
<div class="line"><span class="lineno">  365</span><span class="stringliteral">    methods.</span></div>
<div class="line"><span class="lineno">  366</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  367</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  368</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  369</span><span class="stringliteral">    func : callable ``f(x, *args)``</span></div>
<div class="line"><span class="lineno">  370</span><span class="stringliteral">        Function to be optimized.  ``args`` can be passed as an optional item</span></div>
<div class="line"><span class="lineno">  371</span><span class="stringliteral">        in the dict ``minimizer_kwargs``</span></div>
<div class="line"><span class="lineno">  372</span><span class="stringliteral">    x0 : array_like</span></div>
<div class="line"><span class="lineno">  373</span><span class="stringliteral">        Initial guess.</span></div>
<div class="line"><span class="lineno">  374</span><span class="stringliteral">    niter : integer, optional</span></div>
<div class="line"><span class="lineno">  375</span><span class="stringliteral">        The number of basin-hopping iterations. There will be a total of</span></div>
<div class="line"><span class="lineno">  376</span><span class="stringliteral">        ``niter + 1`` runs of the local minimizer.</span></div>
<div class="line"><span class="lineno">  377</span><span class="stringliteral">    T : float, optional</span></div>
<div class="line"><span class="lineno">  378</span><span class="stringliteral">        The &quot;temperature&quot; parameter for the accept or reject criterion. Higher</span></div>
<div class="line"><span class="lineno">  379</span><span class="stringliteral">        &quot;temperatures&quot; mean that larger jumps in function value will be</span></div>
<div class="line"><span class="lineno">  380</span><span class="stringliteral">        accepted.  For best results ``T`` should be comparable to the</span></div>
<div class="line"><span class="lineno">  381</span><span class="stringliteral">        separation (in function value) between local minima.</span></div>
<div class="line"><span class="lineno">  382</span><span class="stringliteral">    stepsize : float, optional</span></div>
<div class="line"><span class="lineno">  383</span><span class="stringliteral">        Maximum step size for use in the random displacement.</span></div>
<div class="line"><span class="lineno">  384</span><span class="stringliteral">    minimizer_kwargs : dict, optional</span></div>
<div class="line"><span class="lineno">  385</span><span class="stringliteral">        Extra keyword arguments to be passed to the local minimizer</span></div>
<div class="line"><span class="lineno">  386</span><span class="stringliteral">        ``scipy.optimize.minimize()`` Some important options could be:</span></div>
<div class="line"><span class="lineno">  387</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  388</span><span class="stringliteral">            method : str</span></div>
<div class="line"><span class="lineno">  389</span><span class="stringliteral">                The minimization method (e.g. ``&quot;L-BFGS-B&quot;``)</span></div>
<div class="line"><span class="lineno">  390</span><span class="stringliteral">            args : tuple</span></div>
<div class="line"><span class="lineno">  391</span><span class="stringliteral">                Extra arguments passed to the objective function (``func``) and</span></div>
<div class="line"><span class="lineno">  392</span><span class="stringliteral">                its derivatives (Jacobian, Hessian).</span></div>
<div class="line"><span class="lineno">  393</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  394</span><span class="stringliteral">    take_step : callable ``take_step(x)``, optional</span></div>
<div class="line"><span class="lineno">  395</span><span class="stringliteral">        Replace the default step-taking routine with this routine. The default</span></div>
<div class="line"><span class="lineno">  396</span><span class="stringliteral">        step-taking routine is a random displacement of the coordinates, but</span></div>
<div class="line"><span class="lineno">  397</span><span class="stringliteral">        other step-taking algorithms may be better for some systems.</span></div>
<div class="line"><span class="lineno">  398</span><span class="stringliteral">        ``take_step`` can optionally have the attribute ``take_step.stepsize``.</span></div>
<div class="line"><span class="lineno">  399</span><span class="stringliteral">        If this attribute exists, then ``basinhopping`` will adjust</span></div>
<div class="line"><span class="lineno">  400</span><span class="stringliteral">        ``take_step.stepsize`` in order to try to optimize the global minimum</span></div>
<div class="line"><span class="lineno">  401</span><span class="stringliteral">        search.</span></div>
<div class="line"><span class="lineno">  402</span><span class="stringliteral">    accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional</span></div>
<div class="line"><span class="lineno">  403</span><span class="stringliteral">        Define a test which will be used to judge whether or not to accept the</span></div>
<div class="line"><span class="lineno">  404</span><span class="stringliteral">        step.  This will be used in addition to the Metropolis test based on</span></div>
<div class="line"><span class="lineno">  405</span><span class="stringliteral">        &quot;temperature&quot; ``T``.  The acceptable return values are True,</span></div>
<div class="line"><span class="lineno">  406</span><span class="stringliteral">        False, or ``&quot;force accept&quot;``. If any of the tests return False</span></div>
<div class="line"><span class="lineno">  407</span><span class="stringliteral">        then the step is rejected. If the latter, then this will override any</span></div>
<div class="line"><span class="lineno">  408</span><span class="stringliteral">        other tests in order to accept the step. This can be used, for example,</span></div>
<div class="line"><span class="lineno">  409</span><span class="stringliteral">        to forcefully escape from a local minimum that ``basinhopping`` is</span></div>
<div class="line"><span class="lineno">  410</span><span class="stringliteral">        trapped in.</span></div>
<div class="line"><span class="lineno">  411</span><span class="stringliteral">    callback : callable, ``callback(x, f, accept)``, optional</span></div>
<div class="line"><span class="lineno">  412</span><span class="stringliteral">        A callback function which will be called for all minima found. ``x``</span></div>
<div class="line"><span class="lineno">  413</span><span class="stringliteral">        and ``f`` are the coordinates and function value of the trial minimum,</span></div>
<div class="line"><span class="lineno">  414</span><span class="stringliteral">        and ``accept`` is whether or not that minimum was accepted. This can</span></div>
<div class="line"><span class="lineno">  415</span><span class="stringliteral">        be used, for example, to save the lowest N minima found. Also,</span></div>
<div class="line"><span class="lineno">  416</span><span class="stringliteral">        ``callback`` can be used to specify a user defined stop criterion by</span></div>
<div class="line"><span class="lineno">  417</span><span class="stringliteral">        optionally returning True to stop the ``basinhopping`` routine.</span></div>
<div class="line"><span class="lineno">  418</span><span class="stringliteral">    interval : integer, optional</span></div>
<div class="line"><span class="lineno">  419</span><span class="stringliteral">        interval for how often to update the ``stepsize``</span></div>
<div class="line"><span class="lineno">  420</span><span class="stringliteral">    disp : bool, optional</span></div>
<div class="line"><span class="lineno">  421</span><span class="stringliteral">        Set to True to print status messages</span></div>
<div class="line"><span class="lineno">  422</span><span class="stringliteral">    niter_success : integer, optional</span></div>
<div class="line"><span class="lineno">  423</span><span class="stringliteral">        Stop the run if the global minimum candidate remains the same for this</span></div>
<div class="line"><span class="lineno">  424</span><span class="stringliteral">        number of iterations.</span></div>
<div class="line"><span class="lineno">  425</span><span class="stringliteral">    seed : {None, int, `numpy.random.Generator`,</span></div>
<div class="line"><span class="lineno">  426</span><span class="stringliteral">            `numpy.random.RandomState`}, optional</span></div>
<div class="line"><span class="lineno">  427</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  428</span><span class="stringliteral">        If `seed` is None (or `np.random`), the `numpy.random.RandomState`</span></div>
<div class="line"><span class="lineno">  429</span><span class="stringliteral">        singleton is used.</span></div>
<div class="line"><span class="lineno">  430</span><span class="stringliteral">        If `seed` is an int, a new ``RandomState`` instance is used,</span></div>
<div class="line"><span class="lineno">  431</span><span class="stringliteral">        seeded with `seed`.</span></div>
<div class="line"><span class="lineno">  432</span><span class="stringliteral">        If `seed` is already a ``Generator`` or ``RandomState`` instance then</span></div>
<div class="line"><span class="lineno">  433</span><span class="stringliteral">        that instance is used.</span></div>
<div class="line"><span class="lineno">  434</span><span class="stringliteral">        Specify `seed` for repeatable minimizations. The random numbers</span></div>
<div class="line"><span class="lineno">  435</span><span class="stringliteral">        generated with this seed only affect the default Metropolis</span></div>
<div class="line"><span class="lineno">  436</span><span class="stringliteral">        `accept_test` and the default `take_step`. If you supply your own</span></div>
<div class="line"><span class="lineno">  437</span><span class="stringliteral">        `take_step` and `accept_test`, and these functions use random</span></div>
<div class="line"><span class="lineno">  438</span><span class="stringliteral">        number generation, then those functions are responsible for the state</span></div>
<div class="line"><span class="lineno">  439</span><span class="stringliteral">        of their random number generator.</span></div>
<div class="line"><span class="lineno">  440</span><span class="stringliteral">    target_accept_rate : float, optional</span></div>
<div class="line"><span class="lineno">  441</span><span class="stringliteral">        The target acceptance rate that is used to adjust the `stepsize`.</span></div>
<div class="line"><span class="lineno">  442</span><span class="stringliteral">        If the current acceptance rate is greater than the target,</span></div>
<div class="line"><span class="lineno">  443</span><span class="stringliteral">        then the `stepsize` is increased. Otherwise, it is decreased.</span></div>
<div class="line"><span class="lineno">  444</span><span class="stringliteral">        Range is (0, 1). Default is 0.5.</span></div>
<div class="line"><span class="lineno">  445</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  446</span><span class="stringliteral">        .. versionadded:: 1.8.0</span></div>
<div class="line"><span class="lineno">  447</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  448</span><span class="stringliteral">    stepwise_factor : float, optional</span></div>
<div class="line"><span class="lineno">  449</span><span class="stringliteral">        The `stepsize` is multiplied or divided by this stepwise factor upon</span></div>
<div class="line"><span class="lineno">  450</span><span class="stringliteral">        each update. Range is (0, 1). Default is 0.9.</span></div>
<div class="line"><span class="lineno">  451</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  452</span><span class="stringliteral">        .. versionadded:: 1.8.0</span></div>
<div class="line"><span class="lineno">  453</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  454</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  455</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  456</span><span class="stringliteral">    res : OptimizeResult</span></div>
<div class="line"><span class="lineno">  457</span><span class="stringliteral">        The optimization result represented as a ``OptimizeResult`` object.</span></div>
<div class="line"><span class="lineno">  458</span><span class="stringliteral">        Important attributes are: ``x`` the solution array, ``fun`` the value</span></div>
<div class="line"><span class="lineno">  459</span><span class="stringliteral">        of the function at the solution, and ``message`` which describes the</span></div>
<div class="line"><span class="lineno">  460</span><span class="stringliteral">        cause of the termination. The ``OptimizeResult`` object returned by the</span></div>
<div class="line"><span class="lineno">  461</span><span class="stringliteral">        selected minimizer at the lowest minimum is also contained within this</span></div>
<div class="line"><span class="lineno">  462</span><span class="stringliteral">        object and can be accessed through the ``lowest_optimization_result``</span></div>
<div class="line"><span class="lineno">  463</span><span class="stringliteral">        attribute.  See `OptimizeResult` for a description of other attributes.</span></div>
<div class="line"><span class="lineno">  464</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  465</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  466</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  467</span><span class="stringliteral">    minimize :</span></div>
<div class="line"><span class="lineno">  468</span><span class="stringliteral">        The local minimization function called once for each basinhopping step.</span></div>
<div class="line"><span class="lineno">  469</span><span class="stringliteral">        ``minimizer_kwargs`` is passed to this routine.</span></div>
<div class="line"><span class="lineno">  470</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  471</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  472</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  473</span><span class="stringliteral">    Basin-hopping is a stochastic algorithm which attempts to find the global</span></div>
<div class="line"><span class="lineno">  474</span><span class="stringliteral">    minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_</span></div>
<div class="line"><span class="lineno">  475</span><span class="stringliteral">    [4]_. The algorithm in its current form was described by David Wales and</span></div>
<div class="line"><span class="lineno">  476</span><span class="stringliteral">    Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.</span></div>
<div class="line"><span class="lineno">  477</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  478</span><span class="stringliteral">    The algorithm is iterative with each cycle composed of the following</span></div>
<div class="line"><span class="lineno">  479</span><span class="stringliteral">    features</span></div>
<div class="line"><span class="lineno">  480</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  481</span><span class="stringliteral">    1) random perturbation of the coordinates</span></div>
<div class="line"><span class="lineno">  482</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  483</span><span class="stringliteral">    2) local minimization</span></div>
<div class="line"><span class="lineno">  484</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  485</span><span class="stringliteral">    3) accept or reject the new coordinates based on the minimized function</span></div>
<div class="line"><span class="lineno">  486</span><span class="stringliteral">       value</span></div>
<div class="line"><span class="lineno">  487</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  488</span><span class="stringliteral">    The acceptance test used here is the Metropolis criterion of standard Monte</span></div>
<div class="line"><span class="lineno">  489</span><span class="stringliteral">    Carlo algorithms, although there are many other possibilities [3]_.</span></div>
<div class="line"><span class="lineno">  490</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  491</span><span class="stringliteral">    This global minimization method has been shown to be extremely efficient</span></div>
<div class="line"><span class="lineno">  492</span><span class="stringliteral">    for a wide variety of problems in physics and chemistry. It is</span></div>
<div class="line"><span class="lineno">  493</span><span class="stringliteral">    particularly useful when the function has many minima separated by large</span></div>
<div class="line"><span class="lineno">  494</span><span class="stringliteral">    barriers. See the Cambridge Cluster Database</span></div>
<div class="line"><span class="lineno">  495</span><span class="stringliteral">    http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems</span></div>
<div class="line"><span class="lineno">  496</span><span class="stringliteral">    that have been optimized primarily using basin-hopping. This database</span></div>
<div class="line"><span class="lineno">  497</span><span class="stringliteral">    includes minimization problems exceeding 300 degrees of freedom.</span></div>
<div class="line"><span class="lineno">  498</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  499</span><span class="stringliteral">    See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for</span></div>
<div class="line"><span class="lineno">  500</span><span class="stringliteral">    a Fortran implementation of basin-hopping. This implementation has many</span></div>
<div class="line"><span class="lineno">  501</span><span class="stringliteral">    different variations of the procedure described above, including more</span></div>
<div class="line"><span class="lineno">  502</span><span class="stringliteral">    advanced step taking algorithms and alternate acceptance criterion.</span></div>
<div class="line"><span class="lineno">  503</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  504</span><span class="stringliteral">    For stochastic global optimization there is no way to determine if the true</span></div>
<div class="line"><span class="lineno">  505</span><span class="stringliteral">    global minimum has actually been found. Instead, as a consistency check,</span></div>
<div class="line"><span class="lineno">  506</span><span class="stringliteral">    the algorithm can be run from a number of different random starting points</span></div>
<div class="line"><span class="lineno">  507</span><span class="stringliteral">    to ensure the lowest minimum found in each example has converged to the</span></div>
<div class="line"><span class="lineno">  508</span><span class="stringliteral">    global minimum. For this reason, ``basinhopping`` will by default simply</span></div>
<div class="line"><span class="lineno">  509</span><span class="stringliteral">    run for the number of iterations ``niter`` and return the lowest minimum</span></div>
<div class="line"><span class="lineno">  510</span><span class="stringliteral">    found. It is left to the user to ensure that this is in fact the global</span></div>
<div class="line"><span class="lineno">  511</span><span class="stringliteral">    minimum.</span></div>
<div class="line"><span class="lineno">  512</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  513</span><span class="stringliteral">    Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and</span></div>
<div class="line"><span class="lineno">  514</span><span class="stringliteral">    depends on the problem being solved. The step is chosen uniformly in the</span></div>
<div class="line"><span class="lineno">  515</span><span class="stringliteral">    region from x0-stepsize to x0+stepsize, in each dimension. Ideally, it</span></div>
<div class="line"><span class="lineno">  516</span><span class="stringliteral">    should be comparable to the typical separation (in argument values) between</span></div>
<div class="line"><span class="lineno">  517</span><span class="stringliteral">    local minima of the function being optimized. ``basinhopping`` will, by</span></div>
<div class="line"><span class="lineno">  518</span><span class="stringliteral">    default, adjust ``stepsize`` to find an optimal value, but this may take</span></div>
<div class="line"><span class="lineno">  519</span><span class="stringliteral">    many iterations. You will get quicker results if you set a sensible</span></div>
<div class="line"><span class="lineno">  520</span><span class="stringliteral">    initial value for ``stepsize``.</span></div>
<div class="line"><span class="lineno">  521</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  522</span><span class="stringliteral">    Choosing ``T``: The parameter ``T`` is the &quot;temperature&quot; used in the</span></div>
<div class="line"><span class="lineno">  523</span><span class="stringliteral">    Metropolis criterion. Basinhopping steps are always accepted if</span></div>
<div class="line"><span class="lineno">  524</span><span class="stringliteral">    ``func(xnew) &lt; func(xold)``. Otherwise, they are accepted with</span></div>
<div class="line"><span class="lineno">  525</span><span class="stringliteral">    probability::</span></div>
<div class="line"><span class="lineno">  526</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  527</span><span class="stringliteral">        exp( -(func(xnew) - func(xold)) / T )</span></div>
<div class="line"><span class="lineno">  528</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  529</span><span class="stringliteral">    So, for best results, ``T`` should to be comparable to the typical</span></div>
<div class="line"><span class="lineno">  530</span><span class="stringliteral">    difference (in function values) between local minima. (The height of</span></div>
<div class="line"><span class="lineno">  531</span><span class="stringliteral">    &quot;walls&quot; between local minima is irrelevant.)</span></div>
<div class="line"><span class="lineno">  532</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  533</span><span class="stringliteral">    If ``T`` is 0, the algorithm becomes Monotonic Basin-Hopping, in which all</span></div>
<div class="line"><span class="lineno">  534</span><span class="stringliteral">    steps that increase energy are rejected.</span></div>
<div class="line"><span class="lineno">  535</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  536</span><span class="stringliteral">    .. versionadded:: 0.12.0</span></div>
<div class="line"><span class="lineno">  537</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  538</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  539</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  540</span><span class="stringliteral">    .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,</span></div>
<div class="line"><span class="lineno">  541</span><span class="stringliteral">        Cambridge, UK.</span></div>
<div class="line"><span class="lineno">  542</span><span class="stringliteral">    .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and</span></div>
<div class="line"><span class="lineno">  543</span><span class="stringliteral">        the Lowest Energy Structures of Lennard-Jones Clusters Containing up to</span></div>
<div class="line"><span class="lineno">  544</span><span class="stringliteral">        110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.</span></div>
<div class="line"><span class="lineno">  545</span><span class="stringliteral">    .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the</span></div>
<div class="line"><span class="lineno">  546</span><span class="stringliteral">        multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,</span></div>
<div class="line"><span class="lineno">  547</span><span class="stringliteral">        1987, 84, 6611.</span></div>
<div class="line"><span class="lineno">  548</span><span class="stringliteral">    .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,</span></div>
<div class="line"><span class="lineno">  549</span><span class="stringliteral">        crystals, and biomolecules, Science, 1999, 285, 1368.</span></div>
<div class="line"><span class="lineno">  550</span><span class="stringliteral">    .. [5] Olson, B., Hashmi, I., Molloy, K., and Shehu1, A., Basin Hopping as</span></div>
<div class="line"><span class="lineno">  551</span><span class="stringliteral">        a General and Versatile Optimization Framework for the Characterization</span></div>
<div class="line"><span class="lineno">  552</span><span class="stringliteral">        of Biological Macromolecules, Advances in Artificial Intelligence,</span></div>
<div class="line"><span class="lineno">  553</span><span class="stringliteral">        Volume 2012 (2012), Article ID 674832, :doi:`10.1155/2012/674832`</span></div>
<div class="line"><span class="lineno">  554</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  555</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno">  556</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  557</span><span class="stringliteral">    The following example is a 1-D minimization problem, with many</span></div>
<div class="line"><span class="lineno">  558</span><span class="stringliteral">    local minima superimposed on a parabola.</span></div>
<div class="line"><span class="lineno">  559</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  560</span><span class="stringliteral">    &gt;&gt;&gt; from scipy.optimize import basinhopping</span></div>
<div class="line"><span class="lineno">  561</span><span class="stringliteral">    &gt;&gt;&gt; func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x</span></div>
<div class="line"><span class="lineno">  562</span><span class="stringliteral">    &gt;&gt;&gt; x0=[1.]</span></div>
<div class="line"><span class="lineno">  563</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  564</span><span class="stringliteral">    Basinhopping, internally, uses a local minimization algorithm. We will use</span></div>
<div class="line"><span class="lineno">  565</span><span class="stringliteral">    the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to</span></div>
<div class="line"><span class="lineno">  566</span><span class="stringliteral">    use and how to set up that minimizer. This parameter will be passed to</span></div>
<div class="line"><span class="lineno">  567</span><span class="stringliteral">    ``scipy.optimize.minimize()``.</span></div>
<div class="line"><span class="lineno">  568</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  569</span><span class="stringliteral">    &gt;&gt;&gt; minimizer_kwargs = {&quot;method&quot;: &quot;BFGS&quot;}</span></div>
<div class="line"><span class="lineno">  570</span><span class="stringliteral">    &gt;&gt;&gt; ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,</span></div>
<div class="line"><span class="lineno">  571</span><span class="stringliteral">    ...                    niter=200)</span></div>
<div class="line"><span class="lineno">  572</span><span class="stringliteral">    &gt;&gt;&gt; print(&quot;global minimum: x = %.4f, f(x) = %.4f&quot; % (ret.x, ret.fun))</span></div>
<div class="line"><span class="lineno">  573</span><span class="stringliteral">    global minimum: x = -0.1951, f(x) = -1.0009</span></div>
<div class="line"><span class="lineno">  574</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  575</span><span class="stringliteral">    Next consider a 2-D minimization problem. Also, this time, we</span></div>
<div class="line"><span class="lineno">  576</span><span class="stringliteral">    will use gradient information to significantly speed up the search.</span></div>
<div class="line"><span class="lineno">  577</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  578</span><span class="stringliteral">    &gt;&gt;&gt; def func2d(x):</span></div>
<div class="line"><span class="lineno">  579</span><span class="stringliteral">    ...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +</span></div>
<div class="line"><span class="lineno">  580</span><span class="stringliteral">    ...                                                            0.2) * x[0]</span></div>
<div class="line"><span class="lineno">  581</span><span class="stringliteral">    ...     df = np.zeros(2)</span></div>
<div class="line"><span class="lineno">  582</span><span class="stringliteral">    ...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2</span></div>
<div class="line"><span class="lineno">  583</span><span class="stringliteral">    ...     df[1] = 2. * x[1] + 0.2</span></div>
<div class="line"><span class="lineno">  584</span><span class="stringliteral">    ...     return f, df</span></div>
<div class="line"><span class="lineno">  585</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  586</span><span class="stringliteral">    We&#39;ll also use a different local minimization algorithm. Also, we must tell</span></div>
<div class="line"><span class="lineno">  587</span><span class="stringliteral">    the minimizer that our function returns both energy and gradient (Jacobian).</span></div>
<div class="line"><span class="lineno">  588</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  589</span><span class="stringliteral">    &gt;&gt;&gt; minimizer_kwargs = {&quot;method&quot;:&quot;L-BFGS-B&quot;, &quot;jac&quot;:True}</span></div>
<div class="line"><span class="lineno">  590</span><span class="stringliteral">    &gt;&gt;&gt; x0 = [1.0, 1.0]</span></div>
<div class="line"><span class="lineno">  591</span><span class="stringliteral">    &gt;&gt;&gt; ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,</span></div>
<div class="line"><span class="lineno">  592</span><span class="stringliteral">    ...                    niter=200)</span></div>
<div class="line"><span class="lineno">  593</span><span class="stringliteral">    &gt;&gt;&gt; print(&quot;global minimum: x = [%.4f, %.4f], f(x) = %.4f&quot; % (ret.x[0],</span></div>
<div class="line"><span class="lineno">  594</span><span class="stringliteral">    ...                                                           ret.x[1],</span></div>
<div class="line"><span class="lineno">  595</span><span class="stringliteral">    ...                                                           ret.fun))</span></div>
<div class="line"><span class="lineno">  596</span><span class="stringliteral">    global minimum: x = [-0.1951, -0.1000], f(x) = -1.0109</span></div>
<div class="line"><span class="lineno">  597</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  598</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  599</span><span class="stringliteral">    Here is an example using a custom step-taking routine. Imagine you want</span></div>
<div class="line"><span class="lineno">  600</span><span class="stringliteral">    the first coordinate to take larger steps than the rest of the coordinates.</span></div>
<div class="line"><span class="lineno">  601</span><span class="stringliteral">    This can be implemented like so:</span></div>
<div class="line"><span class="lineno">  602</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  603</span><span class="stringliteral">    &gt;&gt;&gt; class MyTakeStep:</span></div>
<div class="line"><span class="lineno">  604</span><span class="stringliteral">    ...    def __init__(self, stepsize=0.5):</span></div>
<div class="line"><span class="lineno">  605</span><span class="stringliteral">    ...        self.stepsize = stepsize</span></div>
<div class="line"><span class="lineno">  606</span><span class="stringliteral">    ...        self.rng = np.random.default_rng()</span></div>
<div class="line"><span class="lineno">  607</span><span class="stringliteral">    ...    def __call__(self, x):</span></div>
<div class="line"><span class="lineno">  608</span><span class="stringliteral">    ...        s = self.stepsize</span></div>
<div class="line"><span class="lineno">  609</span><span class="stringliteral">    ...        x[0] += self.rng.uniform(-2.*s, 2.*s)</span></div>
<div class="line"><span class="lineno">  610</span><span class="stringliteral">    ...        x[1:] += self.rng.uniform(-s, s, x[1:].shape)</span></div>
<div class="line"><span class="lineno">  611</span><span class="stringliteral">    ...        return x</span></div>
<div class="line"><span class="lineno">  612</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  613</span><span class="stringliteral">    Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude</span></div>
<div class="line"><span class="lineno">  614</span><span class="stringliteral">    of ``stepsize`` to optimize the search. We&#39;ll use the same 2-D function as</span></div>
<div class="line"><span class="lineno">  615</span><span class="stringliteral">    before</span></div>
<div class="line"><span class="lineno">  616</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  617</span><span class="stringliteral">    &gt;&gt;&gt; mytakestep = MyTakeStep()</span></div>
<div class="line"><span class="lineno">  618</span><span class="stringliteral">    &gt;&gt;&gt; ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,</span></div>
<div class="line"><span class="lineno">  619</span><span class="stringliteral">    ...                    niter=200, take_step=mytakestep)</span></div>
<div class="line"><span class="lineno">  620</span><span class="stringliteral">    &gt;&gt;&gt; print(&quot;global minimum: x = [%.4f, %.4f], f(x) = %.4f&quot; % (ret.x[0],</span></div>
<div class="line"><span class="lineno">  621</span><span class="stringliteral">    ...                                                           ret.x[1],</span></div>
<div class="line"><span class="lineno">  622</span><span class="stringliteral">    ...                                                           ret.fun))</span></div>
<div class="line"><span class="lineno">  623</span><span class="stringliteral">    global minimum: x = [-0.1951, -0.1000], f(x) = -1.0109</span></div>
<div class="line"><span class="lineno">  624</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  625</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  626</span><span class="stringliteral">    Now, let&#39;s do an example using a custom callback function which prints the</span></div>
<div class="line"><span class="lineno">  627</span><span class="stringliteral">    value of every minimum found</span></div>
<div class="line"><span class="lineno">  628</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  629</span><span class="stringliteral">    &gt;&gt;&gt; def print_fun(x, f, accepted):</span></div>
<div class="line"><span class="lineno">  630</span><span class="stringliteral">    ...         print(&quot;at minimum %.4f accepted %d&quot; % (f, int(accepted)))</span></div>
<div class="line"><span class="lineno">  631</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  632</span><span class="stringliteral">    We&#39;ll run it for only 10 basinhopping steps this time.</span></div>
<div class="line"><span class="lineno">  633</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  634</span><span class="stringliteral">    &gt;&gt;&gt; rng = np.random.default_rng()</span></div>
<div class="line"><span class="lineno">  635</span><span class="stringliteral">    &gt;&gt;&gt; ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,</span></div>
<div class="line"><span class="lineno">  636</span><span class="stringliteral">    ...                    niter=10, callback=print_fun, seed=rng)</span></div>
<div class="line"><span class="lineno">  637</span><span class="stringliteral">    at minimum 0.4159 accepted 1</span></div>
<div class="line"><span class="lineno">  638</span><span class="stringliteral">    at minimum -0.4317 accepted 1</span></div>
<div class="line"><span class="lineno">  639</span><span class="stringliteral">    at minimum -1.0109 accepted 1</span></div>
<div class="line"><span class="lineno">  640</span><span class="stringliteral">    at minimum -0.9073 accepted 1</span></div>
<div class="line"><span class="lineno">  641</span><span class="stringliteral">    at minimum -0.4317 accepted 0</span></div>
<div class="line"><span class="lineno">  642</span><span class="stringliteral">    at minimum -0.1021 accepted 1</span></div>
<div class="line"><span class="lineno">  643</span><span class="stringliteral">    at minimum -0.7425 accepted 1</span></div>
<div class="line"><span class="lineno">  644</span><span class="stringliteral">    at minimum -0.9073 accepted 1</span></div>
<div class="line"><span class="lineno">  645</span><span class="stringliteral">    at minimum -0.4317 accepted 0</span></div>
<div class="line"><span class="lineno">  646</span><span class="stringliteral">    at minimum -0.7425 accepted 1</span></div>
<div class="line"><span class="lineno">  647</span><span class="stringliteral">    at minimum -0.9073 accepted 1</span></div>
<div class="line"><span class="lineno">  648</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  649</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  650</span><span class="stringliteral">    The minimum at -1.0109 is actually the global minimum, found already on the</span></div>
<div class="line"><span class="lineno">  651</span><span class="stringliteral">    8th iteration.</span></div>
<div class="line"><span class="lineno">  652</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  653</span><span class="stringliteral">    Now let&#39;s implement bounds on the problem using a custom ``accept_test``:</span></div>
<div class="line"><span class="lineno">  654</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  655</span><span class="stringliteral">    &gt;&gt;&gt; class MyBounds:</span></div>
<div class="line"><span class="lineno">  656</span><span class="stringliteral">    ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):</span></div>
<div class="line"><span class="lineno">  657</span><span class="stringliteral">    ...         self.xmax = np.array(xmax)</span></div>
<div class="line"><span class="lineno">  658</span><span class="stringliteral">    ...         self.xmin = np.array(xmin)</span></div>
<div class="line"><span class="lineno">  659</span><span class="stringliteral">    ...     def __call__(self, **kwargs):</span></div>
<div class="line"><span class="lineno">  660</span><span class="stringliteral">    ...         x = kwargs[&quot;x_new&quot;]</span></div>
<div class="line"><span class="lineno">  661</span><span class="stringliteral">    ...         tmax = bool(np.all(x &lt;= self.xmax))</span></div>
<div class="line"><span class="lineno">  662</span><span class="stringliteral">    ...         tmin = bool(np.all(x &gt;= self.xmin))</span></div>
<div class="line"><span class="lineno">  663</span><span class="stringliteral">    ...         return tmax and tmin</span></div>
<div class="line"><span class="lineno">  664</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  665</span><span class="stringliteral">    &gt;&gt;&gt; mybounds = MyBounds()</span></div>
<div class="line"><span class="lineno">  666</span><span class="stringliteral">    &gt;&gt;&gt; ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,</span></div>
<div class="line"><span class="lineno">  667</span><span class="stringliteral">    ...                    niter=10, accept_test=mybounds)</span></div>
<div class="line"><span class="lineno">  668</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  669</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  670</span>    <span class="keywordflow">if</span> target_accept_rate &lt;= 0. <span class="keywordflow">or</span> target_accept_rate &gt;= 1.:</div>
<div class="line"><span class="lineno">  671</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;target_accept_rate has to be in range (0, 1)&#39;</span>)</div>
<div class="line"><span class="lineno">  672</span>    <span class="keywordflow">if</span> stepwise_factor &lt;= 0. <span class="keywordflow">or</span> stepwise_factor &gt;= 1.:</div>
<div class="line"><span class="lineno">  673</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;stepwise_factor has to be in range (0, 1)&#39;</span>)</div>
<div class="line"><span class="lineno">  674</span> </div>
<div class="line"><span class="lineno">  675</span>    x0 = np.array(x0)</div>
<div class="line"><span class="lineno">  676</span> </div>
<div class="line"><span class="lineno">  677</span>    <span class="comment"># set up the np.random generator</span></div>
<div class="line"><span class="lineno">  678</span>    rng = check_random_state(seed)</div>
<div class="line"><span class="lineno">  679</span> </div>
<div class="line"><span class="lineno">  680</span>    <span class="comment"># set up minimizer</span></div>
<div class="line"><span class="lineno">  681</span>    <span class="keywordflow">if</span> minimizer_kwargs <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  682</span>        minimizer_kwargs = dict()</div>
<div class="line"><span class="lineno">  683</span>    wrapped_minimizer = MinimizerWrapper(scipy.optimize.minimize, func,</div>
<div class="line"><span class="lineno">  684</span>                                         **minimizer_kwargs)</div>
<div class="line"><span class="lineno">  685</span> </div>
<div class="line"><span class="lineno">  686</span>    <span class="comment"># set up step-taking algorithm</span></div>
<div class="line"><span class="lineno">  687</span>    <span class="keywordflow">if</span> take_step <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  688</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> callable(take_step):</div>
<div class="line"><span class="lineno">  689</span>            <span class="keywordflow">raise</span> TypeError(<span class="stringliteral">&quot;take_step must be callable&quot;</span>)</div>
<div class="line"><span class="lineno">  690</span>        <span class="comment"># if take_step.stepsize exists then use AdaptiveStepsize to control</span></div>
<div class="line"><span class="lineno">  691</span>        <span class="comment"># take_step.stepsize</span></div>
<div class="line"><span class="lineno">  692</span>        <span class="keywordflow">if</span> hasattr(take_step, <span class="stringliteral">&quot;stepsize&quot;</span>):</div>
<div class="line"><span class="lineno">  693</span>            take_step_wrapped = AdaptiveStepsize(</div>
<div class="line"><span class="lineno">  694</span>                take_step, interval=interval,</div>
<div class="line"><span class="lineno">  695</span>                accept_rate=target_accept_rate,</div>
<div class="line"><span class="lineno">  696</span>                factor=stepwise_factor,</div>
<div class="line"><span class="lineno">  697</span>                verbose=disp)</div>
<div class="line"><span class="lineno">  698</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  699</span>            take_step_wrapped = take_step</div>
<div class="line"><span class="lineno">  700</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  701</span>        <span class="comment"># use default</span></div>
<div class="line"><span class="lineno">  702</span>        displace = RandomDisplacement(stepsize=stepsize, random_gen=rng)</div>
<div class="line"><span class="lineno">  703</span>        take_step_wrapped = AdaptiveStepsize(displace, interval=interval,</div>
<div class="line"><span class="lineno">  704</span>                                             accept_rate=target_accept_rate,</div>
<div class="line"><span class="lineno">  705</span>                                             factor=stepwise_factor,</div>
<div class="line"><span class="lineno">  706</span>                                             verbose=disp)</div>
<div class="line"><span class="lineno">  707</span> </div>
<div class="line"><span class="lineno">  708</span>    <span class="comment"># set up accept tests</span></div>
<div class="line"><span class="lineno">  709</span>    accept_tests = []</div>
<div class="line"><span class="lineno">  710</span>    <span class="keywordflow">if</span> accept_test <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  711</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> callable(accept_test):</div>
<div class="line"><span class="lineno">  712</span>            <span class="keywordflow">raise</span> TypeError(<span class="stringliteral">&quot;accept_test must be callable&quot;</span>)</div>
<div class="line"><span class="lineno">  713</span>        accept_tests = [accept_test]</div>
<div class="line"><span class="lineno">  714</span> </div>
<div class="line"><span class="lineno">  715</span>    <span class="comment"># use default</span></div>
<div class="line"><span class="lineno">  716</span>    metropolis = Metropolis(T, random_gen=rng)</div>
<div class="line"><span class="lineno">  717</span>    accept_tests.append(metropolis)</div>
<div class="line"><span class="lineno">  718</span> </div>
<div class="line"><span class="lineno">  719</span>    <span class="keywordflow">if</span> niter_success <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  720</span>        niter_success = niter + 2</div>
<div class="line"><span class="lineno">  721</span> </div>
<div class="line"><span class="lineno">  722</span>    bh = BasinHoppingRunner(x0, wrapped_minimizer, take_step_wrapped,</div>
<div class="line"><span class="lineno">  723</span>                            accept_tests, disp=disp)</div>
<div class="line"><span class="lineno">  724</span> </div>
<div class="line"><span class="lineno">  725</span>    <span class="comment"># The wrapped minimizer is called once during construction of</span></div>
<div class="line"><span class="lineno">  726</span>    <span class="comment"># BasinHoppingRunner, so run the callback</span></div>
<div class="line"><span class="lineno">  727</span>    <span class="keywordflow">if</span> callable(callback):</div>
<div class="line"><span class="lineno">  728</span>        callback(bh.storage.minres.x, bh.storage.minres.fun, <span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  729</span> </div>
<div class="line"><span class="lineno">  730</span>    <span class="comment"># start main iteration loop</span></div>
<div class="line"><span class="lineno">  731</span>    count, i = 0, 0</div>
<div class="line"><span class="lineno">  732</span>    message = [<span class="stringliteral">&quot;requested number of basinhopping iterations completed&quot;</span></div>
<div class="line"><span class="lineno">  733</span>               <span class="stringliteral">&quot; successfully&quot;</span>]</div>
<div class="line"><span class="lineno">  734</span>    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(niter):</div>
<div class="line"><span class="lineno">  735</span>        new_global_min = bh.one_cycle()</div>
<div class="line"><span class="lineno">  736</span> </div>
<div class="line"><span class="lineno">  737</span>        <span class="keywordflow">if</span> callable(callback):</div>
<div class="line"><span class="lineno">  738</span>            <span class="comment"># should we pass a copy of x?</span></div>
<div class="line"><span class="lineno">  739</span>            val = callback(bh.xtrial, bh.energy_trial, bh.accept)</div>
<div class="line"><span class="lineno">  740</span>            <span class="keywordflow">if</span> val <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  741</span>                <span class="keywordflow">if</span> val:</div>
<div class="line"><span class="lineno">  742</span>                    message = [<span class="stringliteral">&quot;callback function requested stop early by&quot;</span></div>
<div class="line"><span class="lineno">  743</span>                               <span class="stringliteral">&quot;returning True&quot;</span>]</div>
<div class="line"><span class="lineno">  744</span>                    <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  745</span> </div>
<div class="line"><span class="lineno">  746</span>        count += 1</div>
<div class="line"><span class="lineno">  747</span>        <span class="keywordflow">if</span> new_global_min:</div>
<div class="line"><span class="lineno">  748</span>            count = 0</div>
<div class="line"><span class="lineno">  749</span>        <span class="keywordflow">elif</span> count &gt; niter_success:</div>
<div class="line"><span class="lineno">  750</span>            message = [<span class="stringliteral">&quot;success condition satisfied&quot;</span>]</div>
<div class="line"><span class="lineno">  751</span>            <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  752</span> </div>
<div class="line"><span class="lineno">  753</span>    <span class="comment"># prepare return object</span></div>
<div class="line"><span class="lineno">  754</span>    res = bh.res</div>
<div class="line"><span class="lineno">  755</span>    res.lowest_optimization_result = bh.storage.get_lowest()</div>
<div class="line"><span class="lineno">  756</span>    res.x = np.copy(res.lowest_optimization_result.x)</div>
<div class="line"><span class="lineno">  757</span>    res.fun = res.lowest_optimization_result.fun</div>
<div class="line"><span class="lineno">  758</span>    res.message = message</div>
<div class="line"><span class="lineno">  759</span>    res.nit = i + 1</div>
<div class="line"><span class="lineno">  760</span>    res.success = res.lowest_optimization_result.success</div>
<div class="line"><span class="lineno">  761</span>    <span class="keywordflow">return</span> res</div>
<div class="line"><span class="lineno">  762</span> </div>
<div class="line"><span class="lineno">  763</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a7880aca351e6c23c94195c1d38e8d6ed" name="a7880aca351e6c23c94195c1d38e8d6ed"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7880aca351e6c23c94195c1d38e8d6ed">&#9670;&#160;</a></span>kwargs</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">dict scipy.optimize._basinhopping.kwargs = {&quot;method&quot;: &quot;L-<a class="el" href="classscipy_1_1optimize_1_1__hessian__update__strategy_1_1_b_f_g_s.html">BFGS</a>-B&quot;}</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a8da73ccef779576742d10fa46da14880" name="a8da73ccef779576742d10fa46da14880"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8da73ccef779576742d10fa46da14880">&#9670;&#160;</a></span>ret</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._basinhopping.ret</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><span class="lineno">    1</span>=  basinhopping(_test_func2d_nograd, x0, minimizer_kwargs=kwargs,</div>
<div class="line"><span class="lineno">    2</span>                       niter=200, disp=<span class="keyword">False</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="acaac440cada1a865aa697e918189e3ab" name="acaac440cada1a865aa697e918189e3ab"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acaac440cada1a865aa697e918189e3ab">&#9670;&#160;</a></span>x0</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">scipy.optimize._basinhopping.x0 = np.array([1.0, 1.])</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
