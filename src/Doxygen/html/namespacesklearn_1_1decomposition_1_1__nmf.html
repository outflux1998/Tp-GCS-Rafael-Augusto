<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.decomposition._nmf Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1decomposition.html">decomposition</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html">_nmf</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">sklearn.decomposition._nmf Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1decomposition_1_1__nmf_1_1___base_n_m_f.html">_BaseNMF</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1decomposition_1_1__nmf_1_1_mini_batch_n_m_f.html">MiniBatchNMF</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1decomposition_1_1__nmf_1_1_n_m_f.html">NMF</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a6e559cdf3a40c3c84e2f872f8cb79bad" id="r_a6e559cdf3a40c3c84e2f872f8cb79bad"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a6e559cdf3a40c3c84e2f872f8cb79bad">norm</a> (x)</td></tr>
<tr class="separator:a6e559cdf3a40c3c84e2f872f8cb79bad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4b8fde89e737a10205d4e0480f4b678d" id="r_a4b8fde89e737a10205d4e0480f4b678d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a4b8fde89e737a10205d4e0480f4b678d">trace_dot</a> (X, Y)</td></tr>
<tr class="separator:a4b8fde89e737a10205d4e0480f4b678d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7db5413fcde9cbca39cb7bef150ccf83" id="r_a7db5413fcde9cbca39cb7bef150ccf83"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a7db5413fcde9cbca39cb7bef150ccf83">_check_init</a> (A, shape, whom)</td></tr>
<tr class="separator:a7db5413fcde9cbca39cb7bef150ccf83"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af28d297dd2bd8cbfbbec46f183a19f73" id="r_af28d297dd2bd8cbfbbec46f183a19f73"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#af28d297dd2bd8cbfbbec46f183a19f73">_beta_divergence</a> (X, W, H, beta, square_root=False)</td></tr>
<tr class="separator:af28d297dd2bd8cbfbbec46f183a19f73"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8e1e7eda092577e8f9515f13646d4dd3" id="r_a8e1e7eda092577e8f9515f13646d4dd3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a8e1e7eda092577e8f9515f13646d4dd3">_special_sparse_dot</a> (W, H, X)</td></tr>
<tr class="separator:a8e1e7eda092577e8f9515f13646d4dd3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ecfb21949e642b20aa31334a24022a6" id="r_a2ecfb21949e642b20aa31334a24022a6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a2ecfb21949e642b20aa31334a24022a6">_beta_loss_to_float</a> (beta_loss)</td></tr>
<tr class="separator:a2ecfb21949e642b20aa31334a24022a6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae6b88b757d9a2b51559c6851fcf794fc" id="r_ae6b88b757d9a2b51559c6851fcf794fc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#ae6b88b757d9a2b51559c6851fcf794fc">_initialize_nmf</a> (X, n_components, init=None, <a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>=1e-6, random_state=None)</td></tr>
<tr class="separator:ae6b88b757d9a2b51559c6851fcf794fc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7625681941ef7d3eaa301844b70f6a37" id="r_a7625681941ef7d3eaa301844b70f6a37"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a7625681941ef7d3eaa301844b70f6a37">_update_coordinate_descent</a> (X, W, Ht, l1_reg, l2_reg, shuffle, random_state)</td></tr>
<tr class="separator:a7625681941ef7d3eaa301844b70f6a37"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1e419b7547218e3e63acbb67417c729a" id="r_a1e419b7547218e3e63acbb67417c729a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a1e419b7547218e3e63acbb67417c729a">_fit_coordinate_descent</a> (X, W, H, <a class="el" href="__lapack__subroutines_8h.html#a0357339a1a1f7b51953875ca01447445">tol</a>=1e-4, max_iter=200, l1_reg_W=0, l1_reg_H=0, l2_reg_W=0, l2_reg_H=0, update_H=True, verbose=0, shuffle=False, random_state=None)</td></tr>
<tr class="separator:a1e419b7547218e3e63acbb67417c729a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa96c1b1190107de9198dcf265ec2ae07" id="r_aa96c1b1190107de9198dcf265ec2ae07"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#aa96c1b1190107de9198dcf265ec2ae07">_multiplicative_update_w</a> (X, W, H, beta_loss, l1_reg_W, l2_reg_W, gamma, H_sum=None, HHt=None, XHt=None, update_H=True)</td></tr>
<tr class="separator:aa96c1b1190107de9198dcf265ec2ae07"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a755378ea5231288eb0861db8c8b8018b" id="r_a755378ea5231288eb0861db8c8b8018b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a755378ea5231288eb0861db8c8b8018b">_multiplicative_update_h</a> (X, W, H, beta_loss, l1_reg_H, l2_reg_H, gamma, A=None, B=None, <a class="el" href="__lapack__subroutines_8h.html#a9bb256f5d273cef048eb659e4ee52fe2">rho</a>=None)</td></tr>
<tr class="separator:a755378ea5231288eb0861db8c8b8018b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2396834d55cd7ee5c334433651161bfd" id="r_a2396834d55cd7ee5c334433651161bfd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a2396834d55cd7ee5c334433651161bfd">_fit_multiplicative_update</a> (X, W, H, beta_loss=&quot;frobenius&quot;, max_iter=200, <a class="el" href="__lapack__subroutines_8h.html#a0357339a1a1f7b51953875ca01447445">tol</a>=1e-4, l1_reg_W=0, l1_reg_H=0, l2_reg_W=0, l2_reg_H=0, update_H=True, verbose=0)</td></tr>
<tr class="separator:a2396834d55cd7ee5c334433651161bfd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1ae7847a21828ee3f94aa00600c57fed" id="r_a1ae7847a21828ee3f94aa00600c57fed"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a1ae7847a21828ee3f94aa00600c57fed">non_negative_factorization</a> (X, W=None, H=None, n_components=None, *init=None, update_H=True, solver=&quot;cd&quot;, beta_loss=&quot;frobenius&quot;, <a class="el" href="__lapack__subroutines_8h.html#a0357339a1a1f7b51953875ca01447445">tol</a>=1e-4, max_iter=200, alpha_W=0.0, alpha_H=&quot;same&quot;, l1_ratio=0.0, random_state=None, verbose=0, shuffle=False)</td></tr>
<tr class="separator:a1ae7847a21828ee3f94aa00600c57fed"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a9bfb8c23341015a3bcd74d3c3f4a363c" id="r_a9bfb8c23341015a3bcd74d3c3f4a363c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1decomposition_1_1__nmf.html#a9bfb8c23341015a3bcd74d3c3f4a363c">EPSILON</a> = np.finfo(np.float32).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a></td></tr>
<tr class="separator:a9bfb8c23341015a3bcd74d3c3f4a363c"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment"> Non-negative matrix factorization.
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="af28d297dd2bd8cbfbbec46f183a19f73" name="af28d297dd2bd8cbfbbec46f183a19f73"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af28d297dd2bd8cbfbbec46f183a19f73">&#9670;&#160;</a></span>_beta_divergence()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._beta_divergence </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>H</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>beta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>square_root</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the beta-divergence of X and dot(W, H).

Parameters
----------
X : float or array-like of shape (n_samples, n_features)

W : float or array-like of shape (n_samples, n_components)

H : float or array-like of shape (n_components, n_features)

beta : float or {'frobenius', 'kullback-leibler', 'itakura-saito'}
    Parameter of the beta-divergence.
    If beta == 2, this is half the Frobenius *squared* norm.
    If beta == 1, this is the generalized Kullback-Leibler divergence.
    If beta == 0, this is the Itakura-Saito divergence.
    Else, this is the general beta-divergence.

square_root : bool, default=False
    If True, return np.sqrt(2 * res)
    For beta == 2, it corresponds to the Frobenius norm.

Returns
-------
    res : float
        Beta divergence of X and np.dot(X, H).
</pre> <div class="fragment"><div class="line"><span class="lineno">   77</span><span class="keyword">def </span>_beta_divergence(X, W, H, beta, square_root=False):</div>
<div class="line"><span class="lineno">   78</span>    <span class="stringliteral">&quot;&quot;&quot;Compute the beta-divergence of X and dot(W, H).</span></div>
<div class="line"><span class="lineno">   79</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   80</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   81</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   82</span><span class="stringliteral">    X : float or array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">   83</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   84</span><span class="stringliteral">    W : float or array-like of shape (n_samples, n_components)</span></div>
<div class="line"><span class="lineno">   85</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   86</span><span class="stringliteral">    H : float or array-like of shape (n_components, n_features)</span></div>
<div class="line"><span class="lineno">   87</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   88</span><span class="stringliteral">    beta : float or {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, &#39;itakura-saito&#39;}</span></div>
<div class="line"><span class="lineno">   89</span><span class="stringliteral">        Parameter of the beta-divergence.</span></div>
<div class="line"><span class="lineno">   90</span><span class="stringliteral">        If beta == 2, this is half the Frobenius *squared* norm.</span></div>
<div class="line"><span class="lineno">   91</span><span class="stringliteral">        If beta == 1, this is the generalized Kullback-Leibler divergence.</span></div>
<div class="line"><span class="lineno">   92</span><span class="stringliteral">        If beta == 0, this is the Itakura-Saito divergence.</span></div>
<div class="line"><span class="lineno">   93</span><span class="stringliteral">        Else, this is the general beta-divergence.</span></div>
<div class="line"><span class="lineno">   94</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   95</span><span class="stringliteral">    square_root : bool, default=False</span></div>
<div class="line"><span class="lineno">   96</span><span class="stringliteral">        If True, return np.sqrt(2 * res)</span></div>
<div class="line"><span class="lineno">   97</span><span class="stringliteral">        For beta == 2, it corresponds to the Frobenius norm.</span></div>
<div class="line"><span class="lineno">   98</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   99</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  100</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  101</span><span class="stringliteral">        res : float</span></div>
<div class="line"><span class="lineno">  102</span><span class="stringliteral">            Beta divergence of X and np.dot(X, H).</span></div>
<div class="line"><span class="lineno">  103</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  104</span>    beta = _beta_loss_to_float(beta)</div>
<div class="line"><span class="lineno">  105</span> </div>
<div class="line"><span class="lineno">  106</span>    <span class="comment"># The method can be called with scalars</span></div>
<div class="line"><span class="lineno">  107</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> sp.issparse(X):</div>
<div class="line"><span class="lineno">  108</span>        X = np.atleast_2d(X)</div>
<div class="line"><span class="lineno">  109</span>    W = np.atleast_2d(W)</div>
<div class="line"><span class="lineno">  110</span>    H = np.atleast_2d(H)</div>
<div class="line"><span class="lineno">  111</span> </div>
<div class="line"><span class="lineno">  112</span>    <span class="comment"># Frobenius norm</span></div>
<div class="line"><span class="lineno">  113</span>    <span class="keywordflow">if</span> beta == 2:</div>
<div class="line"><span class="lineno">  114</span>        <span class="comment"># Avoid the creation of the dense np.dot(W, H) if X is sparse.</span></div>
<div class="line"><span class="lineno">  115</span>        <span class="keywordflow">if</span> sp.issparse(X):</div>
<div class="line"><span class="lineno">  116</span>            norm_X = np.dot(X.data, X.data)</div>
<div class="line"><span class="lineno">  117</span>            norm_WH = trace_dot(np.linalg.multi_dot([W.T, W, H]), H)</div>
<div class="line"><span class="lineno">  118</span>            cross_prod = trace_dot((X * H.T), W)</div>
<div class="line"><span class="lineno">  119</span>            res = (norm_X + norm_WH - 2.0 * cross_prod) / 2.0</div>
<div class="line"><span class="lineno">  120</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  121</span>            res = squared_norm(X - np.dot(W, H)) / 2.0</div>
<div class="line"><span class="lineno">  122</span> </div>
<div class="line"><span class="lineno">  123</span>        <span class="keywordflow">if</span> square_root:</div>
<div class="line"><span class="lineno">  124</span>            <span class="keywordflow">return</span> np.sqrt(res * 2)</div>
<div class="line"><span class="lineno">  125</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  126</span>            <span class="keywordflow">return</span> res</div>
<div class="line"><span class="lineno">  127</span> </div>
<div class="line"><span class="lineno">  128</span>    <span class="keywordflow">if</span> sp.issparse(X):</div>
<div class="line"><span class="lineno">  129</span>        <span class="comment"># compute np.dot(W, H) only where X is nonzero</span></div>
<div class="line"><span class="lineno">  130</span>        WH_data = _special_sparse_dot(W, H, X).data</div>
<div class="line"><span class="lineno">  131</span>        X_data = X.data</div>
<div class="line"><span class="lineno">  132</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  133</span>        WH = np.dot(W, H)</div>
<div class="line"><span class="lineno">  134</span>        WH_data = WH.ravel()</div>
<div class="line"><span class="lineno">  135</span>        X_data = X.ravel()</div>
<div class="line"><span class="lineno">  136</span> </div>
<div class="line"><span class="lineno">  137</span>    <span class="comment"># do not affect the zeros: here 0 ** (-1) = 0 and not infinity</span></div>
<div class="line"><span class="lineno">  138</span>    indices = X_data &gt; EPSILON</div>
<div class="line"><span class="lineno">  139</span>    WH_data = WH_data[indices]</div>
<div class="line"><span class="lineno">  140</span>    X_data = X_data[indices]</div>
<div class="line"><span class="lineno">  141</span> </div>
<div class="line"><span class="lineno">  142</span>    <span class="comment"># used to avoid division by zero</span></div>
<div class="line"><span class="lineno">  143</span>    WH_data[WH_data == 0] = EPSILON</div>
<div class="line"><span class="lineno">  144</span> </div>
<div class="line"><span class="lineno">  145</span>    <span class="comment"># generalized Kullback-Leibler divergence</span></div>
<div class="line"><span class="lineno">  146</span>    <span class="keywordflow">if</span> beta == 1:</div>
<div class="line"><span class="lineno">  147</span>        <span class="comment"># fast and memory efficient computation of np.sum(np.dot(W, H))</span></div>
<div class="line"><span class="lineno">  148</span>        sum_WH = np.dot(np.sum(W, axis=0), np.sum(H, axis=1))</div>
<div class="line"><span class="lineno">  149</span>        <span class="comment"># computes np.sum(X * log(X / WH)) only where X is nonzero</span></div>
<div class="line"><span class="lineno">  150</span>        div = X_data / WH_data</div>
<div class="line"><span class="lineno">  151</span>        res = np.dot(X_data, np.log(div))</div>
<div class="line"><span class="lineno">  152</span>        <span class="comment"># add full np.sum(np.dot(W, H)) - np.sum(X)</span></div>
<div class="line"><span class="lineno">  153</span>        res += sum_WH - X_data.sum()</div>
<div class="line"><span class="lineno">  154</span> </div>
<div class="line"><span class="lineno">  155</span>    <span class="comment"># Itakura-Saito divergence</span></div>
<div class="line"><span class="lineno">  156</span>    <span class="keywordflow">elif</span> beta == 0:</div>
<div class="line"><span class="lineno">  157</span>        div = X_data / WH_data</div>
<div class="line"><span class="lineno">  158</span>        res = np.sum(div) - np.product(X.shape) - np.sum(np.log(div))</div>
<div class="line"><span class="lineno">  159</span> </div>
<div class="line"><span class="lineno">  160</span>    <span class="comment"># beta-divergence, beta not in (0, 1, 2)</span></div>
<div class="line"><span class="lineno">  161</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  162</span>        <span class="keywordflow">if</span> sp.issparse(X):</div>
<div class="line"><span class="lineno">  163</span>            <span class="comment"># slow loop, but memory efficient computation of :</span></div>
<div class="line"><span class="lineno">  164</span>            <span class="comment"># np.sum(np.dot(W, H) ** beta)</span></div>
<div class="line"><span class="lineno">  165</span>            sum_WH_beta = 0</div>
<div class="line"><span class="lineno">  166</span>            <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(X.shape[1]):</div>
<div class="line"><span class="lineno">  167</span>                sum_WH_beta += np.sum(np.dot(W, H[:, i]) ** beta)</div>
<div class="line"><span class="lineno">  168</span> </div>
<div class="line"><span class="lineno">  169</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  170</span>            sum_WH_beta = np.sum(WH**beta)</div>
<div class="line"><span class="lineno">  171</span> </div>
<div class="line"><span class="lineno">  172</span>        sum_X_WH = np.dot(X_data, WH_data ** (beta - 1))</div>
<div class="line"><span class="lineno">  173</span>        res = (X_data**beta).sum() - beta * sum_X_WH</div>
<div class="line"><span class="lineno">  174</span>        res += sum_WH_beta * (beta - 1)</div>
<div class="line"><span class="lineno">  175</span>        res /= beta * (beta - 1)</div>
<div class="line"><span class="lineno">  176</span> </div>
<div class="line"><span class="lineno">  177</span>    <span class="keywordflow">if</span> square_root:</div>
<div class="line"><span class="lineno">  178</span>        res = max(res, 0)  <span class="comment"># avoid negative number due to rounding errors</span></div>
<div class="line"><span class="lineno">  179</span>        <span class="keywordflow">return</span> np.sqrt(2 * res)</div>
<div class="line"><span class="lineno">  180</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  181</span>        <span class="keywordflow">return</span> res</div>
<div class="line"><span class="lineno">  182</span> </div>
<div class="line"><span class="lineno">  183</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a2ecfb21949e642b20aa31334a24022a6" name="a2ecfb21949e642b20aa31334a24022a6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2ecfb21949e642b20aa31334a24022a6">&#9670;&#160;</a></span>_beta_loss_to_float()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._beta_loss_to_float </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>beta_loss</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Convert string beta_loss to float.</pre> <div class="fragment"><div class="line"><span class="lineno">  205</span><span class="keyword">def </span>_beta_loss_to_float(beta_loss):</div>
<div class="line"><span class="lineno">  206</span>    <span class="stringliteral">&quot;&quot;&quot;Convert string beta_loss to float.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  207</span>    beta_loss_map = {<span class="stringliteral">&quot;frobenius&quot;</span>: 2, <span class="stringliteral">&quot;kullback-leibler&quot;</span>: 1, <span class="stringliteral">&quot;itakura-saito&quot;</span>: 0}</div>
<div class="line"><span class="lineno">  208</span>    <span class="keywordflow">if</span> isinstance(beta_loss, str):</div>
<div class="line"><span class="lineno">  209</span>        beta_loss = beta_loss_map[beta_loss]</div>
<div class="line"><span class="lineno">  210</span>    <span class="keywordflow">return</span> beta_loss</div>
<div class="line"><span class="lineno">  211</span> </div>
<div class="line"><span class="lineno">  212</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a7db5413fcde9cbca39cb7bef150ccf83" name="a7db5413fcde9cbca39cb7bef150ccf83"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7db5413fcde9cbca39cb7bef150ccf83">&#9670;&#160;</a></span>_check_init()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._check_init </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>shape</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>whom</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   65</span><span class="keyword">def </span>_check_init(A, shape, whom):</div>
<div class="line"><span class="lineno">   66</span>    A = check_array(A)</div>
<div class="line"><span class="lineno">   67</span>    <span class="keywordflow">if</span> np.shape(A) != shape:</div>
<div class="line"><span class="lineno">   68</span>        <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">   69</span>            <span class="stringliteral">&quot;Array with wrong shape passed to %s. Expected %s, but got %s &quot;</span></div>
<div class="line"><span class="lineno">   70</span>            % (whom, shape, np.shape(A))</div>
<div class="line"><span class="lineno">   71</span>        )</div>
<div class="line"><span class="lineno">   72</span>    check_non_negative(A, whom)</div>
<div class="line"><span class="lineno">   73</span>    <span class="keywordflow">if</span> np.max(A) == 0:</div>
<div class="line"><span class="lineno">   74</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Array passed to %s is full of zeros.&quot;</span> % whom)</div>
<div class="line"><span class="lineno">   75</span> </div>
<div class="line"><span class="lineno">   76</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a1e419b7547218e3e63acbb67417c729a" name="a1e419b7547218e3e63acbb67417c729a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1e419b7547218e3e63acbb67417c729a">&#9670;&#160;</a></span>_fit_coordinate_descent()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._fit_coordinate_descent </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>H</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tol</em> = <code>1e-4</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_iter</em> = <code>200</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1_reg_W</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1_reg_H</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_W</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_H</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>update_H</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>shuffle</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute Non-negative Matrix Factorization (NMF) with Coordinate Descent

The objective function is minimized with an alternating minimization of W
and H. Each minimization is done with a cyclic (up to a permutation of the
features) Coordinate Descent.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    Constant matrix.

W : array-like of shape (n_samples, n_components)
    Initial guess for the solution.

H : array-like of shape (n_components, n_features)
    Initial guess for the solution.

tol : float, default=1e-4
    Tolerance of the stopping condition.

max_iter : int, default=200
    Maximum number of iterations before timing out.

l1_reg_W : float, default=0.
    L1 regularization parameter for W.

l1_reg_H : float, default=0.
    L1 regularization parameter for H.

l2_reg_W : float, default=0.
    L2 regularization parameter for W.

l2_reg_H : float, default=0.
    L2 regularization parameter for H.

update_H : bool, default=True
    Set to True, both W and H will be estimated from initial guesses.
    Set to False, only W will be estimated.

verbose : int, default=0
    The verbosity level.

shuffle : bool, default=False
    If true, randomize the order of coordinates in the CD solver.

random_state : int, RandomState instance or None, default=None
    Used to randomize the coordinates in the CD solver, when
    ``shuffle`` is set to ``True``. Pass an int for reproducible
    results across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

Returns
-------
W : ndarray of shape (n_samples, n_components)
    Solution to the non-negative least squares problem.

H : ndarray of shape (n_components, n_features)
    Solution to the non-negative least squares problem.

n_iter : int
    The number of iterations done by the algorithm.

References
----------
.. [1] :doi:`"Fast local algorithms for large scale nonnegative matrix and tensor
   factorizations" &lt;10.1587/transfun.E92.A.708&gt;`
   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals
   of electronics, communications and computer sciences 92.3: 708-721, 2009.
</pre> <div class="fragment"><div class="line"><span class="lineno">  412</span>):</div>
<div class="line"><span class="lineno">  413</span>    <span class="stringliteral">&quot;&quot;&quot;Compute Non-negative Matrix Factorization (NMF) with Coordinate Descent</span></div>
<div class="line"><span class="lineno">  414</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  415</span><span class="stringliteral">    The objective function is minimized with an alternating minimization of W</span></div>
<div class="line"><span class="lineno">  416</span><span class="stringliteral">    and H. Each minimization is done with a cyclic (up to a permutation of the</span></div>
<div class="line"><span class="lineno">  417</span><span class="stringliteral">    features) Coordinate Descent.</span></div>
<div class="line"><span class="lineno">  418</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  419</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  420</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  421</span><span class="stringliteral">    X : array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  422</span><span class="stringliteral">        Constant matrix.</span></div>
<div class="line"><span class="lineno">  423</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  424</span><span class="stringliteral">    W : array-like of shape (n_samples, n_components)</span></div>
<div class="line"><span class="lineno">  425</span><span class="stringliteral">        Initial guess for the solution.</span></div>
<div class="line"><span class="lineno">  426</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  427</span><span class="stringliteral">    H : array-like of shape (n_components, n_features)</span></div>
<div class="line"><span class="lineno">  428</span><span class="stringliteral">        Initial guess for the solution.</span></div>
<div class="line"><span class="lineno">  429</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  430</span><span class="stringliteral">    tol : float, default=1e-4</span></div>
<div class="line"><span class="lineno">  431</span><span class="stringliteral">        Tolerance of the stopping condition.</span></div>
<div class="line"><span class="lineno">  432</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  433</span><span class="stringliteral">    max_iter : int, default=200</span></div>
<div class="line"><span class="lineno">  434</span><span class="stringliteral">        Maximum number of iterations before timing out.</span></div>
<div class="line"><span class="lineno">  435</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  436</span><span class="stringliteral">    l1_reg_W : float, default=0.</span></div>
<div class="line"><span class="lineno">  437</span><span class="stringliteral">        L1 regularization parameter for W.</span></div>
<div class="line"><span class="lineno">  438</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  439</span><span class="stringliteral">    l1_reg_H : float, default=0.</span></div>
<div class="line"><span class="lineno">  440</span><span class="stringliteral">        L1 regularization parameter for H.</span></div>
<div class="line"><span class="lineno">  441</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  442</span><span class="stringliteral">    l2_reg_W : float, default=0.</span></div>
<div class="line"><span class="lineno">  443</span><span class="stringliteral">        L2 regularization parameter for W.</span></div>
<div class="line"><span class="lineno">  444</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  445</span><span class="stringliteral">    l2_reg_H : float, default=0.</span></div>
<div class="line"><span class="lineno">  446</span><span class="stringliteral">        L2 regularization parameter for H.</span></div>
<div class="line"><span class="lineno">  447</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  448</span><span class="stringliteral">    update_H : bool, default=True</span></div>
<div class="line"><span class="lineno">  449</span><span class="stringliteral">        Set to True, both W and H will be estimated from initial guesses.</span></div>
<div class="line"><span class="lineno">  450</span><span class="stringliteral">        Set to False, only W will be estimated.</span></div>
<div class="line"><span class="lineno">  451</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  452</span><span class="stringliteral">    verbose : int, default=0</span></div>
<div class="line"><span class="lineno">  453</span><span class="stringliteral">        The verbosity level.</span></div>
<div class="line"><span class="lineno">  454</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  455</span><span class="stringliteral">    shuffle : bool, default=False</span></div>
<div class="line"><span class="lineno">  456</span><span class="stringliteral">        If true, randomize the order of coordinates in the CD solver.</span></div>
<div class="line"><span class="lineno">  457</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  458</span><span class="stringliteral">    random_state : int, RandomState instance or None, default=None</span></div>
<div class="line"><span class="lineno">  459</span><span class="stringliteral">        Used to randomize the coordinates in the CD solver, when</span></div>
<div class="line"><span class="lineno">  460</span><span class="stringliteral">        ``shuffle`` is set to ``True``. Pass an int for reproducible</span></div>
<div class="line"><span class="lineno">  461</span><span class="stringliteral">        results across multiple function calls.</span></div>
<div class="line"><span class="lineno">  462</span><span class="stringliteral">        See :term:`Glossary &lt;random_state&gt;`.</span></div>
<div class="line"><span class="lineno">  463</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  464</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  465</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  466</span><span class="stringliteral">    W : ndarray of shape (n_samples, n_components)</span></div>
<div class="line"><span class="lineno">  467</span><span class="stringliteral">        Solution to the non-negative least squares problem.</span></div>
<div class="line"><span class="lineno">  468</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  469</span><span class="stringliteral">    H : ndarray of shape (n_components, n_features)</span></div>
<div class="line"><span class="lineno">  470</span><span class="stringliteral">        Solution to the non-negative least squares problem.</span></div>
<div class="line"><span class="lineno">  471</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  472</span><span class="stringliteral">    n_iter : int</span></div>
<div class="line"><span class="lineno">  473</span><span class="stringliteral">        The number of iterations done by the algorithm.</span></div>
<div class="line"><span class="lineno">  474</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  475</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  476</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  477</span><span class="stringliteral">    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor</span></div>
<div class="line"><span class="lineno">  478</span><span class="stringliteral">       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;`</span></div>
<div class="line"><span class="lineno">  479</span><span class="stringliteral">       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals</span></div>
<div class="line"><span class="lineno">  480</span><span class="stringliteral">       of electronics, communications and computer sciences 92.3: 708-721, 2009.</span></div>
<div class="line"><span class="lineno">  481</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  482</span>    <span class="comment"># so W and Ht are both in C order in memory</span></div>
<div class="line"><span class="lineno">  483</span>    Ht = check_array(H.T, order=<span class="stringliteral">&quot;C&quot;</span>)</div>
<div class="line"><span class="lineno">  484</span>    X = check_array(X, accept_sparse=<span class="stringliteral">&quot;csr&quot;</span>)</div>
<div class="line"><span class="lineno">  485</span> </div>
<div class="line"><span class="lineno">  486</span>    rng = check_random_state(random_state)</div>
<div class="line"><span class="lineno">  487</span> </div>
<div class="line"><span class="lineno">  488</span>    <span class="keywordflow">for</span> n_iter <span class="keywordflow">in</span> range(1, max_iter + 1):</div>
<div class="line"><span class="lineno">  489</span>        violation = 0.0</div>
<div class="line"><span class="lineno">  490</span> </div>
<div class="line"><span class="lineno">  491</span>        <span class="comment"># Update W</span></div>
<div class="line"><span class="lineno">  492</span>        violation += _update_coordinate_descent(</div>
<div class="line"><span class="lineno">  493</span>            X, W, Ht, l1_reg_W, l2_reg_W, shuffle, rng</div>
<div class="line"><span class="lineno">  494</span>        )</div>
<div class="line"><span class="lineno">  495</span>        <span class="comment"># Update H</span></div>
<div class="line"><span class="lineno">  496</span>        <span class="keywordflow">if</span> update_H:</div>
<div class="line"><span class="lineno">  497</span>            violation += _update_coordinate_descent(</div>
<div class="line"><span class="lineno">  498</span>                X.T, Ht, W, l1_reg_H, l2_reg_H, shuffle, rng</div>
<div class="line"><span class="lineno">  499</span>            )</div>
<div class="line"><span class="lineno">  500</span> </div>
<div class="line"><span class="lineno">  501</span>        <span class="keywordflow">if</span> n_iter == 1:</div>
<div class="line"><span class="lineno">  502</span>            violation_init = violation</div>
<div class="line"><span class="lineno">  503</span> </div>
<div class="line"><span class="lineno">  504</span>        <span class="keywordflow">if</span> violation_init == 0:</div>
<div class="line"><span class="lineno">  505</span>            <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  506</span> </div>
<div class="line"><span class="lineno">  507</span>        <span class="keywordflow">if</span> verbose:</div>
<div class="line"><span class="lineno">  508</span>            print(<span class="stringliteral">&quot;violation:&quot;</span>, violation / violation_init)</div>
<div class="line"><span class="lineno">  509</span> </div>
<div class="line"><span class="lineno">  510</span>        <span class="keywordflow">if</span> violation / violation_init &lt;= tol:</div>
<div class="line"><span class="lineno">  511</span>            <span class="keywordflow">if</span> verbose:</div>
<div class="line"><span class="lineno">  512</span>                print(<span class="stringliteral">&quot;Converged at iteration&quot;</span>, n_iter + 1)</div>
<div class="line"><span class="lineno">  513</span>            <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  514</span> </div>
<div class="line"><span class="lineno">  515</span>    <span class="keywordflow">return</span> W, Ht.T, n_iter</div>
<div class="line"><span class="lineno">  516</span> </div>
<div class="line"><span class="lineno">  517</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a2396834d55cd7ee5c334433651161bfd" name="a2396834d55cd7ee5c334433651161bfd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2396834d55cd7ee5c334433651161bfd">&#9670;&#160;</a></span>_fit_multiplicative_update()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._fit_multiplicative_update </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>H</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>beta_loss</em> = <code>&quot;frobenius&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_iter</em> = <code>200</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tol</em> = <code>1e-4</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1_reg_W</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1_reg_H</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_W</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_H</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>update_H</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute Non-negative Matrix Factorization with Multiplicative Update.

The objective function is _beta_divergence(X, WH) and is minimized with an
alternating minimization of W and H. Each minimization is done with a
Multiplicative Update.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    Constant input matrix.

W : array-like of shape (n_samples, n_components)
    Initial guess for the solution.

H : array-like of shape (n_components, n_features)
    Initial guess for the solution.

beta_loss : float or {'frobenius', 'kullback-leibler', \
        'itakura-saito'}, default='frobenius'
    String must be in {'frobenius', 'kullback-leibler', 'itakura-saito'}.
    Beta divergence to be minimized, measuring the distance between X
    and the dot product WH. Note that values different from 'frobenius'
    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower
    fits. Note that for beta_loss &lt;= 0 (or 'itakura-saito'), the input
    matrix X cannot contain zeros.

max_iter : int, default=200
    Number of iterations.

tol : float, default=1e-4
    Tolerance of the stopping condition.

l1_reg_W : float, default=0.
    L1 regularization parameter for W.

l1_reg_H : float, default=0.
    L1 regularization parameter for H.

l2_reg_W : float, default=0.
    L2 regularization parameter for W.

l2_reg_H : float, default=0.
    L2 regularization parameter for H.

update_H : bool, default=True
    Set to True, both W and H will be estimated from initial guesses.
    Set to False, only W will be estimated.

verbose : int, default=0
    The verbosity level.

Returns
-------
W : ndarray of shape (n_samples, n_components)
    Solution to the non-negative least squares problem.

H : ndarray of shape (n_components, n_features)
    Solution to the non-negative least squares problem.

n_iter : int
    The number of iterations done by the algorithm.

References
----------
Lee, D. D., &amp; Seung, H., S. (2001). Algorithms for Non-negative Matrix
Factorization. Adv. Neural Inform. Process. Syst.. 13.
Fevotte, C., &amp; Idier, J. (2011). Algorithms for nonnegative matrix
factorization with the beta-divergence. Neural Computation, 23(9).
</pre> <div class="fragment"><div class="line"><span class="lineno">  736</span>):</div>
<div class="line"><span class="lineno">  737</span>    <span class="stringliteral">&quot;&quot;&quot;Compute Non-negative Matrix Factorization with Multiplicative Update.</span></div>
<div class="line"><span class="lineno">  738</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  739</span><span class="stringliteral">    The objective function is _beta_divergence(X, WH) and is minimized with an</span></div>
<div class="line"><span class="lineno">  740</span><span class="stringliteral">    alternating minimization of W and H. Each minimization is done with a</span></div>
<div class="line"><span class="lineno">  741</span><span class="stringliteral">    Multiplicative Update.</span></div>
<div class="line"><span class="lineno">  742</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  743</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  744</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  745</span><span class="stringliteral">    X : array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  746</span><span class="stringliteral">        Constant input matrix.</span></div>
<div class="line"><span class="lineno">  747</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  748</span><span class="stringliteral">    W : array-like of shape (n_samples, n_components)</span></div>
<div class="line"><span class="lineno">  749</span><span class="stringliteral">        Initial guess for the solution.</span></div>
<div class="line"><span class="lineno">  750</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  751</span><span class="stringliteral">    H : array-like of shape (n_components, n_features)</span></div>
<div class="line"><span class="lineno">  752</span><span class="stringliteral">        Initial guess for the solution.</span></div>
<div class="line"><span class="lineno">  753</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  754</span><span class="stringliteral">    beta_loss : float or {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, \</span></div>
<div class="line"><span class="lineno">  755</span><span class="stringliteral">            &#39;itakura-saito&#39;}, default=&#39;frobenius&#39;</span></div>
<div class="line"><span class="lineno">  756</span><span class="stringliteral">        String must be in {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, &#39;itakura-saito&#39;}.</span></div>
<div class="line"><span class="lineno">  757</span><span class="stringliteral">        Beta divergence to be minimized, measuring the distance between X</span></div>
<div class="line"><span class="lineno">  758</span><span class="stringliteral">        and the dot product WH. Note that values different from &#39;frobenius&#39;</span></div>
<div class="line"><span class="lineno">  759</span><span class="stringliteral">        (or 2) and &#39;kullback-leibler&#39; (or 1) lead to significantly slower</span></div>
<div class="line"><span class="lineno">  760</span><span class="stringliteral">        fits. Note that for beta_loss &lt;= 0 (or &#39;itakura-saito&#39;), the input</span></div>
<div class="line"><span class="lineno">  761</span><span class="stringliteral">        matrix X cannot contain zeros.</span></div>
<div class="line"><span class="lineno">  762</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  763</span><span class="stringliteral">    max_iter : int, default=200</span></div>
<div class="line"><span class="lineno">  764</span><span class="stringliteral">        Number of iterations.</span></div>
<div class="line"><span class="lineno">  765</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  766</span><span class="stringliteral">    tol : float, default=1e-4</span></div>
<div class="line"><span class="lineno">  767</span><span class="stringliteral">        Tolerance of the stopping condition.</span></div>
<div class="line"><span class="lineno">  768</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  769</span><span class="stringliteral">    l1_reg_W : float, default=0.</span></div>
<div class="line"><span class="lineno">  770</span><span class="stringliteral">        L1 regularization parameter for W.</span></div>
<div class="line"><span class="lineno">  771</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  772</span><span class="stringliteral">    l1_reg_H : float, default=0.</span></div>
<div class="line"><span class="lineno">  773</span><span class="stringliteral">        L1 regularization parameter for H.</span></div>
<div class="line"><span class="lineno">  774</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  775</span><span class="stringliteral">    l2_reg_W : float, default=0.</span></div>
<div class="line"><span class="lineno">  776</span><span class="stringliteral">        L2 regularization parameter for W.</span></div>
<div class="line"><span class="lineno">  777</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  778</span><span class="stringliteral">    l2_reg_H : float, default=0.</span></div>
<div class="line"><span class="lineno">  779</span><span class="stringliteral">        L2 regularization parameter for H.</span></div>
<div class="line"><span class="lineno">  780</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  781</span><span class="stringliteral">    update_H : bool, default=True</span></div>
<div class="line"><span class="lineno">  782</span><span class="stringliteral">        Set to True, both W and H will be estimated from initial guesses.</span></div>
<div class="line"><span class="lineno">  783</span><span class="stringliteral">        Set to False, only W will be estimated.</span></div>
<div class="line"><span class="lineno">  784</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  785</span><span class="stringliteral">    verbose : int, default=0</span></div>
<div class="line"><span class="lineno">  786</span><span class="stringliteral">        The verbosity level.</span></div>
<div class="line"><span class="lineno">  787</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  788</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  789</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  790</span><span class="stringliteral">    W : ndarray of shape (n_samples, n_components)</span></div>
<div class="line"><span class="lineno">  791</span><span class="stringliteral">        Solution to the non-negative least squares problem.</span></div>
<div class="line"><span class="lineno">  792</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  793</span><span class="stringliteral">    H : ndarray of shape (n_components, n_features)</span></div>
<div class="line"><span class="lineno">  794</span><span class="stringliteral">        Solution to the non-negative least squares problem.</span></div>
<div class="line"><span class="lineno">  795</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  796</span><span class="stringliteral">    n_iter : int</span></div>
<div class="line"><span class="lineno">  797</span><span class="stringliteral">        The number of iterations done by the algorithm.</span></div>
<div class="line"><span class="lineno">  798</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  799</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  800</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  801</span><span class="stringliteral">    Lee, D. D., &amp; Seung, H., S. (2001). Algorithms for Non-negative Matrix</span></div>
<div class="line"><span class="lineno">  802</span><span class="stringliteral">    Factorization. Adv. Neural Inform. Process. Syst.. 13.</span></div>
<div class="line"><span class="lineno">  803</span><span class="stringliteral">    Fevotte, C., &amp; Idier, J. (2011). Algorithms for nonnegative matrix</span></div>
<div class="line"><span class="lineno">  804</span><span class="stringliteral">    factorization with the beta-divergence. Neural Computation, 23(9).</span></div>
<div class="line"><span class="lineno">  805</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  806</span>    start_time = time.time()</div>
<div class="line"><span class="lineno">  807</span> </div>
<div class="line"><span class="lineno">  808</span>    beta_loss = _beta_loss_to_float(beta_loss)</div>
<div class="line"><span class="lineno">  809</span> </div>
<div class="line"><span class="lineno">  810</span>    <span class="comment"># gamma for Maximization-Minimization (MM) algorithm [Fevotte 2011]</span></div>
<div class="line"><span class="lineno">  811</span>    <span class="keywordflow">if</span> beta_loss &lt; 1:</div>
<div class="line"><span class="lineno">  812</span>        gamma = 1.0 / (2.0 - beta_loss)</div>
<div class="line"><span class="lineno">  813</span>    <span class="keywordflow">elif</span> beta_loss &gt; 2:</div>
<div class="line"><span class="lineno">  814</span>        gamma = 1.0 / (beta_loss - 1.0)</div>
<div class="line"><span class="lineno">  815</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  816</span>        gamma = 1.0</div>
<div class="line"><span class="lineno">  817</span> </div>
<div class="line"><span class="lineno">  818</span>    <span class="comment"># used for the convergence criterion</span></div>
<div class="line"><span class="lineno">  819</span>    error_at_init = _beta_divergence(X, W, H, beta_loss, square_root=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  820</span>    previous_error = error_at_init</div>
<div class="line"><span class="lineno">  821</span> </div>
<div class="line"><span class="lineno">  822</span>    H_sum, HHt, XHt = <span class="keywordtype">None</span>, <span class="keywordtype">None</span>, <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  823</span>    <span class="keywordflow">for</span> n_iter <span class="keywordflow">in</span> range(1, max_iter + 1):</div>
<div class="line"><span class="lineno">  824</span>        <span class="comment"># update W</span></div>
<div class="line"><span class="lineno">  825</span>        <span class="comment"># H_sum, HHt and XHt are saved and reused if not update_H</span></div>
<div class="line"><span class="lineno">  826</span>        W, H_sum, HHt, XHt = _multiplicative_update_w(</div>
<div class="line"><span class="lineno">  827</span>            X,</div>
<div class="line"><span class="lineno">  828</span>            W,</div>
<div class="line"><span class="lineno">  829</span>            H,</div>
<div class="line"><span class="lineno">  830</span>            beta_loss=beta_loss,</div>
<div class="line"><span class="lineno">  831</span>            l1_reg_W=l1_reg_W,</div>
<div class="line"><span class="lineno">  832</span>            l2_reg_W=l2_reg_W,</div>
<div class="line"><span class="lineno">  833</span>            gamma=gamma,</div>
<div class="line"><span class="lineno">  834</span>            H_sum=H_sum,</div>
<div class="line"><span class="lineno">  835</span>            HHt=HHt,</div>
<div class="line"><span class="lineno">  836</span>            XHt=XHt,</div>
<div class="line"><span class="lineno">  837</span>            update_H=update_H,</div>
<div class="line"><span class="lineno">  838</span>        )</div>
<div class="line"><span class="lineno">  839</span> </div>
<div class="line"><span class="lineno">  840</span>        <span class="comment"># necessary for stability with beta_loss &lt; 1</span></div>
<div class="line"><span class="lineno">  841</span>        <span class="keywordflow">if</span> beta_loss &lt; 1:</div>
<div class="line"><span class="lineno">  842</span>            W[W &lt; np.finfo(np.float64).eps] = 0.0</div>
<div class="line"><span class="lineno">  843</span> </div>
<div class="line"><span class="lineno">  844</span>        <span class="comment"># update H (only at fit or fit_transform)</span></div>
<div class="line"><span class="lineno">  845</span>        <span class="keywordflow">if</span> update_H:</div>
<div class="line"><span class="lineno">  846</span>            H = _multiplicative_update_h(</div>
<div class="line"><span class="lineno">  847</span>                X,</div>
<div class="line"><span class="lineno">  848</span>                W,</div>
<div class="line"><span class="lineno">  849</span>                H,</div>
<div class="line"><span class="lineno">  850</span>                beta_loss=beta_loss,</div>
<div class="line"><span class="lineno">  851</span>                l1_reg_H=l1_reg_H,</div>
<div class="line"><span class="lineno">  852</span>                l2_reg_H=l2_reg_H,</div>
<div class="line"><span class="lineno">  853</span>                gamma=gamma,</div>
<div class="line"><span class="lineno">  854</span>            )</div>
<div class="line"><span class="lineno">  855</span> </div>
<div class="line"><span class="lineno">  856</span>            <span class="comment"># These values will be recomputed since H changed</span></div>
<div class="line"><span class="lineno">  857</span>            H_sum, HHt, XHt = <span class="keywordtype">None</span>, <span class="keywordtype">None</span>, <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  858</span> </div>
<div class="line"><span class="lineno">  859</span>            <span class="comment"># necessary for stability with beta_loss &lt; 1</span></div>
<div class="line"><span class="lineno">  860</span>            <span class="keywordflow">if</span> beta_loss &lt;= 1:</div>
<div class="line"><span class="lineno">  861</span>                H[H &lt; np.finfo(np.float64).eps] = 0.0</div>
<div class="line"><span class="lineno">  862</span> </div>
<div class="line"><span class="lineno">  863</span>        <span class="comment"># test convergence criterion every 10 iterations</span></div>
<div class="line"><span class="lineno">  864</span>        <span class="keywordflow">if</span> tol &gt; 0 <span class="keywordflow">and</span> n_iter % 10 == 0:</div>
<div class="line"><span class="lineno">  865</span>            error = _beta_divergence(X, W, H, beta_loss, square_root=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  866</span> </div>
<div class="line"><span class="lineno">  867</span>            <span class="keywordflow">if</span> verbose:</div>
<div class="line"><span class="lineno">  868</span>                iter_time = time.time()</div>
<div class="line"><span class="lineno">  869</span>                print(</div>
<div class="line"><span class="lineno">  870</span>                    <span class="stringliteral">&quot;Epoch %02d reached after %.3f seconds, error: %f&quot;</span></div>
<div class="line"><span class="lineno">  871</span>                    % (n_iter, iter_time - start_time, error)</div>
<div class="line"><span class="lineno">  872</span>                )</div>
<div class="line"><span class="lineno">  873</span> </div>
<div class="line"><span class="lineno">  874</span>            <span class="keywordflow">if</span> (previous_error - error) / error_at_init &lt; tol:</div>
<div class="line"><span class="lineno">  875</span>                <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  876</span>            previous_error = error</div>
<div class="line"><span class="lineno">  877</span> </div>
<div class="line"><span class="lineno">  878</span>    <span class="comment"># do not print if we have already printed in the convergence test</span></div>
<div class="line"><span class="lineno">  879</span>    <span class="keywordflow">if</span> verbose <span class="keywordflow">and</span> (tol == 0 <span class="keywordflow">or</span> n_iter % 10 != 0):</div>
<div class="line"><span class="lineno">  880</span>        end_time = time.time()</div>
<div class="line"><span class="lineno">  881</span>        print(</div>
<div class="line"><span class="lineno">  882</span>            <span class="stringliteral">&quot;Epoch %02d reached after %.3f seconds.&quot;</span> % (n_iter, end_time - start_time)</div>
<div class="line"><span class="lineno">  883</span>        )</div>
<div class="line"><span class="lineno">  884</span> </div>
<div class="line"><span class="lineno">  885</span>    <span class="keywordflow">return</span> W, H, n_iter</div>
<div class="line"><span class="lineno">  886</span> </div>
<div class="line"><span class="lineno">  887</span> </div>
<div class="line"><span class="lineno">  888</span><span class="preprocessor">@validate_params</span>(</div>
<div class="line"><span class="lineno">  889</span>    {</div>
<div class="line"><span class="lineno">  890</span>        <span class="stringliteral">&quot;X&quot;</span>: [<span class="stringliteral">&quot;array-like&quot;</span>, <span class="stringliteral">&quot;sparse matrix&quot;</span>],</div>
<div class="line"><span class="lineno">  891</span>        <span class="stringliteral">&quot;W&quot;</span>: [<span class="stringliteral">&quot;array-like&quot;</span>, <span class="keywordtype">None</span>],</div>
<div class="line"><span class="lineno">  892</span>        <span class="stringliteral">&quot;H&quot;</span>: [<span class="stringliteral">&quot;array-like&quot;</span>, <span class="keywordtype">None</span>],</div>
<div class="line"><span class="lineno">  893</span>        <span class="stringliteral">&quot;update_H&quot;</span>: [<span class="stringliteral">&quot;boolean&quot;</span>],</div>
<div class="line"><span class="lineno">  894</span>    }</div>
<div class="line"><span class="lineno">  895</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae6b88b757d9a2b51559c6851fcf794fc" name="ae6b88b757d9a2b51559c6851fcf794fc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae6b88b757d9a2b51559c6851fcf794fc">&#9670;&#160;</a></span>_initialize_nmf()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._initialize_nmf </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_components</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>init</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>eps</em> = <code>1e-6</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Algorithms for NMF initialization.

Computes an initial guess for the non-negative
rank k matrix approximation for X: X = WH.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    The data matrix to be decomposed.

n_components : int
    The number of components desired in the approximation.

init :  {'random', 'nndsvd', 'nndsvda', 'nndsvdar'}, default=None
    Method used to initialize the procedure.
    Valid options:

    - None: 'nndsvda' if n_components &lt;= min(n_samples, n_features),
        otherwise 'random'.

    - 'random': non-negative random matrices, scaled with:
        sqrt(X.mean() / n_components)

    - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)
        initialization (better for sparseness)

    - 'nndsvda': NNDSVD with zeros filled with the average of X
        (better when sparsity is not desired)

    - 'nndsvdar': NNDSVD with zeros filled with small random values
        (generally faster, less accurate alternative to NNDSVDa
        for when sparsity is not desired)

    - 'custom': use custom matrices W and H

    .. versionchanged:: 1.1
        When `init=None` and n_components is less than n_samples and n_features
        defaults to `nndsvda` instead of `nndsvd`.

eps : float, default=1e-6
    Truncate all values less then this in output to zero.

random_state : int, RandomState instance or None, default=None
    Used when ``init`` == 'nndsvdar' or 'random'. Pass an int for
    reproducible results across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

Returns
-------
W : array-like of shape (n_samples, n_components)
    Initial guesses for solving X ~= WH.

H : array-like of shape (n_components, n_features)
    Initial guesses for solving X ~= WH.

References
----------
C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for
nonnegative matrix factorization - Pattern Recognition, 2008
http://tinyurl.com/nndsvd
</pre> <div class="fragment"><div class="line"><span class="lineno">  213</span><span class="keyword">def </span>_initialize_nmf(X, n_components, init=None, eps=1e-6, random_state=None):</div>
<div class="line"><span class="lineno">  214</span>    <span class="stringliteral">&quot;&quot;&quot;Algorithms for NMF initialization.</span></div>
<div class="line"><span class="lineno">  215</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  216</span><span class="stringliteral">    Computes an initial guess for the non-negative</span></div>
<div class="line"><span class="lineno">  217</span><span class="stringliteral">    rank k matrix approximation for X: X = WH.</span></div>
<div class="line"><span class="lineno">  218</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  219</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  220</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  221</span><span class="stringliteral">    X : array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  222</span><span class="stringliteral">        The data matrix to be decomposed.</span></div>
<div class="line"><span class="lineno">  223</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  224</span><span class="stringliteral">    n_components : int</span></div>
<div class="line"><span class="lineno">  225</span><span class="stringliteral">        The number of components desired in the approximation.</span></div>
<div class="line"><span class="lineno">  226</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  227</span><span class="stringliteral">    init :  {&#39;random&#39;, &#39;nndsvd&#39;, &#39;nndsvda&#39;, &#39;nndsvdar&#39;}, default=None</span></div>
<div class="line"><span class="lineno">  228</span><span class="stringliteral">        Method used to initialize the procedure.</span></div>
<div class="line"><span class="lineno">  229</span><span class="stringliteral">        Valid options:</span></div>
<div class="line"><span class="lineno">  230</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  231</span><span class="stringliteral">        - None: &#39;nndsvda&#39; if n_components &lt;= min(n_samples, n_features),</span></div>
<div class="line"><span class="lineno">  232</span><span class="stringliteral">            otherwise &#39;random&#39;.</span></div>
<div class="line"><span class="lineno">  233</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  234</span><span class="stringliteral">        - &#39;random&#39;: non-negative random matrices, scaled with:</span></div>
<div class="line"><span class="lineno">  235</span><span class="stringliteral">            sqrt(X.mean() / n_components)</span></div>
<div class="line"><span class="lineno">  236</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  237</span><span class="stringliteral">        - &#39;nndsvd&#39;: Nonnegative Double Singular Value Decomposition (NNDSVD)</span></div>
<div class="line"><span class="lineno">  238</span><span class="stringliteral">            initialization (better for sparseness)</span></div>
<div class="line"><span class="lineno">  239</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  240</span><span class="stringliteral">        - &#39;nndsvda&#39;: NNDSVD with zeros filled with the average of X</span></div>
<div class="line"><span class="lineno">  241</span><span class="stringliteral">            (better when sparsity is not desired)</span></div>
<div class="line"><span class="lineno">  242</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  243</span><span class="stringliteral">        - &#39;nndsvdar&#39;: NNDSVD with zeros filled with small random values</span></div>
<div class="line"><span class="lineno">  244</span><span class="stringliteral">            (generally faster, less accurate alternative to NNDSVDa</span></div>
<div class="line"><span class="lineno">  245</span><span class="stringliteral">            for when sparsity is not desired)</span></div>
<div class="line"><span class="lineno">  246</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral">        - &#39;custom&#39;: use custom matrices W and H</span></div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">        .. versionchanged:: 1.1</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral">            When `init=None` and n_components is less than n_samples and n_features</span></div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral">            defaults to `nndsvda` instead of `nndsvd`.</span></div>
<div class="line"><span class="lineno">  252</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  253</span><span class="stringliteral">    eps : float, default=1e-6</span></div>
<div class="line"><span class="lineno">  254</span><span class="stringliteral">        Truncate all values less then this in output to zero.</span></div>
<div class="line"><span class="lineno">  255</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  256</span><span class="stringliteral">    random_state : int, RandomState instance or None, default=None</span></div>
<div class="line"><span class="lineno">  257</span><span class="stringliteral">        Used when ``init`` == &#39;nndsvdar&#39; or &#39;random&#39;. Pass an int for</span></div>
<div class="line"><span class="lineno">  258</span><span class="stringliteral">        reproducible results across multiple function calls.</span></div>
<div class="line"><span class="lineno">  259</span><span class="stringliteral">        See :term:`Glossary &lt;random_state&gt;`.</span></div>
<div class="line"><span class="lineno">  260</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  261</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  262</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  263</span><span class="stringliteral">    W : array-like of shape (n_samples, n_components)</span></div>
<div class="line"><span class="lineno">  264</span><span class="stringliteral">        Initial guesses for solving X ~= WH.</span></div>
<div class="line"><span class="lineno">  265</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  266</span><span class="stringliteral">    H : array-like of shape (n_components, n_features)</span></div>
<div class="line"><span class="lineno">  267</span><span class="stringliteral">        Initial guesses for solving X ~= WH.</span></div>
<div class="line"><span class="lineno">  268</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  269</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  270</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  271</span><span class="stringliteral">    C. Boutsidis, E. Gallopoulos: SVD based initialization: A head start for</span></div>
<div class="line"><span class="lineno">  272</span><span class="stringliteral">    nonnegative matrix factorization - Pattern Recognition, 2008</span></div>
<div class="line"><span class="lineno">  273</span><span class="stringliteral">    http://tinyurl.com/nndsvd</span></div>
<div class="line"><span class="lineno">  274</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  275</span>    check_non_negative(X, <span class="stringliteral">&quot;NMF initialization&quot;</span>)</div>
<div class="line"><span class="lineno">  276</span>    n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno">  277</span> </div>
<div class="line"><span class="lineno">  278</span>    <span class="keywordflow">if</span> (</div>
<div class="line"><span class="lineno">  279</span>        init <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  280</span>        <span class="keywordflow">and</span> init != <span class="stringliteral">&quot;random&quot;</span></div>
<div class="line"><span class="lineno">  281</span>        <span class="keywordflow">and</span> n_components &gt; min(n_samples, n_features)</div>
<div class="line"><span class="lineno">  282</span>    ):</div>
<div class="line"><span class="lineno">  283</span>        <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">  284</span>            <span class="stringliteral">&quot;init = &#39;{}&#39; can only be used when &quot;</span></div>
<div class="line"><span class="lineno">  285</span>            <span class="stringliteral">&quot;n_components &lt;= min(n_samples, n_features)&quot;</span>.format(init)</div>
<div class="line"><span class="lineno">  286</span>        )</div>
<div class="line"><span class="lineno">  287</span> </div>
<div class="line"><span class="lineno">  288</span>    <span class="keywordflow">if</span> init <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  289</span>        <span class="keywordflow">if</span> n_components &lt;= min(n_samples, n_features):</div>
<div class="line"><span class="lineno">  290</span>            init = <span class="stringliteral">&quot;nndsvda&quot;</span></div>
<div class="line"><span class="lineno">  291</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  292</span>            init = <span class="stringliteral">&quot;random&quot;</span></div>
<div class="line"><span class="lineno">  293</span> </div>
<div class="line"><span class="lineno">  294</span>    <span class="comment"># Random initialization</span></div>
<div class="line"><span class="lineno">  295</span>    <span class="keywordflow">if</span> init == <span class="stringliteral">&quot;random&quot;</span>:</div>
<div class="line"><span class="lineno">  296</span>        avg = np.sqrt(X.mean() / n_components)</div>
<div class="line"><span class="lineno">  297</span>        rng = check_random_state(random_state)</div>
<div class="line"><span class="lineno">  298</span>        H = avg * rng.standard_normal(size=(n_components, n_features)).astype(</div>
<div class="line"><span class="lineno">  299</span>            X.dtype, copy=<span class="keyword">False</span></div>
<div class="line"><span class="lineno">  300</span>        )</div>
<div class="line"><span class="lineno">  301</span>        W = avg * rng.standard_normal(size=(n_samples, n_components)).astype(</div>
<div class="line"><span class="lineno">  302</span>            X.dtype, copy=<span class="keyword">False</span></div>
<div class="line"><span class="lineno">  303</span>        )</div>
<div class="line"><span class="lineno">  304</span>        np.abs(H, out=H)</div>
<div class="line"><span class="lineno">  305</span>        np.abs(W, out=W)</div>
<div class="line"><span class="lineno">  306</span>        <span class="keywordflow">return</span> W, H</div>
<div class="line"><span class="lineno">  307</span> </div>
<div class="line"><span class="lineno">  308</span>    <span class="comment"># NNDSVD initialization</span></div>
<div class="line"><span class="lineno">  309</span>    U, S, V = randomized_svd(X, n_components, random_state=random_state)</div>
<div class="line"><span class="lineno">  310</span>    W = np.zeros_like(U)</div>
<div class="line"><span class="lineno">  311</span>    H = np.zeros_like(V)</div>
<div class="line"><span class="lineno">  312</span> </div>
<div class="line"><span class="lineno">  313</span>    <span class="comment"># The leading singular triplet is non-negative</span></div>
<div class="line"><span class="lineno">  314</span>    <span class="comment"># so it can be used as is for initialization.</span></div>
<div class="line"><span class="lineno">  315</span>    W[:, 0] = np.sqrt(S[0]) * np.abs(U[:, 0])</div>
<div class="line"><span class="lineno">  316</span>    H[0, :] = np.sqrt(S[0]) * np.abs(V[0, :])</div>
<div class="line"><span class="lineno">  317</span> </div>
<div class="line"><span class="lineno">  318</span>    <span class="keywordflow">for</span> j <span class="keywordflow">in</span> range(1, n_components):</div>
<div class="line"><span class="lineno">  319</span>        x, y = U[:, j], V[j, :]</div>
<div class="line"><span class="lineno">  320</span> </div>
<div class="line"><span class="lineno">  321</span>        <span class="comment"># extract positive and negative parts of column vectors</span></div>
<div class="line"><span class="lineno">  322</span>        x_p, y_p = np.maximum(x, 0), np.maximum(y, 0)</div>
<div class="line"><span class="lineno">  323</span>        x_n, y_n = np.abs(np.minimum(x, 0)), np.abs(np.minimum(y, 0))</div>
<div class="line"><span class="lineno">  324</span> </div>
<div class="line"><span class="lineno">  325</span>        <span class="comment"># and their norms</span></div>
<div class="line"><span class="lineno">  326</span>        x_p_nrm, y_p_nrm = norm(x_p), norm(y_p)</div>
<div class="line"><span class="lineno">  327</span>        x_n_nrm, y_n_nrm = norm(x_n), norm(y_n)</div>
<div class="line"><span class="lineno">  328</span> </div>
<div class="line"><span class="lineno">  329</span>        m_p, m_n = x_p_nrm * y_p_nrm, x_n_nrm * y_n_nrm</div>
<div class="line"><span class="lineno">  330</span> </div>
<div class="line"><span class="lineno">  331</span>        <span class="comment"># choose update</span></div>
<div class="line"><span class="lineno">  332</span>        <span class="keywordflow">if</span> m_p &gt; m_n:</div>
<div class="line"><span class="lineno">  333</span>            u = x_p / x_p_nrm</div>
<div class="line"><span class="lineno">  334</span>            v = y_p / y_p_nrm</div>
<div class="line"><span class="lineno">  335</span>            sigma = m_p</div>
<div class="line"><span class="lineno">  336</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  337</span>            u = x_n / x_n_nrm</div>
<div class="line"><span class="lineno">  338</span>            v = y_n / y_n_nrm</div>
<div class="line"><span class="lineno">  339</span>            sigma = m_n</div>
<div class="line"><span class="lineno">  340</span> </div>
<div class="line"><span class="lineno">  341</span>        lbd = np.sqrt(S[j] * sigma)</div>
<div class="line"><span class="lineno">  342</span>        W[:, j] = lbd * u</div>
<div class="line"><span class="lineno">  343</span>        H[j, :] = lbd * v</div>
<div class="line"><span class="lineno">  344</span> </div>
<div class="line"><span class="lineno">  345</span>    W[W &lt; eps] = 0</div>
<div class="line"><span class="lineno">  346</span>    H[H &lt; eps] = 0</div>
<div class="line"><span class="lineno">  347</span> </div>
<div class="line"><span class="lineno">  348</span>    <span class="keywordflow">if</span> init == <span class="stringliteral">&quot;nndsvd&quot;</span>:</div>
<div class="line"><span class="lineno">  349</span>        <span class="keywordflow">pass</span></div>
<div class="line"><span class="lineno">  350</span>    <span class="keywordflow">elif</span> init == <span class="stringliteral">&quot;nndsvda&quot;</span>:</div>
<div class="line"><span class="lineno">  351</span>        avg = X.mean()</div>
<div class="line"><span class="lineno">  352</span>        W[W == 0] = avg</div>
<div class="line"><span class="lineno">  353</span>        H[H == 0] = avg</div>
<div class="line"><span class="lineno">  354</span>    <span class="keywordflow">elif</span> init == <span class="stringliteral">&quot;nndsvdar&quot;</span>:</div>
<div class="line"><span class="lineno">  355</span>        rng = check_random_state(random_state)</div>
<div class="line"><span class="lineno">  356</span>        avg = X.mean()</div>
<div class="line"><span class="lineno">  357</span>        W[W == 0] = abs(avg * rng.standard_normal(size=len(W[W == 0])) / 100)</div>
<div class="line"><span class="lineno">  358</span>        H[H == 0] = abs(avg * rng.standard_normal(size=len(H[H == 0])) / 100)</div>
<div class="line"><span class="lineno">  359</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  360</span>        <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">  361</span>            <span class="stringliteral">&quot;Invalid init parameter: got %r instead of one of %r&quot;</span></div>
<div class="line"><span class="lineno">  362</span>            % (init, (<span class="keywordtype">None</span>, <span class="stringliteral">&quot;random&quot;</span>, <span class="stringliteral">&quot;nndsvd&quot;</span>, <span class="stringliteral">&quot;nndsvda&quot;</span>, <span class="stringliteral">&quot;nndsvdar&quot;</span>))</div>
<div class="line"><span class="lineno">  363</span>        )</div>
<div class="line"><span class="lineno">  364</span> </div>
<div class="line"><span class="lineno">  365</span>    <span class="keywordflow">return</span> W, H</div>
<div class="line"><span class="lineno">  366</span> </div>
<div class="line"><span class="lineno">  367</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a755378ea5231288eb0861db8c8b8018b" name="a755378ea5231288eb0861db8c8b8018b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a755378ea5231288eb0861db8c8b8018b">&#9670;&#160;</a></span>_multiplicative_update_h()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._multiplicative_update_h </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>H</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>beta_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1_reg_H</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_H</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>A</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>B</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>rho</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">update H in Multiplicative Update NMF.</pre> <div class="fragment"><div class="line"><span class="lineno">  628</span>):</div>
<div class="line"><span class="lineno">  629</span>    <span class="stringliteral">&quot;&quot;&quot;update H in Multiplicative Update NMF.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  630</span>    <span class="keywordflow">if</span> beta_loss == 2:</div>
<div class="line"><span class="lineno">  631</span>        numerator = safe_sparse_dot(W.T, X)</div>
<div class="line"><span class="lineno">  632</span>        denominator = np.linalg.multi_dot([W.T, W, H])</div>
<div class="line"><span class="lineno">  633</span> </div>
<div class="line"><span class="lineno">  634</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  635</span>        <span class="comment"># Numerator</span></div>
<div class="line"><span class="lineno">  636</span>        WH_safe_X = _special_sparse_dot(W, H, X)</div>
<div class="line"><span class="lineno">  637</span>        <span class="keywordflow">if</span> sp.issparse(X):</div>
<div class="line"><span class="lineno">  638</span>            WH_safe_X_data = WH_safe_X.data</div>
<div class="line"><span class="lineno">  639</span>            X_data = X.data</div>
<div class="line"><span class="lineno">  640</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  641</span>            WH_safe_X_data = WH_safe_X</div>
<div class="line"><span class="lineno">  642</span>            X_data = X</div>
<div class="line"><span class="lineno">  643</span>            <span class="comment"># copy used in the Denominator</span></div>
<div class="line"><span class="lineno">  644</span>            WH = WH_safe_X.copy()</div>
<div class="line"><span class="lineno">  645</span>            <span class="keywordflow">if</span> beta_loss - 1.0 &lt; 0:</div>
<div class="line"><span class="lineno">  646</span>                WH[WH == 0] = EPSILON</div>
<div class="line"><span class="lineno">  647</span> </div>
<div class="line"><span class="lineno">  648</span>        <span class="comment"># to avoid division by zero</span></div>
<div class="line"><span class="lineno">  649</span>        <span class="keywordflow">if</span> beta_loss - 2.0 &lt; 0:</div>
<div class="line"><span class="lineno">  650</span>            WH_safe_X_data[WH_safe_X_data == 0] = EPSILON</div>
<div class="line"><span class="lineno">  651</span> </div>
<div class="line"><span class="lineno">  652</span>        <span class="keywordflow">if</span> beta_loss == 1:</div>
<div class="line"><span class="lineno">  653</span>            np.divide(X_data, WH_safe_X_data, out=WH_safe_X_data)</div>
<div class="line"><span class="lineno">  654</span>        <span class="keywordflow">elif</span> beta_loss == 0:</div>
<div class="line"><span class="lineno">  655</span>            <span class="comment"># speeds up computation time</span></div>
<div class="line"><span class="lineno">  656</span>            <span class="comment"># refer to /numpy/numpy/issues/9363</span></div>
<div class="line"><span class="lineno">  657</span>            WH_safe_X_data **= -1</div>
<div class="line"><span class="lineno">  658</span>            WH_safe_X_data **= 2</div>
<div class="line"><span class="lineno">  659</span>            <span class="comment"># element-wise multiplication</span></div>
<div class="line"><span class="lineno">  660</span>            WH_safe_X_data *= X_data</div>
<div class="line"><span class="lineno">  661</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  662</span>            WH_safe_X_data **= beta_loss - 2</div>
<div class="line"><span class="lineno">  663</span>            <span class="comment"># element-wise multiplication</span></div>
<div class="line"><span class="lineno">  664</span>            WH_safe_X_data *= X_data</div>
<div class="line"><span class="lineno">  665</span> </div>
<div class="line"><span class="lineno">  666</span>        <span class="comment"># here numerator = dot(W.T, (dot(W, H) ** (beta_loss - 2)) * X)</span></div>
<div class="line"><span class="lineno">  667</span>        numerator = safe_sparse_dot(W.T, WH_safe_X)</div>
<div class="line"><span class="lineno">  668</span> </div>
<div class="line"><span class="lineno">  669</span>        <span class="comment"># Denominator</span></div>
<div class="line"><span class="lineno">  670</span>        <span class="keywordflow">if</span> beta_loss == 1:</div>
<div class="line"><span class="lineno">  671</span>            W_sum = np.sum(W, axis=0)  <span class="comment"># shape(n_components, )</span></div>
<div class="line"><span class="lineno">  672</span>            W_sum[W_sum == 0] = 1.0</div>
<div class="line"><span class="lineno">  673</span>            denominator = W_sum[:, np.newaxis]</div>
<div class="line"><span class="lineno">  674</span> </div>
<div class="line"><span class="lineno">  675</span>        <span class="comment"># beta_loss not in (1, 2)</span></div>
<div class="line"><span class="lineno">  676</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  677</span>            <span class="comment"># computation of WtWH = dot(W.T, dot(W, H) ** beta_loss - 1)</span></div>
<div class="line"><span class="lineno">  678</span>            <span class="keywordflow">if</span> sp.issparse(X):</div>
<div class="line"><span class="lineno">  679</span>                <span class="comment"># memory efficient computation</span></div>
<div class="line"><span class="lineno">  680</span>                <span class="comment"># (compute column by column, avoiding the dense matrix WH)</span></div>
<div class="line"><span class="lineno">  681</span>                WtWH = np.empty(H.shape)</div>
<div class="line"><span class="lineno">  682</span>                <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(X.shape[1]):</div>
<div class="line"><span class="lineno">  683</span>                    WHi = np.dot(W, H[:, i])</div>
<div class="line"><span class="lineno">  684</span>                    <span class="keywordflow">if</span> beta_loss - 1 &lt; 0:</div>
<div class="line"><span class="lineno">  685</span>                        WHi[WHi == 0] = EPSILON</div>
<div class="line"><span class="lineno">  686</span>                    WHi **= beta_loss - 1</div>
<div class="line"><span class="lineno">  687</span>                    WtWH[:, i] = np.dot(W.T, WHi)</div>
<div class="line"><span class="lineno">  688</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  689</span>                WH **= beta_loss - 1</div>
<div class="line"><span class="lineno">  690</span>                WtWH = np.dot(W.T, WH)</div>
<div class="line"><span class="lineno">  691</span>            denominator = WtWH</div>
<div class="line"><span class="lineno">  692</span> </div>
<div class="line"><span class="lineno">  693</span>    <span class="comment"># Add L1 and L2 regularization</span></div>
<div class="line"><span class="lineno">  694</span>    <span class="keywordflow">if</span> l1_reg_H &gt; 0:</div>
<div class="line"><span class="lineno">  695</span>        denominator += l1_reg_H</div>
<div class="line"><span class="lineno">  696</span>    <span class="keywordflow">if</span> l2_reg_H &gt; 0:</div>
<div class="line"><span class="lineno">  697</span>        denominator = denominator + l2_reg_H * H</div>
<div class="line"><span class="lineno">  698</span>    denominator[denominator == 0] = EPSILON</div>
<div class="line"><span class="lineno">  699</span> </div>
<div class="line"><span class="lineno">  700</span>    <span class="keywordflow">if</span> A <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> B <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  701</span>        <span class="comment"># Updates for the online nmf</span></div>
<div class="line"><span class="lineno">  702</span>        <span class="keywordflow">if</span> gamma != 1:</div>
<div class="line"><span class="lineno">  703</span>            H **= 1 / gamma</div>
<div class="line"><span class="lineno">  704</span>        numerator *= H</div>
<div class="line"><span class="lineno">  705</span>        A *= rho</div>
<div class="line"><span class="lineno">  706</span>        B *= rho</div>
<div class="line"><span class="lineno">  707</span>        A += numerator</div>
<div class="line"><span class="lineno">  708</span>        B += denominator</div>
<div class="line"><span class="lineno">  709</span>        H = A / B</div>
<div class="line"><span class="lineno">  710</span> </div>
<div class="line"><span class="lineno">  711</span>        <span class="keywordflow">if</span> gamma != 1:</div>
<div class="line"><span class="lineno">  712</span>            H **= gamma</div>
<div class="line"><span class="lineno">  713</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  714</span>        delta_H = numerator</div>
<div class="line"><span class="lineno">  715</span>        delta_H /= denominator</div>
<div class="line"><span class="lineno">  716</span>        <span class="keywordflow">if</span> gamma != 1:</div>
<div class="line"><span class="lineno">  717</span>            delta_H **= gamma</div>
<div class="line"><span class="lineno">  718</span>        H *= delta_H</div>
<div class="line"><span class="lineno">  719</span> </div>
<div class="line"><span class="lineno">  720</span>    <span class="keywordflow">return</span> H</div>
<div class="line"><span class="lineno">  721</span> </div>
<div class="line"><span class="lineno">  722</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aa96c1b1190107de9198dcf265ec2ae07" name="aa96c1b1190107de9198dcf265ec2ae07"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa96c1b1190107de9198dcf265ec2ae07">&#9670;&#160;</a></span>_multiplicative_update_w()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._multiplicative_update_w </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>H</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>beta_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1_reg_W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>H_sum</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>HHt</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>XHt</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>update_H</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Update W in Multiplicative Update NMF.</pre> <div class="fragment"><div class="line"><span class="lineno">  530</span>):</div>
<div class="line"><span class="lineno">  531</span>    <span class="stringliteral">&quot;&quot;&quot;Update W in Multiplicative Update NMF.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  532</span>    <span class="keywordflow">if</span> beta_loss == 2:</div>
<div class="line"><span class="lineno">  533</span>        <span class="comment"># Numerator</span></div>
<div class="line"><span class="lineno">  534</span>        <span class="keywordflow">if</span> XHt <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  535</span>            XHt = safe_sparse_dot(X, H.T)</div>
<div class="line"><span class="lineno">  536</span>        <span class="keywordflow">if</span> update_H:</div>
<div class="line"><span class="lineno">  537</span>            <span class="comment"># avoid a copy of XHt, which will be re-computed (update_H=True)</span></div>
<div class="line"><span class="lineno">  538</span>            numerator = XHt</div>
<div class="line"><span class="lineno">  539</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  540</span>            <span class="comment"># preserve the XHt, which is not re-computed (update_H=False)</span></div>
<div class="line"><span class="lineno">  541</span>            numerator = XHt.copy()</div>
<div class="line"><span class="lineno">  542</span> </div>
<div class="line"><span class="lineno">  543</span>        <span class="comment"># Denominator</span></div>
<div class="line"><span class="lineno">  544</span>        <span class="keywordflow">if</span> HHt <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  545</span>            HHt = np.dot(H, H.T)</div>
<div class="line"><span class="lineno">  546</span>        denominator = np.dot(W, HHt)</div>
<div class="line"><span class="lineno">  547</span> </div>
<div class="line"><span class="lineno">  548</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  549</span>        <span class="comment"># Numerator</span></div>
<div class="line"><span class="lineno">  550</span>        <span class="comment"># if X is sparse, compute WH only where X is non zero</span></div>
<div class="line"><span class="lineno">  551</span>        WH_safe_X = _special_sparse_dot(W, H, X)</div>
<div class="line"><span class="lineno">  552</span>        <span class="keywordflow">if</span> sp.issparse(X):</div>
<div class="line"><span class="lineno">  553</span>            WH_safe_X_data = WH_safe_X.data</div>
<div class="line"><span class="lineno">  554</span>            X_data = X.data</div>
<div class="line"><span class="lineno">  555</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  556</span>            WH_safe_X_data = WH_safe_X</div>
<div class="line"><span class="lineno">  557</span>            X_data = X</div>
<div class="line"><span class="lineno">  558</span>            <span class="comment"># copy used in the Denominator</span></div>
<div class="line"><span class="lineno">  559</span>            WH = WH_safe_X.copy()</div>
<div class="line"><span class="lineno">  560</span>            <span class="keywordflow">if</span> beta_loss - 1.0 &lt; 0:</div>
<div class="line"><span class="lineno">  561</span>                WH[WH == 0] = EPSILON</div>
<div class="line"><span class="lineno">  562</span> </div>
<div class="line"><span class="lineno">  563</span>        <span class="comment"># to avoid taking a negative power of zero</span></div>
<div class="line"><span class="lineno">  564</span>        <span class="keywordflow">if</span> beta_loss - 2.0 &lt; 0:</div>
<div class="line"><span class="lineno">  565</span>            WH_safe_X_data[WH_safe_X_data == 0] = EPSILON</div>
<div class="line"><span class="lineno">  566</span> </div>
<div class="line"><span class="lineno">  567</span>        <span class="keywordflow">if</span> beta_loss == 1:</div>
<div class="line"><span class="lineno">  568</span>            np.divide(X_data, WH_safe_X_data, out=WH_safe_X_data)</div>
<div class="line"><span class="lineno">  569</span>        <span class="keywordflow">elif</span> beta_loss == 0:</div>
<div class="line"><span class="lineno">  570</span>            <span class="comment"># speeds up computation time</span></div>
<div class="line"><span class="lineno">  571</span>            <span class="comment"># refer to /numpy/numpy/issues/9363</span></div>
<div class="line"><span class="lineno">  572</span>            WH_safe_X_data **= -1</div>
<div class="line"><span class="lineno">  573</span>            WH_safe_X_data **= 2</div>
<div class="line"><span class="lineno">  574</span>            <span class="comment"># element-wise multiplication</span></div>
<div class="line"><span class="lineno">  575</span>            WH_safe_X_data *= X_data</div>
<div class="line"><span class="lineno">  576</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  577</span>            WH_safe_X_data **= beta_loss - 2</div>
<div class="line"><span class="lineno">  578</span>            <span class="comment"># element-wise multiplication</span></div>
<div class="line"><span class="lineno">  579</span>            WH_safe_X_data *= X_data</div>
<div class="line"><span class="lineno">  580</span> </div>
<div class="line"><span class="lineno">  581</span>        <span class="comment"># here numerator = dot(X * (dot(W, H) ** (beta_loss - 2)), H.T)</span></div>
<div class="line"><span class="lineno">  582</span>        numerator = safe_sparse_dot(WH_safe_X, H.T)</div>
<div class="line"><span class="lineno">  583</span> </div>
<div class="line"><span class="lineno">  584</span>        <span class="comment"># Denominator</span></div>
<div class="line"><span class="lineno">  585</span>        <span class="keywordflow">if</span> beta_loss == 1:</div>
<div class="line"><span class="lineno">  586</span>            <span class="keywordflow">if</span> H_sum <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  587</span>                H_sum = np.sum(H, axis=1)  <span class="comment"># shape(n_components, )</span></div>
<div class="line"><span class="lineno">  588</span>            denominator = H_sum[np.newaxis, :]</div>
<div class="line"><span class="lineno">  589</span> </div>
<div class="line"><span class="lineno">  590</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  591</span>            <span class="comment"># computation of WHHt = dot(dot(W, H) ** beta_loss - 1, H.T)</span></div>
<div class="line"><span class="lineno">  592</span>            <span class="keywordflow">if</span> sp.issparse(X):</div>
<div class="line"><span class="lineno">  593</span>                <span class="comment"># memory efficient computation</span></div>
<div class="line"><span class="lineno">  594</span>                <span class="comment"># (compute row by row, avoiding the dense matrix WH)</span></div>
<div class="line"><span class="lineno">  595</span>                WHHt = np.empty(W.shape)</div>
<div class="line"><span class="lineno">  596</span>                <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(X.shape[0]):</div>
<div class="line"><span class="lineno">  597</span>                    WHi = np.dot(W[i, :], H)</div>
<div class="line"><span class="lineno">  598</span>                    <span class="keywordflow">if</span> beta_loss - 1 &lt; 0:</div>
<div class="line"><span class="lineno">  599</span>                        WHi[WHi == 0] = EPSILON</div>
<div class="line"><span class="lineno">  600</span>                    WHi **= beta_loss - 1</div>
<div class="line"><span class="lineno">  601</span>                    WHHt[i, :] = np.dot(WHi, H.T)</div>
<div class="line"><span class="lineno">  602</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  603</span>                WH **= beta_loss - 1</div>
<div class="line"><span class="lineno">  604</span>                WHHt = np.dot(WH, H.T)</div>
<div class="line"><span class="lineno">  605</span>            denominator = WHHt</div>
<div class="line"><span class="lineno">  606</span> </div>
<div class="line"><span class="lineno">  607</span>    <span class="comment"># Add L1 and L2 regularization</span></div>
<div class="line"><span class="lineno">  608</span>    <span class="keywordflow">if</span> l1_reg_W &gt; 0:</div>
<div class="line"><span class="lineno">  609</span>        denominator += l1_reg_W</div>
<div class="line"><span class="lineno">  610</span>    <span class="keywordflow">if</span> l2_reg_W &gt; 0:</div>
<div class="line"><span class="lineno">  611</span>        denominator = denominator + l2_reg_W * W</div>
<div class="line"><span class="lineno">  612</span>    denominator[denominator == 0] = EPSILON</div>
<div class="line"><span class="lineno">  613</span> </div>
<div class="line"><span class="lineno">  614</span>    numerator /= denominator</div>
<div class="line"><span class="lineno">  615</span>    delta_W = numerator</div>
<div class="line"><span class="lineno">  616</span> </div>
<div class="line"><span class="lineno">  617</span>    <span class="comment"># gamma is in ]0, 1]</span></div>
<div class="line"><span class="lineno">  618</span>    <span class="keywordflow">if</span> gamma != 1:</div>
<div class="line"><span class="lineno">  619</span>        delta_W **= gamma</div>
<div class="line"><span class="lineno">  620</span> </div>
<div class="line"><span class="lineno">  621</span>    W *= delta_W</div>
<div class="line"><span class="lineno">  622</span> </div>
<div class="line"><span class="lineno">  623</span>    <span class="keywordflow">return</span> W, H_sum, HHt, XHt</div>
<div class="line"><span class="lineno">  624</span> </div>
<div class="line"><span class="lineno">  625</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a8e1e7eda092577e8f9515f13646d4dd3" name="a8e1e7eda092577e8f9515f13646d4dd3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8e1e7eda092577e8f9515f13646d4dd3">&#9670;&#160;</a></span>_special_sparse_dot()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._special_sparse_dot </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>H</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Computes np.dot(W, H), only where X is non zero.</pre> <div class="fragment"><div class="line"><span class="lineno">  184</span><span class="keyword">def </span>_special_sparse_dot(W, H, X):</div>
<div class="line"><span class="lineno">  185</span>    <span class="stringliteral">&quot;&quot;&quot;Computes np.dot(W, H), only where X is non zero.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  186</span>    <span class="keywordflow">if</span> sp.issparse(X):</div>
<div class="line"><span class="lineno">  187</span>        ii, jj = X.nonzero()</div>
<div class="line"><span class="lineno">  188</span>        n_vals = ii.shape[0]</div>
<div class="line"><span class="lineno">  189</span>        dot_vals = np.empty(n_vals)</div>
<div class="line"><span class="lineno">  190</span>        n_components = W.shape[1]</div>
<div class="line"><span class="lineno">  191</span> </div>
<div class="line"><span class="lineno">  192</span>        batch_size = max(n_components, n_vals // n_components)</div>
<div class="line"><span class="lineno">  193</span>        <span class="keywordflow">for</span> start <span class="keywordflow">in</span> range(0, n_vals, batch_size):</div>
<div class="line"><span class="lineno">  194</span>            batch = slice(start, start + batch_size)</div>
<div class="line"><span class="lineno">  195</span>            dot_vals[batch] = np.multiply(W[ii[batch], :], H.T[jj[batch], :]).sum(</div>
<div class="line"><span class="lineno">  196</span>                axis=1</div>
<div class="line"><span class="lineno">  197</span>            )</div>
<div class="line"><span class="lineno">  198</span> </div>
<div class="line"><span class="lineno">  199</span>        WH = sp.coo_matrix((dot_vals, (ii, jj)), shape=X.shape)</div>
<div class="line"><span class="lineno">  200</span>        <span class="keywordflow">return</span> WH.tocsr()</div>
<div class="line"><span class="lineno">  201</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  202</span>        <span class="keywordflow">return</span> np.dot(W, H)</div>
<div class="line"><span class="lineno">  203</span> </div>
<div class="line"><span class="lineno">  204</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a7625681941ef7d3eaa301844b70f6a37" name="a7625681941ef7d3eaa301844b70f6a37"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7625681941ef7d3eaa301844b70f6a37">&#9670;&#160;</a></span>_update_coordinate_descent()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf._update_coordinate_descent </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Ht</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1_reg</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>shuffle</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Helper function for _fit_coordinate_descent.

Update W to minimize the objective function, iterating once over all
coordinates. By symmetry, to update H, one can call
_update_coordinate_descent(X.T, Ht, W, ...).</pre> <div class="fragment"><div class="line"><span class="lineno">  368</span><span class="keyword">def </span>_update_coordinate_descent(X, W, Ht, l1_reg, l2_reg, shuffle, random_state):</div>
<div class="line"><span class="lineno">  369</span>    <span class="stringliteral">&quot;&quot;&quot;Helper function for _fit_coordinate_descent.</span></div>
<div class="line"><span class="lineno">  370</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  371</span><span class="stringliteral">    Update W to minimize the objective function, iterating once over all</span></div>
<div class="line"><span class="lineno">  372</span><span class="stringliteral">    coordinates. By symmetry, to update H, one can call</span></div>
<div class="line"><span class="lineno">  373</span><span class="stringliteral">    _update_coordinate_descent(X.T, Ht, W, ...).</span></div>
<div class="line"><span class="lineno">  374</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  375</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  376</span>    n_components = Ht.shape[1]</div>
<div class="line"><span class="lineno">  377</span> </div>
<div class="line"><span class="lineno">  378</span>    HHt = np.dot(Ht.T, Ht)</div>
<div class="line"><span class="lineno">  379</span>    XHt = safe_sparse_dot(X, Ht)</div>
<div class="line"><span class="lineno">  380</span> </div>
<div class="line"><span class="lineno">  381</span>    <span class="comment"># L2 regularization corresponds to increase of the diagonal of HHt</span></div>
<div class="line"><span class="lineno">  382</span>    <span class="keywordflow">if</span> l2_reg != 0.0:</div>
<div class="line"><span class="lineno">  383</span>        <span class="comment"># adds l2_reg only on the diagonal</span></div>
<div class="line"><span class="lineno">  384</span>        HHt.flat[:: n_components + 1] += l2_reg</div>
<div class="line"><span class="lineno">  385</span>    <span class="comment"># L1 regularization corresponds to decrease of each element of XHt</span></div>
<div class="line"><span class="lineno">  386</span>    <span class="keywordflow">if</span> l1_reg != 0.0:</div>
<div class="line"><span class="lineno">  387</span>        XHt -= l1_reg</div>
<div class="line"><span class="lineno">  388</span> </div>
<div class="line"><span class="lineno">  389</span>    <span class="keywordflow">if</span> shuffle:</div>
<div class="line"><span class="lineno">  390</span>        permutation = random_state.permutation(n_components)</div>
<div class="line"><span class="lineno">  391</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  392</span>        permutation = np.arange(n_components)</div>
<div class="line"><span class="lineno">  393</span>    <span class="comment"># The following seems to be required on 64-bit Windows w/ Python 3.5.</span></div>
<div class="line"><span class="lineno">  394</span>    permutation = np.asarray(permutation, dtype=np.intp)</div>
<div class="line"><span class="lineno">  395</span>    <span class="keywordflow">return</span> _update_cdnmf_fast(W, HHt, XHt, permutation)</div>
<div class="line"><span class="lineno">  396</span> </div>
<div class="line"><span class="lineno">  397</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a1ae7847a21828ee3f94aa00600c57fed" name="a1ae7847a21828ee3f94aa00600c57fed"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1ae7847a21828ee3f94aa00600c57fed">&#9670;&#160;</a></span>non_negative_factorization()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf.non_negative_factorization </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>W</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>H</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_components</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>init</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>update_H</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>solver</em> = <code>&quot;cd&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>beta_loss</em> = <code>&quot;frobenius&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tol</em> = <code>1e-4</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_iter</em> = <code>200</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha_W</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha_H</em> = <code>&quot;same&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1_ratio</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>shuffle</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute Non-negative Matrix Factorization (NMF).

Find two non-negative matrices (W, H) whose product approximates the non-
negative matrix X. This factorization can be used for example for
dimensionality reduction, source separation or topic extraction.

The objective function is:

    .. math::

        L(W, H) &amp;= 0.5 * ||X - WH||_{loss}^2

        &amp;+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1

        &amp;+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1

        &amp;+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2

        &amp;+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2

Where:

:math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)

:math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)

The generic norm :math:`||X - WH||_{loss}^2` may represent
the Frobenius norm or another supported beta-divergence loss.
The choice between options is controlled by the `beta_loss` parameter.

The regularization terms are scaled by `n_features` for `W` and by `n_samples` for
`H` to keep their impact balanced with respect to one another and to the data fit
term as independent as possible of the size `n_samples` of the training set.

The objective function is minimized with an alternating minimization of W
and H. If H is given and update_H=False, it solves for W only.

Note that the transformed data is named W and the components matrix is named H. In
the NMF literature, the naming convention is usually the opposite since the data
matrix X is transposed.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Constant matrix.

W : array-like of shape (n_samples, n_components), default=None
    If init='custom', it is used as initial guess for the solution.

H : array-like of shape (n_components, n_features), default=None
    If init='custom', it is used as initial guess for the solution.
    If update_H=False, it is used as a constant, to solve for W only.

n_components : int, default=None
    Number of components, if n_components is not set all features
    are kept.

init : {'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None
    Method used to initialize the procedure.

    Valid options:

    - None: 'nndsvda' if n_components &lt; n_features, otherwise 'random'.
    - 'random': non-negative random matrices, scaled with:
      `sqrt(X.mean() / n_components)`
    - 'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)
      initialization (better for sparseness)
    - 'nndsvda': NNDSVD with zeros filled with the average of X
      (better when sparsity is not desired)
    - 'nndsvdar': NNDSVD with zeros filled with small random values
      (generally faster, less accurate alternative to NNDSVDa
      for when sparsity is not desired)
    - 'custom': use custom matrices W and H if `update_H=True`. If
      `update_H=False`, then only custom matrix H is used.

    .. versionchanged:: 0.23
        The default value of `init` changed from 'random' to None in 0.23.

    .. versionchanged:: 1.1
        When `init=None` and n_components is less than n_samples and n_features
        defaults to `nndsvda` instead of `nndsvd`.

update_H : bool, default=True
    Set to True, both W and H will be estimated from initial guesses.
    Set to False, only W will be estimated.

solver : {'cd', 'mu'}, default='cd'
    Numerical solver to use:

    - 'cd' is a Coordinate Descent solver that uses Fast Hierarchical
      Alternating Least Squares (Fast HALS).
    - 'mu' is a Multiplicative Update solver.

    .. versionadded:: 0.17
       Coordinate Descent solver.

    .. versionadded:: 0.19
       Multiplicative Update solver.

beta_loss : float or {'frobenius', 'kullback-leibler', \
        'itakura-saito'}, default='frobenius'
    Beta divergence to be minimized, measuring the distance between X
    and the dot product WH. Note that values different from 'frobenius'
    (or 2) and 'kullback-leibler' (or 1) lead to significantly slower
    fits. Note that for beta_loss &lt;= 0 (or 'itakura-saito'), the input
    matrix X cannot contain zeros. Used only in 'mu' solver.

    .. versionadded:: 0.19

tol : float, default=1e-4
    Tolerance of the stopping condition.

max_iter : int, default=200
    Maximum number of iterations before timing out.

alpha_W : float, default=0.0
    Constant that multiplies the regularization terms of `W`. Set it to zero
    (default) to have no regularization on `W`.

    .. versionadded:: 1.0

alpha_H : float or "same", default="same"
    Constant that multiplies the regularization terms of `H`. Set it to zero to
    have no regularization on `H`. If "same" (default), it takes the same value as
    `alpha_W`.

    .. versionadded:: 1.0

l1_ratio : float, default=0.0
    The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
    For l1_ratio = 0 the penalty is an elementwise L2 penalty
    (aka Frobenius Norm).
    For l1_ratio = 1 it is an elementwise L1 penalty.
    For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.

random_state : int, RandomState instance or None, default=None
    Used for NMF initialisation (when ``init`` == 'nndsvdar' or
    'random'), and in Coordinate Descent. Pass an int for reproducible
    results across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

verbose : int, default=0
    The verbosity level.

shuffle : bool, default=False
    If true, randomize the order of coordinates in the CD solver.

Returns
-------
W : ndarray of shape (n_samples, n_components)
    Solution to the non-negative least squares problem.

H : ndarray of shape (n_components, n_features)
    Solution to the non-negative least squares problem.

n_iter : int
    Actual number of iterations.

References
----------
.. [1] :doi:`"Fast local algorithms for large scale nonnegative matrix and tensor
   factorizations" &lt;10.1587/transfun.E92.A.708&gt;`
   Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals
   of electronics, communications and computer sciences 92.3: 708-721, 2009.

.. [2] :doi:`"Algorithms for nonnegative matrix factorization with the
   beta-divergence" &lt;10.1162/NECO_a_00168&gt;`
   Fevotte, C., &amp; Idier, J. (2011). Neural Computation, 23(9).

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
&gt;&gt;&gt; from sklearn.decomposition import non_negative_factorization
&gt;&gt;&gt; W, H, n_iter = non_negative_factorization(
...     X, n_components=2, init='random', random_state=0)
</pre> <div class="fragment"><div class="line"><span class="lineno">  914</span>):</div>
<div class="line"><span class="lineno">  915</span>    <span class="stringliteral">&quot;&quot;&quot;Compute Non-negative Matrix Factorization (NMF).</span></div>
<div class="line"><span class="lineno">  916</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  917</span><span class="stringliteral">    Find two non-negative matrices (W, H) whose product approximates the non-</span></div>
<div class="line"><span class="lineno">  918</span><span class="stringliteral">    negative matrix X. This factorization can be used for example for</span></div>
<div class="line"><span class="lineno">  919</span><span class="stringliteral">    dimensionality reduction, source separation or topic extraction.</span></div>
<div class="line"><span class="lineno">  920</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  921</span><span class="stringliteral">    The objective function is:</span></div>
<div class="line"><span class="lineno">  922</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  923</span><span class="stringliteral">        .. math::</span></div>
<div class="line"><span class="lineno">  924</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  925</span><span class="stringliteral">            L(W, H) &amp;= 0.5 * ||X - WH||_{loss}^2</span></div>
<div class="line"><span class="lineno">  926</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  927</span><span class="stringliteral">            &amp;+ alpha\\_W * l1\\_ratio * n\\_features * ||vec(W)||_1</span></div>
<div class="line"><span class="lineno">  928</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  929</span><span class="stringliteral">            &amp;+ alpha\\_H * l1\\_ratio * n\\_samples * ||vec(H)||_1</span></div>
<div class="line"><span class="lineno">  930</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  931</span><span class="stringliteral">            &amp;+ 0.5 * alpha\\_W * (1 - l1\\_ratio) * n\\_features * ||W||_{Fro}^2</span></div>
<div class="line"><span class="lineno">  932</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  933</span><span class="stringliteral">            &amp;+ 0.5 * alpha\\_H * (1 - l1\\_ratio) * n\\_samples * ||H||_{Fro}^2</span></div>
<div class="line"><span class="lineno">  934</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  935</span><span class="stringliteral">    Where:</span></div>
<div class="line"><span class="lineno">  936</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  937</span><span class="stringliteral">    :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)</span></div>
<div class="line"><span class="lineno">  938</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  939</span><span class="stringliteral">    :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)</span></div>
<div class="line"><span class="lineno">  940</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  941</span><span class="stringliteral">    The generic norm :math:`||X - WH||_{loss}^2` may represent</span></div>
<div class="line"><span class="lineno">  942</span><span class="stringliteral">    the Frobenius norm or another supported beta-divergence loss.</span></div>
<div class="line"><span class="lineno">  943</span><span class="stringliteral">    The choice between options is controlled by the `beta_loss` parameter.</span></div>
<div class="line"><span class="lineno">  944</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  945</span><span class="stringliteral">    The regularization terms are scaled by `n_features` for `W` and by `n_samples` for</span></div>
<div class="line"><span class="lineno">  946</span><span class="stringliteral">    `H` to keep their impact balanced with respect to one another and to the data fit</span></div>
<div class="line"><span class="lineno">  947</span><span class="stringliteral">    term as independent as possible of the size `n_samples` of the training set.</span></div>
<div class="line"><span class="lineno">  948</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  949</span><span class="stringliteral">    The objective function is minimized with an alternating minimization of W</span></div>
<div class="line"><span class="lineno">  950</span><span class="stringliteral">    and H. If H is given and update_H=False, it solves for W only.</span></div>
<div class="line"><span class="lineno">  951</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  952</span><span class="stringliteral">    Note that the transformed data is named W and the components matrix is named H. In</span></div>
<div class="line"><span class="lineno">  953</span><span class="stringliteral">    the NMF literature, the naming convention is usually the opposite since the data</span></div>
<div class="line"><span class="lineno">  954</span><span class="stringliteral">    matrix X is transposed.</span></div>
<div class="line"><span class="lineno">  955</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  956</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  957</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  958</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  959</span><span class="stringliteral">        Constant matrix.</span></div>
<div class="line"><span class="lineno">  960</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  961</span><span class="stringliteral">    W : array-like of shape (n_samples, n_components), default=None</span></div>
<div class="line"><span class="lineno">  962</span><span class="stringliteral">        If init=&#39;custom&#39;, it is used as initial guess for the solution.</span></div>
<div class="line"><span class="lineno">  963</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  964</span><span class="stringliteral">    H : array-like of shape (n_components, n_features), default=None</span></div>
<div class="line"><span class="lineno">  965</span><span class="stringliteral">        If init=&#39;custom&#39;, it is used as initial guess for the solution.</span></div>
<div class="line"><span class="lineno">  966</span><span class="stringliteral">        If update_H=False, it is used as a constant, to solve for W only.</span></div>
<div class="line"><span class="lineno">  967</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  968</span><span class="stringliteral">    n_components : int, default=None</span></div>
<div class="line"><span class="lineno">  969</span><span class="stringliteral">        Number of components, if n_components is not set all features</span></div>
<div class="line"><span class="lineno">  970</span><span class="stringliteral">        are kept.</span></div>
<div class="line"><span class="lineno">  971</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  972</span><span class="stringliteral">    init : {&#39;random&#39;, &#39;nndsvd&#39;, &#39;nndsvda&#39;, &#39;nndsvdar&#39;, &#39;custom&#39;}, default=None</span></div>
<div class="line"><span class="lineno">  973</span><span class="stringliteral">        Method used to initialize the procedure.</span></div>
<div class="line"><span class="lineno">  974</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  975</span><span class="stringliteral">        Valid options:</span></div>
<div class="line"><span class="lineno">  976</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  977</span><span class="stringliteral">        - None: &#39;nndsvda&#39; if n_components &lt; n_features, otherwise &#39;random&#39;.</span></div>
<div class="line"><span class="lineno">  978</span><span class="stringliteral">        - &#39;random&#39;: non-negative random matrices, scaled with:</span></div>
<div class="line"><span class="lineno">  979</span><span class="stringliteral">          `sqrt(X.mean() / n_components)`</span></div>
<div class="line"><span class="lineno">  980</span><span class="stringliteral">        - &#39;nndsvd&#39;: Nonnegative Double Singular Value Decomposition (NNDSVD)</span></div>
<div class="line"><span class="lineno">  981</span><span class="stringliteral">          initialization (better for sparseness)</span></div>
<div class="line"><span class="lineno">  982</span><span class="stringliteral">        - &#39;nndsvda&#39;: NNDSVD with zeros filled with the average of X</span></div>
<div class="line"><span class="lineno">  983</span><span class="stringliteral">          (better when sparsity is not desired)</span></div>
<div class="line"><span class="lineno">  984</span><span class="stringliteral">        - &#39;nndsvdar&#39;: NNDSVD with zeros filled with small random values</span></div>
<div class="line"><span class="lineno">  985</span><span class="stringliteral">          (generally faster, less accurate alternative to NNDSVDa</span></div>
<div class="line"><span class="lineno">  986</span><span class="stringliteral">          for when sparsity is not desired)</span></div>
<div class="line"><span class="lineno">  987</span><span class="stringliteral">        - &#39;custom&#39;: use custom matrices W and H if `update_H=True`. If</span></div>
<div class="line"><span class="lineno">  988</span><span class="stringliteral">          `update_H=False`, then only custom matrix H is used.</span></div>
<div class="line"><span class="lineno">  989</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  990</span><span class="stringliteral">        .. versionchanged:: 0.23</span></div>
<div class="line"><span class="lineno">  991</span><span class="stringliteral">            The default value of `init` changed from &#39;random&#39; to None in 0.23.</span></div>
<div class="line"><span class="lineno">  992</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  993</span><span class="stringliteral">        .. versionchanged:: 1.1</span></div>
<div class="line"><span class="lineno">  994</span><span class="stringliteral">            When `init=None` and n_components is less than n_samples and n_features</span></div>
<div class="line"><span class="lineno">  995</span><span class="stringliteral">            defaults to `nndsvda` instead of `nndsvd`.</span></div>
<div class="line"><span class="lineno">  996</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  997</span><span class="stringliteral">    update_H : bool, default=True</span></div>
<div class="line"><span class="lineno">  998</span><span class="stringliteral">        Set to True, both W and H will be estimated from initial guesses.</span></div>
<div class="line"><span class="lineno">  999</span><span class="stringliteral">        Set to False, only W will be estimated.</span></div>
<div class="line"><span class="lineno"> 1000</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1001</span><span class="stringliteral">    solver : {&#39;cd&#39;, &#39;mu&#39;}, default=&#39;cd&#39;</span></div>
<div class="line"><span class="lineno"> 1002</span><span class="stringliteral">        Numerical solver to use:</span></div>
<div class="line"><span class="lineno"> 1003</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1004</span><span class="stringliteral">        - &#39;cd&#39; is a Coordinate Descent solver that uses Fast Hierarchical</span></div>
<div class="line"><span class="lineno"> 1005</span><span class="stringliteral">          Alternating Least Squares (Fast HALS).</span></div>
<div class="line"><span class="lineno"> 1006</span><span class="stringliteral">        - &#39;mu&#39; is a Multiplicative Update solver.</span></div>
<div class="line"><span class="lineno"> 1007</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1008</span><span class="stringliteral">        .. versionadded:: 0.17</span></div>
<div class="line"><span class="lineno"> 1009</span><span class="stringliteral">           Coordinate Descent solver.</span></div>
<div class="line"><span class="lineno"> 1010</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1011</span><span class="stringliteral">        .. versionadded:: 0.19</span></div>
<div class="line"><span class="lineno"> 1012</span><span class="stringliteral">           Multiplicative Update solver.</span></div>
<div class="line"><span class="lineno"> 1013</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1014</span><span class="stringliteral">    beta_loss : float or {&#39;frobenius&#39;, &#39;kullback-leibler&#39;, \</span></div>
<div class="line"><span class="lineno"> 1015</span><span class="stringliteral">            &#39;itakura-saito&#39;}, default=&#39;frobenius&#39;</span></div>
<div class="line"><span class="lineno"> 1016</span><span class="stringliteral">        Beta divergence to be minimized, measuring the distance between X</span></div>
<div class="line"><span class="lineno"> 1017</span><span class="stringliteral">        and the dot product WH. Note that values different from &#39;frobenius&#39;</span></div>
<div class="line"><span class="lineno"> 1018</span><span class="stringliteral">        (or 2) and &#39;kullback-leibler&#39; (or 1) lead to significantly slower</span></div>
<div class="line"><span class="lineno"> 1019</span><span class="stringliteral">        fits. Note that for beta_loss &lt;= 0 (or &#39;itakura-saito&#39;), the input</span></div>
<div class="line"><span class="lineno"> 1020</span><span class="stringliteral">        matrix X cannot contain zeros. Used only in &#39;mu&#39; solver.</span></div>
<div class="line"><span class="lineno"> 1021</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1022</span><span class="stringliteral">        .. versionadded:: 0.19</span></div>
<div class="line"><span class="lineno"> 1023</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1024</span><span class="stringliteral">    tol : float, default=1e-4</span></div>
<div class="line"><span class="lineno"> 1025</span><span class="stringliteral">        Tolerance of the stopping condition.</span></div>
<div class="line"><span class="lineno"> 1026</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1027</span><span class="stringliteral">    max_iter : int, default=200</span></div>
<div class="line"><span class="lineno"> 1028</span><span class="stringliteral">        Maximum number of iterations before timing out.</span></div>
<div class="line"><span class="lineno"> 1029</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1030</span><span class="stringliteral">    alpha_W : float, default=0.0</span></div>
<div class="line"><span class="lineno"> 1031</span><span class="stringliteral">        Constant that multiplies the regularization terms of `W`. Set it to zero</span></div>
<div class="line"><span class="lineno"> 1032</span><span class="stringliteral">        (default) to have no regularization on `W`.</span></div>
<div class="line"><span class="lineno"> 1033</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1034</span><span class="stringliteral">        .. versionadded:: 1.0</span></div>
<div class="line"><span class="lineno"> 1035</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1036</span><span class="stringliteral">    alpha_H : float or &quot;same&quot;, default=&quot;same&quot;</span></div>
<div class="line"><span class="lineno"> 1037</span><span class="stringliteral">        Constant that multiplies the regularization terms of `H`. Set it to zero to</span></div>
<div class="line"><span class="lineno"> 1038</span><span class="stringliteral">        have no regularization on `H`. If &quot;same&quot; (default), it takes the same value as</span></div>
<div class="line"><span class="lineno"> 1039</span><span class="stringliteral">        `alpha_W`.</span></div>
<div class="line"><span class="lineno"> 1040</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1041</span><span class="stringliteral">        .. versionadded:: 1.0</span></div>
<div class="line"><span class="lineno"> 1042</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1043</span><span class="stringliteral">    l1_ratio : float, default=0.0</span></div>
<div class="line"><span class="lineno"> 1044</span><span class="stringliteral">        The regularization mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.</span></div>
<div class="line"><span class="lineno"> 1045</span><span class="stringliteral">        For l1_ratio = 0 the penalty is an elementwise L2 penalty</span></div>
<div class="line"><span class="lineno"> 1046</span><span class="stringliteral">        (aka Frobenius Norm).</span></div>
<div class="line"><span class="lineno"> 1047</span><span class="stringliteral">        For l1_ratio = 1 it is an elementwise L1 penalty.</span></div>
<div class="line"><span class="lineno"> 1048</span><span class="stringliteral">        For 0 &lt; l1_ratio &lt; 1, the penalty is a combination of L1 and L2.</span></div>
<div class="line"><span class="lineno"> 1049</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1050</span><span class="stringliteral">    random_state : int, RandomState instance or None, default=None</span></div>
<div class="line"><span class="lineno"> 1051</span><span class="stringliteral">        Used for NMF initialisation (when ``init`` == &#39;nndsvdar&#39; or</span></div>
<div class="line"><span class="lineno"> 1052</span><span class="stringliteral">        &#39;random&#39;), and in Coordinate Descent. Pass an int for reproducible</span></div>
<div class="line"><span class="lineno"> 1053</span><span class="stringliteral">        results across multiple function calls.</span></div>
<div class="line"><span class="lineno"> 1054</span><span class="stringliteral">        See :term:`Glossary &lt;random_state&gt;`.</span></div>
<div class="line"><span class="lineno"> 1055</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1056</span><span class="stringliteral">    verbose : int, default=0</span></div>
<div class="line"><span class="lineno"> 1057</span><span class="stringliteral">        The verbosity level.</span></div>
<div class="line"><span class="lineno"> 1058</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1059</span><span class="stringliteral">    shuffle : bool, default=False</span></div>
<div class="line"><span class="lineno"> 1060</span><span class="stringliteral">        If true, randomize the order of coordinates in the CD solver.</span></div>
<div class="line"><span class="lineno"> 1061</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1062</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 1063</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 1064</span><span class="stringliteral">    W : ndarray of shape (n_samples, n_components)</span></div>
<div class="line"><span class="lineno"> 1065</span><span class="stringliteral">        Solution to the non-negative least squares problem.</span></div>
<div class="line"><span class="lineno"> 1066</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1067</span><span class="stringliteral">    H : ndarray of shape (n_components, n_features)</span></div>
<div class="line"><span class="lineno"> 1068</span><span class="stringliteral">        Solution to the non-negative least squares problem.</span></div>
<div class="line"><span class="lineno"> 1069</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1070</span><span class="stringliteral">    n_iter : int</span></div>
<div class="line"><span class="lineno"> 1071</span><span class="stringliteral">        Actual number of iterations.</span></div>
<div class="line"><span class="lineno"> 1072</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1073</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno"> 1074</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 1075</span><span class="stringliteral">    .. [1] :doi:`&quot;Fast local algorithms for large scale nonnegative matrix and tensor</span></div>
<div class="line"><span class="lineno"> 1076</span><span class="stringliteral">       factorizations&quot; &lt;10.1587/transfun.E92.A.708&gt;`</span></div>
<div class="line"><span class="lineno"> 1077</span><span class="stringliteral">       Cichocki, Andrzej, and P. H. A. N. Anh-Huy. IEICE transactions on fundamentals</span></div>
<div class="line"><span class="lineno"> 1078</span><span class="stringliteral">       of electronics, communications and computer sciences 92.3: 708-721, 2009.</span></div>
<div class="line"><span class="lineno"> 1079</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1080</span><span class="stringliteral">    .. [2] :doi:`&quot;Algorithms for nonnegative matrix factorization with the</span></div>
<div class="line"><span class="lineno"> 1081</span><span class="stringliteral">       beta-divergence&quot; &lt;10.1162/NECO_a_00168&gt;`</span></div>
<div class="line"><span class="lineno"> 1082</span><span class="stringliteral">       Fevotte, C., &amp; Idier, J. (2011). Neural Computation, 23(9).</span></div>
<div class="line"><span class="lineno"> 1083</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1084</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno"> 1085</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 1086</span><span class="stringliteral">    &gt;&gt;&gt; import numpy as np</span></div>
<div class="line"><span class="lineno"> 1087</span><span class="stringliteral">    &gt;&gt;&gt; X = np.array([[1,1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])</span></div>
<div class="line"><span class="lineno"> 1088</span><span class="stringliteral">    &gt;&gt;&gt; from sklearn.decomposition import non_negative_factorization</span></div>
<div class="line"><span class="lineno"> 1089</span><span class="stringliteral">    &gt;&gt;&gt; W, H, n_iter = non_negative_factorization(</span></div>
<div class="line"><span class="lineno"> 1090</span><span class="stringliteral">    ...     X, n_components=2, init=&#39;random&#39;, random_state=0)</span></div>
<div class="line"><span class="lineno"> 1091</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1092</span>    est = NMF(</div>
<div class="line"><span class="lineno"> 1093</span>        n_components=n_components,</div>
<div class="line"><span class="lineno"> 1094</span>        init=init,</div>
<div class="line"><span class="lineno"> 1095</span>        solver=solver,</div>
<div class="line"><span class="lineno"> 1096</span>        beta_loss=beta_loss,</div>
<div class="line"><span class="lineno"> 1097</span>        tol=tol,</div>
<div class="line"><span class="lineno"> 1098</span>        max_iter=max_iter,</div>
<div class="line"><span class="lineno"> 1099</span>        random_state=random_state,</div>
<div class="line"><span class="lineno"> 1100</span>        alpha_W=alpha_W,</div>
<div class="line"><span class="lineno"> 1101</span>        alpha_H=alpha_H,</div>
<div class="line"><span class="lineno"> 1102</span>        l1_ratio=l1_ratio,</div>
<div class="line"><span class="lineno"> 1103</span>        verbose=verbose,</div>
<div class="line"><span class="lineno"> 1104</span>        shuffle=shuffle,</div>
<div class="line"><span class="lineno"> 1105</span>    )</div>
<div class="line"><span class="lineno"> 1106</span>    est._validate_params()</div>
<div class="line"><span class="lineno"> 1107</span> </div>
<div class="line"><span class="lineno"> 1108</span>    X = check_array(X, accept_sparse=(<span class="stringliteral">&quot;csr&quot;</span>, <span class="stringliteral">&quot;csc&quot;</span>), dtype=[np.float64, np.float32])</div>
<div class="line"><span class="lineno"> 1109</span> </div>
<div class="line"><span class="lineno"> 1110</span>    <span class="keyword">with</span> config_context(assume_finite=<span class="keyword">True</span>):</div>
<div class="line"><span class="lineno"> 1111</span>        W, H, n_iter = est._fit_transform(X, W=W, H=H, update_H=update_H)</div>
<div class="line"><span class="lineno"> 1112</span> </div>
<div class="line"><span class="lineno"> 1113</span>    <span class="keywordflow">return</span> W, H, n_iter</div>
<div class="line"><span class="lineno"> 1114</span> </div>
<div class="line"><span class="lineno"> 1115</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a6e559cdf3a40c3c84e2f872f8cb79bad" name="a6e559cdf3a40c3c84e2f872f8cb79bad"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6e559cdf3a40c3c84e2f872f8cb79bad">&#9670;&#160;</a></span>norm()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf.norm </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Dot product-based Euclidean norm implementation.

See: http://fa.bianp.net/blog/2011/computing-the-vector-norm/

Parameters
----------
x : array-like
    Vector for which to compute the norm.
</pre> <div class="fragment"><div class="line"><span class="lineno">   39</span><span class="keyword">def </span>norm(x):</div>
<div class="line"><span class="lineno">   40</span>    <span class="stringliteral">&quot;&quot;&quot;Dot product-based Euclidean norm implementation.</span></div>
<div class="line"><span class="lineno">   41</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   42</span><span class="stringliteral">    See: http://fa.bianp.net/blog/2011/computing-the-vector-norm/</span></div>
<div class="line"><span class="lineno">   43</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   44</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   45</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   46</span><span class="stringliteral">    x : array-like</span></div>
<div class="line"><span class="lineno">   47</span><span class="stringliteral">        Vector for which to compute the norm.</span></div>
<div class="line"><span class="lineno">   48</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   49</span>    <span class="keywordflow">return</span> sqrt(squared_norm(x))</div>
<div class="line"><span class="lineno">   50</span> </div>
<div class="line"><span class="lineno">   51</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a4b8fde89e737a10205d4e0480f4b678d" name="a4b8fde89e737a10205d4e0480f4b678d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4b8fde89e737a10205d4e0480f4b678d">&#9670;&#160;</a></span>trace_dot()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf.trace_dot </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Y</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Trace of np.dot(X, Y.T).

Parameters
----------
X : array-like
    First matrix.
Y : array-like
    Second matrix.
</pre> <div class="fragment"><div class="line"><span class="lineno">   52</span><span class="keyword">def </span>trace_dot(X, Y):</div>
<div class="line"><span class="lineno">   53</span>    <span class="stringliteral">&quot;&quot;&quot;Trace of np.dot(X, Y.T).</span></div>
<div class="line"><span class="lineno">   54</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   55</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   56</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   57</span><span class="stringliteral">    X : array-like</span></div>
<div class="line"><span class="lineno">   58</span><span class="stringliteral">        First matrix.</span></div>
<div class="line"><span class="lineno">   59</span><span class="stringliteral">    Y : array-like</span></div>
<div class="line"><span class="lineno">   60</span><span class="stringliteral">        Second matrix.</span></div>
<div class="line"><span class="lineno">   61</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   62</span>    <span class="keywordflow">return</span> np.dot(X.ravel(), Y.ravel())</div>
<div class="line"><span class="lineno">   63</span> </div>
<div class="line"><span class="lineno">   64</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a9bfb8c23341015a3bcd74d3c3f4a363c" name="a9bfb8c23341015a3bcd74d3c3f4a363c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9bfb8c23341015a3bcd74d3c3f4a363c">&#9670;&#160;</a></span>EPSILON</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.decomposition._nmf.EPSILON = np.finfo(np.float32).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
