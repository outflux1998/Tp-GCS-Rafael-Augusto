<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: gensim.models.word2vec Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacegensim.html">gensim</a></li><li class="navelem"><a class="el" href="namespacegensim_1_1models.html">models</a></li><li class="navelem"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html">word2vec</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">gensim.models.word2vec Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1word2vec_1_1_brown_corpus.html">BrownCorpus</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1word2vec_1_1_line_sentence.html">LineSentence</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1word2vec_1_1_path_line_sentences.html">PathLineSentences</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1word2vec_1_1_text8_corpus.html">Text8Corpus</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1word2vec_1_1_word2_vec.html">Word2Vec</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1word2vec_1_1_word2_vec_trainables.html">Word2VecTrainables</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classgensim_1_1models_1_1word2vec_1_1_word2_vec_vocab.html">Word2VecVocab</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:afce5ff823927a826c3d3eaecb865f73e" id="r_afce5ff823927a826c3d3eaecb865f73e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#afce5ff823927a826c3d3eaecb865f73e">train_epoch_sg</a> (<a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a45b862a2ce56cef7ecedf6f2d74269f7">model</a>, corpus_file, <a class="el" href="__lapack__subroutines_8h.html#ac5c1dfc0f77d6570b83bf10cfe850d4e">offset</a>, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)</td></tr>
<tr class="separator:afce5ff823927a826c3d3eaecb865f73e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a62eb4990e3c5e88794946127d8f99d0f" id="r_a62eb4990e3c5e88794946127d8f99d0f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a62eb4990e3c5e88794946127d8f99d0f">train_epoch_cbow</a> (<a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a45b862a2ce56cef7ecedf6f2d74269f7">model</a>, corpus_file, <a class="el" href="__lapack__subroutines_8h.html#ac5c1dfc0f77d6570b83bf10cfe850d4e">offset</a>, _cython_vocab, _cur_epoch, _expected_examples, _expected_words, _work, _neu1, compute_loss)</td></tr>
<tr class="separator:a62eb4990e3c5e88794946127d8f99d0f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a86b83c2ca8d6f19602c036d3a7022488" id="r_a86b83c2ca8d6f19602c036d3a7022488"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a86b83c2ca8d6f19602c036d3a7022488">train_sg_pair</a> (<a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a45b862a2ce56cef7ecedf6f2d74269f7">model</a>, word, context_index, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>, learn_vectors=True, learn_hidden=True, context_vectors=None, context_locks=None, compute_loss=False, is_ft=False)</td></tr>
<tr class="separator:a86b83c2ca8d6f19602c036d3a7022488"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a66cc3ba47d89f96e946369a8abfef7ee" id="r_a66cc3ba47d89f96e946369a8abfef7ee"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a66cc3ba47d89f96e946369a8abfef7ee">train_cbow_pair</a> (<a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a45b862a2ce56cef7ecedf6f2d74269f7">model</a>, word, input_word_indices, l1, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>, learn_vectors=True, learn_hidden=True, compute_loss=False, context_vectors=None, context_locks=None, is_ft=False)</td></tr>
<tr class="separator:a66cc3ba47d89f96e946369a8abfef7ee"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa359d889ce3f8dafc8275865e60858cb" id="r_aa359d889ce3f8dafc8275865e60858cb"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#aa359d889ce3f8dafc8275865e60858cb">score_sg_pair</a> (<a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a45b862a2ce56cef7ecedf6f2d74269f7">model</a>, word, word2)</td></tr>
<tr class="separator:aa359d889ce3f8dafc8275865e60858cb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a018ac6e88aae8cfff2e6405378adbcd7" id="r_a018ac6e88aae8cfff2e6405378adbcd7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a018ac6e88aae8cfff2e6405378adbcd7">score_cbow_pair</a> (<a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a45b862a2ce56cef7ecedf6f2d74269f7">model</a>, word, l1)</td></tr>
<tr class="separator:a018ac6e88aae8cfff2e6405378adbcd7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a332f61160ba060c259cb5a3941cbaa73" id="r_a332f61160ba060c259cb5a3941cbaa73"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a332f61160ba060c259cb5a3941cbaa73">_scan_vocab_worker</a> (stream, progress_queue, max_vocab_size=None, trim_rule=None)</td></tr>
<tr class="separator:a332f61160ba060c259cb5a3941cbaa73"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a694096390ea50f923bd85e875b4d1d10" id="r_a694096390ea50f923bd85e875b4d1d10"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a694096390ea50f923bd85e875b4d1d10">_build_heap</a> (vocab)</td></tr>
<tr class="separator:a694096390ea50f923bd85e875b4d1d10"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a168a67695df8a2c800af1fa4a05de72a" id="r_a168a67695df8a2c800af1fa4a05de72a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a168a67695df8a2c800af1fa4a05de72a">_assign_binary_codes</a> (vocab)</td></tr>
<tr class="separator:a168a67695df8a2c800af1fa4a05de72a"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a4b704d4a8dc8e3898a8c69baffbace0f" id="r_a4b704d4a8dc8e3898a8c69baffbace0f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a4b704d4a8dc8e3898a8c69baffbace0f">logger</a> = logging.getLogger(__name__)</td></tr>
<tr class="separator:a4b704d4a8dc8e3898a8c69baffbace0f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a80a7e892502a81ba32c4cfc93ec7d059" id="r_a80a7e892502a81ba32c4cfc93ec7d059"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#afcae40e2c97fbb41616072e53d221642">int</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a80a7e892502a81ba32c4cfc93ec7d059">CORPUSFILE_VERSION</a> = -1</td></tr>
<tr class="separator:a80a7e892502a81ba32c4cfc93ec7d059"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a74985c2ab1bc5af5777eaa3993826a50" id="r_a74985c2ab1bc5af5777eaa3993826a50"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a74985c2ab1bc5af5777eaa3993826a50">format</a></td></tr>
<tr class="separator:a74985c2ab1bc5af5777eaa3993826a50"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8a6a867cb07f8f85b6a572a15b367689" id="r_a8a6a867cb07f8f85b6a572a15b367689"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a8a6a867cb07f8f85b6a572a15b367689">level</a></td></tr>
<tr class="separator:a8a6a867cb07f8f85b6a572a15b367689"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab26e507875a1da4a0f3628581e5842f2" id="r_ab26e507875a1da4a0f3628581e5842f2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#ab26e507875a1da4a0f3628581e5842f2">program</a> = os.path.basename(sys.argv[0])</td></tr>
<tr class="separator:ab26e507875a1da4a0f3628581e5842f2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a52e7ed29ca2f2cc40bddb3368c75cf4d" id="r_a52e7ed29ca2f2cc40bddb3368c75cf4d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a52e7ed29ca2f2cc40bddb3368c75cf4d">all</a></td></tr>
<tr class="separator:a52e7ed29ca2f2cc40bddb3368c75cf4d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae935aca877d1dff23e4d1433a9e6ee28" id="r_ae935aca877d1dff23e4d1433a9e6ee28"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#ae935aca877d1dff23e4d1433a9e6ee28">parser</a> = argparse.ArgumentParser()</td></tr>
<tr class="separator:ae935aca877d1dff23e4d1433a9e6ee28"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acf5ec38db943f51304ef34ef86a24597" id="r_acf5ec38db943f51304ef34ef86a24597"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#acf5ec38db943f51304ef34ef86a24597">help</a></td></tr>
<tr class="separator:acf5ec38db943f51304ef34ef86a24597"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abcff72c826327b27d4bb80c43e28e262" id="r_abcff72c826327b27d4bb80c43e28e262"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#abcff72c826327b27d4bb80c43e28e262">required</a></td></tr>
<tr class="separator:abcff72c826327b27d4bb80c43e28e262"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4639e260e0ebc4e22425085e431b54d1" id="r_a4639e260e0ebc4e22425085e431b54d1"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a4639e260e0ebc4e22425085e431b54d1">type</a></td></tr>
<tr class="separator:a4639e260e0ebc4e22425085e431b54d1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afcae40e2c97fbb41616072e53d221642" id="r_afcae40e2c97fbb41616072e53d221642"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#afcae40e2c97fbb41616072e53d221642">int</a></td></tr>
<tr class="separator:afcae40e2c97fbb41616072e53d221642"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a73309d21ea8a40c72de81d1e392b25fc" id="r_a73309d21ea8a40c72de81d1e392b25fc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a73309d21ea8a40c72de81d1e392b25fc">default</a></td></tr>
<tr class="separator:a73309d21ea8a40c72de81d1e392b25fc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa11a8022fa912c7e32762d8314cf3000" id="r_aa11a8022fa912c7e32762d8314cf3000"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#aa11a8022fa912c7e32762d8314cf3000">float</a></td></tr>
<tr class="separator:aa11a8022fa912c7e32762d8314cf3000"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6725bc51a56a3dc78b73a2d5b515ac0b" id="r_a6725bc51a56a3dc78b73a2d5b515ac0b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a6725bc51a56a3dc78b73a2d5b515ac0b">choices</a></td></tr>
<tr class="separator:a6725bc51a56a3dc78b73a2d5b515ac0b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a55bfd583221165d36c05d9baed009cce" id="r_a55bfd583221165d36c05d9baed009cce"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a55bfd583221165d36c05d9baed009cce">args</a> = parser.parse_args()</td></tr>
<tr class="separator:a55bfd583221165d36c05d9baed009cce"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8c795c7a9e3b8743738d86224a558221" id="r_a8c795c7a9e3b8743738d86224a558221"><td class="memItemLeft" align="right" valign="top"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#afcae40e2c97fbb41616072e53d221642">int</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a8c795c7a9e3b8743738d86224a558221">skipgram</a> = 1</td></tr>
<tr class="separator:a8c795c7a9e3b8743738d86224a558221"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a25728273e3804cf30fa6888998476b43" id="r_a25728273e3804cf30fa6888998476b43"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a25728273e3804cf30fa6888998476b43">corpus</a> = <a class="el" href="classgensim_1_1models_1_1word2vec_1_1_line_sentence.html">LineSentence</a>(args.train)</td></tr>
<tr class="separator:a25728273e3804cf30fa6888998476b43"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a45b862a2ce56cef7ecedf6f2d74269f7" id="r_a45b862a2ce56cef7ecedf6f2d74269f7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a45b862a2ce56cef7ecedf6f2d74269f7">model</a></td></tr>
<tr class="separator:a45b862a2ce56cef7ecedf6f2d74269f7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab700ff05d63c22834442bf24867db00e" id="r_ab700ff05d63c22834442bf24867db00e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#ab700ff05d63c22834442bf24867db00e">outfile</a> = args.output</td></tr>
<tr class="separator:ab700ff05d63c22834442bf24867db00e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a40324d8b9a2cb8fcaa7a8941a3c50d15" id="r_a40324d8b9a2cb8fcaa7a8941a3c50d15"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#a40324d8b9a2cb8fcaa7a8941a3c50d15">binary</a></td></tr>
<tr class="separator:a40324d8b9a2cb8fcaa7a8941a3c50d15"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">This module implements the word2vec family of algorithms, using highly optimized C routines,
data streaming and Pythonic interfaces.

The word2vec algorithms include skip-gram and CBOW models, using either
hierarchical softmax or negative sampling: `Tomas Mikolov et al: Efficient Estimation of Word Representations
in Vector Space &lt;https://arxiv.org/pdf/1301.3781.pdf&gt;`_, `Tomas Mikolov et al: Distributed Representations of Words
and Phrases and their Compositionality &lt;https://arxiv.org/abs/1310.4546&gt;`_.

Other embeddings
================

There are more ways to train word vectors in Gensim than just Word2Vec.
See also :class:`~gensim.models.doc2vec.Doc2Vec`, :class:`~gensim.models.fasttext.FastText` and
wrappers for :class:`~gensim.models.wrappers.VarEmbed` and :class:`~gensim.models.wrappers.WordRank`.

The training algorithms were originally ported from the C package https://code.google.com/p/word2vec/
and extended with additional functionality and optimizations over the years.

For a tutorial on Gensim word2vec, with an interactive web app trained on GoogleNews,
visit https://rare-technologies.com/word2vec-tutorial/.

**Make sure you have a C compiler before installing Gensim, to use the optimized word2vec routines**
(70x speedup compared to plain NumPy implementation, https://rare-technologies.com/parallelizing-word2vec-in-python/).

Usage examples
==============

Initialize a model with e.g.:

.. sourcecode:: pycon

    &gt;&gt;&gt; from gensim.test.utils import common_texts, get_tmpfile
    &gt;&gt;&gt; from gensim.models import Word2Vec
    &gt;&gt;&gt;
    &gt;&gt;&gt; path = get_tmpfile("word2vec.model")
    &gt;&gt;&gt;
    &gt;&gt;&gt; model = Word2Vec(common_texts, size=100, window=5, min_count=1, workers=4)
    &gt;&gt;&gt; model.save("word2vec.model")

The training is streamed, meaning `sentences` can be a generator, reading input data
from disk on-the-fly, without loading the entire corpus into RAM.

It also means you can continue training the model later:

.. sourcecode:: pycon

    &gt;&gt;&gt; model = Word2Vec.load("word2vec.model")
    &gt;&gt;&gt; model.train([["hello", "world"]], total_examples=1, epochs=1)
    (0, 2)

The trained word vectors are stored in a :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `model.wv`:

.. sourcecode:: pycon

    &gt;&gt;&gt; vector = model.wv['computer']  # numpy vector of a word

The reason for separating the trained vectors into `KeyedVectors` is that if you don't
need the full model state any more (don't need to continue training), the state can discarded,
resulting in a much smaller and faster object that can be mmapped for lightning
fast loading and sharing the vectors in RAM between processes:

.. sourcecode:: pycon

    &gt;&gt;&gt; from gensim.models import KeyedVectors
    &gt;&gt;&gt;
    &gt;&gt;&gt; path = get_tmpfile("wordvectors.kv")
    &gt;&gt;&gt;
    &gt;&gt;&gt; model.wv.save(path)
    &gt;&gt;&gt; wv = KeyedVectors.load("model.wv", mmap='r')
    &gt;&gt;&gt; vector = wv['computer']  # numpy vector of a word

Gensim can also load word vectors in the "word2vec C format", as a
:class:`~gensim.models.keyedvectors.KeyedVectors` instance:

.. sourcecode:: pycon

    &gt;&gt;&gt; from gensim.test.utils import datapath
    &gt;&gt;&gt;
    &gt;&gt;&gt; wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)  # C text format
    &gt;&gt;&gt; wv_from_bin = KeyedVectors.load_word2vec_format(datapath("euclidean_vectors.bin"), binary=True)  # C bin format

It is impossible to continue training the vectors loaded from the C format because the hidden weights,
vocabulary frequencies and the binary tree are missing. To continue training, you'll need the
full :class:`~gensim.models.word2vec.Word2Vec` object state, as stored by :meth:`~gensim.models.word2vec.Word2Vec.save`,
not just the :class:`~gensim.models.keyedvectors.KeyedVectors`.

You can perform various NLP word tasks with a trained model. Some of them
are already built-in - you can see it in :mod:`gensim.models.keyedvectors`.

If you're finished training a model (i.e. no more updates, only querying),
you can switch to the :class:`~gensim.models.keyedvectors.KeyedVectors` instance:

.. sourcecode:: pycon

    &gt;&gt;&gt; word_vectors = model.wv
    &gt;&gt;&gt; del model

to trim unneeded model state = use much less RAM and allow fast loading and memory sharing (mmap).

Note that there is a :mod:`gensim.models.phrases` module which lets you automatically
detect phrases longer than one word. Using phrases, you can learn a word2vec model
where "words" are actually multiword expressions, such as `new_york_times` or `financial_crisis`:

.. sourcecode:: pycon

    &gt;&gt;&gt; from gensim.test.utils import common_texts
    &gt;&gt;&gt; from gensim.models import Phrases
    &gt;&gt;&gt;
    &gt;&gt;&gt; bigram_transformer = Phrases(common_texts)
    &gt;&gt;&gt; model = Word2Vec(bigram_transformer[common_texts], min_count=1)</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="a168a67695df8a2c800af1fa4a05de72a" name="a168a67695df8a2c800af1fa4a05de72a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a168a67695df8a2c800af1fa4a05de72a">&#9670;&#160;</a></span>_assign_binary_codes()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec._assign_binary_codes </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>vocab</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Appends a binary code to each vocab term.

Parameters
----------
vocab : dict
    A dictionary of :class:`gensim.models.word2vec.Vocab` objects.

Notes
-----
Expects each term to have an .index attribute that contains the order in
which the term was added to the vocabulary.  E.g. term.index == 0 means the
term was added to the vocab first.

Sets the .code and .point attributes of each node.
Each code is a numpy.array containing 0s and 1s.
Each point is an integer.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1628</span><span class="keyword">def </span>_assign_binary_codes(vocab):</div>
<div class="line"><span class="lineno"> 1629</span>    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1630</span><span class="stringliteral">    Appends a binary code to each vocab term.</span></div>
<div class="line"><span class="lineno"> 1631</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1632</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 1633</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 1634</span><span class="stringliteral">    vocab : dict</span></div>
<div class="line"><span class="lineno"> 1635</span><span class="stringliteral">        A dictionary of :class:`gensim.models.word2vec.Vocab` objects.</span></div>
<div class="line"><span class="lineno"> 1636</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1637</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno"> 1638</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno"> 1639</span><span class="stringliteral">    Expects each term to have an .index attribute that contains the order in</span></div>
<div class="line"><span class="lineno"> 1640</span><span class="stringliteral">    which the term was added to the vocabulary.  E.g. term.index == 0 means the</span></div>
<div class="line"><span class="lineno"> 1641</span><span class="stringliteral">    term was added to the vocab first.</span></div>
<div class="line"><span class="lineno"> 1642</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1643</span><span class="stringliteral">    Sets the .code and .point attributes of each node.</span></div>
<div class="line"><span class="lineno"> 1644</span><span class="stringliteral">    Each code is a numpy.array containing 0s and 1s.</span></div>
<div class="line"><span class="lineno"> 1645</span><span class="stringliteral">    Each point is an integer.</span></div>
<div class="line"><span class="lineno"> 1646</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1647</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1648</span>    logger.info(<span class="stringliteral">&quot;constructing a huffman tree from %i words&quot;</span>, len(vocab))</div>
<div class="line"><span class="lineno"> 1649</span> </div>
<div class="line"><span class="lineno"> 1650</span>    heap = _build_heap(vocab)</div>
<div class="line"><span class="lineno"> 1651</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> heap:</div>
<div class="line"><span class="lineno"> 1652</span>        <span class="comment">#</span></div>
<div class="line"><span class="lineno"> 1653</span>        <span class="comment"># TODO: how can we end up with an empty heap?</span></div>
<div class="line"><span class="lineno"> 1654</span>        <span class="comment">#</span></div>
<div class="line"><span class="lineno"> 1655</span>        logger.info(<span class="stringliteral">&quot;built huffman tree with maximum node depth 0&quot;</span>)</div>
<div class="line"><span class="lineno"> 1656</span>        <span class="keywordflow">return</span></div>
<div class="line"><span class="lineno"> 1657</span> </div>
<div class="line"><span class="lineno"> 1658</span>    <span class="comment"># recurse over the tree, assigning a binary code to each vocabulary word</span></div>
<div class="line"><span class="lineno"> 1659</span>    max_depth = 0</div>
<div class="line"><span class="lineno"> 1660</span>    stack = [(heap[0], [], [])]</div>
<div class="line"><span class="lineno"> 1661</span>    <span class="keywordflow">while</span> stack:</div>
<div class="line"><span class="lineno"> 1662</span>        node, codes, points = stack.pop()</div>
<div class="line"><span class="lineno"> 1663</span>        <span class="keywordflow">if</span> node.index &lt; len(vocab):</div>
<div class="line"><span class="lineno"> 1664</span>            <span class="comment"># leaf node =&gt; store its path from the root</span></div>
<div class="line"><span class="lineno"> 1665</span>            node.code, node.point = codes, points</div>
<div class="line"><span class="lineno"> 1666</span>            max_depth = max(len(codes), max_depth)</div>
<div class="line"><span class="lineno"> 1667</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1668</span>            <span class="comment"># inner node =&gt; continue recursion</span></div>
<div class="line"><span class="lineno"> 1669</span>            points = array(list(points) + [node.index - len(vocab)], dtype=uint32)</div>
<div class="line"><span class="lineno"> 1670</span>            stack.append((node.left, array(list(codes) + [0], dtype=uint8), points))</div>
<div class="line"><span class="lineno"> 1671</span>            stack.append((node.right, array(list(codes) + [1], dtype=uint8), points))</div>
<div class="line"><span class="lineno"> 1672</span> </div>
<div class="line"><span class="lineno"> 1673</span>    logger.info(<span class="stringliteral">&quot;built huffman tree with maximum node depth %i&quot;</span>, max_depth)</div>
<div class="line"><span class="lineno"> 1674</span> </div>
<div class="line"><span class="lineno"> 1675</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a694096390ea50f923bd85e875b4d1d10" name="a694096390ea50f923bd85e875b4d1d10"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a694096390ea50f923bd85e875b4d1d10">&#9670;&#160;</a></span>_build_heap()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec._build_heap </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>vocab</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1617</span><span class="keyword">def </span>_build_heap(vocab):</div>
<div class="line"><span class="lineno"> 1618</span>    heap = list(itervalues(vocab))</div>
<div class="line"><span class="lineno"> 1619</span>    heapq.heapify(heap)</div>
<div class="line"><span class="lineno"> 1620</span>    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(len(vocab) - 1):</div>
<div class="line"><span class="lineno"> 1621</span>        min1, min2 = heapq.heappop(heap), heapq.heappop(heap)</div>
<div class="line"><span class="lineno"> 1622</span>        heapq.heappush(</div>
<div class="line"><span class="lineno"> 1623</span>            heap, Vocab(count=min1.count + min2.count, index=i + len(vocab), left=min1, right=min2)</div>
<div class="line"><span class="lineno"> 1624</span>        )</div>
<div class="line"><span class="lineno"> 1625</span>    <span class="keywordflow">return</span> heap</div>
<div class="line"><span class="lineno"> 1626</span> </div>
<div class="line"><span class="lineno"> 1627</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a332f61160ba060c259cb5a3941cbaa73" name="a332f61160ba060c259cb5a3941cbaa73"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a332f61160ba060c259cb5a3941cbaa73">&#9670;&#160;</a></span>_scan_vocab_worker()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec._scan_vocab_worker </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>stream</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>progress_queue</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_vocab_size</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>trim_rule</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Do an initial scan of all words appearing in stream.

Note: This function can not be Word2VecVocab's method because
of multiprocessing synchronization specifics in Python.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1317</span><span class="keyword">def </span>_scan_vocab_worker(stream, progress_queue, max_vocab_size=None, trim_rule=None):</div>
<div class="line"><span class="lineno"> 1318</span>    <span class="stringliteral">&quot;&quot;&quot;Do an initial scan of all words appearing in stream.</span></div>
<div class="line"><span class="lineno"> 1319</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1320</span><span class="stringliteral">    Note: This function can not be Word2VecVocab&#39;s method because</span></div>
<div class="line"><span class="lineno"> 1321</span><span class="stringliteral">    of multiprocessing synchronization specifics in Python.</span></div>
<div class="line"><span class="lineno"> 1322</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1323</span>    min_reduce = 1</div>
<div class="line"><span class="lineno"> 1324</span>    vocab = defaultdict(int)</div>
<div class="line"><span class="lineno"> 1325</span>    checked_string_types = 0</div>
<div class="line"><span class="lineno"> 1326</span>    sentence_no = -1</div>
<div class="line"><span class="lineno"> 1327</span>    total_words = 0</div>
<div class="line"><span class="lineno"> 1328</span>    <span class="keywordflow">for</span> sentence_no, sentence <span class="keywordflow">in</span> enumerate(stream):</div>
<div class="line"><span class="lineno"> 1329</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> checked_string_types:</div>
<div class="line"><span class="lineno"> 1330</span>            <span class="keywordflow">if</span> isinstance(sentence, string_types):</div>
<div class="line"><span class="lineno"> 1331</span>                log_msg = <span class="stringliteral">&quot;Each &#39;sentences&#39; item should be a list of words (usually unicode strings). &quot;</span> \</div>
<div class="line"><span class="lineno"> 1332</span>                          <span class="stringliteral">&quot;First item here is instead plain %s.&quot;</span> % type(sentence)</div>
<div class="line"><span class="lineno"> 1333</span>                progress_queue.put(log_msg)</div>
<div class="line"><span class="lineno"> 1334</span> </div>
<div class="line"><span class="lineno"> 1335</span>            checked_string_types += 1</div>
<div class="line"><span class="lineno"> 1336</span> </div>
<div class="line"><span class="lineno"> 1337</span>        <span class="keywordflow">for</span> word <span class="keywordflow">in</span> sentence:</div>
<div class="line"><span class="lineno"> 1338</span>            vocab[word] += 1</div>
<div class="line"><span class="lineno"> 1339</span> </div>
<div class="line"><span class="lineno"> 1340</span>        <span class="keywordflow">if</span> max_vocab_size <span class="keywordflow">and</span> len(vocab) &gt; max_vocab_size:</div>
<div class="line"><span class="lineno"> 1341</span>            utils.prune_vocab(vocab, min_reduce, trim_rule=trim_rule)</div>
<div class="line"><span class="lineno"> 1342</span>            min_reduce += 1</div>
<div class="line"><span class="lineno"> 1343</span> </div>
<div class="line"><span class="lineno"> 1344</span>        total_words += len(sentence)</div>
<div class="line"><span class="lineno"> 1345</span> </div>
<div class="line"><span class="lineno"> 1346</span>    progress_queue.put((total_words, sentence_no + 1))</div>
<div class="line"><span class="lineno"> 1347</span>    progress_queue.put(<span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno"> 1348</span>    <span class="keywordflow">return</span> vocab</div>
<div class="line"><span class="lineno"> 1349</span> </div>
<div class="line"><span class="lineno"> 1350</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a018ac6e88aae8cfff2e6405378adbcd7" name="a018ac6e88aae8cfff2e6405378adbcd7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a018ac6e88aae8cfff2e6405378adbcd7">&#9670;&#160;</a></span>score_cbow_pair()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.score_cbow_pair </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Score the trained CBOW model on a pair of words.

Parameters
----------
model : :class:`~gensim.models.word2vec.Word2Vec`
    The trained model.
word : :class:`~gensim.models.keyedvectors.Vocab`
    Vocabulary representation of the first word.
l1 : list of float
    Vector representation of the second word.

Returns
-------
float
    Logarithm of the sum of exponentiations of input words.</pre> <div class="fragment"><div class="line"><span class="lineno">  419</span><span class="keyword">def </span>score_cbow_pair(model, word, l1):</div>
<div class="line"><span class="lineno">  420</span>    <span class="stringliteral">&quot;&quot;&quot;Score the trained CBOW model on a pair of words.</span></div>
<div class="line"><span class="lineno">  421</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  422</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  423</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  424</span><span class="stringliteral">    model : :class:`~gensim.models.word2vec.Word2Vec`</span></div>
<div class="line"><span class="lineno">  425</span><span class="stringliteral">        The trained model.</span></div>
<div class="line"><span class="lineno">  426</span><span class="stringliteral">    word : :class:`~gensim.models.keyedvectors.Vocab`</span></div>
<div class="line"><span class="lineno">  427</span><span class="stringliteral">        Vocabulary representation of the first word.</span></div>
<div class="line"><span class="lineno">  428</span><span class="stringliteral">    l1 : list of float</span></div>
<div class="line"><span class="lineno">  429</span><span class="stringliteral">        Vector representation of the second word.</span></div>
<div class="line"><span class="lineno">  430</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  431</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  432</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  433</span><span class="stringliteral">    float</span></div>
<div class="line"><span class="lineno">  434</span><span class="stringliteral">        Logarithm of the sum of exponentiations of input words.</span></div>
<div class="line"><span class="lineno">  435</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  436</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  437</span>    l2a = model.syn1[word.point]  <span class="comment"># 2d matrix, codelen x layer1_size</span></div>
<div class="line"><span class="lineno">  438</span>    sgn = (-1.0) ** word.code  <span class="comment"># ch function, 0-&gt; 1, 1 -&gt; -1</span></div>
<div class="line"><span class="lineno">  439</span>    lprob = -logaddexp(0, -sgn * dot(l1, l2a.T))</div>
<div class="line"><span class="lineno">  440</span>    <span class="keywordflow">return</span> sum(lprob)</div>
<div class="line"><span class="lineno">  441</span> </div>
<div class="line"><span class="lineno">  442</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aa359d889ce3f8dafc8275865e60858cb" name="aa359d889ce3f8dafc8275865e60858cb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa359d889ce3f8dafc8275865e60858cb">&#9670;&#160;</a></span>score_sg_pair()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.score_sg_pair </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word2</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Score the trained Skip-gram model on a pair of words.

Parameters
----------
model : :class:`~gensim.models.word2vec.Word2Vec`
    The trained model.
word : :class:`~gensim.models.keyedvectors.Vocab`
    Vocabulary representation of the first word.
word2 : :class:`~gensim.models.keyedvectors.Vocab`
    Vocabulary representation of the second word.

Returns
-------
float
    Logarithm of the sum of exponentiations of input words.</pre> <div class="fragment"><div class="line"><span class="lineno">  394</span><span class="keyword">def </span>score_sg_pair(model, word, word2):</div>
<div class="line"><span class="lineno">  395</span>    <span class="stringliteral">&quot;&quot;&quot;Score the trained Skip-gram model on a pair of words.</span></div>
<div class="line"><span class="lineno">  396</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  397</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  398</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  399</span><span class="stringliteral">    model : :class:`~gensim.models.word2vec.Word2Vec`</span></div>
<div class="line"><span class="lineno">  400</span><span class="stringliteral">        The trained model.</span></div>
<div class="line"><span class="lineno">  401</span><span class="stringliteral">    word : :class:`~gensim.models.keyedvectors.Vocab`</span></div>
<div class="line"><span class="lineno">  402</span><span class="stringliteral">        Vocabulary representation of the first word.</span></div>
<div class="line"><span class="lineno">  403</span><span class="stringliteral">    word2 : :class:`~gensim.models.keyedvectors.Vocab`</span></div>
<div class="line"><span class="lineno">  404</span><span class="stringliteral">        Vocabulary representation of the second word.</span></div>
<div class="line"><span class="lineno">  405</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  406</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  407</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  408</span><span class="stringliteral">    float</span></div>
<div class="line"><span class="lineno">  409</span><span class="stringliteral">        Logarithm of the sum of exponentiations of input words.</span></div>
<div class="line"><span class="lineno">  410</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  411</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  412</span>    l1 = model.wv.syn0[word2.index]</div>
<div class="line"><span class="lineno">  413</span>    l2a = deepcopy(model.syn1[word.point])  <span class="comment"># 2d matrix, codelen x layer1_size</span></div>
<div class="line"><span class="lineno">  414</span>    sgn = (-1.0) ** word.code  <span class="comment"># ch function, 0-&gt; 1, 1 -&gt; -1</span></div>
<div class="line"><span class="lineno">  415</span>    lprob = -logaddexp(0, -sgn * dot(l1, l2a.T))</div>
<div class="line"><span class="lineno">  416</span>    <span class="keywordflow">return</span> sum(lprob)</div>
<div class="line"><span class="lineno">  417</span> </div>
<div class="line"><span class="lineno">  418</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a66cc3ba47d89f96e946369a8abfef7ee" name="a66cc3ba47d89f96e946369a8abfef7ee"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a66cc3ba47d89f96e946369a8abfef7ee">&#9670;&#160;</a></span>train_cbow_pair()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.train_cbow_pair </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>input_word_indices</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learn_vectors</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learn_hidden</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_loss</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_vectors</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_locks</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>is_ft</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Train the passed model instance on a word and its context, using the CBOW algorithm.

Parameters
----------
model : :class:`~gensim.models.word2vec.Word2Vec`
    The model to be trained.
word : str
    The label (predicted) word.
input_word_indices : list of int
    The vocabulary indices of the words in the context.
l1 : list of float
    Vector representation of the label word.
alpha : float
    Learning rate.
learn_vectors : bool, optional
    Whether the vectors should be updated.
learn_hidden : bool, optional
    Whether the weights of the hidden layer should be updated.
compute_loss : bool, optional
    Whether or not the training loss should be computed.
context_vectors : list of list of float, optional
    Vector representations of the words in the context. If None, these will be retrieved from the model.
context_locks : list of float, optional
    The lock factors for each word in the context.
is_ft : bool, optional
    If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`
    instead of `model.wv.syn0`.

Returns
-------
numpy.ndarray
    Error vector to be back-propagated.</pre> <div class="fragment"><div class="line"><span class="lineno">  292</span>                    compute_loss=<span class="keyword">False</span>, context_vectors=<span class="keywordtype">None</span>, context_locks=<span class="keywordtype">None</span>, is_ft=<span class="keyword">False</span>):</div>
<div class="line"><span class="lineno">  293</span>    <span class="stringliteral">&quot;&quot;&quot;Train the passed model instance on a word and its context, using the CBOW algorithm.</span></div>
<div class="line"><span class="lineno">  294</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  295</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  296</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  297</span><span class="stringliteral">    model : :class:`~gensim.models.word2vec.Word2Vec`</span></div>
<div class="line"><span class="lineno">  298</span><span class="stringliteral">        The model to be trained.</span></div>
<div class="line"><span class="lineno">  299</span><span class="stringliteral">    word : str</span></div>
<div class="line"><span class="lineno">  300</span><span class="stringliteral">        The label (predicted) word.</span></div>
<div class="line"><span class="lineno">  301</span><span class="stringliteral">    input_word_indices : list of int</span></div>
<div class="line"><span class="lineno">  302</span><span class="stringliteral">        The vocabulary indices of the words in the context.</span></div>
<div class="line"><span class="lineno">  303</span><span class="stringliteral">    l1 : list of float</span></div>
<div class="line"><span class="lineno">  304</span><span class="stringliteral">        Vector representation of the label word.</span></div>
<div class="line"><span class="lineno">  305</span><span class="stringliteral">    alpha : float</span></div>
<div class="line"><span class="lineno">  306</span><span class="stringliteral">        Learning rate.</span></div>
<div class="line"><span class="lineno">  307</span><span class="stringliteral">    learn_vectors : bool, optional</span></div>
<div class="line"><span class="lineno">  308</span><span class="stringliteral">        Whether the vectors should be updated.</span></div>
<div class="line"><span class="lineno">  309</span><span class="stringliteral">    learn_hidden : bool, optional</span></div>
<div class="line"><span class="lineno">  310</span><span class="stringliteral">        Whether the weights of the hidden layer should be updated.</span></div>
<div class="line"><span class="lineno">  311</span><span class="stringliteral">    compute_loss : bool, optional</span></div>
<div class="line"><span class="lineno">  312</span><span class="stringliteral">        Whether or not the training loss should be computed.</span></div>
<div class="line"><span class="lineno">  313</span><span class="stringliteral">    context_vectors : list of list of float, optional</span></div>
<div class="line"><span class="lineno">  314</span><span class="stringliteral">        Vector representations of the words in the context. If None, these will be retrieved from the model.</span></div>
<div class="line"><span class="lineno">  315</span><span class="stringliteral">    context_locks : list of float, optional</span></div>
<div class="line"><span class="lineno">  316</span><span class="stringliteral">        The lock factors for each word in the context.</span></div>
<div class="line"><span class="lineno">  317</span><span class="stringliteral">    is_ft : bool, optional</span></div>
<div class="line"><span class="lineno">  318</span><span class="stringliteral">        If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`</span></div>
<div class="line"><span class="lineno">  319</span><span class="stringliteral">        instead of `model.wv.syn0`.</span></div>
<div class="line"><span class="lineno">  320</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  321</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  322</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  323</span><span class="stringliteral">    numpy.ndarray</span></div>
<div class="line"><span class="lineno">  324</span><span class="stringliteral">        Error vector to be back-propagated.</span></div>
<div class="line"><span class="lineno">  325</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  326</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  327</span>    <span class="keywordflow">if</span> context_vectors <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  328</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  329</span>            context_vectors_vocab = model.wv.syn0_vocab</div>
<div class="line"><span class="lineno">  330</span>            context_vectors_ngrams = model.wv.syn0_ngrams</div>
<div class="line"><span class="lineno">  331</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  332</span>            context_vectors = model.wv.syn0</div>
<div class="line"><span class="lineno">  333</span>    <span class="keywordflow">if</span> context_locks <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  334</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  335</span>            context_locks_vocab = model.syn0_vocab_lockf</div>
<div class="line"><span class="lineno">  336</span>            context_locks_ngrams = model.syn0_ngrams_lockf</div>
<div class="line"><span class="lineno">  337</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  338</span>            context_locks = model.syn0_lockf</div>
<div class="line"><span class="lineno">  339</span> </div>
<div class="line"><span class="lineno">  340</span>    neu1e = zeros(l1.shape)</div>
<div class="line"><span class="lineno">  341</span> </div>
<div class="line"><span class="lineno">  342</span>    <span class="keywordflow">if</span> model.hs:</div>
<div class="line"><span class="lineno">  343</span>        l2a = model.syn1[word.point]  <span class="comment"># 2d matrix, codelen x layer1_size</span></div>
<div class="line"><span class="lineno">  344</span>        prod_term = dot(l1, l2a.T)</div>
<div class="line"><span class="lineno">  345</span>        fa = expit(prod_term)  <span class="comment"># propagate hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  346</span>        ga = (1. - word.code - fa) * alpha  <span class="comment"># vector of error gradients multiplied by the learning rate</span></div>
<div class="line"><span class="lineno">  347</span>        <span class="keywordflow">if</span> learn_hidden:</div>
<div class="line"><span class="lineno">  348</span>            model.syn1[word.point] += outer(ga, l1)  <span class="comment"># learn hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  349</span>        neu1e += dot(ga, l2a)  <span class="comment"># save error</span></div>
<div class="line"><span class="lineno">  350</span> </div>
<div class="line"><span class="lineno">  351</span>        <span class="comment"># loss component corresponding to hierarchical softmax</span></div>
<div class="line"><span class="lineno">  352</span>        <span class="keywordflow">if</span> compute_loss:</div>
<div class="line"><span class="lineno">  353</span>            sgn = (-1.0) ** word.code  <span class="comment"># ch function, 0-&gt; 1, 1 -&gt; -1</span></div>
<div class="line"><span class="lineno">  354</span>            model.running_training_loss += sum(-log(expit(-sgn * prod_term)))</div>
<div class="line"><span class="lineno">  355</span> </div>
<div class="line"><span class="lineno">  356</span>    <span class="keywordflow">if</span> model.negative:</div>
<div class="line"><span class="lineno">  357</span>        <span class="comment"># use this word (label = 1) + `negative` other random words not from this sentence (label = 0)</span></div>
<div class="line"><span class="lineno">  358</span>        word_indices = [word.index]</div>
<div class="line"><span class="lineno">  359</span>        <span class="keywordflow">while</span> len(word_indices) &lt; model.negative + 1:</div>
<div class="line"><span class="lineno">  360</span>            w = model.cum_table.searchsorted(model.random.randint(model.cum_table[-1]))</div>
<div class="line"><span class="lineno">  361</span>            <span class="keywordflow">if</span> w != word.index:</div>
<div class="line"><span class="lineno">  362</span>                word_indices.append(w)</div>
<div class="line"><span class="lineno">  363</span>        l2b = model.syn1neg[word_indices]  <span class="comment"># 2d matrix, k+1 x layer1_size</span></div>
<div class="line"><span class="lineno">  364</span>        prod_term = dot(l1, l2b.T)</div>
<div class="line"><span class="lineno">  365</span>        fb = expit(prod_term)  <span class="comment"># propagate hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  366</span>        gb = (model.neg_labels - fb) * alpha  <span class="comment"># vector of error gradients multiplied by the learning rate</span></div>
<div class="line"><span class="lineno">  367</span>        <span class="keywordflow">if</span> learn_hidden:</div>
<div class="line"><span class="lineno">  368</span>            model.syn1neg[word_indices] += outer(gb, l1)  <span class="comment"># learn hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  369</span>        neu1e += dot(gb, l2b)  <span class="comment"># save error</span></div>
<div class="line"><span class="lineno">  370</span> </div>
<div class="line"><span class="lineno">  371</span>        <span class="comment"># loss component corresponding to negative sampling</span></div>
<div class="line"><span class="lineno">  372</span>        <span class="keywordflow">if</span> compute_loss:</div>
<div class="line"><span class="lineno">  373</span>            model.running_training_loss -= sum(log(expit(-1 * prod_term[1:])))  <span class="comment"># for the sampled words</span></div>
<div class="line"><span class="lineno">  374</span>            model.running_training_loss -= log(expit(prod_term[0]))  <span class="comment"># for the output word</span></div>
<div class="line"><span class="lineno">  375</span> </div>
<div class="line"><span class="lineno">  376</span>    <span class="keywordflow">if</span> learn_vectors:</div>
<div class="line"><span class="lineno">  377</span>        <span class="comment"># learn input -&gt; hidden, here for all words in the window separately</span></div>
<div class="line"><span class="lineno">  378</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  379</span>            <span class="keywordflow">if</span> <span class="keywordflow">not</span> model.cbow_mean <span class="keywordflow">and</span> input_word_indices:</div>
<div class="line"><span class="lineno">  380</span>                neu1e /= (len(input_word_indices[0]) + len(input_word_indices[1]))</div>
<div class="line"><span class="lineno">  381</span>            <span class="keywordflow">for</span> i <span class="keywordflow">in</span> input_word_indices[0]:</div>
<div class="line"><span class="lineno">  382</span>                context_vectors_vocab[i] += neu1e * context_locks_vocab[i]</div>
<div class="line"><span class="lineno">  383</span>            <span class="keywordflow">for</span> i <span class="keywordflow">in</span> input_word_indices[1]:</div>
<div class="line"><span class="lineno">  384</span>                context_vectors_ngrams[i] += neu1e * context_locks_ngrams[i]</div>
<div class="line"><span class="lineno">  385</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  386</span>            <span class="keywordflow">if</span> <span class="keywordflow">not</span> model.cbow_mean <span class="keywordflow">and</span> input_word_indices:</div>
<div class="line"><span class="lineno">  387</span>                neu1e /= len(input_word_indices)</div>
<div class="line"><span class="lineno">  388</span>            <span class="keywordflow">for</span> i <span class="keywordflow">in</span> input_word_indices:</div>
<div class="line"><span class="lineno">  389</span>                context_vectors[i] += neu1e * context_locks[i]</div>
<div class="line"><span class="lineno">  390</span> </div>
<div class="line"><span class="lineno">  391</span>    <span class="keywordflow">return</span> neu1e</div>
<div class="line"><span class="lineno">  392</span> </div>
<div class="line"><span class="lineno">  393</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a62eb4990e3c5e88794946127d8f99d0f" name="a62eb4990e3c5e88794946127d8f99d0f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a62eb4990e3c5e88794946127d8f99d0f">&#9670;&#160;</a></span>train_epoch_cbow()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.train_epoch_cbow </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>corpus_file</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>offset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_cython_vocab</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_cur_epoch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_expected_examples</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_expected_words</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_work</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_neu1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_loss</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  179</span>                         _work, _neu1, compute_loss):</div>
<div class="line"><span class="lineno">  180</span>        <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&quot;Training with corpus_file argument is not supported&quot;</span>)</div>
<div class="line"><span class="lineno">  181</span> </div>
<div class="line"><span class="lineno">  182</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="afce5ff823927a826c3d3eaecb865f73e" name="afce5ff823927a826c3d3eaecb865f73e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afce5ff823927a826c3d3eaecb865f73e">&#9670;&#160;</a></span>train_epoch_sg()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.train_epoch_sg </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>corpus_file</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>offset</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_cython_vocab</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_cur_epoch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_expected_examples</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_expected_words</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_work</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>_neu1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_loss</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  175</span>                       _work, _neu1, compute_loss):</div>
<div class="line"><span class="lineno">  176</span>        <span class="keywordflow">raise</span> RuntimeError(<span class="stringliteral">&quot;Training with corpus_file argument is not supported&quot;</span>)</div>
<div class="line"><span class="lineno">  177</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a86b83c2ca8d6f19602c036d3a7022488" name="a86b83c2ca8d6f19602c036d3a7022488"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a86b83c2ca8d6f19602c036d3a7022488">&#9670;&#160;</a></span>train_sg_pair()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.train_sg_pair </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>model</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>word</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learn_vectors</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learn_hidden</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_vectors</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>context_locks</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_loss</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>is_ft</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Train the passed model instance on a word and its context, using the Skip-gram algorithm.

Parameters
----------
model : :class:`~gensim.models.word2vec.Word2Vec`
    The model to be trained.
word : str
    The label (predicted) word.
context_index : list of int
    The vocabulary indices of the words in the context.
alpha : float
    Learning rate.
learn_vectors : bool, optional
    Whether the vectors should be updated.
learn_hidden : bool, optional
    Whether the weights of the hidden layer should be updated.
context_vectors : list of list of float, optional
    Vector representations of the words in the context. If None, these will be retrieved from the model.
context_locks : list of float, optional
    The lock factors for each word in the context.
compute_loss : bool, optional
    Whether or not the training loss should be computed.
is_ft : bool, optional
    If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`
    instead of `model.wv.syn0`.

Returns
-------
numpy.ndarray
    Error vector to be back-propagated.</pre> <div class="fragment"><div class="line"><span class="lineno">  184</span>                  context_vectors=<span class="keywordtype">None</span>, context_locks=<span class="keywordtype">None</span>, compute_loss=<span class="keyword">False</span>, is_ft=<span class="keyword">False</span>):</div>
<div class="line"><span class="lineno">  185</span>    <span class="stringliteral">&quot;&quot;&quot;Train the passed model instance on a word and its context, using the Skip-gram algorithm.</span></div>
<div class="line"><span class="lineno">  186</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  187</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  188</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  189</span><span class="stringliteral">    model : :class:`~gensim.models.word2vec.Word2Vec`</span></div>
<div class="line"><span class="lineno">  190</span><span class="stringliteral">        The model to be trained.</span></div>
<div class="line"><span class="lineno">  191</span><span class="stringliteral">    word : str</span></div>
<div class="line"><span class="lineno">  192</span><span class="stringliteral">        The label (predicted) word.</span></div>
<div class="line"><span class="lineno">  193</span><span class="stringliteral">    context_index : list of int</span></div>
<div class="line"><span class="lineno">  194</span><span class="stringliteral">        The vocabulary indices of the words in the context.</span></div>
<div class="line"><span class="lineno">  195</span><span class="stringliteral">    alpha : float</span></div>
<div class="line"><span class="lineno">  196</span><span class="stringliteral">        Learning rate.</span></div>
<div class="line"><span class="lineno">  197</span><span class="stringliteral">    learn_vectors : bool, optional</span></div>
<div class="line"><span class="lineno">  198</span><span class="stringliteral">        Whether the vectors should be updated.</span></div>
<div class="line"><span class="lineno">  199</span><span class="stringliteral">    learn_hidden : bool, optional</span></div>
<div class="line"><span class="lineno">  200</span><span class="stringliteral">        Whether the weights of the hidden layer should be updated.</span></div>
<div class="line"><span class="lineno">  201</span><span class="stringliteral">    context_vectors : list of list of float, optional</span></div>
<div class="line"><span class="lineno">  202</span><span class="stringliteral">        Vector representations of the words in the context. If None, these will be retrieved from the model.</span></div>
<div class="line"><span class="lineno">  203</span><span class="stringliteral">    context_locks : list of float, optional</span></div>
<div class="line"><span class="lineno">  204</span><span class="stringliteral">        The lock factors for each word in the context.</span></div>
<div class="line"><span class="lineno">  205</span><span class="stringliteral">    compute_loss : bool, optional</span></div>
<div class="line"><span class="lineno">  206</span><span class="stringliteral">        Whether or not the training loss should be computed.</span></div>
<div class="line"><span class="lineno">  207</span><span class="stringliteral">    is_ft : bool, optional</span></div>
<div class="line"><span class="lineno">  208</span><span class="stringliteral">        If True, weights will be computed using `model.wv.syn0_vocab` and `model.wv.syn0_ngrams`</span></div>
<div class="line"><span class="lineno">  209</span><span class="stringliteral">        instead of `model.wv.syn0`.</span></div>
<div class="line"><span class="lineno">  210</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  211</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  212</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  213</span><span class="stringliteral">    numpy.ndarray</span></div>
<div class="line"><span class="lineno">  214</span><span class="stringliteral">        Error vector to be back-propagated.</span></div>
<div class="line"><span class="lineno">  215</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  216</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  217</span>    <span class="keywordflow">if</span> context_vectors <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  218</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  219</span>            context_vectors_vocab = model.wv.syn0_vocab</div>
<div class="line"><span class="lineno">  220</span>            context_vectors_ngrams = model.wv.syn0_ngrams</div>
<div class="line"><span class="lineno">  221</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  222</span>            context_vectors = model.wv.syn0</div>
<div class="line"><span class="lineno">  223</span>    <span class="keywordflow">if</span> context_locks <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  224</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  225</span>            context_locks_vocab = model.syn0_vocab_lockf</div>
<div class="line"><span class="lineno">  226</span>            context_locks_ngrams = model.syn0_ngrams_lockf</div>
<div class="line"><span class="lineno">  227</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  228</span>            context_locks = model.syn0_lockf</div>
<div class="line"><span class="lineno">  229</span> </div>
<div class="line"><span class="lineno">  230</span>    <span class="keywordflow">if</span> word <span class="keywordflow">not</span> <span class="keywordflow">in</span> model.wv.vocab:</div>
<div class="line"><span class="lineno">  231</span>        <span class="keywordflow">return</span></div>
<div class="line"><span class="lineno">  232</span>    predict_word = model.wv.vocab[word]  <span class="comment"># target word (NN output)</span></div>
<div class="line"><span class="lineno">  233</span> </div>
<div class="line"><span class="lineno">  234</span>    <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  235</span>        l1_vocab = context_vectors_vocab[context_index[0]]</div>
<div class="line"><span class="lineno">  236</span>        l1_ngrams = np_sum(context_vectors_ngrams[context_index[1:]], axis=0)</div>
<div class="line"><span class="lineno">  237</span>        <span class="keywordflow">if</span> context_index:</div>
<div class="line"><span class="lineno">  238</span>            l1 = np_sum([l1_vocab, l1_ngrams], axis=0) / len(context_index)</div>
<div class="line"><span class="lineno">  239</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  240</span>        l1 = context_vectors[context_index]  <span class="comment"># input word (NN input/projection layer)</span></div>
<div class="line"><span class="lineno">  241</span>        lock_factor = context_locks[context_index]</div>
<div class="line"><span class="lineno">  242</span> </div>
<div class="line"><span class="lineno">  243</span>    neu1e = zeros(l1.shape)</div>
<div class="line"><span class="lineno">  244</span> </div>
<div class="line"><span class="lineno">  245</span>    <span class="keywordflow">if</span> model.hs:</div>
<div class="line"><span class="lineno">  246</span>        <span class="comment"># work on the entire tree at once, to push as much work into numpy&#39;s C routines as possible (performance)</span></div>
<div class="line"><span class="lineno">  247</span>        l2a = deepcopy(model.syn1[predict_word.point])  <span class="comment"># 2d matrix, codelen x layer1_size</span></div>
<div class="line"><span class="lineno">  248</span>        prod_term = dot(l1, l2a.T)</div>
<div class="line"><span class="lineno">  249</span>        fa = expit(prod_term)  <span class="comment"># propagate hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  250</span>        ga = (1 - predict_word.code - fa) * alpha  <span class="comment"># vector of error gradients multiplied by the learning rate</span></div>
<div class="line"><span class="lineno">  251</span>        <span class="keywordflow">if</span> learn_hidden:</div>
<div class="line"><span class="lineno">  252</span>            model.syn1[predict_word.point] += outer(ga, l1)  <span class="comment"># learn hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  253</span>        neu1e += dot(ga, l2a)  <span class="comment"># save error</span></div>
<div class="line"><span class="lineno">  254</span> </div>
<div class="line"><span class="lineno">  255</span>        <span class="comment"># loss component corresponding to hierarchical softmax</span></div>
<div class="line"><span class="lineno">  256</span>        <span class="keywordflow">if</span> compute_loss:</div>
<div class="line"><span class="lineno">  257</span>            sgn = (-1.0) ** predict_word.code  <span class="comment"># `ch` function, 0 -&gt; 1, 1 -&gt; -1</span></div>
<div class="line"><span class="lineno">  258</span>            lprob = -log(expit(-sgn * prod_term))</div>
<div class="line"><span class="lineno">  259</span>            model.running_training_loss += sum(lprob)</div>
<div class="line"><span class="lineno">  260</span> </div>
<div class="line"><span class="lineno">  261</span>    <span class="keywordflow">if</span> model.negative:</div>
<div class="line"><span class="lineno">  262</span>        <span class="comment"># use this word (label = 1) + `negative` other random words not from this sentence (label = 0)</span></div>
<div class="line"><span class="lineno">  263</span>        word_indices = [predict_word.index]</div>
<div class="line"><span class="lineno">  264</span>        <span class="keywordflow">while</span> len(word_indices) &lt; model.negative + 1:</div>
<div class="line"><span class="lineno">  265</span>            w = model.cum_table.searchsorted(model.random.randint(model.cum_table[-1]))</div>
<div class="line"><span class="lineno">  266</span>            <span class="keywordflow">if</span> w != predict_word.index:</div>
<div class="line"><span class="lineno">  267</span>                word_indices.append(w)</div>
<div class="line"><span class="lineno">  268</span>        l2b = model.syn1neg[word_indices]  <span class="comment"># 2d matrix, k+1 x layer1_size</span></div>
<div class="line"><span class="lineno">  269</span>        prod_term = dot(l1, l2b.T)</div>
<div class="line"><span class="lineno">  270</span>        fb = expit(prod_term)  <span class="comment"># propagate hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  271</span>        gb = (model.neg_labels - fb) * alpha  <span class="comment"># vector of error gradients multiplied by the learning rate</span></div>
<div class="line"><span class="lineno">  272</span>        <span class="keywordflow">if</span> learn_hidden:</div>
<div class="line"><span class="lineno">  273</span>            model.syn1neg[word_indices] += outer(gb, l1)  <span class="comment"># learn hidden -&gt; output</span></div>
<div class="line"><span class="lineno">  274</span>        neu1e += dot(gb, l2b)  <span class="comment"># save error</span></div>
<div class="line"><span class="lineno">  275</span> </div>
<div class="line"><span class="lineno">  276</span>        <span class="comment"># loss component corresponding to negative sampling</span></div>
<div class="line"><span class="lineno">  277</span>        <span class="keywordflow">if</span> compute_loss:</div>
<div class="line"><span class="lineno">  278</span>            model.running_training_loss -= sum(log(expit(-1 * prod_term[1:])))  <span class="comment"># for the sampled words</span></div>
<div class="line"><span class="lineno">  279</span>            model.running_training_loss -= log(expit(prod_term[0]))  <span class="comment"># for the output word</span></div>
<div class="line"><span class="lineno">  280</span> </div>
<div class="line"><span class="lineno">  281</span>    <span class="keywordflow">if</span> learn_vectors:</div>
<div class="line"><span class="lineno">  282</span>        <span class="keywordflow">if</span> is_ft:</div>
<div class="line"><span class="lineno">  283</span>            model.wv.syn0_vocab[context_index[0]] += neu1e * context_locks_vocab[context_index[0]]</div>
<div class="line"><span class="lineno">  284</span>            <span class="keywordflow">for</span> i <span class="keywordflow">in</span> context_index[1:]:</div>
<div class="line"><span class="lineno">  285</span>                model.wv.syn0_ngrams[i] += neu1e * context_locks_ngrams[i]</div>
<div class="line"><span class="lineno">  286</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  287</span>            l1 += neu1e * lock_factor  <span class="comment"># learn input -&gt; hidden (mutates model.wv.syn0[word2.index], if that is l1)</span></div>
<div class="line"><span class="lineno">  288</span>    <span class="keywordflow">return</span> neu1e</div>
<div class="line"><span class="lineno">  289</span> </div>
<div class="line"><span class="lineno">  290</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a52e7ed29ca2f2cc40bddb3368c75cf4d" name="a52e7ed29ca2f2cc40bddb3368c75cf4d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a52e7ed29ca2f2cc40bddb3368c75cf4d">&#9670;&#160;</a></span>all</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.all</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a55bfd583221165d36c05d9baed009cce" name="a55bfd583221165d36c05d9baed009cce"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a55bfd583221165d36c05d9baed009cce">&#9670;&#160;</a></span>args</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.args = parser.parse_args()</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a40324d8b9a2cb8fcaa7a8941a3c50d15" name="a40324d8b9a2cb8fcaa7a8941a3c50d15"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a40324d8b9a2cb8fcaa7a8941a3c50d15">&#9670;&#160;</a></span>binary</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.binary</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a6725bc51a56a3dc78b73a2d5b515ac0b" name="a6725bc51a56a3dc78b73a2d5b515ac0b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6725bc51a56a3dc78b73a2d5b515ac0b">&#9670;&#160;</a></span>choices</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.choices</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a25728273e3804cf30fa6888998476b43" name="a25728273e3804cf30fa6888998476b43"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a25728273e3804cf30fa6888998476b43">&#9670;&#160;</a></span>corpus</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.corpus = <a class="el" href="classgensim_1_1models_1_1word2vec_1_1_line_sentence.html">LineSentence</a>(args.train)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a80a7e892502a81ba32c4cfc93ec7d059" name="a80a7e892502a81ba32c4cfc93ec7d059"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a80a7e892502a81ba32c4cfc93ec7d059">&#9670;&#160;</a></span>CORPUSFILE_VERSION</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#afcae40e2c97fbb41616072e53d221642">int</a> gensim.models.word2vec.CORPUSFILE_VERSION = -1</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a73309d21ea8a40c72de81d1e392b25fc" name="a73309d21ea8a40c72de81d1e392b25fc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a73309d21ea8a40c72de81d1e392b25fc">&#9670;&#160;</a></span>default</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.default</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="aa11a8022fa912c7e32762d8314cf3000" name="aa11a8022fa912c7e32762d8314cf3000"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa11a8022fa912c7e32762d8314cf3000">&#9670;&#160;</a></span>float</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.float</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a74985c2ab1bc5af5777eaa3993826a50" name="a74985c2ab1bc5af5777eaa3993826a50"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a74985c2ab1bc5af5777eaa3993826a50">&#9670;&#160;</a></span>format</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.format</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="acf5ec38db943f51304ef34ef86a24597" name="acf5ec38db943f51304ef34ef86a24597"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acf5ec38db943f51304ef34ef86a24597">&#9670;&#160;</a></span>help</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.help</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="afcae40e2c97fbb41616072e53d221642" name="afcae40e2c97fbb41616072e53d221642"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afcae40e2c97fbb41616072e53d221642">&#9670;&#160;</a></span>int</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.int</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a8a6a867cb07f8f85b6a572a15b367689" name="a8a6a867cb07f8f85b6a572a15b367689"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8a6a867cb07f8f85b6a572a15b367689">&#9670;&#160;</a></span>level</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.level</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a4b704d4a8dc8e3898a8c69baffbace0f" name="a4b704d4a8dc8e3898a8c69baffbace0f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4b704d4a8dc8e3898a8c69baffbace0f">&#9670;&#160;</a></span>logger</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.logger = logging.getLogger(__name__)</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a45b862a2ce56cef7ecedf6f2d74269f7" name="a45b862a2ce56cef7ecedf6f2d74269f7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a45b862a2ce56cef7ecedf6f2d74269f7">&#9670;&#160;</a></span>model</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.model</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><span class="lineno">    1</span>=  Word2Vec(</div>
<div class="line"><span class="lineno">    2</span>        corpus, size=args.size, min_count=args.min_count, workers=args.threads,</div>
<div class="line"><span class="lineno">    3</span>        window=args.window, sample=args.sample, sg=skipgram, hs=args.hs,</div>
<div class="line"><span class="lineno">    4</span>        negative=args.negative, cbow_mean=1, iter=args.iter</div>
<div class="line"><span class="lineno">    5</span>    )</div>
</div><!-- fragment -->
</div>
</div>
<a id="ab700ff05d63c22834442bf24867db00e" name="ab700ff05d63c22834442bf24867db00e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab700ff05d63c22834442bf24867db00e">&#9670;&#160;</a></span>outfile</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.outfile = args.output</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ae935aca877d1dff23e4d1433a9e6ee28" name="ae935aca877d1dff23e4d1433a9e6ee28"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae935aca877d1dff23e4d1433a9e6ee28">&#9670;&#160;</a></span>parser</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.parser = argparse.ArgumentParser()</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ab26e507875a1da4a0f3628581e5842f2" name="ab26e507875a1da4a0f3628581e5842f2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab26e507875a1da4a0f3628581e5842f2">&#9670;&#160;</a></span>program</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.program = os.path.basename(sys.argv[0])</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="abcff72c826327b27d4bb80c43e28e262" name="abcff72c826327b27d4bb80c43e28e262"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abcff72c826327b27d4bb80c43e28e262">&#9670;&#160;</a></span>required</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.required</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a8c795c7a9e3b8743738d86224a558221" name="a8c795c7a9e3b8743738d86224a558221"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8c795c7a9e3b8743738d86224a558221">&#9670;&#160;</a></span>skipgram</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="namespacegensim_1_1models_1_1word2vec.html#afcae40e2c97fbb41616072e53d221642">int</a> gensim.models.word2vec.skipgram = 1</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a4639e260e0ebc4e22425085e431b54d1" name="a4639e260e0ebc4e22425085e431b54d1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4639e260e0ebc4e22425085e431b54d1">&#9670;&#160;</a></span>type</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">gensim.models.word2vec.type</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
