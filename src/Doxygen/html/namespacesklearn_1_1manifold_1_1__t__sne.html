<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.manifold._t_sne Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1manifold.html">manifold</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1manifold_1_1__t__sne.html">_t_sne</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">sklearn.manifold._t_sne Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1manifold_1_1__t__sne_1_1_t_s_n_e.html">TSNE</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ad8b7c25c5600bbaa275abe387cef55d3" id="r_ad8b7c25c5600bbaa275abe387cef55d3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1manifold_1_1__t__sne.html#ad8b7c25c5600bbaa275abe387cef55d3">_joint_probabilities</a> (distances, desired_perplexity, verbose)</td></tr>
<tr class="separator:ad8b7c25c5600bbaa275abe387cef55d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae142c57fc0866dacf6978fe62facd79c" id="r_ae142c57fc0866dacf6978fe62facd79c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1manifold_1_1__t__sne.html#ae142c57fc0866dacf6978fe62facd79c">_joint_probabilities_nn</a> (distances, desired_perplexity, verbose)</td></tr>
<tr class="separator:ae142c57fc0866dacf6978fe62facd79c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a26ce7302d5085e587421770508bd7cad" id="r_a26ce7302d5085e587421770508bd7cad"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1manifold_1_1__t__sne.html#a26ce7302d5085e587421770508bd7cad">_kl_divergence</a> (params, P, degrees_of_freedom, n_samples, n_components, skip_num_points=0, compute_error=True)</td></tr>
<tr class="separator:a26ce7302d5085e587421770508bd7cad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a454a53a34098a51ad06ebc4890b7b961" id="r_a454a53a34098a51ad06ebc4890b7b961"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1manifold_1_1__t__sne.html#a454a53a34098a51ad06ebc4890b7b961">_kl_divergence_bh</a> (params, P, degrees_of_freedom, n_samples, n_components, angle=0.5, skip_num_points=0, verbose=False, compute_error=True, num_threads=1)</td></tr>
<tr class="separator:a454a53a34098a51ad06ebc4890b7b961"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aacb9aeeb7263e40fbada7b88a0130351" id="r_aacb9aeeb7263e40fbada7b88a0130351"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1manifold_1_1__t__sne.html#aacb9aeeb7263e40fbada7b88a0130351">_gradient_descent</a> (objective, p0, it, n_iter, n_iter_check=1, n_iter_without_progress=300, momentum=0.8, learning_rate=200.0, min_gain=0.01, min_grad_norm=1e-7, verbose=0, args=None, kwargs=None)</td></tr>
<tr class="separator:aacb9aeeb7263e40fbada7b88a0130351"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1946a3f79fae0098860ec8b6aa8d97c9" id="r_a1946a3f79fae0098860ec8b6aa8d97c9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1manifold_1_1__t__sne.html#a1946a3f79fae0098860ec8b6aa8d97c9">trustworthiness</a> (X, X_embedded, *n_neighbors=5, metric=&quot;euclidean&quot;)</td></tr>
<tr class="separator:a1946a3f79fae0098860ec8b6aa8d97c9"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a996e540cc45c8dd2e580bd50e79425c3" id="r_a996e540cc45c8dd2e580bd50e79425c3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1manifold_1_1__t__sne.html#a996e540cc45c8dd2e580bd50e79425c3">MACHINE_EPSILON</a> = np.finfo(np.double).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a></td></tr>
<tr class="separator:a996e540cc45c8dd2e580bd50e79425c3"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Function Documentation</h2>
<a id="aacb9aeeb7263e40fbada7b88a0130351" name="aacb9aeeb7263e40fbada7b88a0130351"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aacb9aeeb7263e40fbada7b88a0130351">&#9670;&#160;</a></span>_gradient_descent()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.manifold._t_sne._gradient_descent </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>objective</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>p0</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>it</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_iter</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_iter_check</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_iter_without_progress</em> = <code>300</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>momentum</em> = <code>0.8</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>learning_rate</em> = <code>200.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>min_gain</em> = <code>0.01</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>min_grad_norm</em> = <code>1e-7</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>args</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>kwargs</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Batch gradient descent with momentum and individual gains.

Parameters
----------
objective : callable
    Should return a tuple of cost and gradient for a given parameter
    vector. When expensive to compute, the cost can optionally
    be None and can be computed every n_iter_check steps using
    the objective_error function.

p0 : array-like of shape (n_params,)
    Initial parameter vector.

it : int
    Current number of iterations (this function will be called more than
    once during the optimization).

n_iter : int
    Maximum number of gradient descent iterations.

n_iter_check : int, default=1
    Number of iterations before evaluating the global error. If the error
    is sufficiently low, we abort the optimization.

n_iter_without_progress : int, default=300
    Maximum number of iterations without progress before we abort the
    optimization.

momentum : float within (0.0, 1.0), default=0.8
    The momentum generates a weight for previous gradients that decays
    exponentially.

learning_rate : float, default=200.0
    The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If
    the learning rate is too high, the data may look like a 'ball' with any
    point approximately equidistant from its nearest neighbours. If the
    learning rate is too low, most points may look compressed in a dense
    cloud with few outliers.

min_gain : float, default=0.01
    Minimum individual gain for each parameter.

min_grad_norm : float, default=1e-7
    If the gradient norm is below this threshold, the optimization will
    be aborted.

verbose : int, default=0
    Verbosity level.

args : sequence, default=None
    Arguments to pass to objective function.

kwargs : dict, default=None
    Keyword arguments to pass to objective function.

Returns
-------
p : ndarray of shape (n_params,)
    Optimum parameters.

error : float
    Optimum.

i : int
    Last iteration.
</pre> <div class="fragment"><div class="line"><span class="lineno">  314</span>):</div>
<div class="line"><span class="lineno">  315</span>    <span class="stringliteral">&quot;&quot;&quot;Batch gradient descent with momentum and individual gains.</span></div>
<div class="line"><span class="lineno">  316</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  317</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  318</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  319</span><span class="stringliteral">    objective : callable</span></div>
<div class="line"><span class="lineno">  320</span><span class="stringliteral">        Should return a tuple of cost and gradient for a given parameter</span></div>
<div class="line"><span class="lineno">  321</span><span class="stringliteral">        vector. When expensive to compute, the cost can optionally</span></div>
<div class="line"><span class="lineno">  322</span><span class="stringliteral">        be None and can be computed every n_iter_check steps using</span></div>
<div class="line"><span class="lineno">  323</span><span class="stringliteral">        the objective_error function.</span></div>
<div class="line"><span class="lineno">  324</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  325</span><span class="stringliteral">    p0 : array-like of shape (n_params,)</span></div>
<div class="line"><span class="lineno">  326</span><span class="stringliteral">        Initial parameter vector.</span></div>
<div class="line"><span class="lineno">  327</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  328</span><span class="stringliteral">    it : int</span></div>
<div class="line"><span class="lineno">  329</span><span class="stringliteral">        Current number of iterations (this function will be called more than</span></div>
<div class="line"><span class="lineno">  330</span><span class="stringliteral">        once during the optimization).</span></div>
<div class="line"><span class="lineno">  331</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  332</span><span class="stringliteral">    n_iter : int</span></div>
<div class="line"><span class="lineno">  333</span><span class="stringliteral">        Maximum number of gradient descent iterations.</span></div>
<div class="line"><span class="lineno">  334</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  335</span><span class="stringliteral">    n_iter_check : int, default=1</span></div>
<div class="line"><span class="lineno">  336</span><span class="stringliteral">        Number of iterations before evaluating the global error. If the error</span></div>
<div class="line"><span class="lineno">  337</span><span class="stringliteral">        is sufficiently low, we abort the optimization.</span></div>
<div class="line"><span class="lineno">  338</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  339</span><span class="stringliteral">    n_iter_without_progress : int, default=300</span></div>
<div class="line"><span class="lineno">  340</span><span class="stringliteral">        Maximum number of iterations without progress before we abort the</span></div>
<div class="line"><span class="lineno">  341</span><span class="stringliteral">        optimization.</span></div>
<div class="line"><span class="lineno">  342</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  343</span><span class="stringliteral">    momentum : float within (0.0, 1.0), default=0.8</span></div>
<div class="line"><span class="lineno">  344</span><span class="stringliteral">        The momentum generates a weight for previous gradients that decays</span></div>
<div class="line"><span class="lineno">  345</span><span class="stringliteral">        exponentially.</span></div>
<div class="line"><span class="lineno">  346</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  347</span><span class="stringliteral">    learning_rate : float, default=200.0</span></div>
<div class="line"><span class="lineno">  348</span><span class="stringliteral">        The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If</span></div>
<div class="line"><span class="lineno">  349</span><span class="stringliteral">        the learning rate is too high, the data may look like a &#39;ball&#39; with any</span></div>
<div class="line"><span class="lineno">  350</span><span class="stringliteral">        point approximately equidistant from its nearest neighbours. If the</span></div>
<div class="line"><span class="lineno">  351</span><span class="stringliteral">        learning rate is too low, most points may look compressed in a dense</span></div>
<div class="line"><span class="lineno">  352</span><span class="stringliteral">        cloud with few outliers.</span></div>
<div class="line"><span class="lineno">  353</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  354</span><span class="stringliteral">    min_gain : float, default=0.01</span></div>
<div class="line"><span class="lineno">  355</span><span class="stringliteral">        Minimum individual gain for each parameter.</span></div>
<div class="line"><span class="lineno">  356</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  357</span><span class="stringliteral">    min_grad_norm : float, default=1e-7</span></div>
<div class="line"><span class="lineno">  358</span><span class="stringliteral">        If the gradient norm is below this threshold, the optimization will</span></div>
<div class="line"><span class="lineno">  359</span><span class="stringliteral">        be aborted.</span></div>
<div class="line"><span class="lineno">  360</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  361</span><span class="stringliteral">    verbose : int, default=0</span></div>
<div class="line"><span class="lineno">  362</span><span class="stringliteral">        Verbosity level.</span></div>
<div class="line"><span class="lineno">  363</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  364</span><span class="stringliteral">    args : sequence, default=None</span></div>
<div class="line"><span class="lineno">  365</span><span class="stringliteral">        Arguments to pass to objective function.</span></div>
<div class="line"><span class="lineno">  366</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  367</span><span class="stringliteral">    kwargs : dict, default=None</span></div>
<div class="line"><span class="lineno">  368</span><span class="stringliteral">        Keyword arguments to pass to objective function.</span></div>
<div class="line"><span class="lineno">  369</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  370</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  371</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  372</span><span class="stringliteral">    p : ndarray of shape (n_params,)</span></div>
<div class="line"><span class="lineno">  373</span><span class="stringliteral">        Optimum parameters.</span></div>
<div class="line"><span class="lineno">  374</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  375</span><span class="stringliteral">    error : float</span></div>
<div class="line"><span class="lineno">  376</span><span class="stringliteral">        Optimum.</span></div>
<div class="line"><span class="lineno">  377</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  378</span><span class="stringliteral">    i : int</span></div>
<div class="line"><span class="lineno">  379</span><span class="stringliteral">        Last iteration.</span></div>
<div class="line"><span class="lineno">  380</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  381</span>    <span class="keywordflow">if</span> args <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  382</span>        args = []</div>
<div class="line"><span class="lineno">  383</span>    <span class="keywordflow">if</span> kwargs <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  384</span>        kwargs = {}</div>
<div class="line"><span class="lineno">  385</span> </div>
<div class="line"><span class="lineno">  386</span>    p = p0.copy().ravel()</div>
<div class="line"><span class="lineno">  387</span>    update = np.zeros_like(p)</div>
<div class="line"><span class="lineno">  388</span>    gains = np.ones_like(p)</div>
<div class="line"><span class="lineno">  389</span>    error = np.finfo(float).max</div>
<div class="line"><span class="lineno">  390</span>    best_error = np.finfo(float).max</div>
<div class="line"><span class="lineno">  391</span>    best_iter = i = it</div>
<div class="line"><span class="lineno">  392</span> </div>
<div class="line"><span class="lineno">  393</span>    tic = time()</div>
<div class="line"><span class="lineno">  394</span>    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(it, n_iter):</div>
<div class="line"><span class="lineno">  395</span>        check_convergence = (i + 1) % n_iter_check == 0</div>
<div class="line"><span class="lineno">  396</span>        <span class="comment"># only compute the error when needed</span></div>
<div class="line"><span class="lineno">  397</span>        kwargs[<span class="stringliteral">&quot;compute_error&quot;</span>] = check_convergence <span class="keywordflow">or</span> i == n_iter - 1</div>
<div class="line"><span class="lineno">  398</span> </div>
<div class="line"><span class="lineno">  399</span>        error, grad = objective(p, *args, **kwargs)</div>
<div class="line"><span class="lineno">  400</span> </div>
<div class="line"><span class="lineno">  401</span>        inc = update * grad &lt; 0.0</div>
<div class="line"><span class="lineno">  402</span>        dec = np.invert(inc)</div>
<div class="line"><span class="lineno">  403</span>        gains[inc] += 0.2</div>
<div class="line"><span class="lineno">  404</span>        gains[dec] *= 0.8</div>
<div class="line"><span class="lineno">  405</span>        np.clip(gains, min_gain, np.inf, out=gains)</div>
<div class="line"><span class="lineno">  406</span>        grad *= gains</div>
<div class="line"><span class="lineno">  407</span>        update = momentum * update - learning_rate * grad</div>
<div class="line"><span class="lineno">  408</span>        p += update</div>
<div class="line"><span class="lineno">  409</span> </div>
<div class="line"><span class="lineno">  410</span>        <span class="keywordflow">if</span> check_convergence:</div>
<div class="line"><span class="lineno">  411</span>            toc = time()</div>
<div class="line"><span class="lineno">  412</span>            duration = toc - tic</div>
<div class="line"><span class="lineno">  413</span>            tic = toc</div>
<div class="line"><span class="lineno">  414</span>            grad_norm = linalg.norm(grad)</div>
<div class="line"><span class="lineno">  415</span> </div>
<div class="line"><span class="lineno">  416</span>            <span class="keywordflow">if</span> verbose &gt;= 2:</div>
<div class="line"><span class="lineno">  417</span>                print(</div>
<div class="line"><span class="lineno">  418</span>                    <span class="stringliteral">&quot;[t-SNE] Iteration %d: error = %.7f,&quot;</span></div>
<div class="line"><span class="lineno">  419</span>                    <span class="stringliteral">&quot; gradient norm = %.7f&quot;</span></div>
<div class="line"><span class="lineno">  420</span>                    <span class="stringliteral">&quot; (%s iterations in %0.3fs)&quot;</span></div>
<div class="line"><span class="lineno">  421</span>                    % (i + 1, error, grad_norm, n_iter_check, duration)</div>
<div class="line"><span class="lineno">  422</span>                )</div>
<div class="line"><span class="lineno">  423</span> </div>
<div class="line"><span class="lineno">  424</span>            <span class="keywordflow">if</span> error &lt; best_error:</div>
<div class="line"><span class="lineno">  425</span>                best_error = error</div>
<div class="line"><span class="lineno">  426</span>                best_iter = i</div>
<div class="line"><span class="lineno">  427</span>            <span class="keywordflow">elif</span> i - best_iter &gt; n_iter_without_progress:</div>
<div class="line"><span class="lineno">  428</span>                <span class="keywordflow">if</span> verbose &gt;= 2:</div>
<div class="line"><span class="lineno">  429</span>                    print(</div>
<div class="line"><span class="lineno">  430</span>                        <span class="stringliteral">&quot;[t-SNE] Iteration %d: did not make any progress &quot;</span></div>
<div class="line"><span class="lineno">  431</span>                        <span class="stringliteral">&quot;during the last %d episodes. Finished.&quot;</span></div>
<div class="line"><span class="lineno">  432</span>                        % (i + 1, n_iter_without_progress)</div>
<div class="line"><span class="lineno">  433</span>                    )</div>
<div class="line"><span class="lineno">  434</span>                <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  435</span>            <span class="keywordflow">if</span> grad_norm &lt;= min_grad_norm:</div>
<div class="line"><span class="lineno">  436</span>                <span class="keywordflow">if</span> verbose &gt;= 2:</div>
<div class="line"><span class="lineno">  437</span>                    print(</div>
<div class="line"><span class="lineno">  438</span>                        <span class="stringliteral">&quot;[t-SNE] Iteration %d: gradient norm %f. Finished.&quot;</span></div>
<div class="line"><span class="lineno">  439</span>                        % (i + 1, grad_norm)</div>
<div class="line"><span class="lineno">  440</span>                    )</div>
<div class="line"><span class="lineno">  441</span>                <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  442</span> </div>
<div class="line"><span class="lineno">  443</span>    <span class="keywordflow">return</span> p, error, i</div>
<div class="line"><span class="lineno">  444</span> </div>
<div class="line"><span class="lineno">  445</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad8b7c25c5600bbaa275abe387cef55d3" name="ad8b7c25c5600bbaa275abe387cef55d3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad8b7c25c5600bbaa275abe387cef55d3">&#9670;&#160;</a></span>_joint_probabilities()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.manifold._t_sne._joint_probabilities </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>distances</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>desired_perplexity</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute joint probabilities p_ij from distances.

Parameters
----------
distances : ndarray of shape (n_samples * (n_samples-1) / 2,)
    Distances of samples are stored as condensed matrices, i.e.
    we omit the diagonal and duplicate entries and store everything
    in a one-dimensional array.

desired_perplexity : float
    Desired perplexity of the joint probability distributions.

verbose : int
    Verbosity level.

Returns
-------
P : ndarray of shape (n_samples * (n_samples-1) / 2,)
    Condensed joint probability matrix.
</pre> <div class="fragment"><div class="line"><span class="lineno">   37</span><span class="keyword">def </span>_joint_probabilities(distances, desired_perplexity, verbose):</div>
<div class="line"><span class="lineno">   38</span>    <span class="stringliteral">&quot;&quot;&quot;Compute joint probabilities p_ij from distances.</span></div>
<div class="line"><span class="lineno">   39</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   40</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   41</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   42</span><span class="stringliteral">    distances : ndarray of shape (n_samples * (n_samples-1) / 2,)</span></div>
<div class="line"><span class="lineno">   43</span><span class="stringliteral">        Distances of samples are stored as condensed matrices, i.e.</span></div>
<div class="line"><span class="lineno">   44</span><span class="stringliteral">        we omit the diagonal and duplicate entries and store everything</span></div>
<div class="line"><span class="lineno">   45</span><span class="stringliteral">        in a one-dimensional array.</span></div>
<div class="line"><span class="lineno">   46</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   47</span><span class="stringliteral">    desired_perplexity : float</span></div>
<div class="line"><span class="lineno">   48</span><span class="stringliteral">        Desired perplexity of the joint probability distributions.</span></div>
<div class="line"><span class="lineno">   49</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   50</span><span class="stringliteral">    verbose : int</span></div>
<div class="line"><span class="lineno">   51</span><span class="stringliteral">        Verbosity level.</span></div>
<div class="line"><span class="lineno">   52</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   53</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">   54</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">   55</span><span class="stringliteral">    P : ndarray of shape (n_samples * (n_samples-1) / 2,)</span></div>
<div class="line"><span class="lineno">   56</span><span class="stringliteral">        Condensed joint probability matrix.</span></div>
<div class="line"><span class="lineno">   57</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   58</span>    <span class="comment"># Compute conditional probabilities such that they approximately match</span></div>
<div class="line"><span class="lineno">   59</span>    <span class="comment"># the desired perplexity</span></div>
<div class="line"><span class="lineno">   60</span>    distances = distances.astype(np.float32, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">   61</span>    conditional_P = _utils._binary_search_perplexity(</div>
<div class="line"><span class="lineno">   62</span>        distances, desired_perplexity, verbose</div>
<div class="line"><span class="lineno">   63</span>    )</div>
<div class="line"><span class="lineno">   64</span>    P = conditional_P + conditional_P.T</div>
<div class="line"><span class="lineno">   65</span>    sum_P = np.maximum(np.sum(P), MACHINE_EPSILON)</div>
<div class="line"><span class="lineno">   66</span>    P = np.maximum(squareform(P) / sum_P, MACHINE_EPSILON)</div>
<div class="line"><span class="lineno">   67</span>    <span class="keywordflow">return</span> P</div>
<div class="line"><span class="lineno">   68</span> </div>
<div class="line"><span class="lineno">   69</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ae142c57fc0866dacf6978fe62facd79c" name="ae142c57fc0866dacf6978fe62facd79c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae142c57fc0866dacf6978fe62facd79c">&#9670;&#160;</a></span>_joint_probabilities_nn()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.manifold._t_sne._joint_probabilities_nn </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>distances</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>desired_perplexity</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute joint probabilities p_ij from distances using just nearest
neighbors.

This method is approximately equal to _joint_probabilities. The latter
is O(N), but limiting the joint probability to nearest neighbors improves
this substantially to O(uN).

Parameters
----------
distances : sparse matrix of shape (n_samples, n_samples)
    Distances of samples to its n_neighbors nearest neighbors. All other
    distances are left to zero (and are not materialized in memory).
    Matrix should be of CSR format.

desired_perplexity : float
    Desired perplexity of the joint probability distributions.

verbose : int
    Verbosity level.

Returns
-------
P : sparse matrix of shape (n_samples, n_samples)
    Condensed joint probability matrix with only nearest neighbors. Matrix
    will be of CSR format.
</pre> <div class="fragment"><div class="line"><span class="lineno">   70</span><span class="keyword">def </span>_joint_probabilities_nn(distances, desired_perplexity, verbose):</div>
<div class="line"><span class="lineno">   71</span>    <span class="stringliteral">&quot;&quot;&quot;Compute joint probabilities p_ij from distances using just nearest</span></div>
<div class="line"><span class="lineno">   72</span><span class="stringliteral">    neighbors.</span></div>
<div class="line"><span class="lineno">   73</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   74</span><span class="stringliteral">    This method is approximately equal to _joint_probabilities. The latter</span></div>
<div class="line"><span class="lineno">   75</span><span class="stringliteral">    is O(N), but limiting the joint probability to nearest neighbors improves</span></div>
<div class="line"><span class="lineno">   76</span><span class="stringliteral">    this substantially to O(uN).</span></div>
<div class="line"><span class="lineno">   77</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   78</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   79</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   80</span><span class="stringliteral">    distances : sparse matrix of shape (n_samples, n_samples)</span></div>
<div class="line"><span class="lineno">   81</span><span class="stringliteral">        Distances of samples to its n_neighbors nearest neighbors. All other</span></div>
<div class="line"><span class="lineno">   82</span><span class="stringliteral">        distances are left to zero (and are not materialized in memory).</span></div>
<div class="line"><span class="lineno">   83</span><span class="stringliteral">        Matrix should be of CSR format.</span></div>
<div class="line"><span class="lineno">   84</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   85</span><span class="stringliteral">    desired_perplexity : float</span></div>
<div class="line"><span class="lineno">   86</span><span class="stringliteral">        Desired perplexity of the joint probability distributions.</span></div>
<div class="line"><span class="lineno">   87</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   88</span><span class="stringliteral">    verbose : int</span></div>
<div class="line"><span class="lineno">   89</span><span class="stringliteral">        Verbosity level.</span></div>
<div class="line"><span class="lineno">   90</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   91</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">   92</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">   93</span><span class="stringliteral">    P : sparse matrix of shape (n_samples, n_samples)</span></div>
<div class="line"><span class="lineno">   94</span><span class="stringliteral">        Condensed joint probability matrix with only nearest neighbors. Matrix</span></div>
<div class="line"><span class="lineno">   95</span><span class="stringliteral">        will be of CSR format.</span></div>
<div class="line"><span class="lineno">   96</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   97</span>    t0 = time()</div>
<div class="line"><span class="lineno">   98</span>    <span class="comment"># Compute conditional probabilities such that they approximately match</span></div>
<div class="line"><span class="lineno">   99</span>    <span class="comment"># the desired perplexity</span></div>
<div class="line"><span class="lineno">  100</span>    distances.sort_indices()</div>
<div class="line"><span class="lineno">  101</span>    n_samples = distances.shape[0]</div>
<div class="line"><span class="lineno">  102</span>    distances_data = distances.data.reshape(n_samples, -1)</div>
<div class="line"><span class="lineno">  103</span>    distances_data = distances_data.astype(np.float32, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  104</span>    conditional_P = _utils._binary_search_perplexity(</div>
<div class="line"><span class="lineno">  105</span>        distances_data, desired_perplexity, verbose</div>
<div class="line"><span class="lineno">  106</span>    )</div>
<div class="line"><span class="lineno">  107</span>    <span class="keyword">assert</span> np.all(np.isfinite(conditional_P)), <span class="stringliteral">&quot;All probabilities should be finite&quot;</span></div>
<div class="line"><span class="lineno">  108</span> </div>
<div class="line"><span class="lineno">  109</span>    <span class="comment"># Symmetrize the joint probability distribution using sparse operations</span></div>
<div class="line"><span class="lineno">  110</span>    P = csr_matrix(</div>
<div class="line"><span class="lineno">  111</span>        (conditional_P.ravel(), distances.indices, distances.indptr),</div>
<div class="line"><span class="lineno">  112</span>        shape=(n_samples, n_samples),</div>
<div class="line"><span class="lineno">  113</span>    )</div>
<div class="line"><span class="lineno">  114</span>    P = P + P.T</div>
<div class="line"><span class="lineno">  115</span> </div>
<div class="line"><span class="lineno">  116</span>    <span class="comment"># Normalize the joint probability distribution</span></div>
<div class="line"><span class="lineno">  117</span>    sum_P = np.maximum(P.sum(), MACHINE_EPSILON)</div>
<div class="line"><span class="lineno">  118</span>    P /= sum_P</div>
<div class="line"><span class="lineno">  119</span> </div>
<div class="line"><span class="lineno">  120</span>    <span class="keyword">assert</span> np.all(np.abs(P.data) &lt;= 1.0)</div>
<div class="line"><span class="lineno">  121</span>    <span class="keywordflow">if</span> verbose &gt;= 2:</div>
<div class="line"><span class="lineno">  122</span>        duration = time() - t0</div>
<div class="line"><span class="lineno">  123</span>        print(<span class="stringliteral">&quot;[t-SNE] Computed conditional probabilities in {:.3f}s&quot;</span>.format(duration))</div>
<div class="line"><span class="lineno">  124</span>    <span class="keywordflow">return</span> P</div>
<div class="line"><span class="lineno">  125</span> </div>
<div class="line"><span class="lineno">  126</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a26ce7302d5085e587421770508bd7cad" name="a26ce7302d5085e587421770508bd7cad"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a26ce7302d5085e587421770508bd7cad">&#9670;&#160;</a></span>_kl_divergence()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.manifold._t_sne._kl_divergence </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>P</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>degrees_of_freedom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_components</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_num_points</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_error</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">t-SNE objective function: gradient of the KL divergence
of p_ijs and q_ijs and the absolute error.

Parameters
----------
params : ndarray of shape (n_params,)
    Unraveled embedding.

P : ndarray of shape (n_samples * (n_samples-1) / 2,)
    Condensed joint probability matrix.

degrees_of_freedom : int
    Degrees of freedom of the Student's-t distribution.

n_samples : int
    Number of samples.

n_components : int
    Dimension of the embedded space.

skip_num_points : int, default=0
    This does not compute the gradient for points with indices below
    `skip_num_points`. This is useful when computing transforms of new
    data where you'd like to keep the old data fixed.

compute_error: bool, default=True
    If False, the kl_divergence is not computed and returns NaN.

Returns
-------
kl_divergence : float
    Kullback-Leibler divergence of p_ij and q_ij.

grad : ndarray of shape (n_params,)
    Unraveled gradient of the Kullback-Leibler divergence with respect to
    the embedding.
</pre> <div class="fragment"><div class="line"><span class="lineno">  135</span>):</div>
<div class="line"><span class="lineno">  136</span>    <span class="stringliteral">&quot;&quot;&quot;t-SNE objective function: gradient of the KL divergence</span></div>
<div class="line"><span class="lineno">  137</span><span class="stringliteral">    of p_ijs and q_ijs and the absolute error.</span></div>
<div class="line"><span class="lineno">  138</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  139</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  140</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  141</span><span class="stringliteral">    params : ndarray of shape (n_params,)</span></div>
<div class="line"><span class="lineno">  142</span><span class="stringliteral">        Unraveled embedding.</span></div>
<div class="line"><span class="lineno">  143</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  144</span><span class="stringliteral">    P : ndarray of shape (n_samples * (n_samples-1) / 2,)</span></div>
<div class="line"><span class="lineno">  145</span><span class="stringliteral">        Condensed joint probability matrix.</span></div>
<div class="line"><span class="lineno">  146</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  147</span><span class="stringliteral">    degrees_of_freedom : int</span></div>
<div class="line"><span class="lineno">  148</span><span class="stringliteral">        Degrees of freedom of the Student&#39;s-t distribution.</span></div>
<div class="line"><span class="lineno">  149</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  150</span><span class="stringliteral">    n_samples : int</span></div>
<div class="line"><span class="lineno">  151</span><span class="stringliteral">        Number of samples.</span></div>
<div class="line"><span class="lineno">  152</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  153</span><span class="stringliteral">    n_components : int</span></div>
<div class="line"><span class="lineno">  154</span><span class="stringliteral">        Dimension of the embedded space.</span></div>
<div class="line"><span class="lineno">  155</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  156</span><span class="stringliteral">    skip_num_points : int, default=0</span></div>
<div class="line"><span class="lineno">  157</span><span class="stringliteral">        This does not compute the gradient for points with indices below</span></div>
<div class="line"><span class="lineno">  158</span><span class="stringliteral">        `skip_num_points`. This is useful when computing transforms of new</span></div>
<div class="line"><span class="lineno">  159</span><span class="stringliteral">        data where you&#39;d like to keep the old data fixed.</span></div>
<div class="line"><span class="lineno">  160</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  161</span><span class="stringliteral">    compute_error: bool, default=True</span></div>
<div class="line"><span class="lineno">  162</span><span class="stringliteral">        If False, the kl_divergence is not computed and returns NaN.</span></div>
<div class="line"><span class="lineno">  163</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  164</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  165</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  166</span><span class="stringliteral">    kl_divergence : float</span></div>
<div class="line"><span class="lineno">  167</span><span class="stringliteral">        Kullback-Leibler divergence of p_ij and q_ij.</span></div>
<div class="line"><span class="lineno">  168</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  169</span><span class="stringliteral">    grad : ndarray of shape (n_params,)</span></div>
<div class="line"><span class="lineno">  170</span><span class="stringliteral">        Unraveled gradient of the Kullback-Leibler divergence with respect to</span></div>
<div class="line"><span class="lineno">  171</span><span class="stringliteral">        the embedding.</span></div>
<div class="line"><span class="lineno">  172</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  173</span>    X_embedded = params.reshape(n_samples, n_components)</div>
<div class="line"><span class="lineno">  174</span> </div>
<div class="line"><span class="lineno">  175</span>    <span class="comment"># Q is a heavy-tailed distribution: Student&#39;s t-distribution</span></div>
<div class="line"><span class="lineno">  176</span>    dist = pdist(X_embedded, <span class="stringliteral">&quot;sqeuclidean&quot;</span>)</div>
<div class="line"><span class="lineno">  177</span>    dist /= degrees_of_freedom</div>
<div class="line"><span class="lineno">  178</span>    dist += 1.0</div>
<div class="line"><span class="lineno">  179</span>    dist **= (degrees_of_freedom + 1.0) / -2.0</div>
<div class="line"><span class="lineno">  180</span>    Q = np.maximum(dist / (2.0 * np.sum(dist)), MACHINE_EPSILON)</div>
<div class="line"><span class="lineno">  181</span> </div>
<div class="line"><span class="lineno">  182</span>    <span class="comment"># Optimization trick below: np.dot(x, y) is faster than</span></div>
<div class="line"><span class="lineno">  183</span>    <span class="comment"># np.sum(x * y) because it calls BLAS</span></div>
<div class="line"><span class="lineno">  184</span> </div>
<div class="line"><span class="lineno">  185</span>    <span class="comment"># Objective: C (Kullback-Leibler divergence of P and Q)</span></div>
<div class="line"><span class="lineno">  186</span>    <span class="keywordflow">if</span> compute_error:</div>
<div class="line"><span class="lineno">  187</span>        kl_divergence = 2.0 * np.dot(P, np.log(np.maximum(P, MACHINE_EPSILON) / Q))</div>
<div class="line"><span class="lineno">  188</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  189</span>        kl_divergence = np.nan</div>
<div class="line"><span class="lineno">  190</span> </div>
<div class="line"><span class="lineno">  191</span>    <span class="comment"># Gradient: dC/dY</span></div>
<div class="line"><span class="lineno">  192</span>    <span class="comment"># pdist always returns double precision distances. Thus we need to take</span></div>
<div class="line"><span class="lineno">  193</span>    grad = np.ndarray((n_samples, n_components), dtype=params.dtype)</div>
<div class="line"><span class="lineno">  194</span>    PQd = squareform((P - Q) * dist)</div>
<div class="line"><span class="lineno">  195</span>    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(skip_num_points, n_samples):</div>
<div class="line"><span class="lineno">  196</span>        grad[i] = np.dot(np.ravel(PQd[i], order=<span class="stringliteral">&quot;K&quot;</span>), X_embedded[i] - X_embedded)</div>
<div class="line"><span class="lineno">  197</span>    grad = grad.ravel()</div>
<div class="line"><span class="lineno">  198</span>    c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom</div>
<div class="line"><span class="lineno">  199</span>    grad *= c</div>
<div class="line"><span class="lineno">  200</span> </div>
<div class="line"><span class="lineno">  201</span>    <span class="keywordflow">return</span> kl_divergence, grad</div>
<div class="line"><span class="lineno">  202</span> </div>
<div class="line"><span class="lineno">  203</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a454a53a34098a51ad06ebc4890b7b961" name="a454a53a34098a51ad06ebc4890b7b961"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a454a53a34098a51ad06ebc4890b7b961">&#9670;&#160;</a></span>_kl_divergence_bh()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.manifold._t_sne._kl_divergence_bh </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>P</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>degrees_of_freedom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_components</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>angle</em> = <code>0.5</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>skip_num_points</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>compute_error</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>num_threads</em> = <code>1</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">t-SNE objective function: KL divergence of p_ijs and q_ijs.

Uses Barnes-Hut tree methods to calculate the gradient that
runs in O(NlogN) instead of O(N^2).

Parameters
----------
params : ndarray of shape (n_params,)
    Unraveled embedding.

P : sparse matrix of shape (n_samples, n_sample)
    Sparse approximate joint probability matrix, computed only for the
    k nearest-neighbors and symmetrized. Matrix should be of CSR format.

degrees_of_freedom : int
    Degrees of freedom of the Student's-t distribution.

n_samples : int
    Number of samples.

n_components : int
    Dimension of the embedded space.

angle : float, default=0.5
    This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.
    'angle' is the angular size (referred to as theta in [3]) of a distant
    node as measured from a point. If this size is below 'angle' then it is
    used as a summary node of all points contained within it.
    This method is not very sensitive to changes in this parameter
    in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing
    computation time and angle greater 0.8 has quickly increasing error.

skip_num_points : int, default=0
    This does not compute the gradient for points with indices below
    `skip_num_points`. This is useful when computing transforms of new
    data where you'd like to keep the old data fixed.

verbose : int, default=False
    Verbosity level.

compute_error: bool, default=True
    If False, the kl_divergence is not computed and returns NaN.

num_threads : int, default=1
    Number of threads used to compute the gradient. This is set here to
    avoid calling _openmp_effective_n_threads for each gradient step.

Returns
-------
kl_divergence : float
    Kullback-Leibler divergence of p_ij and q_ij.

grad : ndarray of shape (n_params,)
    Unraveled gradient of the Kullback-Leibler divergence with respect to
    the embedding.
</pre> <div class="fragment"><div class="line"><span class="lineno">  215</span>):</div>
<div class="line"><span class="lineno">  216</span>    <span class="stringliteral">&quot;&quot;&quot;t-SNE objective function: KL divergence of p_ijs and q_ijs.</span></div>
<div class="line"><span class="lineno">  217</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  218</span><span class="stringliteral">    Uses Barnes-Hut tree methods to calculate the gradient that</span></div>
<div class="line"><span class="lineno">  219</span><span class="stringliteral">    runs in O(NlogN) instead of O(N^2).</span></div>
<div class="line"><span class="lineno">  220</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  221</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  222</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  223</span><span class="stringliteral">    params : ndarray of shape (n_params,)</span></div>
<div class="line"><span class="lineno">  224</span><span class="stringliteral">        Unraveled embedding.</span></div>
<div class="line"><span class="lineno">  225</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  226</span><span class="stringliteral">    P : sparse matrix of shape (n_samples, n_sample)</span></div>
<div class="line"><span class="lineno">  227</span><span class="stringliteral">        Sparse approximate joint probability matrix, computed only for the</span></div>
<div class="line"><span class="lineno">  228</span><span class="stringliteral">        k nearest-neighbors and symmetrized. Matrix should be of CSR format.</span></div>
<div class="line"><span class="lineno">  229</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  230</span><span class="stringliteral">    degrees_of_freedom : int</span></div>
<div class="line"><span class="lineno">  231</span><span class="stringliteral">        Degrees of freedom of the Student&#39;s-t distribution.</span></div>
<div class="line"><span class="lineno">  232</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  233</span><span class="stringliteral">    n_samples : int</span></div>
<div class="line"><span class="lineno">  234</span><span class="stringliteral">        Number of samples.</span></div>
<div class="line"><span class="lineno">  235</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  236</span><span class="stringliteral">    n_components : int</span></div>
<div class="line"><span class="lineno">  237</span><span class="stringliteral">        Dimension of the embedded space.</span></div>
<div class="line"><span class="lineno">  238</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  239</span><span class="stringliteral">    angle : float, default=0.5</span></div>
<div class="line"><span class="lineno">  240</span><span class="stringliteral">        This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.</span></div>
<div class="line"><span class="lineno">  241</span><span class="stringliteral">        &#39;angle&#39; is the angular size (referred to as theta in [3]) of a distant</span></div>
<div class="line"><span class="lineno">  242</span><span class="stringliteral">        node as measured from a point. If this size is below &#39;angle&#39; then it is</span></div>
<div class="line"><span class="lineno">  243</span><span class="stringliteral">        used as a summary node of all points contained within it.</span></div>
<div class="line"><span class="lineno">  244</span><span class="stringliteral">        This method is not very sensitive to changes in this parameter</span></div>
<div class="line"><span class="lineno">  245</span><span class="stringliteral">        in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing</span></div>
<div class="line"><span class="lineno">  246</span><span class="stringliteral">        computation time and angle greater 0.8 has quickly increasing error.</span></div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral">    skip_num_points : int, default=0</span></div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">        This does not compute the gradient for points with indices below</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral">        `skip_num_points`. This is useful when computing transforms of new</span></div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral">        data where you&#39;d like to keep the old data fixed.</span></div>
<div class="line"><span class="lineno">  252</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  253</span><span class="stringliteral">    verbose : int, default=False</span></div>
<div class="line"><span class="lineno">  254</span><span class="stringliteral">        Verbosity level.</span></div>
<div class="line"><span class="lineno">  255</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  256</span><span class="stringliteral">    compute_error: bool, default=True</span></div>
<div class="line"><span class="lineno">  257</span><span class="stringliteral">        If False, the kl_divergence is not computed and returns NaN.</span></div>
<div class="line"><span class="lineno">  258</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  259</span><span class="stringliteral">    num_threads : int, default=1</span></div>
<div class="line"><span class="lineno">  260</span><span class="stringliteral">        Number of threads used to compute the gradient. This is set here to</span></div>
<div class="line"><span class="lineno">  261</span><span class="stringliteral">        avoid calling _openmp_effective_n_threads for each gradient step.</span></div>
<div class="line"><span class="lineno">  262</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  263</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  264</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  265</span><span class="stringliteral">    kl_divergence : float</span></div>
<div class="line"><span class="lineno">  266</span><span class="stringliteral">        Kullback-Leibler divergence of p_ij and q_ij.</span></div>
<div class="line"><span class="lineno">  267</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  268</span><span class="stringliteral">    grad : ndarray of shape (n_params,)</span></div>
<div class="line"><span class="lineno">  269</span><span class="stringliteral">        Unraveled gradient of the Kullback-Leibler divergence with respect to</span></div>
<div class="line"><span class="lineno">  270</span><span class="stringliteral">        the embedding.</span></div>
<div class="line"><span class="lineno">  271</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  272</span>    params = params.astype(np.float32, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  273</span>    X_embedded = params.reshape(n_samples, n_components)</div>
<div class="line"><span class="lineno">  274</span> </div>
<div class="line"><span class="lineno">  275</span>    val_P = P.data.astype(np.float32, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  276</span>    neighbors = P.indices.astype(np.int64, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  277</span>    indptr = P.indptr.astype(np.int64, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  278</span> </div>
<div class="line"><span class="lineno">  279</span>    grad = np.zeros(X_embedded.shape, dtype=np.float32)</div>
<div class="line"><span class="lineno">  280</span>    error = _barnes_hut_tsne.gradient(</div>
<div class="line"><span class="lineno">  281</span>        val_P,</div>
<div class="line"><span class="lineno">  282</span>        X_embedded,</div>
<div class="line"><span class="lineno">  283</span>        neighbors,</div>
<div class="line"><span class="lineno">  284</span>        indptr,</div>
<div class="line"><span class="lineno">  285</span>        grad,</div>
<div class="line"><span class="lineno">  286</span>        angle,</div>
<div class="line"><span class="lineno">  287</span>        n_components,</div>
<div class="line"><span class="lineno">  288</span>        verbose,</div>
<div class="line"><span class="lineno">  289</span>        dof=degrees_of_freedom,</div>
<div class="line"><span class="lineno">  290</span>        compute_error=compute_error,</div>
<div class="line"><span class="lineno">  291</span>        num_threads=num_threads,</div>
<div class="line"><span class="lineno">  292</span>    )</div>
<div class="line"><span class="lineno">  293</span>    c = 2.0 * (degrees_of_freedom + 1.0) / degrees_of_freedom</div>
<div class="line"><span class="lineno">  294</span>    grad = grad.ravel()</div>
<div class="line"><span class="lineno">  295</span>    grad *= c</div>
<div class="line"><span class="lineno">  296</span> </div>
<div class="line"><span class="lineno">  297</span>    <span class="keywordflow">return</span> error, grad</div>
<div class="line"><span class="lineno">  298</span> </div>
<div class="line"><span class="lineno">  299</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a1946a3f79fae0098860ec8b6aa8d97c9" name="a1946a3f79fae0098860ec8b6aa8d97c9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1946a3f79fae0098860ec8b6aa8d97c9">&#9670;&#160;</a></span>trustworthiness()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.manifold._t_sne.trustworthiness </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X_embedded</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>n_neighbors</em> = <code>5</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>metric</em> = <code>&quot;euclidean&quot;</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Indicate to what extent the local structure is retained.

The trustworthiness is within [0, 1]. It is defined as

.. math::

T(k) = 1 - \frac{2}{nk (2n - 3k - 1)} \sum^n_{i=1}
\sum_{j \in \mathcal{N}_{i}^{k}} \max(0, (r(i, j) - k))

where for each sample i, :math:`\mathcal{N}_{i}^{k}` are its k nearest
neighbors in the output space, and every sample j is its :math:`r(i, j)`-th
nearest neighbor in the input space. In other words, any unexpected nearest
neighbors in the output space are penalised in proportion to their rank in
the input space.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features) or \
(n_samples, n_samples)
If the metric is 'precomputed' X must be a square distance
matrix. Otherwise it contains a sample per row.

X_embedded : {array-like, sparse matrix} of shape (n_samples, n_components)
Embedding of the training data in low-dimensional space.

n_neighbors : int, default=5
The number of neighbors that will be considered. Should be fewer than
`n_samples / 2` to ensure the trustworthiness to lies within [0, 1], as
mentioned in [1]_. An error will be raised otherwise.

metric : str or callable, default='euclidean'
Which metric to use for computing pairwise distances between samples
from the original input space. If metric is 'precomputed', X must be a
matrix of pairwise distances or squared distances. Otherwise, for a list
of available metrics, see the documentation of argument metric in
`sklearn.pairwise.pairwise_distances` and metrics listed in
`sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the
"cosine" metric uses :func:`~sklearn.metrics.pairwise.cosine_distances`.

.. versionadded:: 0.20

Returns
-------
trustworthiness : float
Trustworthiness of the low-dimensional embedding.

References
----------
.. [1] Jarkko Venna and Samuel Kaski. 2001. Neighborhood
Preservation in Nonlinear Projection Methods: An Experimental Study.
In Proceedings of the International Conference on Artificial Neural Networks
(ICANN '01). Springer-Verlag, Berlin, Heidelberg, 485-491.

.. [2] Laurens van der Maaten. Learning a Parametric Embedding by Preserving
Local Structure. Proceedings of the Twelth International Conference on
Artificial Intelligence and Statistics, PMLR 5:384-391, 2009.
</pre> <div class="fragment"><div class="line"><span class="lineno">  446</span><span class="keyword">def </span>trustworthiness(X, X_embedded, *, n_neighbors=5, metric=&quot;euclidean&quot;):</div>
<div class="line"><span class="lineno">  447</span>    <span class="stringliteral">r&quot;&quot;&quot;Indicate to what extent the local structure is retained.</span></div>
<div class="line"><span class="lineno">  448</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  449</span><span class="stringliteral">    The trustworthiness is within [0, 1]. It is defined as</span></div>
<div class="line"><span class="lineno">  450</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  451</span><span class="stringliteral">    .. math::</span></div>
<div class="line"><span class="lineno">  452</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  453</span><span class="stringliteral">        T(k) = 1 - \frac{2}{nk (2n - 3k - 1)} \sum^n_{i=1}</span></div>
<div class="line"><span class="lineno">  454</span><span class="stringliteral">            \sum_{j \in \mathcal{N}_{i}^{k}} \max(0, (r(i, j) - k))</span></div>
<div class="line"><span class="lineno">  455</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  456</span><span class="stringliteral">    where for each sample i, :math:`\mathcal{N}_{i}^{k}` are its k nearest</span></div>
<div class="line"><span class="lineno">  457</span><span class="stringliteral">    neighbors in the output space, and every sample j is its :math:`r(i, j)`-th</span></div>
<div class="line"><span class="lineno">  458</span><span class="stringliteral">    nearest neighbor in the input space. In other words, any unexpected nearest</span></div>
<div class="line"><span class="lineno">  459</span><span class="stringliteral">    neighbors in the output space are penalised in proportion to their rank in</span></div>
<div class="line"><span class="lineno">  460</span><span class="stringliteral">    the input space.</span></div>
<div class="line"><span class="lineno">  461</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  462</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  463</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  464</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features) or \</span></div>
<div class="line"><span class="lineno">  465</span><span class="stringliteral">        (n_samples, n_samples)</span></div>
<div class="line"><span class="lineno">  466</span><span class="stringliteral">        If the metric is &#39;precomputed&#39; X must be a square distance</span></div>
<div class="line"><span class="lineno">  467</span><span class="stringliteral">        matrix. Otherwise it contains a sample per row.</span></div>
<div class="line"><span class="lineno">  468</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  469</span><span class="stringliteral">    X_embedded : {array-like, sparse matrix} of shape (n_samples, n_components)</span></div>
<div class="line"><span class="lineno">  470</span><span class="stringliteral">        Embedding of the training data in low-dimensional space.</span></div>
<div class="line"><span class="lineno">  471</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  472</span><span class="stringliteral">    n_neighbors : int, default=5</span></div>
<div class="line"><span class="lineno">  473</span><span class="stringliteral">        The number of neighbors that will be considered. Should be fewer than</span></div>
<div class="line"><span class="lineno">  474</span><span class="stringliteral">        `n_samples / 2` to ensure the trustworthiness to lies within [0, 1], as</span></div>
<div class="line"><span class="lineno">  475</span><span class="stringliteral">        mentioned in [1]_. An error will be raised otherwise.</span></div>
<div class="line"><span class="lineno">  476</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  477</span><span class="stringliteral">    metric : str or callable, default=&#39;euclidean&#39;</span></div>
<div class="line"><span class="lineno">  478</span><span class="stringliteral">        Which metric to use for computing pairwise distances between samples</span></div>
<div class="line"><span class="lineno">  479</span><span class="stringliteral">        from the original input space. If metric is &#39;precomputed&#39;, X must be a</span></div>
<div class="line"><span class="lineno">  480</span><span class="stringliteral">        matrix of pairwise distances or squared distances. Otherwise, for a list</span></div>
<div class="line"><span class="lineno">  481</span><span class="stringliteral">        of available metrics, see the documentation of argument metric in</span></div>
<div class="line"><span class="lineno">  482</span><span class="stringliteral">        `sklearn.pairwise.pairwise_distances` and metrics listed in</span></div>
<div class="line"><span class="lineno">  483</span><span class="stringliteral">        `sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS`. Note that the</span></div>
<div class="line"><span class="lineno">  484</span><span class="stringliteral">        &quot;cosine&quot; metric uses :func:`~sklearn.metrics.pairwise.cosine_distances`.</span></div>
<div class="line"><span class="lineno">  485</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  486</span><span class="stringliteral">        .. versionadded:: 0.20</span></div>
<div class="line"><span class="lineno">  487</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  488</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  489</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  490</span><span class="stringliteral">    trustworthiness : float</span></div>
<div class="line"><span class="lineno">  491</span><span class="stringliteral">        Trustworthiness of the low-dimensional embedding.</span></div>
<div class="line"><span class="lineno">  492</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  493</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  494</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  495</span><span class="stringliteral">    .. [1] Jarkko Venna and Samuel Kaski. 2001. Neighborhood</span></div>
<div class="line"><span class="lineno">  496</span><span class="stringliteral">           Preservation in Nonlinear Projection Methods: An Experimental Study.</span></div>
<div class="line"><span class="lineno">  497</span><span class="stringliteral">           In Proceedings of the International Conference on Artificial Neural Networks</span></div>
<div class="line"><span class="lineno">  498</span><span class="stringliteral">           (ICANN &#39;01). Springer-Verlag, Berlin, Heidelberg, 485-491.</span></div>
<div class="line"><span class="lineno">  499</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  500</span><span class="stringliteral">    .. [2] Laurens van der Maaten. Learning a Parametric Embedding by Preserving</span></div>
<div class="line"><span class="lineno">  501</span><span class="stringliteral">           Local Structure. Proceedings of the Twelth International Conference on</span></div>
<div class="line"><span class="lineno">  502</span><span class="stringliteral">           Artificial Intelligence and Statistics, PMLR 5:384-391, 2009.</span></div>
<div class="line"><span class="lineno">  503</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  504</span>    n_samples = X.shape[0]</div>
<div class="line"><span class="lineno">  505</span>    <span class="keywordflow">if</span> n_neighbors &gt;= n_samples / 2:</div>
<div class="line"><span class="lineno">  506</span>        <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">  507</span>            f<span class="stringliteral">&quot;n_neighbors ({n_neighbors}) should be less than n_samples / 2&quot;</span></div>
<div class="line"><span class="lineno">  508</span>            f<span class="stringliteral">&quot; ({n_samples / 2})&quot;</span></div>
<div class="line"><span class="lineno">  509</span>        )</div>
<div class="line"><span class="lineno">  510</span>    dist_X = pairwise_distances(X, metric=metric)</div>
<div class="line"><span class="lineno">  511</span>    <span class="keywordflow">if</span> metric == <span class="stringliteral">&quot;precomputed&quot;</span>:</div>
<div class="line"><span class="lineno">  512</span>        dist_X = dist_X.copy()</div>
<div class="line"><span class="lineno">  513</span>    <span class="comment"># we set the diagonal to np.inf to exclude the points themselves from</span></div>
<div class="line"><span class="lineno">  514</span>    <span class="comment"># their own neighborhood</span></div>
<div class="line"><span class="lineno">  515</span>    np.fill_diagonal(dist_X, np.inf)</div>
<div class="line"><span class="lineno">  516</span>    ind_X = np.argsort(dist_X, axis=1)</div>
<div class="line"><span class="lineno">  517</span>    <span class="comment"># `ind_X[i]` is the index of sorted distances between i and other samples</span></div>
<div class="line"><span class="lineno">  518</span>    ind_X_embedded = (</div>
<div class="line"><span class="lineno">  519</span>        NearestNeighbors(n_neighbors=n_neighbors)</div>
<div class="line"><span class="lineno">  520</span>        .fit(X_embedded)</div>
<div class="line"><span class="lineno">  521</span>        .kneighbors(return_distance=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  522</span>    )</div>
<div class="line"><span class="lineno">  523</span> </div>
<div class="line"><span class="lineno">  524</span>    <span class="comment"># We build an inverted index of neighbors in the input space: For sample i,</span></div>
<div class="line"><span class="lineno">  525</span>    <span class="comment"># we define `inverted_index[i]` as the inverted index of sorted distances:</span></div>
<div class="line"><span class="lineno">  526</span>    <span class="comment"># inverted_index[i][ind_X[i]] = np.arange(1, n_sample + 1)</span></div>
<div class="line"><span class="lineno">  527</span>    inverted_index = np.zeros((n_samples, n_samples), dtype=int)</div>
<div class="line"><span class="lineno">  528</span>    ordered_indices = np.arange(n_samples + 1)</div>
<div class="line"><span class="lineno">  529</span>    inverted_index[ordered_indices[:-1, np.newaxis], ind_X] = ordered_indices[1:]</div>
<div class="line"><span class="lineno">  530</span>    ranks = (</div>
<div class="line"><span class="lineno">  531</span>        inverted_index[ordered_indices[:-1, np.newaxis], ind_X_embedded] - n_neighbors</div>
<div class="line"><span class="lineno">  532</span>    )</div>
<div class="line"><span class="lineno">  533</span>    t = np.sum(ranks[ranks &gt; 0])</div>
<div class="line"><span class="lineno">  534</span>    t = 1.0 - t * (</div>
<div class="line"><span class="lineno">  535</span>        2.0 / (n_samples * n_neighbors * (2.0 * n_samples - 3.0 * n_neighbors - 1.0))</div>
<div class="line"><span class="lineno">  536</span>    )</div>
<div class="line"><span class="lineno">  537</span>    <span class="keywordflow">return</span> t</div>
<div class="line"><span class="lineno">  538</span> </div>
<div class="line"><span class="lineno">  539</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a996e540cc45c8dd2e580bd50e79425c3" name="a996e540cc45c8dd2e580bd50e79425c3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a996e540cc45c8dd2e580bd50e79425c3">&#9670;&#160;</a></span>MACHINE_EPSILON</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.manifold._t_sne.MACHINE_EPSILON = np.finfo(np.double).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
