<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.utils.extmath Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1utils.html">utils</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html">extmath</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">sklearn.utils.extmath Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a6a39b4b4b5f631c8f0652304fda26a54" id="r_a6a39b4b4b5f631c8f0652304fda26a54"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a6a39b4b4b5f631c8f0652304fda26a54">squared_norm</a> (x)</td></tr>
<tr class="separator:a6a39b4b4b5f631c8f0652304fda26a54"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac3d9849e3c602ac89b59603797d8042c" id="r_ac3d9849e3c602ac89b59603797d8042c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#ac3d9849e3c602ac89b59603797d8042c">row_norms</a> (X, squared=False)</td></tr>
<tr class="separator:ac3d9849e3c602ac89b59603797d8042c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9839591c7220d735c3bebc11d4ad603b" id="r_a9839591c7220d735c3bebc11d4ad603b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a9839591c7220d735c3bebc11d4ad603b">fast_logdet</a> (A)</td></tr>
<tr class="separator:a9839591c7220d735c3bebc11d4ad603b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a362ab91825ce0c8bc8d5f8052467d8c3" id="r_a362ab91825ce0c8bc8d5f8052467d8c3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a362ab91825ce0c8bc8d5f8052467d8c3">density</a> (<a class="el" href="__lapack__subroutines_8h.html#a817b85d82af73ef273fafbec623bb90b">w</a>, **kwargs)</td></tr>
<tr class="separator:a362ab91825ce0c8bc8d5f8052467d8c3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a64fcf3937cf878995669fdb47fe66bc5" id="r_a64fcf3937cf878995669fdb47fe66bc5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a64fcf3937cf878995669fdb47fe66bc5">safe_sparse_dot</a> (<a class="el" href="__blas__subroutines_8h.html#a4da0a64c77789ca4c8115aef76120fd2">a</a>, b, *dense_output=False)</td></tr>
<tr class="separator:a64fcf3937cf878995669fdb47fe66bc5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a62197ea0dd2672d138159f71e46f0297" id="r_a62197ea0dd2672d138159f71e46f0297"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a62197ea0dd2672d138159f71e46f0297">randomized_range_finder</a> (A, *size, n_iter, power_iteration_normalizer=&quot;auto&quot;, random_state=None)</td></tr>
<tr class="separator:a62197ea0dd2672d138159f71e46f0297"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2a15a7466b901bbecb1221aa49e95777" id="r_a2a15a7466b901bbecb1221aa49e95777"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a2a15a7466b901bbecb1221aa49e95777">randomized_svd</a> (M, n_components, *n_oversamples=10, n_iter=&quot;auto&quot;, power_iteration_normalizer=&quot;auto&quot;, transpose=&quot;auto&quot;, flip_sign=True, random_state=None, svd_lapack_driver=&quot;gesdd&quot;)</td></tr>
<tr class="separator:a2a15a7466b901bbecb1221aa49e95777"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3639f7271d34424d8db8cc5417335afc" id="r_a3639f7271d34424d8db8cc5417335afc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a3639f7271d34424d8db8cc5417335afc">_randomized_eigsh</a> (M, n_components, *n_oversamples=10, n_iter=&quot;auto&quot;, power_iteration_normalizer=&quot;auto&quot;, selection=&quot;module&quot;, random_state=None)</td></tr>
<tr class="separator:a3639f7271d34424d8db8cc5417335afc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a02c4499812c3353e167e0b535456d72f" id="r_a02c4499812c3353e167e0b535456d72f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a02c4499812c3353e167e0b535456d72f">weighted_mode</a> (<a class="el" href="__blas__subroutines_8h.html#a4da0a64c77789ca4c8115aef76120fd2">a</a>, <a class="el" href="__lapack__subroutines_8h.html#a817b85d82af73ef273fafbec623bb90b">w</a>, *axis=0)</td></tr>
<tr class="separator:a02c4499812c3353e167e0b535456d72f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad2780457312fed7f3b34ff7ad32210e6" id="r_ad2780457312fed7f3b34ff7ad32210e6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#ad2780457312fed7f3b34ff7ad32210e6">cartesian</a> (arrays, out=None)</td></tr>
<tr class="separator:ad2780457312fed7f3b34ff7ad32210e6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeb7d1817c732296d017ee3ebd9c9ded7" id="r_aeb7d1817c732296d017ee3ebd9c9ded7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#aeb7d1817c732296d017ee3ebd9c9ded7">svd_flip</a> (u, v, u_based_decision=True)</td></tr>
<tr class="separator:aeb7d1817c732296d017ee3ebd9c9ded7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1870bfcf7422ea7f026611c5f09bc78d" id="r_a1870bfcf7422ea7f026611c5f09bc78d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a1870bfcf7422ea7f026611c5f09bc78d">log_logistic</a> (X, out=None)</td></tr>
<tr class="separator:a1870bfcf7422ea7f026611c5f09bc78d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5a030f453adb35a00f5f860b31cd3f3e" id="r_a5a030f453adb35a00f5f860b31cd3f3e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a5a030f453adb35a00f5f860b31cd3f3e">softmax</a> (X, copy=True)</td></tr>
<tr class="separator:a5a030f453adb35a00f5f860b31cd3f3e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6effd449b565544a97f4d5a301c09ccb" id="r_a6effd449b565544a97f4d5a301c09ccb"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a6effd449b565544a97f4d5a301c09ccb">make_nonnegative</a> (X, min_value=0)</td></tr>
<tr class="separator:a6effd449b565544a97f4d5a301c09ccb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a19939f2804ad89d9a563a53f0af59ab6" id="r_a19939f2804ad89d9a563a53f0af59ab6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a19939f2804ad89d9a563a53f0af59ab6">_safe_accumulator_op</a> (op, x, *args, **kwargs)</td></tr>
<tr class="separator:a19939f2804ad89d9a563a53f0af59ab6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9b2e4e0420df17e13899235c82352c70" id="r_a9b2e4e0420df17e13899235c82352c70"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a9b2e4e0420df17e13899235c82352c70">_incremental_mean_and_var</a> (X, last_mean, last_variance, last_sample_count, sample_weight=None)</td></tr>
<tr class="separator:a9b2e4e0420df17e13899235c82352c70"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a71ca8363822cbe4ae7cfc7c0efd20d01" id="r_a71ca8363822cbe4ae7cfc7c0efd20d01"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a71ca8363822cbe4ae7cfc7c0efd20d01">_deterministic_vector_sign_flip</a> (u)</td></tr>
<tr class="separator:a71ca8363822cbe4ae7cfc7c0efd20d01"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7c2292c77ff1021f1f2c33dc369da054" id="r_a7c2292c77ff1021f1f2c33dc369da054"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1utils_1_1extmath.html#a7c2292c77ff1021f1f2c33dc369da054">stable_cumsum</a> (arr, axis=None, <a class="el" href="__lapack__subroutines_8h.html#aa4a017e91ee751f9803a1bdb6caf1c06">rtol</a>=1e-05, atol=1e-08)</td></tr>
<tr class="separator:a7c2292c77ff1021f1f2c33dc369da054"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Extended math utilities.
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="a71ca8363822cbe4ae7cfc7c0efd20d01" name="a71ca8363822cbe4ae7cfc7c0efd20d01"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a71ca8363822cbe4ae7cfc7c0efd20d01">&#9670;&#160;</a></span>_deterministic_vector_sign_flip()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath._deterministic_vector_sign_flip </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>u</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Modify the sign of vectors for reproducibility.

Flips the sign of elements of all the vectors (rows of u) such that
the absolute maximum element of each vector is positive.

Parameters
----------
u : ndarray
    Array with vectors as its rows.

Returns
-------
u_flipped : ndarray with same shape as u
    Array with the sign flipped vectors as its rows.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1093</span><span class="keyword">def </span>_deterministic_vector_sign_flip(u):</div>
<div class="line"><span class="lineno"> 1094</span>    <span class="stringliteral">&quot;&quot;&quot;Modify the sign of vectors for reproducibility.</span></div>
<div class="line"><span class="lineno"> 1095</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1096</span><span class="stringliteral">    Flips the sign of elements of all the vectors (rows of u) such that</span></div>
<div class="line"><span class="lineno"> 1097</span><span class="stringliteral">    the absolute maximum element of each vector is positive.</span></div>
<div class="line"><span class="lineno"> 1098</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1099</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 1100</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 1101</span><span class="stringliteral">    u : ndarray</span></div>
<div class="line"><span class="lineno"> 1102</span><span class="stringliteral">        Array with vectors as its rows.</span></div>
<div class="line"><span class="lineno"> 1103</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1104</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 1105</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 1106</span><span class="stringliteral">    u_flipped : ndarray with same shape as u</span></div>
<div class="line"><span class="lineno"> 1107</span><span class="stringliteral">        Array with the sign flipped vectors as its rows.</span></div>
<div class="line"><span class="lineno"> 1108</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1109</span>    max_abs_rows = np.argmax(np.abs(u), axis=1)</div>
<div class="line"><span class="lineno"> 1110</span>    signs = np.sign(u[range(u.shape[0]), max_abs_rows])</div>
<div class="line"><span class="lineno"> 1111</span>    u *= signs[:, np.newaxis]</div>
<div class="line"><span class="lineno"> 1112</span>    <span class="keywordflow">return</span> u</div>
<div class="line"><span class="lineno"> 1113</span> </div>
<div class="line"><span class="lineno"> 1114</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a9b2e4e0420df17e13899235c82352c70" name="a9b2e4e0420df17e13899235c82352c70"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9b2e4e0420df17e13899235c82352c70">&#9670;&#160;</a></span>_incremental_mean_and_var()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath._incremental_mean_and_var </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>last_mean</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>last_variance</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>last_sample_count</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Calculate mean update and a Youngs and Cramer variance update.

If sample_weight is given, the weighted mean and variance is computed.

Update a given mean and (possibly) variance according to new data given
in X. last_mean is always required to compute the new mean.
If last_variance is None, no variance is computed and None return for
updated_variance.

From the paper "Algorithms for computing the sample variance: analysis and
recommendations", by Chan, Golub, and LeVeque.

Parameters
----------
X : array-like of shape (n_samples, n_features)
    Data to use for variance update.

last_mean : array-like of shape (n_features,)

last_variance : array-like of shape (n_features,)

last_sample_count : array-like of shape (n_features,)
    The number of samples encountered until now if sample_weight is None.
    If sample_weight is not None, this is the sum of sample_weight
    encountered.

sample_weight : array-like of shape (n_samples,) or None
    Sample weights. If None, compute the unweighted mean/variance.

Returns
-------
updated_mean : ndarray of shape (n_features,)

updated_variance : ndarray of shape (n_features,)
    None if last_variance was None.

updated_sample_count : ndarray of shape (n_features,)

Notes
-----
NaNs are ignored during the algorithm.

References
----------
T. Chan, G. Golub, R. LeVeque. Algorithms for computing the sample
    variance: recommendations, The American Statistician, Vol. 37, No. 3,
    pp. 242-247

Also, see the sparse implementation of this in
`utils.sparsefuncs.incr_mean_variance_axis` and
`utils.sparsefuncs_fast.incr_mean_variance_axis0`
</pre> <div class="fragment"><div class="line"><span class="lineno">  969</span>):</div>
<div class="line"><span class="lineno">  970</span>    <span class="stringliteral">&quot;&quot;&quot;Calculate mean update and a Youngs and Cramer variance update.</span></div>
<div class="line"><span class="lineno">  971</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  972</span><span class="stringliteral">    If sample_weight is given, the weighted mean and variance is computed.</span></div>
<div class="line"><span class="lineno">  973</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  974</span><span class="stringliteral">    Update a given mean and (possibly) variance according to new data given</span></div>
<div class="line"><span class="lineno">  975</span><span class="stringliteral">    in X. last_mean is always required to compute the new mean.</span></div>
<div class="line"><span class="lineno">  976</span><span class="stringliteral">    If last_variance is None, no variance is computed and None return for</span></div>
<div class="line"><span class="lineno">  977</span><span class="stringliteral">    updated_variance.</span></div>
<div class="line"><span class="lineno">  978</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  979</span><span class="stringliteral">    From the paper &quot;Algorithms for computing the sample variance: analysis and</span></div>
<div class="line"><span class="lineno">  980</span><span class="stringliteral">    recommendations&quot;, by Chan, Golub, and LeVeque.</span></div>
<div class="line"><span class="lineno">  981</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  982</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  983</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  984</span><span class="stringliteral">    X : array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  985</span><span class="stringliteral">        Data to use for variance update.</span></div>
<div class="line"><span class="lineno">  986</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  987</span><span class="stringliteral">    last_mean : array-like of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  988</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  989</span><span class="stringliteral">    last_variance : array-like of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  990</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  991</span><span class="stringliteral">    last_sample_count : array-like of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  992</span><span class="stringliteral">        The number of samples encountered until now if sample_weight is None.</span></div>
<div class="line"><span class="lineno">  993</span><span class="stringliteral">        If sample_weight is not None, this is the sum of sample_weight</span></div>
<div class="line"><span class="lineno">  994</span><span class="stringliteral">        encountered.</span></div>
<div class="line"><span class="lineno">  995</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  996</span><span class="stringliteral">    sample_weight : array-like of shape (n_samples,) or None</span></div>
<div class="line"><span class="lineno">  997</span><span class="stringliteral">        Sample weights. If None, compute the unweighted mean/variance.</span></div>
<div class="line"><span class="lineno">  998</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  999</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 1000</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 1001</span><span class="stringliteral">    updated_mean : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno"> 1002</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1003</span><span class="stringliteral">    updated_variance : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno"> 1004</span><span class="stringliteral">        None if last_variance was None.</span></div>
<div class="line"><span class="lineno"> 1005</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1006</span><span class="stringliteral">    updated_sample_count : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno"> 1007</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1008</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno"> 1009</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno"> 1010</span><span class="stringliteral">    NaNs are ignored during the algorithm.</span></div>
<div class="line"><span class="lineno"> 1011</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1012</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno"> 1013</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 1014</span><span class="stringliteral">    T. Chan, G. Golub, R. LeVeque. Algorithms for computing the sample</span></div>
<div class="line"><span class="lineno"> 1015</span><span class="stringliteral">        variance: recommendations, The American Statistician, Vol. 37, No. 3,</span></div>
<div class="line"><span class="lineno"> 1016</span><span class="stringliteral">        pp. 242-247</span></div>
<div class="line"><span class="lineno"> 1017</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1018</span><span class="stringliteral">    Also, see the sparse implementation of this in</span></div>
<div class="line"><span class="lineno"> 1019</span><span class="stringliteral">    `utils.sparsefuncs.incr_mean_variance_axis` and</span></div>
<div class="line"><span class="lineno"> 1020</span><span class="stringliteral">    `utils.sparsefuncs_fast.incr_mean_variance_axis0`</span></div>
<div class="line"><span class="lineno"> 1021</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1022</span>    <span class="comment"># old = stats until now</span></div>
<div class="line"><span class="lineno"> 1023</span>    <span class="comment"># new = the current increment</span></div>
<div class="line"><span class="lineno"> 1024</span>    <span class="comment"># updated = the aggregated stats</span></div>
<div class="line"><span class="lineno"> 1025</span>    last_sum = last_mean * last_sample_count</div>
<div class="line"><span class="lineno"> 1026</span>    X_nan_mask = np.isnan(X)</div>
<div class="line"><span class="lineno"> 1027</span>    <span class="keywordflow">if</span> np.any(X_nan_mask):</div>
<div class="line"><span class="lineno"> 1028</span>        sum_op = np.nansum</div>
<div class="line"><span class="lineno"> 1029</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1030</span>        sum_op = np.sum</div>
<div class="line"><span class="lineno"> 1031</span>    <span class="keywordflow">if</span> sample_weight <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno"> 1032</span>        <span class="comment"># equivalent to np.nansum(X * sample_weight, axis=0)</span></div>
<div class="line"><span class="lineno"> 1033</span>        <span class="comment"># safer because np.float64(X*W) != np.float64(X)*np.float64(W)</span></div>
<div class="line"><span class="lineno"> 1034</span>        new_sum = _safe_accumulator_op(</div>
<div class="line"><span class="lineno"> 1035</span>            np.matmul, sample_weight, np.where(X_nan_mask, 0, X)</div>
<div class="line"><span class="lineno"> 1036</span>        )</div>
<div class="line"><span class="lineno"> 1037</span>        new_sample_count = _safe_accumulator_op(</div>
<div class="line"><span class="lineno"> 1038</span>            np.sum, sample_weight[:, <span class="keywordtype">None</span>] * (~X_nan_mask), axis=0</div>
<div class="line"><span class="lineno"> 1039</span>        )</div>
<div class="line"><span class="lineno"> 1040</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1041</span>        new_sum = _safe_accumulator_op(sum_op, X, axis=0)</div>
<div class="line"><span class="lineno"> 1042</span>        n_samples = X.shape[0]</div>
<div class="line"><span class="lineno"> 1043</span>        new_sample_count = n_samples - np.sum(X_nan_mask, axis=0)</div>
<div class="line"><span class="lineno"> 1044</span> </div>
<div class="line"><span class="lineno"> 1045</span>    updated_sample_count = last_sample_count + new_sample_count</div>
<div class="line"><span class="lineno"> 1046</span> </div>
<div class="line"><span class="lineno"> 1047</span>    updated_mean = (last_sum + new_sum) / updated_sample_count</div>
<div class="line"><span class="lineno"> 1048</span> </div>
<div class="line"><span class="lineno"> 1049</span>    <span class="keywordflow">if</span> last_variance <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno"> 1050</span>        updated_variance = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1051</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1052</span>        T = new_sum / new_sample_count</div>
<div class="line"><span class="lineno"> 1053</span>        temp = X - T</div>
<div class="line"><span class="lineno"> 1054</span>        <span class="keywordflow">if</span> sample_weight <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno"> 1055</span>            <span class="comment"># equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)</span></div>
<div class="line"><span class="lineno"> 1056</span>            <span class="comment"># safer because np.float64(X*W) != np.float64(X)*np.float64(W)</span></div>
<div class="line"><span class="lineno"> 1057</span>            correction = _safe_accumulator_op(</div>
<div class="line"><span class="lineno"> 1058</span>                np.matmul, sample_weight, np.where(X_nan_mask, 0, temp)</div>
<div class="line"><span class="lineno"> 1059</span>            )</div>
<div class="line"><span class="lineno"> 1060</span>            temp **= 2</div>
<div class="line"><span class="lineno"> 1061</span>            new_unnormalized_variance = _safe_accumulator_op(</div>
<div class="line"><span class="lineno"> 1062</span>                np.matmul, sample_weight, np.where(X_nan_mask, 0, temp)</div>
<div class="line"><span class="lineno"> 1063</span>            )</div>
<div class="line"><span class="lineno"> 1064</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1065</span>            correction = _safe_accumulator_op(sum_op, temp, axis=0)</div>
<div class="line"><span class="lineno"> 1066</span>            temp **= 2</div>
<div class="line"><span class="lineno"> 1067</span>            new_unnormalized_variance = _safe_accumulator_op(sum_op, temp, axis=0)</div>
<div class="line"><span class="lineno"> 1068</span> </div>
<div class="line"><span class="lineno"> 1069</span>        <span class="comment"># correction term of the corrected 2 pass algorithm.</span></div>
<div class="line"><span class="lineno"> 1070</span>        <span class="comment"># See &quot;Algorithms for computing the sample variance: analysis</span></div>
<div class="line"><span class="lineno"> 1071</span>        <span class="comment"># and recommendations&quot;, by Chan, Golub, and LeVeque.</span></div>
<div class="line"><span class="lineno"> 1072</span>        new_unnormalized_variance -= correction**2 / new_sample_count</div>
<div class="line"><span class="lineno"> 1073</span> </div>
<div class="line"><span class="lineno"> 1074</span>        last_unnormalized_variance = last_variance * last_sample_count</div>
<div class="line"><span class="lineno"> 1075</span> </div>
<div class="line"><span class="lineno"> 1076</span>        <span class="keyword">with</span> np.errstate(divide=<span class="stringliteral">&quot;ignore&quot;</span>, invalid=<span class="stringliteral">&quot;ignore&quot;</span>):</div>
<div class="line"><span class="lineno"> 1077</span>            last_over_new_count = last_sample_count / new_sample_count</div>
<div class="line"><span class="lineno"> 1078</span>            updated_unnormalized_variance = (</div>
<div class="line"><span class="lineno"> 1079</span>                last_unnormalized_variance</div>
<div class="line"><span class="lineno"> 1080</span>                + new_unnormalized_variance</div>
<div class="line"><span class="lineno"> 1081</span>                + last_over_new_count</div>
<div class="line"><span class="lineno"> 1082</span>                / updated_sample_count</div>
<div class="line"><span class="lineno"> 1083</span>                * (last_sum / last_over_new_count - new_sum) ** 2</div>
<div class="line"><span class="lineno"> 1084</span>            )</div>
<div class="line"><span class="lineno"> 1085</span> </div>
<div class="line"><span class="lineno"> 1086</span>        zeros = last_sample_count == 0</div>
<div class="line"><span class="lineno"> 1087</span>        updated_unnormalized_variance[zeros] = new_unnormalized_variance[zeros]</div>
<div class="line"><span class="lineno"> 1088</span>        updated_variance = updated_unnormalized_variance / updated_sample_count</div>
<div class="line"><span class="lineno"> 1089</span> </div>
<div class="line"><span class="lineno"> 1090</span>    <span class="keywordflow">return</span> updated_mean, updated_variance, updated_sample_count</div>
<div class="line"><span class="lineno"> 1091</span> </div>
<div class="line"><span class="lineno"> 1092</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a3639f7271d34424d8db8cc5417335afc" name="a3639f7271d34424d8db8cc5417335afc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3639f7271d34424d8db8cc5417335afc">&#9670;&#160;</a></span>_randomized_eigsh()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath._randomized_eigsh </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>M</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_components</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>n_oversamples</em> = <code>10</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_iter</em> = <code>&quot;auto&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>power_iteration_normalizer</em> = <code>&quot;auto&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>selection</em> = <code>&quot;module&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Computes a truncated eigendecomposition using randomized methods

This method solves the fixed-rank approximation problem described in the
Halko et al paper.

The choice of which components to select can be tuned with the `selection`
parameter.

.. versionadded:: 0.24

Parameters
----------
M : ndarray or sparse matrix
    Matrix to decompose, it should be real symmetric square or complex
    hermitian

n_components : int
    Number of eigenvalues and vectors to extract.

n_oversamples : int, default=10
    Additional number of random vectors to sample the range of M so as
    to ensure proper conditioning. The total number of random vectors
    used to find the range of M is n_components + n_oversamples. Smaller
    number can improve speed but can negatively impact the quality of
    approximation of eigenvectors and eigenvalues. Users might wish
    to increase this parameter up to `2*k - n_components` where k is the
    effective rank, for large matrices, noisy problems, matrices with
    slowly decaying spectrums, or to increase precision accuracy. See Halko
    et al (pages 5, 23 and 26).

n_iter : int or 'auto', default='auto'
    Number of power iterations. It can be used to deal with very noisy
    problems. When 'auto', it is set to 4, unless `n_components` is small
    (&lt; .1 * min(X.shape)) in which case `n_iter` is set to 7.
    This improves precision with few components. Note that in general
    users should rather increase `n_oversamples` before increasing `n_iter`
    as the principle of the randomized method is to avoid usage of these
    more costly power iterations steps. When `n_components` is equal
    or greater to the effective matrix rank and the spectrum does not
    present a slow decay, `n_iter=0` or `1` should even work fine in theory
    (see Halko et al paper, page 9).

power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'
    Whether the power iterations are normalized with step-by-step
    QR factorization (the slowest but most accurate), 'none'
    (the fastest but numerically unstable when `n_iter` is large, e.g.
    typically 5 or larger), or 'LU' factorization (numerically stable
    but can lose slightly in accuracy). The 'auto' mode applies no
    normalization if `n_iter` &lt;= 2 and switches to LU otherwise.

selection : {'value', 'module'}, default='module'
    Strategy used to select the n components. When `selection` is `'value'`
    (not yet implemented, will become the default when implemented), the
    components corresponding to the n largest eigenvalues are returned.
    When `selection` is `'module'`, the components corresponding to the n
    eigenvalues with largest modules are returned.

random_state : int, RandomState instance, default=None
    The seed of the pseudo random number generator to use when shuffling
    the data, i.e. getting the random vectors to initialize the algorithm.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

Notes
-----
This algorithm finds a (usually very good) approximate truncated
eigendecomposition using randomized methods to speed up the computations.

This method is particularly fast on large matrices on which
you wish to extract only a small number of components. In order to
obtain further speed up, `n_iter` can be set &lt;=2 (at the cost of
loss of precision). To increase the precision it is recommended to
increase `n_oversamples`, up to `2*k-n_components` where k is the
effective rank. Usually, `n_components` is chosen to be greater than k
so increasing `n_oversamples` up to `n_components` should be enough.

Strategy 'value': not implemented yet.
Algorithms 5.3, 5.4 and 5.5 in the Halko et al paper should provide good
condidates for a future implementation.

Strategy 'module':
The principle is that for diagonalizable matrices, the singular values and
eigenvalues are related: if t is an eigenvalue of A, then :math:`|t|` is a
singular value of A. This method relies on a randomized SVD to find the n
singular components corresponding to the n singular values with largest
modules, and then uses the signs of the singular vectors to find the true
sign of t: if the sign of left and right singular vectors are different
then the corresponding eigenvalue is negative.

Returns
-------
eigvals : 1D array of shape (n_components,) containing the `n_components`
    eigenvalues selected (see ``selection`` parameter).
eigvecs : 2D array of shape (M.shape[0], n_components) containing the
    `n_components` eigenvectors corresponding to the `eigvals`, in the
    corresponding order. Note that this follows the `scipy.linalg.eigh`
    convention.

See Also
--------
:func:`randomized_svd`

References
----------
* :arxiv:`"Finding structure with randomness:
  Stochastic algorithms for constructing approximate matrix decompositions"
  (Algorithm 4.3 for strategy 'module') &lt;0909.4061&gt;`
  Halko, et al. (2009)
</pre> <div class="fragment"><div class="line"><span class="lineno">  487</span>):</div>
<div class="line"><span class="lineno">  488</span>    <span class="stringliteral">&quot;&quot;&quot;Computes a truncated eigendecomposition using randomized methods</span></div>
<div class="line"><span class="lineno">  489</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  490</span><span class="stringliteral">    This method solves the fixed-rank approximation problem described in the</span></div>
<div class="line"><span class="lineno">  491</span><span class="stringliteral">    Halko et al paper.</span></div>
<div class="line"><span class="lineno">  492</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  493</span><span class="stringliteral">    The choice of which components to select can be tuned with the `selection`</span></div>
<div class="line"><span class="lineno">  494</span><span class="stringliteral">    parameter.</span></div>
<div class="line"><span class="lineno">  495</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  496</span><span class="stringliteral">    .. versionadded:: 0.24</span></div>
<div class="line"><span class="lineno">  497</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  498</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  499</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  500</span><span class="stringliteral">    M : ndarray or sparse matrix</span></div>
<div class="line"><span class="lineno">  501</span><span class="stringliteral">        Matrix to decompose, it should be real symmetric square or complex</span></div>
<div class="line"><span class="lineno">  502</span><span class="stringliteral">        hermitian</span></div>
<div class="line"><span class="lineno">  503</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  504</span><span class="stringliteral">    n_components : int</span></div>
<div class="line"><span class="lineno">  505</span><span class="stringliteral">        Number of eigenvalues and vectors to extract.</span></div>
<div class="line"><span class="lineno">  506</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  507</span><span class="stringliteral">    n_oversamples : int, default=10</span></div>
<div class="line"><span class="lineno">  508</span><span class="stringliteral">        Additional number of random vectors to sample the range of M so as</span></div>
<div class="line"><span class="lineno">  509</span><span class="stringliteral">        to ensure proper conditioning. The total number of random vectors</span></div>
<div class="line"><span class="lineno">  510</span><span class="stringliteral">        used to find the range of M is n_components + n_oversamples. Smaller</span></div>
<div class="line"><span class="lineno">  511</span><span class="stringliteral">        number can improve speed but can negatively impact the quality of</span></div>
<div class="line"><span class="lineno">  512</span><span class="stringliteral">        approximation of eigenvectors and eigenvalues. Users might wish</span></div>
<div class="line"><span class="lineno">  513</span><span class="stringliteral">        to increase this parameter up to `2*k - n_components` where k is the</span></div>
<div class="line"><span class="lineno">  514</span><span class="stringliteral">        effective rank, for large matrices, noisy problems, matrices with</span></div>
<div class="line"><span class="lineno">  515</span><span class="stringliteral">        slowly decaying spectrums, or to increase precision accuracy. See Halko</span></div>
<div class="line"><span class="lineno">  516</span><span class="stringliteral">        et al (pages 5, 23 and 26).</span></div>
<div class="line"><span class="lineno">  517</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  518</span><span class="stringliteral">    n_iter : int or &#39;auto&#39;, default=&#39;auto&#39;</span></div>
<div class="line"><span class="lineno">  519</span><span class="stringliteral">        Number of power iterations. It can be used to deal with very noisy</span></div>
<div class="line"><span class="lineno">  520</span><span class="stringliteral">        problems. When &#39;auto&#39;, it is set to 4, unless `n_components` is small</span></div>
<div class="line"><span class="lineno">  521</span><span class="stringliteral">        (&lt; .1 * min(X.shape)) in which case `n_iter` is set to 7.</span></div>
<div class="line"><span class="lineno">  522</span><span class="stringliteral">        This improves precision with few components. Note that in general</span></div>
<div class="line"><span class="lineno">  523</span><span class="stringliteral">        users should rather increase `n_oversamples` before increasing `n_iter`</span></div>
<div class="line"><span class="lineno">  524</span><span class="stringliteral">        as the principle of the randomized method is to avoid usage of these</span></div>
<div class="line"><span class="lineno">  525</span><span class="stringliteral">        more costly power iterations steps. When `n_components` is equal</span></div>
<div class="line"><span class="lineno">  526</span><span class="stringliteral">        or greater to the effective matrix rank and the spectrum does not</span></div>
<div class="line"><span class="lineno">  527</span><span class="stringliteral">        present a slow decay, `n_iter=0` or `1` should even work fine in theory</span></div>
<div class="line"><span class="lineno">  528</span><span class="stringliteral">        (see Halko et al paper, page 9).</span></div>
<div class="line"><span class="lineno">  529</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  530</span><span class="stringliteral">    power_iteration_normalizer : {&#39;auto&#39;, &#39;QR&#39;, &#39;LU&#39;, &#39;none&#39;}, default=&#39;auto&#39;</span></div>
<div class="line"><span class="lineno">  531</span><span class="stringliteral">        Whether the power iterations are normalized with step-by-step</span></div>
<div class="line"><span class="lineno">  532</span><span class="stringliteral">        QR factorization (the slowest but most accurate), &#39;none&#39;</span></div>
<div class="line"><span class="lineno">  533</span><span class="stringliteral">        (the fastest but numerically unstable when `n_iter` is large, e.g.</span></div>
<div class="line"><span class="lineno">  534</span><span class="stringliteral">        typically 5 or larger), or &#39;LU&#39; factorization (numerically stable</span></div>
<div class="line"><span class="lineno">  535</span><span class="stringliteral">        but can lose slightly in accuracy). The &#39;auto&#39; mode applies no</span></div>
<div class="line"><span class="lineno">  536</span><span class="stringliteral">        normalization if `n_iter` &lt;= 2 and switches to LU otherwise.</span></div>
<div class="line"><span class="lineno">  537</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  538</span><span class="stringliteral">    selection : {&#39;value&#39;, &#39;module&#39;}, default=&#39;module&#39;</span></div>
<div class="line"><span class="lineno">  539</span><span class="stringliteral">        Strategy used to select the n components. When `selection` is `&#39;value&#39;`</span></div>
<div class="line"><span class="lineno">  540</span><span class="stringliteral">        (not yet implemented, will become the default when implemented), the</span></div>
<div class="line"><span class="lineno">  541</span><span class="stringliteral">        components corresponding to the n largest eigenvalues are returned.</span></div>
<div class="line"><span class="lineno">  542</span><span class="stringliteral">        When `selection` is `&#39;module&#39;`, the components corresponding to the n</span></div>
<div class="line"><span class="lineno">  543</span><span class="stringliteral">        eigenvalues with largest modules are returned.</span></div>
<div class="line"><span class="lineno">  544</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  545</span><span class="stringliteral">    random_state : int, RandomState instance, default=None</span></div>
<div class="line"><span class="lineno">  546</span><span class="stringliteral">        The seed of the pseudo random number generator to use when shuffling</span></div>
<div class="line"><span class="lineno">  547</span><span class="stringliteral">        the data, i.e. getting the random vectors to initialize the algorithm.</span></div>
<div class="line"><span class="lineno">  548</span><span class="stringliteral">        Pass an int for reproducible results across multiple function calls.</span></div>
<div class="line"><span class="lineno">  549</span><span class="stringliteral">        See :term:`Glossary &lt;random_state&gt;`.</span></div>
<div class="line"><span class="lineno">  550</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  551</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  552</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  553</span><span class="stringliteral">    This algorithm finds a (usually very good) approximate truncated</span></div>
<div class="line"><span class="lineno">  554</span><span class="stringliteral">    eigendecomposition using randomized methods to speed up the computations.</span></div>
<div class="line"><span class="lineno">  555</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  556</span><span class="stringliteral">    This method is particularly fast on large matrices on which</span></div>
<div class="line"><span class="lineno">  557</span><span class="stringliteral">    you wish to extract only a small number of components. In order to</span></div>
<div class="line"><span class="lineno">  558</span><span class="stringliteral">    obtain further speed up, `n_iter` can be set &lt;=2 (at the cost of</span></div>
<div class="line"><span class="lineno">  559</span><span class="stringliteral">    loss of precision). To increase the precision it is recommended to</span></div>
<div class="line"><span class="lineno">  560</span><span class="stringliteral">    increase `n_oversamples`, up to `2*k-n_components` where k is the</span></div>
<div class="line"><span class="lineno">  561</span><span class="stringliteral">    effective rank. Usually, `n_components` is chosen to be greater than k</span></div>
<div class="line"><span class="lineno">  562</span><span class="stringliteral">    so increasing `n_oversamples` up to `n_components` should be enough.</span></div>
<div class="line"><span class="lineno">  563</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  564</span><span class="stringliteral">    Strategy &#39;value&#39;: not implemented yet.</span></div>
<div class="line"><span class="lineno">  565</span><span class="stringliteral">    Algorithms 5.3, 5.4 and 5.5 in the Halko et al paper should provide good</span></div>
<div class="line"><span class="lineno">  566</span><span class="stringliteral">    condidates for a future implementation.</span></div>
<div class="line"><span class="lineno">  567</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  568</span><span class="stringliteral">    Strategy &#39;module&#39;:</span></div>
<div class="line"><span class="lineno">  569</span><span class="stringliteral">    The principle is that for diagonalizable matrices, the singular values and</span></div>
<div class="line"><span class="lineno">  570</span><span class="stringliteral">    eigenvalues are related: if t is an eigenvalue of A, then :math:`|t|` is a</span></div>
<div class="line"><span class="lineno">  571</span><span class="stringliteral">    singular value of A. This method relies on a randomized SVD to find the n</span></div>
<div class="line"><span class="lineno">  572</span><span class="stringliteral">    singular components corresponding to the n singular values with largest</span></div>
<div class="line"><span class="lineno">  573</span><span class="stringliteral">    modules, and then uses the signs of the singular vectors to find the true</span></div>
<div class="line"><span class="lineno">  574</span><span class="stringliteral">    sign of t: if the sign of left and right singular vectors are different</span></div>
<div class="line"><span class="lineno">  575</span><span class="stringliteral">    then the corresponding eigenvalue is negative.</span></div>
<div class="line"><span class="lineno">  576</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  577</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  578</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  579</span><span class="stringliteral">    eigvals : 1D array of shape (n_components,) containing the `n_components`</span></div>
<div class="line"><span class="lineno">  580</span><span class="stringliteral">        eigenvalues selected (see ``selection`` parameter).</span></div>
<div class="line"><span class="lineno">  581</span><span class="stringliteral">    eigvecs : 2D array of shape (M.shape[0], n_components) containing the</span></div>
<div class="line"><span class="lineno">  582</span><span class="stringliteral">        `n_components` eigenvectors corresponding to the `eigvals`, in the</span></div>
<div class="line"><span class="lineno">  583</span><span class="stringliteral">        corresponding order. Note that this follows the `scipy.linalg.eigh`</span></div>
<div class="line"><span class="lineno">  584</span><span class="stringliteral">        convention.</span></div>
<div class="line"><span class="lineno">  585</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  586</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  587</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  588</span><span class="stringliteral">    :func:`randomized_svd`</span></div>
<div class="line"><span class="lineno">  589</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  590</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  591</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  592</span><span class="stringliteral">    * :arxiv:`&quot;Finding structure with randomness:</span></div>
<div class="line"><span class="lineno">  593</span><span class="stringliteral">      Stochastic algorithms for constructing approximate matrix decompositions&quot;</span></div>
<div class="line"><span class="lineno">  594</span><span class="stringliteral">      (Algorithm 4.3 for strategy &#39;module&#39;) &lt;0909.4061&gt;`</span></div>
<div class="line"><span class="lineno">  595</span><span class="stringliteral">      Halko, et al. (2009)</span></div>
<div class="line"><span class="lineno">  596</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  597</span>    <span class="keywordflow">if</span> selection == <span class="stringliteral">&quot;value&quot;</span>:  <span class="comment"># pragma: no cover</span></div>
<div class="line"><span class="lineno">  598</span>        <span class="comment"># to do : an algorithm can be found in the Halko et al reference</span></div>
<div class="line"><span class="lineno">  599</span>        <span class="keywordflow">raise</span> NotImplementedError()</div>
<div class="line"><span class="lineno">  600</span> </div>
<div class="line"><span class="lineno">  601</span>    <span class="keywordflow">elif</span> selection == <span class="stringliteral">&quot;module&quot;</span>:</div>
<div class="line"><span class="lineno">  602</span>        <span class="comment"># Note: no need for deterministic U and Vt (flip_sign=True),</span></div>
<div class="line"><span class="lineno">  603</span>        <span class="comment"># as we only use the dot product UVt afterwards</span></div>
<div class="line"><span class="lineno">  604</span>        U, S, Vt = randomized_svd(</div>
<div class="line"><span class="lineno">  605</span>            M,</div>
<div class="line"><span class="lineno">  606</span>            n_components=n_components,</div>
<div class="line"><span class="lineno">  607</span>            n_oversamples=n_oversamples,</div>
<div class="line"><span class="lineno">  608</span>            n_iter=n_iter,</div>
<div class="line"><span class="lineno">  609</span>            power_iteration_normalizer=power_iteration_normalizer,</div>
<div class="line"><span class="lineno">  610</span>            flip_sign=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno">  611</span>            random_state=random_state,</div>
<div class="line"><span class="lineno">  612</span>        )</div>
<div class="line"><span class="lineno">  613</span> </div>
<div class="line"><span class="lineno">  614</span>        eigvecs = U[:, :n_components]</div>
<div class="line"><span class="lineno">  615</span>        eigvals = S[:n_components]</div>
<div class="line"><span class="lineno">  616</span> </div>
<div class="line"><span class="lineno">  617</span>        <span class="comment"># Conversion of Singular values into Eigenvalues:</span></div>
<div class="line"><span class="lineno">  618</span>        <span class="comment"># For any eigenvalue t, the corresponding singular value is |t|.</span></div>
<div class="line"><span class="lineno">  619</span>        <span class="comment"># So if there is a negative eigenvalue t, the corresponding singular</span></div>
<div class="line"><span class="lineno">  620</span>        <span class="comment"># value will be -t, and the left (U) and right (V) singular vectors</span></div>
<div class="line"><span class="lineno">  621</span>        <span class="comment"># will have opposite signs.</span></div>
<div class="line"><span class="lineno">  622</span>        <span class="comment"># Fastest way: see &lt;https://stackoverflow.com/a/61974002/7262247&gt;</span></div>
<div class="line"><span class="lineno">  623</span>        diag_VtU = np.einsum(<span class="stringliteral">&quot;ji,ij-&gt;j&quot;</span>, Vt[:n_components, :], U[:, :n_components])</div>
<div class="line"><span class="lineno">  624</span>        signs = np.sign(diag_VtU)</div>
<div class="line"><span class="lineno">  625</span>        eigvals = eigvals * signs</div>
<div class="line"><span class="lineno">  626</span> </div>
<div class="line"><span class="lineno">  627</span>    <span class="keywordflow">else</span>:  <span class="comment"># pragma: no cover</span></div>
<div class="line"><span class="lineno">  628</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Invalid `selection`: %r&quot;</span> % selection)</div>
<div class="line"><span class="lineno">  629</span> </div>
<div class="line"><span class="lineno">  630</span>    <span class="keywordflow">return</span> eigvals, eigvecs</div>
<div class="line"><span class="lineno">  631</span> </div>
<div class="line"><span class="lineno">  632</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a19939f2804ad89d9a563a53f0af59ab6" name="a19939f2804ad89d9a563a53f0af59ab6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a19939f2804ad89d9a563a53f0af59ab6">&#9670;&#160;</a></span>_safe_accumulator_op()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath._safe_accumulator_op </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>op</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>args</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">This function provides numpy accumulator functions with a float64 dtype
when used on a floating point input. This prevents accumulator overflow on
smaller floating point dtypes.

Parameters
----------
op : function
    A numpy accumulator function such as np.mean or np.sum.
x : ndarray
    A numpy array to apply the accumulator function.
*args : positional arguments
    Positional arguments passed to the accumulator function after the
    input x.
**kwargs : keyword arguments
    Keyword arguments passed to the accumulator function.

Returns
-------
result
    The output of the accumulator function passed to this function.
</pre> <div class="fragment"><div class="line"><span class="lineno">  937</span><span class="keyword">def </span>_safe_accumulator_op(op, x, *args, **kwargs):</div>
<div class="line"><span class="lineno">  938</span>    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  939</span><span class="stringliteral">    This function provides numpy accumulator functions with a float64 dtype</span></div>
<div class="line"><span class="lineno">  940</span><span class="stringliteral">    when used on a floating point input. This prevents accumulator overflow on</span></div>
<div class="line"><span class="lineno">  941</span><span class="stringliteral">    smaller floating point dtypes.</span></div>
<div class="line"><span class="lineno">  942</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  943</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  944</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  945</span><span class="stringliteral">    op : function</span></div>
<div class="line"><span class="lineno">  946</span><span class="stringliteral">        A numpy accumulator function such as np.mean or np.sum.</span></div>
<div class="line"><span class="lineno">  947</span><span class="stringliteral">    x : ndarray</span></div>
<div class="line"><span class="lineno">  948</span><span class="stringliteral">        A numpy array to apply the accumulator function.</span></div>
<div class="line"><span class="lineno">  949</span><span class="stringliteral">    *args : positional arguments</span></div>
<div class="line"><span class="lineno">  950</span><span class="stringliteral">        Positional arguments passed to the accumulator function after the</span></div>
<div class="line"><span class="lineno">  951</span><span class="stringliteral">        input x.</span></div>
<div class="line"><span class="lineno">  952</span><span class="stringliteral">    **kwargs : keyword arguments</span></div>
<div class="line"><span class="lineno">  953</span><span class="stringliteral">        Keyword arguments passed to the accumulator function.</span></div>
<div class="line"><span class="lineno">  954</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  955</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  956</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  957</span><span class="stringliteral">    result</span></div>
<div class="line"><span class="lineno">  958</span><span class="stringliteral">        The output of the accumulator function passed to this function.</span></div>
<div class="line"><span class="lineno">  959</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  960</span>    <span class="keywordflow">if</span> np.issubdtype(x.dtype, np.floating) <span class="keywordflow">and</span> x.dtype.itemsize &lt; 8:</div>
<div class="line"><span class="lineno">  961</span>        result = op(x, *args, **kwargs, dtype=np.float64)</div>
<div class="line"><span class="lineno">  962</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  963</span>        result = op(x, *args, **kwargs)</div>
<div class="line"><span class="lineno">  964</span>    <span class="keywordflow">return</span> result</div>
<div class="line"><span class="lineno">  965</span> </div>
<div class="line"><span class="lineno">  966</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad2780457312fed7f3b34ff7ad32210e6" name="ad2780457312fed7f3b34ff7ad32210e6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad2780457312fed7f3b34ff7ad32210e6">&#9670;&#160;</a></span>cartesian()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.cartesian </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>arrays</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>out</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Generate a cartesian product of input arrays.

Parameters
----------
arrays : list of array-like
    1-D arrays to form the cartesian product of.
out : ndarray of shape (M, len(arrays)), default=None
    Array to place the cartesian product in.

Returns
-------
out : ndarray of shape (M, len(arrays))
    Array containing the cartesian products formed of input arrays.
    If not provided, the `dtype` of the output array is set to the most
    permissive `dtype` of the input arrays, according to NumPy type
    promotion.

    .. versionadded:: 1.2
       Add support for arrays of different types.

Notes
-----
This function may not be used on more than 32 arrays
because the underlying numpy functions do not support it.

Examples
--------
&gt;&gt;&gt; from sklearn.utils.extmath import cartesian
&gt;&gt;&gt; cartesian(([1, 2, 3], [4, 5], [6, 7]))
array([[1, 4, 6],
       [1, 4, 7],
       [1, 5, 6],
       [1, 5, 7],
       [2, 4, 6],
       [2, 4, 7],
       [2, 5, 6],
       [2, 5, 7],
       [3, 4, 6],
       [3, 4, 7],
       [3, 5, 6],
       [3, 5, 7]])
</pre> <div class="fragment"><div class="line"><span class="lineno">  707</span><span class="keyword">def </span>cartesian(arrays, out=None):</div>
<div class="line"><span class="lineno">  708</span>    <span class="stringliteral">&quot;&quot;&quot;Generate a cartesian product of input arrays.</span></div>
<div class="line"><span class="lineno">  709</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  710</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  711</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  712</span><span class="stringliteral">    arrays : list of array-like</span></div>
<div class="line"><span class="lineno">  713</span><span class="stringliteral">        1-D arrays to form the cartesian product of.</span></div>
<div class="line"><span class="lineno">  714</span><span class="stringliteral">    out : ndarray of shape (M, len(arrays)), default=None</span></div>
<div class="line"><span class="lineno">  715</span><span class="stringliteral">        Array to place the cartesian product in.</span></div>
<div class="line"><span class="lineno">  716</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  717</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  718</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  719</span><span class="stringliteral">    out : ndarray of shape (M, len(arrays))</span></div>
<div class="line"><span class="lineno">  720</span><span class="stringliteral">        Array containing the cartesian products formed of input arrays.</span></div>
<div class="line"><span class="lineno">  721</span><span class="stringliteral">        If not provided, the `dtype` of the output array is set to the most</span></div>
<div class="line"><span class="lineno">  722</span><span class="stringliteral">        permissive `dtype` of the input arrays, according to NumPy type</span></div>
<div class="line"><span class="lineno">  723</span><span class="stringliteral">        promotion.</span></div>
<div class="line"><span class="lineno">  724</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  725</span><span class="stringliteral">        .. versionadded:: 1.2</span></div>
<div class="line"><span class="lineno">  726</span><span class="stringliteral">           Add support for arrays of different types.</span></div>
<div class="line"><span class="lineno">  727</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  728</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  729</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  730</span><span class="stringliteral">    This function may not be used on more than 32 arrays</span></div>
<div class="line"><span class="lineno">  731</span><span class="stringliteral">    because the underlying numpy functions do not support it.</span></div>
<div class="line"><span class="lineno">  732</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  733</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno">  734</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  735</span><span class="stringliteral">    &gt;&gt;&gt; from sklearn.utils.extmath import cartesian</span></div>
<div class="line"><span class="lineno">  736</span><span class="stringliteral">    &gt;&gt;&gt; cartesian(([1, 2, 3], [4, 5], [6, 7]))</span></div>
<div class="line"><span class="lineno">  737</span><span class="stringliteral">    array([[1, 4, 6],</span></div>
<div class="line"><span class="lineno">  738</span><span class="stringliteral">           [1, 4, 7],</span></div>
<div class="line"><span class="lineno">  739</span><span class="stringliteral">           [1, 5, 6],</span></div>
<div class="line"><span class="lineno">  740</span><span class="stringliteral">           [1, 5, 7],</span></div>
<div class="line"><span class="lineno">  741</span><span class="stringliteral">           [2, 4, 6],</span></div>
<div class="line"><span class="lineno">  742</span><span class="stringliteral">           [2, 4, 7],</span></div>
<div class="line"><span class="lineno">  743</span><span class="stringliteral">           [2, 5, 6],</span></div>
<div class="line"><span class="lineno">  744</span><span class="stringliteral">           [2, 5, 7],</span></div>
<div class="line"><span class="lineno">  745</span><span class="stringliteral">           [3, 4, 6],</span></div>
<div class="line"><span class="lineno">  746</span><span class="stringliteral">           [3, 4, 7],</span></div>
<div class="line"><span class="lineno">  747</span><span class="stringliteral">           [3, 5, 6],</span></div>
<div class="line"><span class="lineno">  748</span><span class="stringliteral">           [3, 5, 7]])</span></div>
<div class="line"><span class="lineno">  749</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  750</span>    arrays = [np.asarray(x) <span class="keywordflow">for</span> x <span class="keywordflow">in</span> arrays]</div>
<div class="line"><span class="lineno">  751</span>    shape = (len(x) <span class="keywordflow">for</span> x <span class="keywordflow">in</span> arrays)</div>
<div class="line"><span class="lineno">  752</span> </div>
<div class="line"><span class="lineno">  753</span>    ix = np.indices(shape)</div>
<div class="line"><span class="lineno">  754</span>    ix = ix.reshape(len(arrays), -1).T</div>
<div class="line"><span class="lineno">  755</span> </div>
<div class="line"><span class="lineno">  756</span>    <span class="keywordflow">if</span> out <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  757</span>        dtype = np.result_type(*arrays)  <span class="comment"># find the most permissive dtype</span></div>
<div class="line"><span class="lineno">  758</span>        out = np.empty_like(ix, dtype=dtype)</div>
<div class="line"><span class="lineno">  759</span> </div>
<div class="line"><span class="lineno">  760</span>    <span class="keywordflow">for</span> n, arr <span class="keywordflow">in</span> enumerate(arrays):</div>
<div class="line"><span class="lineno">  761</span>        out[:, n] = arrays[n][ix[:, n]]</div>
<div class="line"><span class="lineno">  762</span> </div>
<div class="line"><span class="lineno">  763</span>    <span class="keywordflow">return</span> out</div>
<div class="line"><span class="lineno">  764</span> </div>
<div class="line"><span class="lineno">  765</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a362ab91825ce0c8bc8d5f8052467d8c3" name="a362ab91825ce0c8bc8d5f8052467d8c3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a362ab91825ce0c8bc8d5f8052467d8c3">&#9670;&#160;</a></span>density()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.density </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>w</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">**&#160;</td>
          <td class="paramname"><em>kwargs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute density of a sparse vector.

Parameters
----------
w : array-like
    The sparse vector.
**kwargs : keyword arguments
    Ignored.

    .. deprecated:: 1.2
        ``**kwargs`` were deprecated in version 1.2 and will be removed in
        1.4.

Returns
-------
float
    The density of w, between 0 and 1.
</pre> <div class="fragment"><div class="line"><span class="lineno">  123</span><span class="keyword">def </span>density(w, **kwargs):</div>
<div class="line"><span class="lineno">  124</span>    <span class="stringliteral">&quot;&quot;&quot;Compute density of a sparse vector.</span></div>
<div class="line"><span class="lineno">  125</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  126</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  127</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  128</span><span class="stringliteral">    w : array-like</span></div>
<div class="line"><span class="lineno">  129</span><span class="stringliteral">        The sparse vector.</span></div>
<div class="line"><span class="lineno">  130</span><span class="stringliteral">    **kwargs : keyword arguments</span></div>
<div class="line"><span class="lineno">  131</span><span class="stringliteral">        Ignored.</span></div>
<div class="line"><span class="lineno">  132</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  133</span><span class="stringliteral">        .. deprecated:: 1.2</span></div>
<div class="line"><span class="lineno">  134</span><span class="stringliteral">            ``**kwargs`` were deprecated in version 1.2 and will be removed in</span></div>
<div class="line"><span class="lineno">  135</span><span class="stringliteral">            1.4.</span></div>
<div class="line"><span class="lineno">  136</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  137</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  138</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  139</span><span class="stringliteral">    float</span></div>
<div class="line"><span class="lineno">  140</span><span class="stringliteral">        The density of w, between 0 and 1.</span></div>
<div class="line"><span class="lineno">  141</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  142</span>    <span class="keywordflow">if</span> kwargs:</div>
<div class="line"><span class="lineno">  143</span>        warnings.warn(</div>
<div class="line"><span class="lineno">  144</span>            <span class="stringliteral">&quot;Additional keyword arguments are deprecated in version 1.2 and will be&quot;</span></div>
<div class="line"><span class="lineno">  145</span>            <span class="stringliteral">&quot; removed in version 1.4.&quot;</span>,</div>
<div class="line"><span class="lineno">  146</span>            FutureWarning,</div>
<div class="line"><span class="lineno">  147</span>        )</div>
<div class="line"><span class="lineno">  148</span> </div>
<div class="line"><span class="lineno">  149</span>    <span class="keywordflow">if</span> hasattr(w, <span class="stringliteral">&quot;toarray&quot;</span>):</div>
<div class="line"><span class="lineno">  150</span>        d = float(w.nnz) / (w.shape[0] * w.shape[1])</div>
<div class="line"><span class="lineno">  151</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  152</span>        d = 0 <span class="keywordflow">if</span> w <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">else</span> float((w != 0).sum()) / w.size</div>
<div class="line"><span class="lineno">  153</span>    <span class="keywordflow">return</span> d</div>
<div class="line"><span class="lineno">  154</span> </div>
<div class="line"><span class="lineno">  155</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a9839591c7220d735c3bebc11d4ad603b" name="a9839591c7220d735c3bebc11d4ad603b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9839591c7220d735c3bebc11d4ad603b">&#9670;&#160;</a></span>fast_logdet()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.fast_logdet </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>A</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute logarithm of determinant of a square matrix.

The (natural) logarithm of the determinant of a square matrix
is returned if det(A) is non-negative and well defined.
If the determinant is zero or negative returns -Inf.

Equivalent to : np.log(np.det(A)) but more robust.

Parameters
----------
A : array_like of shape (n, n)
    The square matrix.

Returns
-------
logdet : float
    When det(A) is strictly positive, log(det(A)) is returned.
    When det(A) is non-positive or not defined, then -inf is returned.

See Also
--------
numpy.linalg.slogdet : Compute the sign and (natural) logarithm of the determinant
    of an array.

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.utils.extmath import fast_logdet
&gt;&gt;&gt; a = np.array([[5, 1], [2, 8]])
&gt;&gt;&gt; fast_logdet(a)
3.6375861597263857
</pre> <div class="fragment"><div class="line"><span class="lineno">   84</span><span class="keyword">def </span>fast_logdet(A):</div>
<div class="line"><span class="lineno">   85</span>    <span class="stringliteral">&quot;&quot;&quot;Compute logarithm of determinant of a square matrix.</span></div>
<div class="line"><span class="lineno">   86</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   87</span><span class="stringliteral">    The (natural) logarithm of the determinant of a square matrix</span></div>
<div class="line"><span class="lineno">   88</span><span class="stringliteral">    is returned if det(A) is non-negative and well defined.</span></div>
<div class="line"><span class="lineno">   89</span><span class="stringliteral">    If the determinant is zero or negative returns -Inf.</span></div>
<div class="line"><span class="lineno">   90</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   91</span><span class="stringliteral">    Equivalent to : np.log(np.det(A)) but more robust.</span></div>
<div class="line"><span class="lineno">   92</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   93</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   94</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   95</span><span class="stringliteral">    A : array_like of shape (n, n)</span></div>
<div class="line"><span class="lineno">   96</span><span class="stringliteral">        The square matrix.</span></div>
<div class="line"><span class="lineno">   97</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   98</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">   99</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  100</span><span class="stringliteral">    logdet : float</span></div>
<div class="line"><span class="lineno">  101</span><span class="stringliteral">        When det(A) is strictly positive, log(det(A)) is returned.</span></div>
<div class="line"><span class="lineno">  102</span><span class="stringliteral">        When det(A) is non-positive or not defined, then -inf is returned.</span></div>
<div class="line"><span class="lineno">  103</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  104</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  105</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  106</span><span class="stringliteral">    numpy.linalg.slogdet : Compute the sign and (natural) logarithm of the determinant</span></div>
<div class="line"><span class="lineno">  107</span><span class="stringliteral">        of an array.</span></div>
<div class="line"><span class="lineno">  108</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  109</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno">  110</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  111</span><span class="stringliteral">    &gt;&gt;&gt; import numpy as np</span></div>
<div class="line"><span class="lineno">  112</span><span class="stringliteral">    &gt;&gt;&gt; from sklearn.utils.extmath import fast_logdet</span></div>
<div class="line"><span class="lineno">  113</span><span class="stringliteral">    &gt;&gt;&gt; a = np.array([[5, 1], [2, 8]])</span></div>
<div class="line"><span class="lineno">  114</span><span class="stringliteral">    &gt;&gt;&gt; fast_logdet(a)</span></div>
<div class="line"><span class="lineno">  115</span><span class="stringliteral">    3.6375861597263857</span></div>
<div class="line"><span class="lineno">  116</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  117</span>    sign, ld = np.linalg.slogdet(A)</div>
<div class="line"><span class="lineno">  118</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> sign &gt; 0:</div>
<div class="line"><span class="lineno">  119</span>        <span class="keywordflow">return</span> -np.inf</div>
<div class="line"><span class="lineno">  120</span>    <span class="keywordflow">return</span> ld</div>
<div class="line"><span class="lineno">  121</span> </div>
<div class="line"><span class="lineno">  122</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a1870bfcf7422ea7f026611c5f09bc78d" name="a1870bfcf7422ea7f026611c5f09bc78d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1870bfcf7422ea7f026611c5f09bc78d">&#9670;&#160;</a></span>log_logistic()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.log_logistic </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>out</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.

This implementation is numerically stable because it splits positive and
negative values::

    -log(1 + exp(-x_i))     if x_i &gt; 0
    x_i - log(1 + exp(x_i)) if x_i &lt;= 0

For the ordinary logistic function, use ``scipy.special.expit``.

Parameters
----------
X : array-like of shape (M, N) or (M,)
    Argument to the logistic function.

out : array-like of shape (M, N) or (M,), default=None
    Preallocated output array.

Returns
-------
out : ndarray of shape (M, N) or (M,)
    Log of the logistic function evaluated at every point in x.

Notes
-----
See the blog post describing this implementation:
http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/
</pre> <div class="fragment"><div class="line"><span class="lineno">  814</span><span class="keyword">def </span>log_logistic(X, out=None):</div>
<div class="line"><span class="lineno">  815</span>    <span class="stringliteral">&quot;&quot;&quot;Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.</span></div>
<div class="line"><span class="lineno">  816</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  817</span><span class="stringliteral">    This implementation is numerically stable because it splits positive and</span></div>
<div class="line"><span class="lineno">  818</span><span class="stringliteral">    negative values::</span></div>
<div class="line"><span class="lineno">  819</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  820</span><span class="stringliteral">        -log(1 + exp(-x_i))     if x_i &gt; 0</span></div>
<div class="line"><span class="lineno">  821</span><span class="stringliteral">        x_i - log(1 + exp(x_i)) if x_i &lt;= 0</span></div>
<div class="line"><span class="lineno">  822</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  823</span><span class="stringliteral">    For the ordinary logistic function, use ``scipy.special.expit``.</span></div>
<div class="line"><span class="lineno">  824</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  825</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  826</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  827</span><span class="stringliteral">    X : array-like of shape (M, N) or (M,)</span></div>
<div class="line"><span class="lineno">  828</span><span class="stringliteral">        Argument to the logistic function.</span></div>
<div class="line"><span class="lineno">  829</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  830</span><span class="stringliteral">    out : array-like of shape (M, N) or (M,), default=None</span></div>
<div class="line"><span class="lineno">  831</span><span class="stringliteral">        Preallocated output array.</span></div>
<div class="line"><span class="lineno">  832</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  833</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  834</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  835</span><span class="stringliteral">    out : ndarray of shape (M, N) or (M,)</span></div>
<div class="line"><span class="lineno">  836</span><span class="stringliteral">        Log of the logistic function evaluated at every point in x.</span></div>
<div class="line"><span class="lineno">  837</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  838</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  839</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  840</span><span class="stringliteral">    See the blog post describing this implementation:</span></div>
<div class="line"><span class="lineno">  841</span><span class="stringliteral">    http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/</span></div>
<div class="line"><span class="lineno">  842</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  843</span>    is_1d = X.ndim == 1</div>
<div class="line"><span class="lineno">  844</span>    X = np.atleast_2d(X)</div>
<div class="line"><span class="lineno">  845</span>    X = check_array(X, dtype=np.float64)</div>
<div class="line"><span class="lineno">  846</span> </div>
<div class="line"><span class="lineno">  847</span>    n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno">  848</span> </div>
<div class="line"><span class="lineno">  849</span>    <span class="keywordflow">if</span> out <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  850</span>        out = np.empty_like(X)</div>
<div class="line"><span class="lineno">  851</span> </div>
<div class="line"><span class="lineno">  852</span>    _log_logistic_sigmoid(n_samples, n_features, X, out)</div>
<div class="line"><span class="lineno">  853</span> </div>
<div class="line"><span class="lineno">  854</span>    <span class="keywordflow">if</span> is_1d:</div>
<div class="line"><span class="lineno">  855</span>        <span class="keywordflow">return</span> np.squeeze(out)</div>
<div class="line"><span class="lineno">  856</span>    <span class="keywordflow">return</span> out</div>
<div class="line"><span class="lineno">  857</span> </div>
<div class="line"><span class="lineno">  858</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a6effd449b565544a97f4d5a301c09ccb" name="a6effd449b565544a97f4d5a301c09ccb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6effd449b565544a97f4d5a301c09ccb">&#9670;&#160;</a></span>make_nonnegative()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.make_nonnegative </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>min_value</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Ensure `X.min()` &gt;= `min_value`.

Parameters
----------
X : array-like
    The matrix to make non-negative.
min_value : float, default=0
    The threshold value.

Returns
-------
array-like
    The thresholded array.

Raises
------
ValueError
    When X is sparse.
</pre> <div class="fragment"><div class="line"><span class="lineno">  901</span><span class="keyword">def </span>make_nonnegative(X, min_value=0):</div>
<div class="line"><span class="lineno">  902</span>    <span class="stringliteral">&quot;&quot;&quot;Ensure `X.min()` &gt;= `min_value`.</span></div>
<div class="line"><span class="lineno">  903</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  904</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  905</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  906</span><span class="stringliteral">    X : array-like</span></div>
<div class="line"><span class="lineno">  907</span><span class="stringliteral">        The matrix to make non-negative.</span></div>
<div class="line"><span class="lineno">  908</span><span class="stringliteral">    min_value : float, default=0</span></div>
<div class="line"><span class="lineno">  909</span><span class="stringliteral">        The threshold value.</span></div>
<div class="line"><span class="lineno">  910</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  911</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  912</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  913</span><span class="stringliteral">    array-like</span></div>
<div class="line"><span class="lineno">  914</span><span class="stringliteral">        The thresholded array.</span></div>
<div class="line"><span class="lineno">  915</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  916</span><span class="stringliteral">    Raises</span></div>
<div class="line"><span class="lineno">  917</span><span class="stringliteral">    ------</span></div>
<div class="line"><span class="lineno">  918</span><span class="stringliteral">    ValueError</span></div>
<div class="line"><span class="lineno">  919</span><span class="stringliteral">        When X is sparse.</span></div>
<div class="line"><span class="lineno">  920</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  921</span>    min_ = X.min()</div>
<div class="line"><span class="lineno">  922</span>    <span class="keywordflow">if</span> min_ &lt; min_value:</div>
<div class="line"><span class="lineno">  923</span>        <span class="keywordflow">if</span> sparse.issparse(X):</div>
<div class="line"><span class="lineno">  924</span>            <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">  925</span>                <span class="stringliteral">&quot;Cannot make the data matrix&quot;</span></div>
<div class="line"><span class="lineno">  926</span>                <span class="stringliteral">&quot; nonnegative because it is sparse.&quot;</span></div>
<div class="line"><span class="lineno">  927</span>                <span class="stringliteral">&quot; Adding a value to every entry would&quot;</span></div>
<div class="line"><span class="lineno">  928</span>                <span class="stringliteral">&quot; make it no longer sparse.&quot;</span></div>
<div class="line"><span class="lineno">  929</span>            )</div>
<div class="line"><span class="lineno">  930</span>        X = X + (min_value - min_)</div>
<div class="line"><span class="lineno">  931</span>    <span class="keywordflow">return</span> X</div>
<div class="line"><span class="lineno">  932</span> </div>
<div class="line"><span class="lineno">  933</span> </div>
<div class="line"><span class="lineno">  934</span><span class="comment"># Use at least float64 for the accumulating functions to avoid precision issue</span></div>
<div class="line"><span class="lineno">  935</span><span class="comment"># see https://github.com/numpy/numpy/issues/9393. The float64 is also retained</span></div>
<div class="line"><span class="lineno">  936</span><span class="comment"># as it is in case the float overflows</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a62197ea0dd2672d138159f71e46f0297" name="a62197ea0dd2672d138159f71e46f0297"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a62197ea0dd2672d138159f71e46f0297">&#9670;&#160;</a></span>randomized_range_finder()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.randomized_range_finder </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_iter</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>power_iteration_normalizer</em> = <code>&quot;auto&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute an orthonormal matrix whose range approximates the range of A.

Parameters
----------
A : 2D array
    The input data matrix.

size : int
    Size of the return array.

n_iter : int
    Number of power iterations used to stabilize the result.

power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'
    Whether the power iterations are normalized with step-by-step
    QR factorization (the slowest but most accurate), 'none'
    (the fastest but numerically unstable when `n_iter` is large, e.g.
    typically 5 or larger), or 'LU' factorization (numerically stable
    but can lose slightly in accuracy). The 'auto' mode applies no
    normalization if `n_iter` &lt;= 2 and switches to LU otherwise.

    .. versionadded:: 0.18

random_state : int, RandomState instance or None, default=None
    The seed of the pseudo random number generator to use when shuffling
    the data, i.e. getting the random vectors to initialize the algorithm.
    Pass an int for reproducible results across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

Returns
-------
Q : ndarray
    A (size x size) projection matrix, the range of which
    approximates well the range of the input matrix A.

Notes
-----

Follows Algorithm 4.3 of
:arxiv:`"Finding structure with randomness:
Stochastic algorithms for constructing approximate matrix decompositions"
&lt;0909.4061&gt;`
Halko, et al. (2009)

An implementation of a randomized algorithm for principal component
analysis
A. Szlam et al. 2014
</pre> <div class="fragment"><div class="line"><span class="lineno">  203</span>):</div>
<div class="line"><span class="lineno">  204</span>    <span class="stringliteral">&quot;&quot;&quot;Compute an orthonormal matrix whose range approximates the range of A.</span></div>
<div class="line"><span class="lineno">  205</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  206</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  207</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  208</span><span class="stringliteral">    A : 2D array</span></div>
<div class="line"><span class="lineno">  209</span><span class="stringliteral">        The input data matrix.</span></div>
<div class="line"><span class="lineno">  210</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  211</span><span class="stringliteral">    size : int</span></div>
<div class="line"><span class="lineno">  212</span><span class="stringliteral">        Size of the return array.</span></div>
<div class="line"><span class="lineno">  213</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  214</span><span class="stringliteral">    n_iter : int</span></div>
<div class="line"><span class="lineno">  215</span><span class="stringliteral">        Number of power iterations used to stabilize the result.</span></div>
<div class="line"><span class="lineno">  216</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  217</span><span class="stringliteral">    power_iteration_normalizer : {&#39;auto&#39;, &#39;QR&#39;, &#39;LU&#39;, &#39;none&#39;}, default=&#39;auto&#39;</span></div>
<div class="line"><span class="lineno">  218</span><span class="stringliteral">        Whether the power iterations are normalized with step-by-step</span></div>
<div class="line"><span class="lineno">  219</span><span class="stringliteral">        QR factorization (the slowest but most accurate), &#39;none&#39;</span></div>
<div class="line"><span class="lineno">  220</span><span class="stringliteral">        (the fastest but numerically unstable when `n_iter` is large, e.g.</span></div>
<div class="line"><span class="lineno">  221</span><span class="stringliteral">        typically 5 or larger), or &#39;LU&#39; factorization (numerically stable</span></div>
<div class="line"><span class="lineno">  222</span><span class="stringliteral">        but can lose slightly in accuracy). The &#39;auto&#39; mode applies no</span></div>
<div class="line"><span class="lineno">  223</span><span class="stringliteral">        normalization if `n_iter` &lt;= 2 and switches to LU otherwise.</span></div>
<div class="line"><span class="lineno">  224</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  225</span><span class="stringliteral">        .. versionadded:: 0.18</span></div>
<div class="line"><span class="lineno">  226</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  227</span><span class="stringliteral">    random_state : int, RandomState instance or None, default=None</span></div>
<div class="line"><span class="lineno">  228</span><span class="stringliteral">        The seed of the pseudo random number generator to use when shuffling</span></div>
<div class="line"><span class="lineno">  229</span><span class="stringliteral">        the data, i.e. getting the random vectors to initialize the algorithm.</span></div>
<div class="line"><span class="lineno">  230</span><span class="stringliteral">        Pass an int for reproducible results across multiple function calls.</span></div>
<div class="line"><span class="lineno">  231</span><span class="stringliteral">        See :term:`Glossary &lt;random_state&gt;`.</span></div>
<div class="line"><span class="lineno">  232</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  233</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  234</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  235</span><span class="stringliteral">    Q : ndarray</span></div>
<div class="line"><span class="lineno">  236</span><span class="stringliteral">        A (size x size) projection matrix, the range of which</span></div>
<div class="line"><span class="lineno">  237</span><span class="stringliteral">        approximates well the range of the input matrix A.</span></div>
<div class="line"><span class="lineno">  238</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  239</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  240</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  241</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  242</span><span class="stringliteral">    Follows Algorithm 4.3 of</span></div>
<div class="line"><span class="lineno">  243</span><span class="stringliteral">    :arxiv:`&quot;Finding structure with randomness:</span></div>
<div class="line"><span class="lineno">  244</span><span class="stringliteral">    Stochastic algorithms for constructing approximate matrix decompositions&quot;</span></div>
<div class="line"><span class="lineno">  245</span><span class="stringliteral">    &lt;0909.4061&gt;`</span></div>
<div class="line"><span class="lineno">  246</span><span class="stringliteral">    Halko, et al. (2009)</span></div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral">    An implementation of a randomized algorithm for principal component</span></div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">    analysis</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral">    A. Szlam et al. 2014</span></div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  252</span>    random_state = check_random_state(random_state)</div>
<div class="line"><span class="lineno">  253</span> </div>
<div class="line"><span class="lineno">  254</span>    <span class="comment"># Generating normal random vectors with shape: (A.shape[1], size)</span></div>
<div class="line"><span class="lineno">  255</span>    Q = random_state.normal(size=(A.shape[1], size))</div>
<div class="line"><span class="lineno">  256</span>    <span class="keywordflow">if</span> hasattr(A, <span class="stringliteral">&quot;dtype&quot;</span>) <span class="keywordflow">and</span> A.dtype.kind == <span class="stringliteral">&quot;f&quot;</span>:</div>
<div class="line"><span class="lineno">  257</span>        <span class="comment"># Ensure f32 is preserved as f32</span></div>
<div class="line"><span class="lineno">  258</span>        Q = Q.astype(A.dtype, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  259</span> </div>
<div class="line"><span class="lineno">  260</span>    <span class="comment"># Deal with &quot;auto&quot; mode</span></div>
<div class="line"><span class="lineno">  261</span>    <span class="keywordflow">if</span> power_iteration_normalizer == <span class="stringliteral">&quot;auto&quot;</span>:</div>
<div class="line"><span class="lineno">  262</span>        <span class="keywordflow">if</span> n_iter &lt;= 2:</div>
<div class="line"><span class="lineno">  263</span>            power_iteration_normalizer = <span class="stringliteral">&quot;none&quot;</span></div>
<div class="line"><span class="lineno">  264</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  265</span>            power_iteration_normalizer = <span class="stringliteral">&quot;LU&quot;</span></div>
<div class="line"><span class="lineno">  266</span> </div>
<div class="line"><span class="lineno">  267</span>    <span class="comment"># Perform power iterations with Q to further &#39;imprint&#39; the top</span></div>
<div class="line"><span class="lineno">  268</span>    <span class="comment"># singular vectors of A in Q</span></div>
<div class="line"><span class="lineno">  269</span>    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(n_iter):</div>
<div class="line"><span class="lineno">  270</span>        <span class="keywordflow">if</span> power_iteration_normalizer == <span class="stringliteral">&quot;none&quot;</span>:</div>
<div class="line"><span class="lineno">  271</span>            Q = safe_sparse_dot(A, Q)</div>
<div class="line"><span class="lineno">  272</span>            Q = safe_sparse_dot(A.T, Q)</div>
<div class="line"><span class="lineno">  273</span>        <span class="keywordflow">elif</span> power_iteration_normalizer == <span class="stringliteral">&quot;LU&quot;</span>:</div>
<div class="line"><span class="lineno">  274</span>            Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  275</span>            Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  276</span>        <span class="keywordflow">elif</span> power_iteration_normalizer == <span class="stringliteral">&quot;QR&quot;</span>:</div>
<div class="line"><span class="lineno">  277</span>            Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode=<span class="stringliteral">&quot;economic&quot;</span>)</div>
<div class="line"><span class="lineno">  278</span>            Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode=<span class="stringliteral">&quot;economic&quot;</span>)</div>
<div class="line"><span class="lineno">  279</span> </div>
<div class="line"><span class="lineno">  280</span>    <span class="comment"># Sample the range of A using by linear projection of Q</span></div>
<div class="line"><span class="lineno">  281</span>    <span class="comment"># Extract an orthonormal basis</span></div>
<div class="line"><span class="lineno">  282</span>    Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode=<span class="stringliteral">&quot;economic&quot;</span>)</div>
<div class="line"><span class="lineno">  283</span> </div>
<div class="line"><span class="lineno">  284</span>    <span class="keywordflow">return</span> Q</div>
<div class="line"><span class="lineno">  285</span> </div>
<div class="line"><span class="lineno">  286</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a2a15a7466b901bbecb1221aa49e95777" name="a2a15a7466b901bbecb1221aa49e95777"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2a15a7466b901bbecb1221aa49e95777">&#9670;&#160;</a></span>randomized_svd()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.randomized_svd </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>M</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_components</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>n_oversamples</em> = <code>10</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_iter</em> = <code>&quot;auto&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>power_iteration_normalizer</em> = <code>&quot;auto&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>transpose</em> = <code>&quot;auto&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>flip_sign</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>svd_lapack_driver</em> = <code>&quot;gesdd&quot;</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute a truncated randomized SVD.

This method solves the fixed-rank approximation problem described in [1]_
(problem (1.5), p5).

Parameters
----------
M : {ndarray, sparse matrix}
    Matrix to decompose.

n_components : int
    Number of singular values and vectors to extract.

n_oversamples : int, default=10
    Additional number of random vectors to sample the range of M so as
    to ensure proper conditioning. The total number of random vectors
    used to find the range of M is n_components + n_oversamples. Smaller
    number can improve speed but can negatively impact the quality of
    approximation of singular vectors and singular values. Users might wish
    to increase this parameter up to `2*k - n_components` where k is the
    effective rank, for large matrices, noisy problems, matrices with
    slowly decaying spectrums, or to increase precision accuracy. See [1]_
    (pages 5, 23 and 26).

n_iter : int or 'auto', default='auto'
    Number of power iterations. It can be used to deal with very noisy
    problems. When 'auto', it is set to 4, unless `n_components` is small
    (&lt; .1 * min(X.shape)) in which case `n_iter` is set to 7.
    This improves precision with few components. Note that in general
    users should rather increase `n_oversamples` before increasing `n_iter`
    as the principle of the randomized method is to avoid usage of these
    more costly power iterations steps. When `n_components` is equal
    or greater to the effective matrix rank and the spectrum does not
    present a slow decay, `n_iter=0` or `1` should even work fine in theory
    (see [1]_ page 9).

    .. versionchanged:: 0.18

power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'
    Whether the power iterations are normalized with step-by-step
    QR factorization (the slowest but most accurate), 'none'
    (the fastest but numerically unstable when `n_iter` is large, e.g.
    typically 5 or larger), or 'LU' factorization (numerically stable
    but can lose slightly in accuracy). The 'auto' mode applies no
    normalization if `n_iter` &lt;= 2 and switches to LU otherwise.

    .. versionadded:: 0.18

transpose : bool or 'auto', default='auto'
    Whether the algorithm should be applied to M.T instead of M. The
    result should approximately be the same. The 'auto' mode will
    trigger the transposition if M.shape[1] &gt; M.shape[0] since this
    implementation of randomized SVD tend to be a little faster in that
    case.

    .. versionchanged:: 0.18

flip_sign : bool, default=True
    The output of a singular value decomposition is only unique up to a
    permutation of the signs of the singular vectors. If `flip_sign` is
    set to `True`, the sign ambiguity is resolved by making the largest
    loadings for each component in the left singular vectors positive.

random_state : int, RandomState instance or None, default='warn'
    The seed of the pseudo random number generator to use when
    shuffling the data, i.e. getting the random vectors to initialize
    the algorithm. Pass an int for reproducible results across multiple
    function calls. See :term:`Glossary &lt;random_state&gt;`.

    .. versionchanged:: 1.2
        The default value changed from 0 to None.

svd_lapack_driver : {"gesdd", "gesvd"}, default="gesdd"
    Whether to use the more efficient divide-and-conquer approach
    (`"gesdd"`) or more general rectangular approach (`"gesvd"`) to compute
    the SVD of the matrix B, which is the projection of M into a low
    dimensional subspace, as described in [1]_.

    .. versionadded:: 1.2

Returns
-------
u : ndarray of shape (n_samples, n_components)
    Unitary matrix having left singular vectors with signs flipped as columns.
s : ndarray of shape (n_components,)
    The singular values, sorted in non-increasing order.
vh : ndarray of shape (n_components, n_features)
    Unitary matrix having right singular vectors with signs flipped as rows.

Notes
-----
This algorithm finds a (usually very good) approximate truncated
singular value decomposition using randomization to speed up the
computations. It is particularly fast on large matrices on which
you wish to extract only a small number of components. In order to
obtain further speed up, `n_iter` can be set &lt;=2 (at the cost of
loss of precision). To increase the precision it is recommended to
increase `n_oversamples`, up to `2*k-n_components` where k is the
effective rank. Usually, `n_components` is chosen to be greater than k
so increasing `n_oversamples` up to `n_components` should be enough.

References
----------
.. [1] :arxiv:`"Finding structure with randomness:
  Stochastic algorithms for constructing approximate matrix decompositions"
  &lt;0909.4061&gt;`
  Halko, et al. (2009)

.. [2] A randomized algorithm for the decomposition of matrices
  Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert

.. [3] An implementation of a randomized algorithm for principal component
  analysis A. Szlam et al. 2014

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.utils.extmath import randomized_svd
&gt;&gt;&gt; a = np.array([[1, 2, 3, 5],
...               [3, 4, 5, 6],
...               [7, 8, 9, 10]])
&gt;&gt;&gt; U, s, Vh = randomized_svd(a, n_components=2, random_state=0)
&gt;&gt;&gt; U.shape, s.shape, Vh.shape
((3, 2), (2,), (2, 4))
</pre> <div class="fragment"><div class="line"><span class="lineno">  298</span>):</div>
<div class="line"><span class="lineno">  299</span>    <span class="stringliteral">&quot;&quot;&quot;Compute a truncated randomized SVD.</span></div>
<div class="line"><span class="lineno">  300</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  301</span><span class="stringliteral">    This method solves the fixed-rank approximation problem described in [1]_</span></div>
<div class="line"><span class="lineno">  302</span><span class="stringliteral">    (problem (1.5), p5).</span></div>
<div class="line"><span class="lineno">  303</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  304</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  305</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  306</span><span class="stringliteral">    M : {ndarray, sparse matrix}</span></div>
<div class="line"><span class="lineno">  307</span><span class="stringliteral">        Matrix to decompose.</span></div>
<div class="line"><span class="lineno">  308</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  309</span><span class="stringliteral">    n_components : int</span></div>
<div class="line"><span class="lineno">  310</span><span class="stringliteral">        Number of singular values and vectors to extract.</span></div>
<div class="line"><span class="lineno">  311</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  312</span><span class="stringliteral">    n_oversamples : int, default=10</span></div>
<div class="line"><span class="lineno">  313</span><span class="stringliteral">        Additional number of random vectors to sample the range of M so as</span></div>
<div class="line"><span class="lineno">  314</span><span class="stringliteral">        to ensure proper conditioning. The total number of random vectors</span></div>
<div class="line"><span class="lineno">  315</span><span class="stringliteral">        used to find the range of M is n_components + n_oversamples. Smaller</span></div>
<div class="line"><span class="lineno">  316</span><span class="stringliteral">        number can improve speed but can negatively impact the quality of</span></div>
<div class="line"><span class="lineno">  317</span><span class="stringliteral">        approximation of singular vectors and singular values. Users might wish</span></div>
<div class="line"><span class="lineno">  318</span><span class="stringliteral">        to increase this parameter up to `2*k - n_components` where k is the</span></div>
<div class="line"><span class="lineno">  319</span><span class="stringliteral">        effective rank, for large matrices, noisy problems, matrices with</span></div>
<div class="line"><span class="lineno">  320</span><span class="stringliteral">        slowly decaying spectrums, or to increase precision accuracy. See [1]_</span></div>
<div class="line"><span class="lineno">  321</span><span class="stringliteral">        (pages 5, 23 and 26).</span></div>
<div class="line"><span class="lineno">  322</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  323</span><span class="stringliteral">    n_iter : int or &#39;auto&#39;, default=&#39;auto&#39;</span></div>
<div class="line"><span class="lineno">  324</span><span class="stringliteral">        Number of power iterations. It can be used to deal with very noisy</span></div>
<div class="line"><span class="lineno">  325</span><span class="stringliteral">        problems. When &#39;auto&#39;, it is set to 4, unless `n_components` is small</span></div>
<div class="line"><span class="lineno">  326</span><span class="stringliteral">        (&lt; .1 * min(X.shape)) in which case `n_iter` is set to 7.</span></div>
<div class="line"><span class="lineno">  327</span><span class="stringliteral">        This improves precision with few components. Note that in general</span></div>
<div class="line"><span class="lineno">  328</span><span class="stringliteral">        users should rather increase `n_oversamples` before increasing `n_iter`</span></div>
<div class="line"><span class="lineno">  329</span><span class="stringliteral">        as the principle of the randomized method is to avoid usage of these</span></div>
<div class="line"><span class="lineno">  330</span><span class="stringliteral">        more costly power iterations steps. When `n_components` is equal</span></div>
<div class="line"><span class="lineno">  331</span><span class="stringliteral">        or greater to the effective matrix rank and the spectrum does not</span></div>
<div class="line"><span class="lineno">  332</span><span class="stringliteral">        present a slow decay, `n_iter=0` or `1` should even work fine in theory</span></div>
<div class="line"><span class="lineno">  333</span><span class="stringliteral">        (see [1]_ page 9).</span></div>
<div class="line"><span class="lineno">  334</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  335</span><span class="stringliteral">        .. versionchanged:: 0.18</span></div>
<div class="line"><span class="lineno">  336</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  337</span><span class="stringliteral">    power_iteration_normalizer : {&#39;auto&#39;, &#39;QR&#39;, &#39;LU&#39;, &#39;none&#39;}, default=&#39;auto&#39;</span></div>
<div class="line"><span class="lineno">  338</span><span class="stringliteral">        Whether the power iterations are normalized with step-by-step</span></div>
<div class="line"><span class="lineno">  339</span><span class="stringliteral">        QR factorization (the slowest but most accurate), &#39;none&#39;</span></div>
<div class="line"><span class="lineno">  340</span><span class="stringliteral">        (the fastest but numerically unstable when `n_iter` is large, e.g.</span></div>
<div class="line"><span class="lineno">  341</span><span class="stringliteral">        typically 5 or larger), or &#39;LU&#39; factorization (numerically stable</span></div>
<div class="line"><span class="lineno">  342</span><span class="stringliteral">        but can lose slightly in accuracy). The &#39;auto&#39; mode applies no</span></div>
<div class="line"><span class="lineno">  343</span><span class="stringliteral">        normalization if `n_iter` &lt;= 2 and switches to LU otherwise.</span></div>
<div class="line"><span class="lineno">  344</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  345</span><span class="stringliteral">        .. versionadded:: 0.18</span></div>
<div class="line"><span class="lineno">  346</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  347</span><span class="stringliteral">    transpose : bool or &#39;auto&#39;, default=&#39;auto&#39;</span></div>
<div class="line"><span class="lineno">  348</span><span class="stringliteral">        Whether the algorithm should be applied to M.T instead of M. The</span></div>
<div class="line"><span class="lineno">  349</span><span class="stringliteral">        result should approximately be the same. The &#39;auto&#39; mode will</span></div>
<div class="line"><span class="lineno">  350</span><span class="stringliteral">        trigger the transposition if M.shape[1] &gt; M.shape[0] since this</span></div>
<div class="line"><span class="lineno">  351</span><span class="stringliteral">        implementation of randomized SVD tend to be a little faster in that</span></div>
<div class="line"><span class="lineno">  352</span><span class="stringliteral">        case.</span></div>
<div class="line"><span class="lineno">  353</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  354</span><span class="stringliteral">        .. versionchanged:: 0.18</span></div>
<div class="line"><span class="lineno">  355</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  356</span><span class="stringliteral">    flip_sign : bool, default=True</span></div>
<div class="line"><span class="lineno">  357</span><span class="stringliteral">        The output of a singular value decomposition is only unique up to a</span></div>
<div class="line"><span class="lineno">  358</span><span class="stringliteral">        permutation of the signs of the singular vectors. If `flip_sign` is</span></div>
<div class="line"><span class="lineno">  359</span><span class="stringliteral">        set to `True`, the sign ambiguity is resolved by making the largest</span></div>
<div class="line"><span class="lineno">  360</span><span class="stringliteral">        loadings for each component in the left singular vectors positive.</span></div>
<div class="line"><span class="lineno">  361</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  362</span><span class="stringliteral">    random_state : int, RandomState instance or None, default=&#39;warn&#39;</span></div>
<div class="line"><span class="lineno">  363</span><span class="stringliteral">        The seed of the pseudo random number generator to use when</span></div>
<div class="line"><span class="lineno">  364</span><span class="stringliteral">        shuffling the data, i.e. getting the random vectors to initialize</span></div>
<div class="line"><span class="lineno">  365</span><span class="stringliteral">        the algorithm. Pass an int for reproducible results across multiple</span></div>
<div class="line"><span class="lineno">  366</span><span class="stringliteral">        function calls. See :term:`Glossary &lt;random_state&gt;`.</span></div>
<div class="line"><span class="lineno">  367</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  368</span><span class="stringliteral">        .. versionchanged:: 1.2</span></div>
<div class="line"><span class="lineno">  369</span><span class="stringliteral">            The default value changed from 0 to None.</span></div>
<div class="line"><span class="lineno">  370</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  371</span><span class="stringliteral">    svd_lapack_driver : {&quot;gesdd&quot;, &quot;gesvd&quot;}, default=&quot;gesdd&quot;</span></div>
<div class="line"><span class="lineno">  372</span><span class="stringliteral">        Whether to use the more efficient divide-and-conquer approach</span></div>
<div class="line"><span class="lineno">  373</span><span class="stringliteral">        (`&quot;gesdd&quot;`) or more general rectangular approach (`&quot;gesvd&quot;`) to compute</span></div>
<div class="line"><span class="lineno">  374</span><span class="stringliteral">        the SVD of the matrix B, which is the projection of M into a low</span></div>
<div class="line"><span class="lineno">  375</span><span class="stringliteral">        dimensional subspace, as described in [1]_.</span></div>
<div class="line"><span class="lineno">  376</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  377</span><span class="stringliteral">        .. versionadded:: 1.2</span></div>
<div class="line"><span class="lineno">  378</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  379</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  380</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  381</span><span class="stringliteral">    u : ndarray of shape (n_samples, n_components)</span></div>
<div class="line"><span class="lineno">  382</span><span class="stringliteral">        Unitary matrix having left singular vectors with signs flipped as columns.</span></div>
<div class="line"><span class="lineno">  383</span><span class="stringliteral">    s : ndarray of shape (n_components,)</span></div>
<div class="line"><span class="lineno">  384</span><span class="stringliteral">        The singular values, sorted in non-increasing order.</span></div>
<div class="line"><span class="lineno">  385</span><span class="stringliteral">    vh : ndarray of shape (n_components, n_features)</span></div>
<div class="line"><span class="lineno">  386</span><span class="stringliteral">        Unitary matrix having right singular vectors with signs flipped as rows.</span></div>
<div class="line"><span class="lineno">  387</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  388</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  389</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  390</span><span class="stringliteral">    This algorithm finds a (usually very good) approximate truncated</span></div>
<div class="line"><span class="lineno">  391</span><span class="stringliteral">    singular value decomposition using randomization to speed up the</span></div>
<div class="line"><span class="lineno">  392</span><span class="stringliteral">    computations. It is particularly fast on large matrices on which</span></div>
<div class="line"><span class="lineno">  393</span><span class="stringliteral">    you wish to extract only a small number of components. In order to</span></div>
<div class="line"><span class="lineno">  394</span><span class="stringliteral">    obtain further speed up, `n_iter` can be set &lt;=2 (at the cost of</span></div>
<div class="line"><span class="lineno">  395</span><span class="stringliteral">    loss of precision). To increase the precision it is recommended to</span></div>
<div class="line"><span class="lineno">  396</span><span class="stringliteral">    increase `n_oversamples`, up to `2*k-n_components` where k is the</span></div>
<div class="line"><span class="lineno">  397</span><span class="stringliteral">    effective rank. Usually, `n_components` is chosen to be greater than k</span></div>
<div class="line"><span class="lineno">  398</span><span class="stringliteral">    so increasing `n_oversamples` up to `n_components` should be enough.</span></div>
<div class="line"><span class="lineno">  399</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  400</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  401</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  402</span><span class="stringliteral">    .. [1] :arxiv:`&quot;Finding structure with randomness:</span></div>
<div class="line"><span class="lineno">  403</span><span class="stringliteral">      Stochastic algorithms for constructing approximate matrix decompositions&quot;</span></div>
<div class="line"><span class="lineno">  404</span><span class="stringliteral">      &lt;0909.4061&gt;`</span></div>
<div class="line"><span class="lineno">  405</span><span class="stringliteral">      Halko, et al. (2009)</span></div>
<div class="line"><span class="lineno">  406</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  407</span><span class="stringliteral">    .. [2] A randomized algorithm for the decomposition of matrices</span></div>
<div class="line"><span class="lineno">  408</span><span class="stringliteral">      Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</span></div>
<div class="line"><span class="lineno">  409</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  410</span><span class="stringliteral">    .. [3] An implementation of a randomized algorithm for principal component</span></div>
<div class="line"><span class="lineno">  411</span><span class="stringliteral">      analysis A. Szlam et al. 2014</span></div>
<div class="line"><span class="lineno">  412</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  413</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno">  414</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  415</span><span class="stringliteral">    &gt;&gt;&gt; import numpy as np</span></div>
<div class="line"><span class="lineno">  416</span><span class="stringliteral">    &gt;&gt;&gt; from sklearn.utils.extmath import randomized_svd</span></div>
<div class="line"><span class="lineno">  417</span><span class="stringliteral">    &gt;&gt;&gt; a = np.array([[1, 2, 3, 5],</span></div>
<div class="line"><span class="lineno">  418</span><span class="stringliteral">    ...               [3, 4, 5, 6],</span></div>
<div class="line"><span class="lineno">  419</span><span class="stringliteral">    ...               [7, 8, 9, 10]])</span></div>
<div class="line"><span class="lineno">  420</span><span class="stringliteral">    &gt;&gt;&gt; U, s, Vh = randomized_svd(a, n_components=2, random_state=0)</span></div>
<div class="line"><span class="lineno">  421</span><span class="stringliteral">    &gt;&gt;&gt; U.shape, s.shape, Vh.shape</span></div>
<div class="line"><span class="lineno">  422</span><span class="stringliteral">    ((3, 2), (2,), (2, 4))</span></div>
<div class="line"><span class="lineno">  423</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  424</span>    <span class="keywordflow">if</span> isinstance(M, (sparse.lil_matrix, sparse.dok_matrix)):</div>
<div class="line"><span class="lineno">  425</span>        warnings.warn(</div>
<div class="line"><span class="lineno">  426</span>            <span class="stringliteral">&quot;Calculating SVD of a {} is expensive. &quot;</span></div>
<div class="line"><span class="lineno">  427</span>            <span class="stringliteral">&quot;csr_matrix is more efficient.&quot;</span>.format(type(M).__name__),</div>
<div class="line"><span class="lineno">  428</span>            sparse.SparseEfficiencyWarning,</div>
<div class="line"><span class="lineno">  429</span>        )</div>
<div class="line"><span class="lineno">  430</span> </div>
<div class="line"><span class="lineno">  431</span>    random_state = check_random_state(random_state)</div>
<div class="line"><span class="lineno">  432</span>    n_random = n_components + n_oversamples</div>
<div class="line"><span class="lineno">  433</span>    n_samples, n_features = M.shape</div>
<div class="line"><span class="lineno">  434</span> </div>
<div class="line"><span class="lineno">  435</span>    <span class="keywordflow">if</span> n_iter == <span class="stringliteral">&quot;auto&quot;</span>:</div>
<div class="line"><span class="lineno">  436</span>        <span class="comment"># Checks if the number of iterations is explicitly specified</span></div>
<div class="line"><span class="lineno">  437</span>        <span class="comment"># Adjust n_iter. 7 was found a good compromise for PCA. See #5299</span></div>
<div class="line"><span class="lineno">  438</span>        n_iter = 7 <span class="keywordflow">if</span> n_components &lt; 0.1 * min(M.shape) <span class="keywordflow">else</span> 4</div>
<div class="line"><span class="lineno">  439</span> </div>
<div class="line"><span class="lineno">  440</span>    <span class="keywordflow">if</span> transpose == <span class="stringliteral">&quot;auto&quot;</span>:</div>
<div class="line"><span class="lineno">  441</span>        transpose = n_samples &lt; n_features</div>
<div class="line"><span class="lineno">  442</span>    <span class="keywordflow">if</span> transpose:</div>
<div class="line"><span class="lineno">  443</span>        <span class="comment"># this implementation is a bit faster with smaller shape[1]</span></div>
<div class="line"><span class="lineno">  444</span>        M = M.T</div>
<div class="line"><span class="lineno">  445</span> </div>
<div class="line"><span class="lineno">  446</span>    Q = randomized_range_finder(</div>
<div class="line"><span class="lineno">  447</span>        M,</div>
<div class="line"><span class="lineno">  448</span>        size=n_random,</div>
<div class="line"><span class="lineno">  449</span>        n_iter=n_iter,</div>
<div class="line"><span class="lineno">  450</span>        power_iteration_normalizer=power_iteration_normalizer,</div>
<div class="line"><span class="lineno">  451</span>        random_state=random_state,</div>
<div class="line"><span class="lineno">  452</span>    )</div>
<div class="line"><span class="lineno">  453</span> </div>
<div class="line"><span class="lineno">  454</span>    <span class="comment"># project M to the (k + p) dimensional space using the basis vectors</span></div>
<div class="line"><span class="lineno">  455</span>    B = safe_sparse_dot(Q.T, M)</div>
<div class="line"><span class="lineno">  456</span> </div>
<div class="line"><span class="lineno">  457</span>    <span class="comment"># compute the SVD on the thin matrix: (k + p) wide</span></div>
<div class="line"><span class="lineno">  458</span>    Uhat, s, Vt = linalg.svd(B, full_matrices=<span class="keyword">False</span>, lapack_driver=svd_lapack_driver)</div>
<div class="line"><span class="lineno">  459</span> </div>
<div class="line"><span class="lineno">  460</span>    del B</div>
<div class="line"><span class="lineno">  461</span>    U = np.dot(Q, Uhat)</div>
<div class="line"><span class="lineno">  462</span> </div>
<div class="line"><span class="lineno">  463</span>    <span class="keywordflow">if</span> flip_sign:</div>
<div class="line"><span class="lineno">  464</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> transpose:</div>
<div class="line"><span class="lineno">  465</span>            U, Vt = svd_flip(U, Vt)</div>
<div class="line"><span class="lineno">  466</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  467</span>            <span class="comment"># In case of transpose u_based_decision=false</span></div>
<div class="line"><span class="lineno">  468</span>            <span class="comment"># to actually flip based on u and not v.</span></div>
<div class="line"><span class="lineno">  469</span>            U, Vt = svd_flip(U, Vt, u_based_decision=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  470</span> </div>
<div class="line"><span class="lineno">  471</span>    <span class="keywordflow">if</span> transpose:</div>
<div class="line"><span class="lineno">  472</span>        <span class="comment"># transpose back the results according to the input convention</span></div>
<div class="line"><span class="lineno">  473</span>        <span class="keywordflow">return</span> Vt[:n_components, :].T, s[:n_components], U[:, :n_components].T</div>
<div class="line"><span class="lineno">  474</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  475</span>        <span class="keywordflow">return</span> U[:, :n_components], s[:n_components], Vt[:n_components, :]</div>
<div class="line"><span class="lineno">  476</span> </div>
<div class="line"><span class="lineno">  477</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ac3d9849e3c602ac89b59603797d8042c" name="ac3d9849e3c602ac89b59603797d8042c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac3d9849e3c602ac89b59603797d8042c">&#9670;&#160;</a></span>row_norms()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.row_norms </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>squared</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Row-wise (squared) Euclidean norm of X.

Equivalent to np.sqrt((X * X).sum(axis=1)), but also supports sparse
matrices and does not create an X.shape-sized temporary.

Performs no input validation.

Parameters
----------
X : array-like
    The input array.
squared : bool, default=False
    If True, return squared norms.

Returns
-------
array-like
    The row-wise (squared) Euclidean norm of X.
</pre> <div class="fragment"><div class="line"><span class="lineno">   52</span><span class="keyword">def </span>row_norms(X, squared=False):</div>
<div class="line"><span class="lineno">   53</span>    <span class="stringliteral">&quot;&quot;&quot;Row-wise (squared) Euclidean norm of X.</span></div>
<div class="line"><span class="lineno">   54</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   55</span><span class="stringliteral">    Equivalent to np.sqrt((X * X).sum(axis=1)), but also supports sparse</span></div>
<div class="line"><span class="lineno">   56</span><span class="stringliteral">    matrices and does not create an X.shape-sized temporary.</span></div>
<div class="line"><span class="lineno">   57</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   58</span><span class="stringliteral">    Performs no input validation.</span></div>
<div class="line"><span class="lineno">   59</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   60</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   61</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   62</span><span class="stringliteral">    X : array-like</span></div>
<div class="line"><span class="lineno">   63</span><span class="stringliteral">        The input array.</span></div>
<div class="line"><span class="lineno">   64</span><span class="stringliteral">    squared : bool, default=False</span></div>
<div class="line"><span class="lineno">   65</span><span class="stringliteral">        If True, return squared norms.</span></div>
<div class="line"><span class="lineno">   66</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   67</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">   68</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">   69</span><span class="stringliteral">    array-like</span></div>
<div class="line"><span class="lineno">   70</span><span class="stringliteral">        The row-wise (squared) Euclidean norm of X.</span></div>
<div class="line"><span class="lineno">   71</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   72</span>    <span class="keywordflow">if</span> sparse.issparse(X):</div>
<div class="line"><span class="lineno">   73</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> isinstance(X, sparse.csr_matrix):</div>
<div class="line"><span class="lineno">   74</span>            X = sparse.csr_matrix(X)</div>
<div class="line"><span class="lineno">   75</span>        norms = csr_row_norms(X)</div>
<div class="line"><span class="lineno">   76</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   77</span>        norms = np.einsum(<span class="stringliteral">&quot;ij,ij-&gt;i&quot;</span>, X, X)</div>
<div class="line"><span class="lineno">   78</span> </div>
<div class="line"><span class="lineno">   79</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> squared:</div>
<div class="line"><span class="lineno">   80</span>        np.sqrt(norms, norms)</div>
<div class="line"><span class="lineno">   81</span>    <span class="keywordflow">return</span> norms</div>
<div class="line"><span class="lineno">   82</span> </div>
<div class="line"><span class="lineno">   83</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a64fcf3937cf878995669fdb47fe66bc5" name="a64fcf3937cf878995669fdb47fe66bc5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a64fcf3937cf878995669fdb47fe66bc5">&#9670;&#160;</a></span>safe_sparse_dot()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.safe_sparse_dot </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>b</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>dense_output</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Dot product that handle the sparse matrix case correctly.

Parameters
----------
a : {ndarray, sparse matrix}
b : {ndarray, sparse matrix}
dense_output : bool, default=False
    When False, ``a`` and ``b`` both being sparse will yield sparse output.
    When True, output will always be a dense array.

Returns
-------
dot_product : {ndarray, sparse matrix}
    Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.
</pre> <div class="fragment"><div class="line"><span class="lineno">  156</span><span class="keyword">def </span>safe_sparse_dot(a, b, *, dense_output=False):</div>
<div class="line"><span class="lineno">  157</span>    <span class="stringliteral">&quot;&quot;&quot;Dot product that handle the sparse matrix case correctly.</span></div>
<div class="line"><span class="lineno">  158</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  159</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  160</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  161</span><span class="stringliteral">    a : {ndarray, sparse matrix}</span></div>
<div class="line"><span class="lineno">  162</span><span class="stringliteral">    b : {ndarray, sparse matrix}</span></div>
<div class="line"><span class="lineno">  163</span><span class="stringliteral">    dense_output : bool, default=False</span></div>
<div class="line"><span class="lineno">  164</span><span class="stringliteral">        When False, ``a`` and ``b`` both being sparse will yield sparse output.</span></div>
<div class="line"><span class="lineno">  165</span><span class="stringliteral">        When True, output will always be a dense array.</span></div>
<div class="line"><span class="lineno">  166</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  167</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  168</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  169</span><span class="stringliteral">    dot_product : {ndarray, sparse matrix}</span></div>
<div class="line"><span class="lineno">  170</span><span class="stringliteral">        Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.</span></div>
<div class="line"><span class="lineno">  171</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  172</span>    <span class="keywordflow">if</span> a.ndim &gt; 2 <span class="keywordflow">or</span> b.ndim &gt; 2:</div>
<div class="line"><span class="lineno">  173</span>        <span class="keywordflow">if</span> sparse.issparse(a):</div>
<div class="line"><span class="lineno">  174</span>            <span class="comment"># sparse is always 2D. Implies b is 3D+</span></div>
<div class="line"><span class="lineno">  175</span>            <span class="comment"># [i, j] @ [k, ..., l, m, n] -&gt; [i, k, ..., l, n]</span></div>
<div class="line"><span class="lineno">  176</span>            b_ = np.rollaxis(b, -2)</div>
<div class="line"><span class="lineno">  177</span>            b_2d = b_.reshape((b.shape[-2], -1))</div>
<div class="line"><span class="lineno">  178</span>            ret = a @ b_2d</div>
<div class="line"><span class="lineno">  179</span>            ret = ret.reshape(a.shape[0], *b_.shape[1:])</div>
<div class="line"><span class="lineno">  180</span>        <span class="keywordflow">elif</span> sparse.issparse(b):</div>
<div class="line"><span class="lineno">  181</span>            <span class="comment"># sparse is always 2D. Implies a is 3D+</span></div>
<div class="line"><span class="lineno">  182</span>            <span class="comment"># [k, ..., l, m] @ [i, j] -&gt; [k, ..., l, j]</span></div>
<div class="line"><span class="lineno">  183</span>            a_2d = a.reshape(-1, a.shape[-1])</div>
<div class="line"><span class="lineno">  184</span>            ret = a_2d @ b</div>
<div class="line"><span class="lineno">  185</span>            ret = ret.reshape(*a.shape[:-1], b.shape[1])</div>
<div class="line"><span class="lineno">  186</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  187</span>            ret = np.dot(a, b)</div>
<div class="line"><span class="lineno">  188</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  189</span>        ret = a @ b</div>
<div class="line"><span class="lineno">  190</span> </div>
<div class="line"><span class="lineno">  191</span>    <span class="keywordflow">if</span> (</div>
<div class="line"><span class="lineno">  192</span>        sparse.issparse(a)</div>
<div class="line"><span class="lineno">  193</span>        <span class="keywordflow">and</span> sparse.issparse(b)</div>
<div class="line"><span class="lineno">  194</span>        <span class="keywordflow">and</span> dense_output</div>
<div class="line"><span class="lineno">  195</span>        <span class="keywordflow">and</span> hasattr(ret, <span class="stringliteral">&quot;toarray&quot;</span>)</div>
<div class="line"><span class="lineno">  196</span>    ):</div>
<div class="line"><span class="lineno">  197</span>        <span class="keywordflow">return</span> ret.toarray()</div>
<div class="line"><span class="lineno">  198</span>    <span class="keywordflow">return</span> ret</div>
<div class="line"><span class="lineno">  199</span> </div>
<div class="line"><span class="lineno">  200</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a5a030f453adb35a00f5f860b31cd3f3e" name="a5a030f453adb35a00f5f860b31cd3f3e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5a030f453adb35a00f5f860b31cd3f3e">&#9670;&#160;</a></span>softmax()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.softmax </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Calculate the softmax function.

The softmax function is calculated by
np.exp(X) / np.sum(np.exp(X), axis=1)

This will cause overflow when large values are exponentiated.
Hence the largest value in each row is subtracted from each data
point to prevent this.

Parameters
----------
X : array-like of float of shape (M, N)
    Argument to the logistic function.

copy : bool, default=True
    Copy X or not.

Returns
-------
out : ndarray of shape (M, N)
    Softmax function evaluated at every point in x.
</pre> <div class="fragment"><div class="line"><span class="lineno">  859</span><span class="keyword">def </span>softmax(X, copy=True):</div>
<div class="line"><span class="lineno">  860</span>    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  861</span><span class="stringliteral">    Calculate the softmax function.</span></div>
<div class="line"><span class="lineno">  862</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  863</span><span class="stringliteral">    The softmax function is calculated by</span></div>
<div class="line"><span class="lineno">  864</span><span class="stringliteral">    np.exp(X) / np.sum(np.exp(X), axis=1)</span></div>
<div class="line"><span class="lineno">  865</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  866</span><span class="stringliteral">    This will cause overflow when large values are exponentiated.</span></div>
<div class="line"><span class="lineno">  867</span><span class="stringliteral">    Hence the largest value in each row is subtracted from each data</span></div>
<div class="line"><span class="lineno">  868</span><span class="stringliteral">    point to prevent this.</span></div>
<div class="line"><span class="lineno">  869</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  870</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  871</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  872</span><span class="stringliteral">    X : array-like of float of shape (M, N)</span></div>
<div class="line"><span class="lineno">  873</span><span class="stringliteral">        Argument to the logistic function.</span></div>
<div class="line"><span class="lineno">  874</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  875</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno">  876</span><span class="stringliteral">        Copy X or not.</span></div>
<div class="line"><span class="lineno">  877</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  878</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  879</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  880</span><span class="stringliteral">    out : ndarray of shape (M, N)</span></div>
<div class="line"><span class="lineno">  881</span><span class="stringliteral">        Softmax function evaluated at every point in x.</span></div>
<div class="line"><span class="lineno">  882</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  883</span>    xp, is_array_api = get_namespace(X)</div>
<div class="line"><span class="lineno">  884</span>    <span class="keywordflow">if</span> copy:</div>
<div class="line"><span class="lineno">  885</span>        X = xp.asarray(X, copy=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  886</span>    max_prob = xp.reshape(xp.max(X, axis=1), (-1, 1))</div>
<div class="line"><span class="lineno">  887</span>    X -= max_prob</div>
<div class="line"><span class="lineno">  888</span> </div>
<div class="line"><span class="lineno">  889</span>    <span class="keywordflow">if</span> xp.__name__ <span class="keywordflow">in</span> {<span class="stringliteral">&quot;numpy&quot;</span>, <span class="stringliteral">&quot;numpy.array_api&quot;</span>}:</div>
<div class="line"><span class="lineno">  890</span>        <span class="comment"># optimization for NumPy arrays</span></div>
<div class="line"><span class="lineno">  891</span>        np.exp(X, out=np.asarray(X))</div>
<div class="line"><span class="lineno">  892</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  893</span>        <span class="comment"># array_api does not have `out=`</span></div>
<div class="line"><span class="lineno">  894</span>        X = xp.exp(X)</div>
<div class="line"><span class="lineno">  895</span> </div>
<div class="line"><span class="lineno">  896</span>    sum_prob = xp.reshape(xp.sum(X, axis=1), (-1, 1))</div>
<div class="line"><span class="lineno">  897</span>    X /= sum_prob</div>
<div class="line"><span class="lineno">  898</span>    <span class="keywordflow">return</span> X</div>
<div class="line"><span class="lineno">  899</span> </div>
<div class="line"><span class="lineno">  900</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a6a39b4b4b5f631c8f0652304fda26a54" name="a6a39b4b4b5f631c8f0652304fda26a54"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6a39b4b4b5f631c8f0652304fda26a54">&#9670;&#160;</a></span>squared_norm()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.squared_norm </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Squared Euclidean or Frobenius norm of x.

Faster than norm(x) ** 2.

Parameters
----------
x : array-like
    The input array which could be either be a vector or a 2 dimensional array.

Returns
-------
float
    The Euclidean norm when x is a vector, the Frobenius norm when x
    is a matrix (2-d array).
</pre> <div class="fragment"><div class="line"><span class="lineno">   26</span><span class="keyword">def </span>squared_norm(x):</div>
<div class="line"><span class="lineno">   27</span>    <span class="stringliteral">&quot;&quot;&quot;Squared Euclidean or Frobenius norm of x.</span></div>
<div class="line"><span class="lineno">   28</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   29</span><span class="stringliteral">    Faster than norm(x) ** 2.</span></div>
<div class="line"><span class="lineno">   30</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   31</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   32</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   33</span><span class="stringliteral">    x : array-like</span></div>
<div class="line"><span class="lineno">   34</span><span class="stringliteral">        The input array which could be either be a vector or a 2 dimensional array.</span></div>
<div class="line"><span class="lineno">   35</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   36</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">   37</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">   38</span><span class="stringliteral">    float</span></div>
<div class="line"><span class="lineno">   39</span><span class="stringliteral">        The Euclidean norm when x is a vector, the Frobenius norm when x</span></div>
<div class="line"><span class="lineno">   40</span><span class="stringliteral">        is a matrix (2-d array).</span></div>
<div class="line"><span class="lineno">   41</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   42</span>    x = np.ravel(x, order=<span class="stringliteral">&quot;K&quot;</span>)</div>
<div class="line"><span class="lineno">   43</span>    <span class="keywordflow">if</span> np.issubdtype(x.dtype, np.integer):</div>
<div class="line"><span class="lineno">   44</span>        warnings.warn(</div>
<div class="line"><span class="lineno">   45</span>            <span class="stringliteral">&quot;Array type is integer, np.dot may overflow. &quot;</span></div>
<div class="line"><span class="lineno">   46</span>            <span class="stringliteral">&quot;Data should be float type to avoid this issue&quot;</span>,</div>
<div class="line"><span class="lineno">   47</span>            UserWarning,</div>
<div class="line"><span class="lineno">   48</span>        )</div>
<div class="line"><span class="lineno">   49</span>    <span class="keywordflow">return</span> np.dot(x, x)</div>
<div class="line"><span class="lineno">   50</span> </div>
<div class="line"><span class="lineno">   51</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a7c2292c77ff1021f1f2c33dc369da054" name="a7c2292c77ff1021f1f2c33dc369da054"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7c2292c77ff1021f1f2c33dc369da054">&#9670;&#160;</a></span>stable_cumsum()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.stable_cumsum </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>arr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>axis</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>rtol</em> = <code>1e-05</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>atol</em> = <code>1e-08</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Use high precision for cumsum and check that final value matches sum.

Warns if the final cumulative sum does not match the sum (up to the chosen
tolerance).

Parameters
----------
arr : array-like
    To be cumulatively summed as flat.
axis : int, default=None
    Axis along which the cumulative sum is computed.
    The default (None) is to compute the cumsum over the flattened array.
rtol : float, default=1e-05
    Relative tolerance, see ``np.allclose``.
atol : float, default=1e-08
    Absolute tolerance, see ``np.allclose``.

Returns
-------
out : ndarray
    Array with the cumulative sums along the chosen axis.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1115</span><span class="keyword">def </span>stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):</div>
<div class="line"><span class="lineno"> 1116</span>    <span class="stringliteral">&quot;&quot;&quot;Use high precision for cumsum and check that final value matches sum.</span></div>
<div class="line"><span class="lineno"> 1117</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1118</span><span class="stringliteral">    Warns if the final cumulative sum does not match the sum (up to the chosen</span></div>
<div class="line"><span class="lineno"> 1119</span><span class="stringliteral">    tolerance).</span></div>
<div class="line"><span class="lineno"> 1120</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1121</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 1122</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno"> 1123</span><span class="stringliteral">    arr : array-like</span></div>
<div class="line"><span class="lineno"> 1124</span><span class="stringliteral">        To be cumulatively summed as flat.</span></div>
<div class="line"><span class="lineno"> 1125</span><span class="stringliteral">    axis : int, default=None</span></div>
<div class="line"><span class="lineno"> 1126</span><span class="stringliteral">        Axis along which the cumulative sum is computed.</span></div>
<div class="line"><span class="lineno"> 1127</span><span class="stringliteral">        The default (None) is to compute the cumsum over the flattened array.</span></div>
<div class="line"><span class="lineno"> 1128</span><span class="stringliteral">    rtol : float, default=1e-05</span></div>
<div class="line"><span class="lineno"> 1129</span><span class="stringliteral">        Relative tolerance, see ``np.allclose``.</span></div>
<div class="line"><span class="lineno"> 1130</span><span class="stringliteral">    atol : float, default=1e-08</span></div>
<div class="line"><span class="lineno"> 1131</span><span class="stringliteral">        Absolute tolerance, see ``np.allclose``.</span></div>
<div class="line"><span class="lineno"> 1132</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1133</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 1134</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno"> 1135</span><span class="stringliteral">    out : ndarray</span></div>
<div class="line"><span class="lineno"> 1136</span><span class="stringliteral">        Array with the cumulative sums along the chosen axis.</span></div>
<div class="line"><span class="lineno"> 1137</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1138</span>    out = np.cumsum(arr, axis=axis, dtype=np.float64)</div>
<div class="line"><span class="lineno"> 1139</span>    expected = np.sum(arr, axis=axis, dtype=np.float64)</div>
<div class="line"><span class="lineno"> 1140</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> np.all(</div>
<div class="line"><span class="lineno"> 1141</span>        np.isclose(</div>
<div class="line"><span class="lineno"> 1142</span>            out.take(-1, axis=axis), expected, rtol=rtol, atol=atol, equal_nan=<span class="keyword">True</span></div>
<div class="line"><span class="lineno"> 1143</span>        )</div>
<div class="line"><span class="lineno"> 1144</span>    ):</div>
<div class="line"><span class="lineno"> 1145</span>        warnings.warn(</div>
<div class="line"><span class="lineno"> 1146</span>            <span class="stringliteral">&quot;cumsum was found to be unstable: &quot;</span></div>
<div class="line"><span class="lineno"> 1147</span>            <span class="stringliteral">&quot;its last element does not correspond to sum&quot;</span>,</div>
<div class="line"><span class="lineno"> 1148</span>            RuntimeWarning,</div>
<div class="line"><span class="lineno"> 1149</span>        )</div>
<div class="line"><span class="lineno"> 1150</span>    <span class="keywordflow">return</span> out</div>
</div><!-- fragment -->
</div>
</div>
<a id="aeb7d1817c732296d017ee3ebd9c9ded7" name="aeb7d1817c732296d017ee3ebd9c9ded7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeb7d1817c732296d017ee3ebd9c9ded7">&#9670;&#160;</a></span>svd_flip()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.svd_flip </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>u</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>v</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>u_based_decision</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Sign correction to ensure deterministic output from SVD.

Adjusts the columns of u and the rows of v such that the loadings in the
columns in u that are largest in absolute value are always positive.

Parameters
----------
u : ndarray
    Parameters u and v are the output of `linalg.svd` or
    :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner
    dimensions so one can compute `np.dot(u * s, v)`.

v : ndarray
    Parameters u and v are the output of `linalg.svd` or
    :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner
    dimensions so one can compute `np.dot(u * s, v)`.
    The input v should really be called vt to be consistent with scipy's
    output.

u_based_decision : bool, default=True
    If True, use the columns of u as the basis for sign flipping.
    Otherwise, use the rows of v. The choice of which variable to base the
    decision on is generally algorithm dependent.

Returns
-------
u_adjusted : ndarray
    Array u with adjusted columns and the same dimensions as u.

v_adjusted : ndarray
    Array v with adjusted rows and the same dimensions as v.
</pre> <div class="fragment"><div class="line"><span class="lineno">  766</span><span class="keyword">def </span>svd_flip(u, v, u_based_decision=True):</div>
<div class="line"><span class="lineno">  767</span>    <span class="stringliteral">&quot;&quot;&quot;Sign correction to ensure deterministic output from SVD.</span></div>
<div class="line"><span class="lineno">  768</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  769</span><span class="stringliteral">    Adjusts the columns of u and the rows of v such that the loadings in the</span></div>
<div class="line"><span class="lineno">  770</span><span class="stringliteral">    columns in u that are largest in absolute value are always positive.</span></div>
<div class="line"><span class="lineno">  771</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  772</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  773</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  774</span><span class="stringliteral">    u : ndarray</span></div>
<div class="line"><span class="lineno">  775</span><span class="stringliteral">        Parameters u and v are the output of `linalg.svd` or</span></div>
<div class="line"><span class="lineno">  776</span><span class="stringliteral">        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner</span></div>
<div class="line"><span class="lineno">  777</span><span class="stringliteral">        dimensions so one can compute `np.dot(u * s, v)`.</span></div>
<div class="line"><span class="lineno">  778</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  779</span><span class="stringliteral">    v : ndarray</span></div>
<div class="line"><span class="lineno">  780</span><span class="stringliteral">        Parameters u and v are the output of `linalg.svd` or</span></div>
<div class="line"><span class="lineno">  781</span><span class="stringliteral">        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner</span></div>
<div class="line"><span class="lineno">  782</span><span class="stringliteral">        dimensions so one can compute `np.dot(u * s, v)`.</span></div>
<div class="line"><span class="lineno">  783</span><span class="stringliteral">        The input v should really be called vt to be consistent with scipy&#39;s</span></div>
<div class="line"><span class="lineno">  784</span><span class="stringliteral">        output.</span></div>
<div class="line"><span class="lineno">  785</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  786</span><span class="stringliteral">    u_based_decision : bool, default=True</span></div>
<div class="line"><span class="lineno">  787</span><span class="stringliteral">        If True, use the columns of u as the basis for sign flipping.</span></div>
<div class="line"><span class="lineno">  788</span><span class="stringliteral">        Otherwise, use the rows of v. The choice of which variable to base the</span></div>
<div class="line"><span class="lineno">  789</span><span class="stringliteral">        decision on is generally algorithm dependent.</span></div>
<div class="line"><span class="lineno">  790</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  791</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  792</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  793</span><span class="stringliteral">    u_adjusted : ndarray</span></div>
<div class="line"><span class="lineno">  794</span><span class="stringliteral">        Array u with adjusted columns and the same dimensions as u.</span></div>
<div class="line"><span class="lineno">  795</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  796</span><span class="stringliteral">    v_adjusted : ndarray</span></div>
<div class="line"><span class="lineno">  797</span><span class="stringliteral">        Array v with adjusted rows and the same dimensions as v.</span></div>
<div class="line"><span class="lineno">  798</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  799</span>    <span class="keywordflow">if</span> u_based_decision:</div>
<div class="line"><span class="lineno">  800</span>        <span class="comment"># columns of u, rows of v</span></div>
<div class="line"><span class="lineno">  801</span>        max_abs_cols = np.argmax(np.abs(u), axis=0)</div>
<div class="line"><span class="lineno">  802</span>        signs = np.sign(u[max_abs_cols, range(u.shape[1])])</div>
<div class="line"><span class="lineno">  803</span>        u *= signs</div>
<div class="line"><span class="lineno">  804</span>        v *= signs[:, np.newaxis]</div>
<div class="line"><span class="lineno">  805</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  806</span>        <span class="comment"># rows of v, columns of u</span></div>
<div class="line"><span class="lineno">  807</span>        max_abs_rows = np.argmax(np.abs(v), axis=1)</div>
<div class="line"><span class="lineno">  808</span>        signs = np.sign(v[range(v.shape[0]), max_abs_rows])</div>
<div class="line"><span class="lineno">  809</span>        u *= signs</div>
<div class="line"><span class="lineno">  810</span>        v *= signs[:, np.newaxis]</div>
<div class="line"><span class="lineno">  811</span>    <span class="keywordflow">return</span> u, v</div>
<div class="line"><span class="lineno">  812</span> </div>
<div class="line"><span class="lineno">  813</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a02c4499812c3353e167e0b535456d72f" name="a02c4499812c3353e167e0b535456d72f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a02c4499812c3353e167e0b535456d72f">&#9670;&#160;</a></span>weighted_mode()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.utils.extmath.weighted_mode </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>w</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>axis</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Return an array of the weighted modal (most common) value in the passed array.

If there is more than one such value, only the first is returned.
The bin-count for the modal bins is also returned.

This is an extension of the algorithm in scipy.stats.mode.

Parameters
----------
a : array-like of shape (n_samples,)
    Array of which values to find mode(s).
w : array-like of shape (n_samples,)
    Array of weights for each value.
axis : int, default=0
    Axis along which to operate. Default is 0, i.e. the first axis.

Returns
-------
vals : ndarray
    Array of modal values.
score : ndarray
    Array of weighted counts for each mode.

See Also
--------
scipy.stats.mode: Calculates the Modal (most common) value of array elements
    along specified axis.

Examples
--------
&gt;&gt;&gt; from sklearn.utils.extmath import weighted_mode
&gt;&gt;&gt; x = [4, 1, 4, 2, 4, 2]
&gt;&gt;&gt; weights = [1, 1, 1, 1, 1, 1]
&gt;&gt;&gt; weighted_mode(x, weights)
(array([4.]), array([3.]))

The value 4 appears three times: with uniform weights, the result is
simply the mode of the distribution.

&gt;&gt;&gt; weights = [1, 3, 0.5, 1.5, 1, 2]  # deweight the 4's
&gt;&gt;&gt; weighted_mode(x, weights)
(array([2.]), array([3.5]))

The value 2 has the highest score: it appears twice with weights of
1.5 and 2: the sum of these is 3.5.
</pre> <div class="fragment"><div class="line"><span class="lineno">  633</span><span class="keyword">def </span>weighted_mode(a, w, *, axis=0):</div>
<div class="line"><span class="lineno">  634</span>    <span class="stringliteral">&quot;&quot;&quot;Return an array of the weighted modal (most common) value in the passed array.</span></div>
<div class="line"><span class="lineno">  635</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  636</span><span class="stringliteral">    If there is more than one such value, only the first is returned.</span></div>
<div class="line"><span class="lineno">  637</span><span class="stringliteral">    The bin-count for the modal bins is also returned.</span></div>
<div class="line"><span class="lineno">  638</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  639</span><span class="stringliteral">    This is an extension of the algorithm in scipy.stats.mode.</span></div>
<div class="line"><span class="lineno">  640</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  641</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  642</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  643</span><span class="stringliteral">    a : array-like of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  644</span><span class="stringliteral">        Array of which values to find mode(s).</span></div>
<div class="line"><span class="lineno">  645</span><span class="stringliteral">    w : array-like of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  646</span><span class="stringliteral">        Array of weights for each value.</span></div>
<div class="line"><span class="lineno">  647</span><span class="stringliteral">    axis : int, default=0</span></div>
<div class="line"><span class="lineno">  648</span><span class="stringliteral">        Axis along which to operate. Default is 0, i.e. the first axis.</span></div>
<div class="line"><span class="lineno">  649</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  650</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  651</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  652</span><span class="stringliteral">    vals : ndarray</span></div>
<div class="line"><span class="lineno">  653</span><span class="stringliteral">        Array of modal values.</span></div>
<div class="line"><span class="lineno">  654</span><span class="stringliteral">    score : ndarray</span></div>
<div class="line"><span class="lineno">  655</span><span class="stringliteral">        Array of weighted counts for each mode.</span></div>
<div class="line"><span class="lineno">  656</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  657</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  658</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  659</span><span class="stringliteral">    scipy.stats.mode: Calculates the Modal (most common) value of array elements</span></div>
<div class="line"><span class="lineno">  660</span><span class="stringliteral">        along specified axis.</span></div>
<div class="line"><span class="lineno">  661</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  662</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno">  663</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  664</span><span class="stringliteral">    &gt;&gt;&gt; from sklearn.utils.extmath import weighted_mode</span></div>
<div class="line"><span class="lineno">  665</span><span class="stringliteral">    &gt;&gt;&gt; x = [4, 1, 4, 2, 4, 2]</span></div>
<div class="line"><span class="lineno">  666</span><span class="stringliteral">    &gt;&gt;&gt; weights = [1, 1, 1, 1, 1, 1]</span></div>
<div class="line"><span class="lineno">  667</span><span class="stringliteral">    &gt;&gt;&gt; weighted_mode(x, weights)</span></div>
<div class="line"><span class="lineno">  668</span><span class="stringliteral">    (array([4.]), array([3.]))</span></div>
<div class="line"><span class="lineno">  669</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  670</span><span class="stringliteral">    The value 4 appears three times: with uniform weights, the result is</span></div>
<div class="line"><span class="lineno">  671</span><span class="stringliteral">    simply the mode of the distribution.</span></div>
<div class="line"><span class="lineno">  672</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  673</span><span class="stringliteral">    &gt;&gt;&gt; weights = [1, 3, 0.5, 1.5, 1, 2]  # deweight the 4&#39;s</span></div>
<div class="line"><span class="lineno">  674</span><span class="stringliteral">    &gt;&gt;&gt; weighted_mode(x, weights)</span></div>
<div class="line"><span class="lineno">  675</span><span class="stringliteral">    (array([2.]), array([3.5]))</span></div>
<div class="line"><span class="lineno">  676</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  677</span><span class="stringliteral">    The value 2 has the highest score: it appears twice with weights of</span></div>
<div class="line"><span class="lineno">  678</span><span class="stringliteral">    1.5 and 2: the sum of these is 3.5.</span></div>
<div class="line"><span class="lineno">  679</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  680</span>    <span class="keywordflow">if</span> axis <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  681</span>        a = np.ravel(a)</div>
<div class="line"><span class="lineno">  682</span>        w = np.ravel(w)</div>
<div class="line"><span class="lineno">  683</span>        axis = 0</div>
<div class="line"><span class="lineno">  684</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  685</span>        a = np.asarray(a)</div>
<div class="line"><span class="lineno">  686</span>        w = np.asarray(w)</div>
<div class="line"><span class="lineno">  687</span> </div>
<div class="line"><span class="lineno">  688</span>    <span class="keywordflow">if</span> a.shape != w.shape:</div>
<div class="line"><span class="lineno">  689</span>        w = np.full(a.shape, w, dtype=w.dtype)</div>
<div class="line"><span class="lineno">  690</span> </div>
<div class="line"><span class="lineno">  691</span>    scores = np.unique(np.ravel(a))  <span class="comment"># get ALL unique values</span></div>
<div class="line"><span class="lineno">  692</span>    testshape = list(a.shape)</div>
<div class="line"><span class="lineno">  693</span>    testshape[axis] = 1</div>
<div class="line"><span class="lineno">  694</span>    oldmostfreq = np.zeros(testshape)</div>
<div class="line"><span class="lineno">  695</span>    oldcounts = np.zeros(testshape)</div>
<div class="line"><span class="lineno">  696</span>    <span class="keywordflow">for</span> score <span class="keywordflow">in</span> scores:</div>
<div class="line"><span class="lineno">  697</span>        template = np.zeros(a.shape)</div>
<div class="line"><span class="lineno">  698</span>        ind = a == score</div>
<div class="line"><span class="lineno">  699</span>        template[ind] = w[ind]</div>
<div class="line"><span class="lineno">  700</span>        counts = np.expand_dims(np.sum(template, axis), axis)</div>
<div class="line"><span class="lineno">  701</span>        mostfrequent = np.where(counts &gt; oldcounts, score, oldmostfreq)</div>
<div class="line"><span class="lineno">  702</span>        oldcounts = np.maximum(counts, oldcounts)</div>
<div class="line"><span class="lineno">  703</span>        oldmostfreq = mostfrequent</div>
<div class="line"><span class="lineno">  704</span>    <span class="keywordflow">return</span> mostfrequent, oldcounts</div>
<div class="line"><span class="lineno">  705</span> </div>
<div class="line"><span class="lineno">  706</span> </div>
</div><!-- fragment -->
</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
