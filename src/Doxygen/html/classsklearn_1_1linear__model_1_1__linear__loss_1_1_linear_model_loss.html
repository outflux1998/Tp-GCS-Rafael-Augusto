<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.linear_model._linear_loss.LinearModelLoss Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model.html">linear_model</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model_1_1__linear__loss.html">_linear_loss</a></li><li class="navelem"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html">LinearModelLoss</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">sklearn.linear_model._linear_loss.LinearModelLoss Class Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a72a722924c5fe32413be8fe95e0f4e0e" id="r_a72a722924c5fe32413be8fe95e0f4e0e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#a72a722924c5fe32413be8fe95e0f4e0e">__init__</a> (self, <a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#ae45960a0f1e675ef15d88e7cc4d459b9">base_loss</a>, <a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#a9ffd044562d8ae3655156076c529a852">fit_intercept</a>)</td></tr>
<tr class="separator:a72a722924c5fe32413be8fe95e0f4e0e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a90d92487a6eb759c86797dfbfe23153c" id="r_a90d92487a6eb759c86797dfbfe23153c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#a90d92487a6eb759c86797dfbfe23153c">init_zero_coef</a> (self, X, dtype=None)</td></tr>
<tr class="separator:a90d92487a6eb759c86797dfbfe23153c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad6aa682bba552f631193aeb838bbb4fb" id="r_ad6aa682bba552f631193aeb838bbb4fb"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#ad6aa682bba552f631193aeb838bbb4fb">weight_intercept</a> (self, coef)</td></tr>
<tr class="separator:ad6aa682bba552f631193aeb838bbb4fb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2c1020b46d5e61ae8bda153835bdaa2b" id="r_a2c1020b46d5e61ae8bda153835bdaa2b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#a2c1020b46d5e61ae8bda153835bdaa2b">weight_intercept_raw</a> (self, coef, X)</td></tr>
<tr class="separator:a2c1020b46d5e61ae8bda153835bdaa2b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a73116d9f130017dab7f045eab9a27dd6" id="r_a73116d9f130017dab7f045eab9a27dd6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#a73116d9f130017dab7f045eab9a27dd6">l2_penalty</a> (self, weights, l2_reg_strength)</td></tr>
<tr class="separator:a73116d9f130017dab7f045eab9a27dd6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a502d1c80fd8bf5c9f2fefeb71c12d15f" id="r_a502d1c80fd8bf5c9f2fefeb71c12d15f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#a502d1c80fd8bf5c9f2fefeb71c12d15f">loss</a> (self, coef, X, y, sample_weight=None, l2_reg_strength=0.0, n_threads=1, raw_prediction=None)</td></tr>
<tr class="separator:a502d1c80fd8bf5c9f2fefeb71c12d15f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aadedf74d59c6f362cac8adef57efc958" id="r_aadedf74d59c6f362cac8adef57efc958"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#aadedf74d59c6f362cac8adef57efc958">loss_gradient</a> (self, coef, X, y, sample_weight=None, l2_reg_strength=0.0, n_threads=1, raw_prediction=None)</td></tr>
<tr class="separator:aadedf74d59c6f362cac8adef57efc958"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad058c2c80b2430c6cece8b6e5f37a3c2" id="r_ad058c2c80b2430c6cece8b6e5f37a3c2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#ad058c2c80b2430c6cece8b6e5f37a3c2">gradient</a> (self, coef, X, y, sample_weight=None, l2_reg_strength=0.0, n_threads=1, raw_prediction=None)</td></tr>
<tr class="separator:ad058c2c80b2430c6cece8b6e5f37a3c2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adfe9025aefcc02979e18a6ef2dd52a32" id="r_adfe9025aefcc02979e18a6ef2dd52a32"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#adfe9025aefcc02979e18a6ef2dd52a32">gradient_hessian</a> (self, coef, X, y, sample_weight=None, l2_reg_strength=0.0, n_threads=1, gradient_out=None, hessian_out=None, raw_prediction=None)</td></tr>
<tr class="separator:adfe9025aefcc02979e18a6ef2dd52a32"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aabd136d35f0b757f2d9d4da9a41ceabd" id="r_aabd136d35f0b757f2d9d4da9a41ceabd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#aabd136d35f0b757f2d9d4da9a41ceabd">gradient_hessian_product</a> (self, coef, X, y, sample_weight=None, l2_reg_strength=0.0, n_threads=1)</td></tr>
<tr class="separator:aabd136d35f0b757f2d9d4da9a41ceabd"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-attribs" name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr class="memitem:ae45960a0f1e675ef15d88e7cc4d459b9" id="r_ae45960a0f1e675ef15d88e7cc4d459b9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#ae45960a0f1e675ef15d88e7cc4d459b9">base_loss</a></td></tr>
<tr class="separator:ae45960a0f1e675ef15d88e7cc4d459b9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9ffd044562d8ae3655156076c529a852" id="r_a9ffd044562d8ae3655156076c529a852"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__linear__loss_1_1_linear_model_loss.html#a9ffd044562d8ae3655156076c529a852">fit_intercept</a></td></tr>
<tr class="separator:a9ffd044562d8ae3655156076c529a852"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">General class for loss functions with raw_prediction = X @ coef + intercept.

Note that raw_prediction is also known as linear predictor.

The loss is the sum of per sample losses and includes a term for L2
regularization::

    loss = sum_i s_i loss(y_i, X_i @ coef + intercept)
           + 1/2 * l2_reg_strength * ||coef||_2^2

with sample weights s_i=1 if sample_weight=None.

Gradient and hessian, for simplicity without intercept, are::

    gradient = X.T @ loss.gradient + l2_reg_strength * coef
    hessian = X.T @ diag(loss.hessian) @ X + l2_reg_strength * identity

Conventions:
    if fit_intercept:
        n_dof =  n_features + 1
    else:
        n_dof = n_features

    if base_loss.is_multiclass:
        coef.shape = (n_classes, n_dof) or ravelled (n_classes * n_dof,)
    else:
        coef.shape = (n_dof,)

    The intercept term is at the end of the coef array:
    if base_loss.is_multiclass:
        if coef.shape (n_classes, n_dof):
            intercept = coef[:, -1]
        if coef.shape (n_classes * n_dof,)
            intercept = coef[n_features::n_dof] = coef[(n_dof-1)::n_dof]
        intercept.shape = (n_classes,)
    else:
        intercept = coef[-1]

Note: If coef has shape (n_classes * n_dof,), the 2d-array can be reconstructed as

    coef.reshape((n_classes, -1), order="F")

The option order="F" makes coef[:, i] contiguous. This, in turn, makes the
coefficients without intercept, coef[:, :-1], contiguous and speeds up
matrix-vector computations.

Note: If the average loss per sample is wanted instead of the sum of the loss per
sample, one can simply use a rescaled sample_weight such that
sum(sample_weight) = 1.

Parameters
----------
base_loss : instance of class BaseLoss from sklearn._loss.
fit_intercept : bool
</pre> </div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a72a722924c5fe32413be8fe95e0f4e0e" name="a72a722924c5fe32413be8fe95e0f4e0e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a72a722924c5fe32413be8fe95e0f4e0e">&#9670;&#160;</a></span>__init__()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.__init__ </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>base_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   66</span>    <span class="keyword">def </span>__init__(self, base_loss, fit_intercept):</div>
<div class="line"><span class="lineno">   67</span>        self.base_loss = base_loss</div>
<div class="line"><span class="lineno">   68</span>        self.fit_intercept = fit_intercept</div>
<div class="line"><span class="lineno">   69</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="ad058c2c80b2430c6cece8b6e5f37a3c2" name="ad058c2c80b2430c6cece8b6e5f37a3c2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad058c2c80b2430c6cece8b6e5f37a3c2">&#9670;&#160;</a></span>gradient()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.gradient </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coef</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_strength</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_threads</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>raw_prediction</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Computes the gradient w.r.t. coef.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space). If provided, these are used. If
    None, then raw_prediction = X @ coef + intercept is calculated.

Returns
-------
gradient : ndarray of shape coef.shape
     The gradient of the loss.
</pre> <div class="fragment"><div class="line"><span class="lineno">  312</span>    ):</div>
<div class="line"><span class="lineno">  313</span>        <span class="stringliteral">&quot;&quot;&quot;Computes the gradient w.r.t. coef.</span></div>
<div class="line"><span class="lineno">  314</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  315</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  316</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  317</span><span class="stringliteral">        coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)</span></div>
<div class="line"><span class="lineno">  318</span><span class="stringliteral">            Coefficients of a linear model.</span></div>
<div class="line"><span class="lineno">  319</span><span class="stringliteral">            If shape (n_classes * n_dof,), the classes of one feature are contiguous,</span></div>
<div class="line"><span class="lineno">  320</span><span class="stringliteral">            i.e. one reconstructs the 2d-array via</span></div>
<div class="line"><span class="lineno">  321</span><span class="stringliteral">            coef.reshape((n_classes, -1), order=&quot;F&quot;).</span></div>
<div class="line"><span class="lineno">  322</span><span class="stringliteral">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  323</span><span class="stringliteral">            Training data.</span></div>
<div class="line"><span class="lineno">  324</span><span class="stringliteral">        y : contiguous array of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  325</span><span class="stringliteral">            Observed, true target values.</span></div>
<div class="line"><span class="lineno">  326</span><span class="stringliteral">        sample_weight : None or contiguous array of shape (n_samples,), default=None</span></div>
<div class="line"><span class="lineno">  327</span><span class="stringliteral">            Sample weights.</span></div>
<div class="line"><span class="lineno">  328</span><span class="stringliteral">        l2_reg_strength : float, default=0.0</span></div>
<div class="line"><span class="lineno">  329</span><span class="stringliteral">            L2 regularization strength</span></div>
<div class="line"><span class="lineno">  330</span><span class="stringliteral">        n_threads : int, default=1</span></div>
<div class="line"><span class="lineno">  331</span><span class="stringliteral">            Number of OpenMP threads to use.</span></div>
<div class="line"><span class="lineno">  332</span><span class="stringliteral">        raw_prediction : C-contiguous array of shape (n_samples,) or array of \</span></div>
<div class="line"><span class="lineno">  333</span><span class="stringliteral">            shape (n_samples, n_classes)</span></div>
<div class="line"><span class="lineno">  334</span><span class="stringliteral">            Raw prediction values (in link space). If provided, these are used. If</span></div>
<div class="line"><span class="lineno">  335</span><span class="stringliteral">            None, then raw_prediction = X @ coef + intercept is calculated.</span></div>
<div class="line"><span class="lineno">  336</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  337</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  338</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  339</span><span class="stringliteral">        gradient : ndarray of shape coef.shape</span></div>
<div class="line"><span class="lineno">  340</span><span class="stringliteral">             The gradient of the loss.</span></div>
<div class="line"><span class="lineno">  341</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  342</span>        n_features, n_classes = X.shape[1], self.base_loss.n_classes</div>
<div class="line"><span class="lineno">  343</span>        n_dof = n_features + int(self.fit_intercept)</div>
<div class="line"><span class="lineno">  344</span> </div>
<div class="line"><span class="lineno">  345</span>        <span class="keywordflow">if</span> raw_prediction <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  346</span>            weights, intercept, raw_prediction = self.weight_intercept_raw(coef, X)</div>
<div class="line"><span class="lineno">  347</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  348</span>            weights, intercept = self.weight_intercept(coef)</div>
<div class="line"><span class="lineno">  349</span> </div>
<div class="line"><span class="lineno">  350</span>        grad_pointwise = self.base_loss.gradient(</div>
<div class="line"><span class="lineno">  351</span>            y_true=y,</div>
<div class="line"><span class="lineno">  352</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  353</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  354</span>            n_threads=n_threads,</div>
<div class="line"><span class="lineno">  355</span>        )</div>
<div class="line"><span class="lineno">  356</span> </div>
<div class="line"><span class="lineno">  357</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  358</span>            grad = np.empty_like(coef, dtype=weights.dtype)</div>
<div class="line"><span class="lineno">  359</span>            grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights</div>
<div class="line"><span class="lineno">  360</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  361</span>                grad[-1] = grad_pointwise.sum()</div>
<div class="line"><span class="lineno">  362</span>            <span class="keywordflow">return</span> grad</div>
<div class="line"><span class="lineno">  363</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  364</span>            grad = np.empty((n_classes, n_dof), dtype=weights.dtype, order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  365</span>            <span class="comment"># gradient.shape = (n_samples, n_classes)</span></div>
<div class="line"><span class="lineno">  366</span>            grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights</div>
<div class="line"><span class="lineno">  367</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  368</span>                grad[:, -1] = grad_pointwise.sum(axis=0)</div>
<div class="line"><span class="lineno">  369</span>            <span class="keywordflow">if</span> coef.ndim == 1:</div>
<div class="line"><span class="lineno">  370</span>                <span class="keywordflow">return</span> grad.ravel(order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  371</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  372</span>                <span class="keywordflow">return</span> grad</div>
<div class="line"><span class="lineno">  373</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="adfe9025aefcc02979e18a6ef2dd52a32" name="adfe9025aefcc02979e18a6ef2dd52a32"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adfe9025aefcc02979e18a6ef2dd52a32">&#9670;&#160;</a></span>gradient_hessian()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.gradient_hessian </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coef</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_strength</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_threads</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>gradient_out</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>hessian_out</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>raw_prediction</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Computes gradient and hessian w.r.t. coef.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.
gradient_out : None or ndarray of shape coef.shape
    A location into which the gradient is stored. If None, a new array
    might be created.
hessian_out : None or ndarray
    A location into which the hessian is stored. If None, a new array
    might be created.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space). If provided, these are used. If
    None, then raw_prediction = X @ coef + intercept is calculated.

Returns
-------
gradient : ndarray of shape coef.shape
     The gradient of the loss.

hessian : ndarray
    Hessian matrix.

hessian_warning : bool
    True if pointwise hessian has more than half of its elements non-positive.
</pre> <div class="fragment"><div class="line"><span class="lineno">  385</span>    ):</div>
<div class="line"><span class="lineno">  386</span>        <span class="stringliteral">&quot;&quot;&quot;Computes gradient and hessian w.r.t. coef.</span></div>
<div class="line"><span class="lineno">  387</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  388</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  389</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  390</span><span class="stringliteral">        coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)</span></div>
<div class="line"><span class="lineno">  391</span><span class="stringliteral">            Coefficients of a linear model.</span></div>
<div class="line"><span class="lineno">  392</span><span class="stringliteral">            If shape (n_classes * n_dof,), the classes of one feature are contiguous,</span></div>
<div class="line"><span class="lineno">  393</span><span class="stringliteral">            i.e. one reconstructs the 2d-array via</span></div>
<div class="line"><span class="lineno">  394</span><span class="stringliteral">            coef.reshape((n_classes, -1), order=&quot;F&quot;).</span></div>
<div class="line"><span class="lineno">  395</span><span class="stringliteral">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  396</span><span class="stringliteral">            Training data.</span></div>
<div class="line"><span class="lineno">  397</span><span class="stringliteral">        y : contiguous array of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  398</span><span class="stringliteral">            Observed, true target values.</span></div>
<div class="line"><span class="lineno">  399</span><span class="stringliteral">        sample_weight : None or contiguous array of shape (n_samples,), default=None</span></div>
<div class="line"><span class="lineno">  400</span><span class="stringliteral">            Sample weights.</span></div>
<div class="line"><span class="lineno">  401</span><span class="stringliteral">        l2_reg_strength : float, default=0.0</span></div>
<div class="line"><span class="lineno">  402</span><span class="stringliteral">            L2 regularization strength</span></div>
<div class="line"><span class="lineno">  403</span><span class="stringliteral">        n_threads : int, default=1</span></div>
<div class="line"><span class="lineno">  404</span><span class="stringliteral">            Number of OpenMP threads to use.</span></div>
<div class="line"><span class="lineno">  405</span><span class="stringliteral">        gradient_out : None or ndarray of shape coef.shape</span></div>
<div class="line"><span class="lineno">  406</span><span class="stringliteral">            A location into which the gradient is stored. If None, a new array</span></div>
<div class="line"><span class="lineno">  407</span><span class="stringliteral">            might be created.</span></div>
<div class="line"><span class="lineno">  408</span><span class="stringliteral">        hessian_out : None or ndarray</span></div>
<div class="line"><span class="lineno">  409</span><span class="stringliteral">            A location into which the hessian is stored. If None, a new array</span></div>
<div class="line"><span class="lineno">  410</span><span class="stringliteral">            might be created.</span></div>
<div class="line"><span class="lineno">  411</span><span class="stringliteral">        raw_prediction : C-contiguous array of shape (n_samples,) or array of \</span></div>
<div class="line"><span class="lineno">  412</span><span class="stringliteral">            shape (n_samples, n_classes)</span></div>
<div class="line"><span class="lineno">  413</span><span class="stringliteral">            Raw prediction values (in link space). If provided, these are used. If</span></div>
<div class="line"><span class="lineno">  414</span><span class="stringliteral">            None, then raw_prediction = X @ coef + intercept is calculated.</span></div>
<div class="line"><span class="lineno">  415</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  416</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  417</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  418</span><span class="stringliteral">        gradient : ndarray of shape coef.shape</span></div>
<div class="line"><span class="lineno">  419</span><span class="stringliteral">             The gradient of the loss.</span></div>
<div class="line"><span class="lineno">  420</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  421</span><span class="stringliteral">        hessian : ndarray</span></div>
<div class="line"><span class="lineno">  422</span><span class="stringliteral">            Hessian matrix.</span></div>
<div class="line"><span class="lineno">  423</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  424</span><span class="stringliteral">        hessian_warning : bool</span></div>
<div class="line"><span class="lineno">  425</span><span class="stringliteral">            True if pointwise hessian has more than half of its elements non-positive.</span></div>
<div class="line"><span class="lineno">  426</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  427</span>        n_samples, n_features = X.shape</div>
<div class="line"><span class="lineno">  428</span>        n_dof = n_features + int(self.fit_intercept)</div>
<div class="line"><span class="lineno">  429</span> </div>
<div class="line"><span class="lineno">  430</span>        <span class="keywordflow">if</span> raw_prediction <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  431</span>            weights, intercept, raw_prediction = self.weight_intercept_raw(coef, X)</div>
<div class="line"><span class="lineno">  432</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  433</span>            weights, intercept = self.weight_intercept(coef)</div>
<div class="line"><span class="lineno">  434</span> </div>
<div class="line"><span class="lineno">  435</span>        grad_pointwise, hess_pointwise = self.base_loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  436</span>            y_true=y,</div>
<div class="line"><span class="lineno">  437</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  438</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  439</span>            n_threads=n_threads,</div>
<div class="line"><span class="lineno">  440</span>        )</div>
<div class="line"><span class="lineno">  441</span> </div>
<div class="line"><span class="lineno">  442</span>        <span class="comment"># For non-canonical link functions and far away from the optimum, the pointwise</span></div>
<div class="line"><span class="lineno">  443</span>        <span class="comment"># hessian can be negative. We take care that 75% ot the hessian entries are</span></div>
<div class="line"><span class="lineno">  444</span>        <span class="comment"># positive.</span></div>
<div class="line"><span class="lineno">  445</span>        hessian_warning = np.mean(hess_pointwise &lt;= 0) &gt; 0.25</div>
<div class="line"><span class="lineno">  446</span>        hess_pointwise = np.abs(hess_pointwise)</div>
<div class="line"><span class="lineno">  447</span> </div>
<div class="line"><span class="lineno">  448</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  449</span>            <span class="comment"># gradient</span></div>
<div class="line"><span class="lineno">  450</span>            <span class="keywordflow">if</span> gradient_out <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  451</span>                grad = np.empty_like(coef, dtype=weights.dtype)</div>
<div class="line"><span class="lineno">  452</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  453</span>                grad = gradient_out</div>
<div class="line"><span class="lineno">  454</span>            grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights</div>
<div class="line"><span class="lineno">  455</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  456</span>                grad[-1] = grad_pointwise.sum()</div>
<div class="line"><span class="lineno">  457</span> </div>
<div class="line"><span class="lineno">  458</span>            <span class="comment"># hessian</span></div>
<div class="line"><span class="lineno">  459</span>            <span class="keywordflow">if</span> hessian_out <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  460</span>                hess = np.empty(shape=(n_dof, n_dof), dtype=weights.dtype)</div>
<div class="line"><span class="lineno">  461</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  462</span>                hess = hessian_out</div>
<div class="line"><span class="lineno">  463</span> </div>
<div class="line"><span class="lineno">  464</span>            <span class="keywordflow">if</span> hessian_warning:</div>
<div class="line"><span class="lineno">  465</span>                <span class="comment"># Exit early without computing the hessian.</span></div>
<div class="line"><span class="lineno">  466</span>                <span class="keywordflow">return</span> grad, hess, hessian_warning</div>
<div class="line"><span class="lineno">  467</span> </div>
<div class="line"><span class="lineno">  468</span>            <span class="comment"># TODO: This &quot;sandwich product&quot;, X&#39; diag(W) X, is the main computational</span></div>
<div class="line"><span class="lineno">  469</span>            <span class="comment"># bottleneck for solvers. A dedicated Cython routine might improve it</span></div>
<div class="line"><span class="lineno">  470</span>            <span class="comment"># exploiting the symmetry (as opposed to, e.g., BLAS gemm).</span></div>
<div class="line"><span class="lineno">  471</span>            <span class="keywordflow">if</span> sparse.issparse(X):</div>
<div class="line"><span class="lineno">  472</span>                hess[:n_features, :n_features] = (</div>
<div class="line"><span class="lineno">  473</span>                    X.T</div>
<div class="line"><span class="lineno">  474</span>                    @ sparse.dia_matrix(</div>
<div class="line"><span class="lineno">  475</span>                        (hess_pointwise, 0), shape=(n_samples, n_samples)</div>
<div class="line"><span class="lineno">  476</span>                    )</div>
<div class="line"><span class="lineno">  477</span>                    @ X</div>
<div class="line"><span class="lineno">  478</span>                ).toarray()</div>
<div class="line"><span class="lineno">  479</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  480</span>                <span class="comment"># np.einsum may use less memory but the following, using BLAS matrix</span></div>
<div class="line"><span class="lineno">  481</span>                <span class="comment"># multiplication (gemm), is by far faster.</span></div>
<div class="line"><span class="lineno">  482</span>                WX = hess_pointwise[:, <span class="keywordtype">None</span>] * X</div>
<div class="line"><span class="lineno">  483</span>                hess[:n_features, :n_features] = np.dot(X.T, WX)</div>
<div class="line"><span class="lineno">  484</span> </div>
<div class="line"><span class="lineno">  485</span>            <span class="keywordflow">if</span> l2_reg_strength &gt; 0:</div>
<div class="line"><span class="lineno">  486</span>                <span class="comment"># The L2 penalty enters the Hessian on the diagonal only. To add those</span></div>
<div class="line"><span class="lineno">  487</span>                <span class="comment"># terms, we use a flattened view on the array.</span></div>
<div class="line"><span class="lineno">  488</span>                hess.reshape(-1)[</div>
<div class="line"><span class="lineno">  489</span>                    : (n_features * n_dof) : (n_dof + 1)</div>
<div class="line"><span class="lineno">  490</span>                ] += l2_reg_strength</div>
<div class="line"><span class="lineno">  491</span> </div>
<div class="line"><span class="lineno">  492</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  493</span>                <span class="comment"># With intercept included as added column to X, the hessian becomes</span></div>
<div class="line"><span class="lineno">  494</span>                <span class="comment"># hess = (X, 1)&#39; @ diag(h) @ (X, 1)</span></div>
<div class="line"><span class="lineno">  495</span>                <span class="comment">#      = (X&#39; @ diag(h) @ X, X&#39; @ h)</span></div>
<div class="line"><span class="lineno">  496</span>                <span class="comment">#        (           h @ X, sum(h))</span></div>
<div class="line"><span class="lineno">  497</span>                <span class="comment"># The left upper part has already been filled, it remains to compute</span></div>
<div class="line"><span class="lineno">  498</span>                <span class="comment"># the last row and the last column.</span></div>
<div class="line"><span class="lineno">  499</span>                Xh = X.T @ hess_pointwise</div>
<div class="line"><span class="lineno">  500</span>                hess[:-1, -1] = Xh</div>
<div class="line"><span class="lineno">  501</span>                hess[-1, :-1] = Xh</div>
<div class="line"><span class="lineno">  502</span>                hess[-1, -1] = hess_pointwise.sum()</div>
<div class="line"><span class="lineno">  503</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  504</span>            <span class="comment"># Here we may safely assume HalfMultinomialLoss aka categorical</span></div>
<div class="line"><span class="lineno">  505</span>            <span class="comment"># cross-entropy.</span></div>
<div class="line"><span class="lineno">  506</span>            <span class="keywordflow">raise</span> NotImplementedError</div>
<div class="line"><span class="lineno">  507</span> </div>
<div class="line"><span class="lineno">  508</span>        <span class="keywordflow">return</span> grad, hess, hessian_warning</div>
<div class="line"><span class="lineno">  509</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aabd136d35f0b757f2d9d4da9a41ceabd" name="aabd136d35f0b757f2d9d4da9a41ceabd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aabd136d35f0b757f2d9d4da9a41ceabd">&#9670;&#160;</a></span>gradient_hessian_product()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.gradient_hessian_product </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coef</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_strength</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_threads</em> = <code>1</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Computes gradient and hessp (hessian product function) w.r.t. coef.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.

Returns
-------
gradient : ndarray of shape coef.shape
     The gradient of the loss.

hessp : callable
    Function that takes in a vector input of shape of gradient and
    and returns matrix-vector product with hessian.
</pre> <div class="fragment"><div class="line"><span class="lineno">  512</span>    ):</div>
<div class="line"><span class="lineno">  513</span>        <span class="stringliteral">&quot;&quot;&quot;Computes gradient and hessp (hessian product function) w.r.t. coef.</span></div>
<div class="line"><span class="lineno">  514</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  515</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  516</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  517</span><span class="stringliteral">        coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)</span></div>
<div class="line"><span class="lineno">  518</span><span class="stringliteral">            Coefficients of a linear model.</span></div>
<div class="line"><span class="lineno">  519</span><span class="stringliteral">            If shape (n_classes * n_dof,), the classes of one feature are contiguous,</span></div>
<div class="line"><span class="lineno">  520</span><span class="stringliteral">            i.e. one reconstructs the 2d-array via</span></div>
<div class="line"><span class="lineno">  521</span><span class="stringliteral">            coef.reshape((n_classes, -1), order=&quot;F&quot;).</span></div>
<div class="line"><span class="lineno">  522</span><span class="stringliteral">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  523</span><span class="stringliteral">            Training data.</span></div>
<div class="line"><span class="lineno">  524</span><span class="stringliteral">        y : contiguous array of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  525</span><span class="stringliteral">            Observed, true target values.</span></div>
<div class="line"><span class="lineno">  526</span><span class="stringliteral">        sample_weight : None or contiguous array of shape (n_samples,), default=None</span></div>
<div class="line"><span class="lineno">  527</span><span class="stringliteral">            Sample weights.</span></div>
<div class="line"><span class="lineno">  528</span><span class="stringliteral">        l2_reg_strength : float, default=0.0</span></div>
<div class="line"><span class="lineno">  529</span><span class="stringliteral">            L2 regularization strength</span></div>
<div class="line"><span class="lineno">  530</span><span class="stringliteral">        n_threads : int, default=1</span></div>
<div class="line"><span class="lineno">  531</span><span class="stringliteral">            Number of OpenMP threads to use.</span></div>
<div class="line"><span class="lineno">  532</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  533</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  534</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  535</span><span class="stringliteral">        gradient : ndarray of shape coef.shape</span></div>
<div class="line"><span class="lineno">  536</span><span class="stringliteral">             The gradient of the loss.</span></div>
<div class="line"><span class="lineno">  537</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  538</span><span class="stringliteral">        hessp : callable</span></div>
<div class="line"><span class="lineno">  539</span><span class="stringliteral">            Function that takes in a vector input of shape of gradient and</span></div>
<div class="line"><span class="lineno">  540</span><span class="stringliteral">            and returns matrix-vector product with hessian.</span></div>
<div class="line"><span class="lineno">  541</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  542</span>        (n_samples, n_features), n_classes = X.shape, self.base_loss.n_classes</div>
<div class="line"><span class="lineno">  543</span>        n_dof = n_features + int(self.fit_intercept)</div>
<div class="line"><span class="lineno">  544</span>        weights, intercept, raw_prediction = self.weight_intercept_raw(coef, X)</div>
<div class="line"><span class="lineno">  545</span> </div>
<div class="line"><span class="lineno">  546</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  547</span>            grad_pointwise, hess_pointwise = self.base_loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  548</span>                y_true=y,</div>
<div class="line"><span class="lineno">  549</span>                raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  550</span>                sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  551</span>                n_threads=n_threads,</div>
<div class="line"><span class="lineno">  552</span>            )</div>
<div class="line"><span class="lineno">  553</span>            grad = np.empty_like(coef, dtype=weights.dtype)</div>
<div class="line"><span class="lineno">  554</span>            grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights</div>
<div class="line"><span class="lineno">  555</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  556</span>                grad[-1] = grad_pointwise.sum()</div>
<div class="line"><span class="lineno">  557</span> </div>
<div class="line"><span class="lineno">  558</span>            <span class="comment"># Precompute as much as possible: hX, hX_sum and hessian_sum</span></div>
<div class="line"><span class="lineno">  559</span>            hessian_sum = hess_pointwise.sum()</div>
<div class="line"><span class="lineno">  560</span>            <span class="keywordflow">if</span> sparse.issparse(X):</div>
<div class="line"><span class="lineno">  561</span>                hX = (</div>
<div class="line"><span class="lineno">  562</span>                    sparse.dia_matrix((hess_pointwise, 0), shape=(n_samples, n_samples))</div>
<div class="line"><span class="lineno">  563</span>                    @ X</div>
<div class="line"><span class="lineno">  564</span>                )</div>
<div class="line"><span class="lineno">  565</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  566</span>                hX = hess_pointwise[:, np.newaxis] * X</div>
<div class="line"><span class="lineno">  567</span> </div>
<div class="line"><span class="lineno">  568</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  569</span>                <span class="comment"># Calculate the double derivative with respect to intercept.</span></div>
<div class="line"><span class="lineno">  570</span>                <span class="comment"># Note: In case hX is sparse, hX.sum is a matrix object.</span></div>
<div class="line"><span class="lineno">  571</span>                hX_sum = np.squeeze(np.asarray(hX.sum(axis=0)))</div>
<div class="line"><span class="lineno">  572</span>                <span class="comment"># prevent squeezing to zero-dim array if n_features == 1</span></div>
<div class="line"><span class="lineno">  573</span>                hX_sum = np.atleast_1d(hX_sum)</div>
<div class="line"><span class="lineno">  574</span> </div>
<div class="line"><span class="lineno">  575</span>            <span class="comment"># With intercept included and l2_reg_strength = 0, hessp returns</span></div>
<div class="line"><span class="lineno">  576</span>            <span class="comment"># res = (X, 1)&#39; @ diag(h) @ (X, 1) @ s</span></div>
<div class="line"><span class="lineno">  577</span>            <span class="comment">#     = (X, 1)&#39; @ (hX @ s[:n_features], sum(h) * s[-1])</span></div>
<div class="line"><span class="lineno">  578</span>            <span class="comment"># res[:n_features] = X&#39; @ hX @ s[:n_features] + sum(h) * s[-1]</span></div>
<div class="line"><span class="lineno">  579</span>            <span class="comment"># res[-1] = 1&#39; @ hX @ s[:n_features] + sum(h) * s[-1]</span></div>
<div class="line"><span class="lineno">  580</span>            <span class="keyword">def </span>hessp(s):</div>
<div class="line"><span class="lineno">  581</span>                ret = np.empty_like(s)</div>
<div class="line"><span class="lineno">  582</span>                <span class="keywordflow">if</span> sparse.issparse(X):</div>
<div class="line"><span class="lineno">  583</span>                    ret[:n_features] = X.T @ (hX @ s[:n_features])</div>
<div class="line"><span class="lineno">  584</span>                <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  585</span>                    ret[:n_features] = np.linalg.multi_dot([X.T, hX, s[:n_features]])</div>
<div class="line"><span class="lineno">  586</span>                ret[:n_features] += l2_reg_strength * s[:n_features]</div>
<div class="line"><span class="lineno">  587</span> </div>
<div class="line"><span class="lineno">  588</span>                <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  589</span>                    ret[:n_features] += s[-1] * hX_sum</div>
<div class="line"><span class="lineno">  590</span>                    ret[-1] = hX_sum @ s[:n_features] + hessian_sum * s[-1]</div>
<div class="line"><span class="lineno">  591</span>                <span class="keywordflow">return</span> ret</div>
<div class="line"><span class="lineno">  592</span> </div>
<div class="line"><span class="lineno">  593</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  594</span>            <span class="comment"># Here we may safely assume HalfMultinomialLoss aka categorical</span></div>
<div class="line"><span class="lineno">  595</span>            <span class="comment"># cross-entropy.</span></div>
<div class="line"><span class="lineno">  596</span>            <span class="comment"># HalfMultinomialLoss computes only the diagonal part of the hessian, i.e.</span></div>
<div class="line"><span class="lineno">  597</span>            <span class="comment"># diagonal in the classes. Here, we want the matrix-vector product of the</span></div>
<div class="line"><span class="lineno">  598</span>            <span class="comment"># full hessian. Therefore, we call gradient_proba.</span></div>
<div class="line"><span class="lineno">  599</span>            grad_pointwise, proba = self.base_loss.gradient_proba(</div>
<div class="line"><span class="lineno">  600</span>                y_true=y,</div>
<div class="line"><span class="lineno">  601</span>                raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  602</span>                sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  603</span>                n_threads=n_threads,</div>
<div class="line"><span class="lineno">  604</span>            )</div>
<div class="line"><span class="lineno">  605</span>            grad = np.empty((n_classes, n_dof), dtype=weights.dtype, order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  606</span>            grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights</div>
<div class="line"><span class="lineno">  607</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  608</span>                grad[:, -1] = grad_pointwise.sum(axis=0)</div>
<div class="line"><span class="lineno">  609</span> </div>
<div class="line"><span class="lineno">  610</span>            <span class="comment"># Full hessian-vector product, i.e. not only the diagonal part of the</span></div>
<div class="line"><span class="lineno">  611</span>            <span class="comment"># hessian. Derivation with some index battle for input vector s:</span></div>
<div class="line"><span class="lineno">  612</span>            <span class="comment">#   - sample index i</span></div>
<div class="line"><span class="lineno">  613</span>            <span class="comment">#   - feature indices j, m</span></div>
<div class="line"><span class="lineno">  614</span>            <span class="comment">#   - class indices k, l</span></div>
<div class="line"><span class="lineno">  615</span>            <span class="comment">#   - 1_{k=l} is one if k=l else 0</span></div>
<div class="line"><span class="lineno">  616</span>            <span class="comment">#   - p_i_k is the (predicted) probability that sample i belongs to class k</span></div>
<div class="line"><span class="lineno">  617</span>            <span class="comment">#     for all i: sum_k p_i_k = 1</span></div>
<div class="line"><span class="lineno">  618</span>            <span class="comment">#   - s_l_m is input vector for class l and feature m</span></div>
<div class="line"><span class="lineno">  619</span>            <span class="comment">#   - X&#39; = X transposed</span></div>
<div class="line"><span class="lineno">  620</span>            <span class="comment">#</span></div>
<div class="line"><span class="lineno">  621</span>            <span class="comment"># Note: Hessian with dropping most indices is just:</span></div>
<div class="line"><span class="lineno">  622</span>            <span class="comment">#       X&#39; @ p_k (1(k=l) - p_l) @ X</span></div>
<div class="line"><span class="lineno">  623</span>            <span class="comment">#</span></div>
<div class="line"><span class="lineno">  624</span>            <span class="comment"># result_{k j} = sum_{i, l, m} Hessian_{i, k j, m l} * s_l_m</span></div>
<div class="line"><span class="lineno">  625</span>            <span class="comment">#   = sum_{i, l, m} (X&#39;)_{ji} * p_i_k * (1_{k=l} - p_i_l)</span></div>
<div class="line"><span class="lineno">  626</span>            <span class="comment">#                   * X_{im} s_l_m</span></div>
<div class="line"><span class="lineno">  627</span>            <span class="comment">#   = sum_{i, m} (X&#39;)_{ji} * p_i_k</span></div>
<div class="line"><span class="lineno">  628</span>            <span class="comment">#                * (X_{im} * s_k_m - sum_l p_i_l * X_{im} * s_l_m)</span></div>
<div class="line"><span class="lineno">  629</span>            <span class="comment">#</span></div>
<div class="line"><span class="lineno">  630</span>            <span class="comment"># See also https://github.com/scikit-learn/scikit-learn/pull/3646#discussion_r17461411  # noqa</span></div>
<div class="line"><span class="lineno">  631</span>            <span class="keyword">def </span>hessp(s):</div>
<div class="line"><span class="lineno">  632</span>                s = s.reshape((n_classes, -1), order=<span class="stringliteral">&quot;F&quot;</span>)  <span class="comment"># shape = (n_classes, n_dof)</span></div>
<div class="line"><span class="lineno">  633</span>                <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  634</span>                    s_intercept = s[:, -1]</div>
<div class="line"><span class="lineno">  635</span>                    s = s[:, :-1]  <span class="comment"># shape = (n_classes, n_features)</span></div>
<div class="line"><span class="lineno">  636</span>                <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  637</span>                    s_intercept = 0</div>
<div class="line"><span class="lineno">  638</span>                tmp = X @ s.T + s_intercept  <span class="comment"># X_{im} * s_k_m</span></div>
<div class="line"><span class="lineno">  639</span>                tmp += (-proba * tmp).sum(axis=1)[:, np.newaxis]  <span class="comment"># - sum_l ..</span></div>
<div class="line"><span class="lineno">  640</span>                tmp *= proba  <span class="comment"># * p_i_k</span></div>
<div class="line"><span class="lineno">  641</span>                <span class="keywordflow">if</span> sample_weight <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  642</span>                    tmp *= sample_weight[:, np.newaxis]</div>
<div class="line"><span class="lineno">  643</span>                <span class="comment"># hess_prod = empty_like(grad), but we ravel grad below and this</span></div>
<div class="line"><span class="lineno">  644</span>                <span class="comment"># function is run after that.</span></div>
<div class="line"><span class="lineno">  645</span>                hess_prod = np.empty((n_classes, n_dof), dtype=weights.dtype, order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  646</span>                hess_prod[:, :n_features] = tmp.T @ X + l2_reg_strength * s</div>
<div class="line"><span class="lineno">  647</span>                <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  648</span>                    hess_prod[:, -1] = tmp.sum(axis=0)</div>
<div class="line"><span class="lineno">  649</span>                <span class="keywordflow">if</span> coef.ndim == 1:</div>
<div class="line"><span class="lineno">  650</span>                    <span class="keywordflow">return</span> hess_prod.ravel(order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  651</span>                <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  652</span>                    <span class="keywordflow">return</span> hess_prod</div>
<div class="line"><span class="lineno">  653</span> </div>
<div class="line"><span class="lineno">  654</span>            <span class="keywordflow">if</span> coef.ndim == 1:</div>
<div class="line"><span class="lineno">  655</span>                <span class="keywordflow">return</span> grad.ravel(order=<span class="stringliteral">&quot;F&quot;</span>), hessp</div>
<div class="line"><span class="lineno">  656</span> </div>
<div class="line"><span class="lineno">  657</span>        <span class="keywordflow">return</span> grad, hessp</div>
</div><!-- fragment -->
</div>
</div>
<a id="a90d92487a6eb759c86797dfbfe23153c" name="a90d92487a6eb759c86797dfbfe23153c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a90d92487a6eb759c86797dfbfe23153c">&#9670;&#160;</a></span>init_zero_coef()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.init_zero_coef </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dtype</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Allocate coef of correct shape with zeros.

Parameters:
-----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
dtype : data-type, default=None
    Overrides the data type of coef. With dtype=None, coef will have the same
    dtype as X.

Returns
-------
coef : ndarray of shape (n_dof,) or (n_classes, n_dof)
    Coefficients of a linear model.
</pre> <div class="fragment"><div class="line"><span class="lineno">   70</span>    <span class="keyword">def </span>init_zero_coef(self, X, dtype=None):</div>
<div class="line"><span class="lineno">   71</span>        <span class="stringliteral">&quot;&quot;&quot;Allocate coef of correct shape with zeros.</span></div>
<div class="line"><span class="lineno">   72</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   73</span><span class="stringliteral">        Parameters:</span></div>
<div class="line"><span class="lineno">   74</span><span class="stringliteral">        -----------</span></div>
<div class="line"><span class="lineno">   75</span><span class="stringliteral">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">   76</span><span class="stringliteral">            Training data.</span></div>
<div class="line"><span class="lineno">   77</span><span class="stringliteral">        dtype : data-type, default=None</span></div>
<div class="line"><span class="lineno">   78</span><span class="stringliteral">            Overrides the data type of coef. With dtype=None, coef will have the same</span></div>
<div class="line"><span class="lineno">   79</span><span class="stringliteral">            dtype as X.</span></div>
<div class="line"><span class="lineno">   80</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   81</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">   82</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">   83</span><span class="stringliteral">        coef : ndarray of shape (n_dof,) or (n_classes, n_dof)</span></div>
<div class="line"><span class="lineno">   84</span><span class="stringliteral">            Coefficients of a linear model.</span></div>
<div class="line"><span class="lineno">   85</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   86</span>        n_features = X.shape[1]</div>
<div class="line"><span class="lineno">   87</span>        n_classes = self.base_loss.n_classes</div>
<div class="line"><span class="lineno">   88</span>        <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">   89</span>            n_dof = n_features + 1</div>
<div class="line"><span class="lineno">   90</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   91</span>            n_dof = n_features</div>
<div class="line"><span class="lineno">   92</span>        <span class="keywordflow">if</span> self.base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">   93</span>            coef = np.zeros_like(X, shape=(n_classes, n_dof), dtype=dtype, order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">   94</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   95</span>            coef = np.zeros_like(X, shape=n_dof, dtype=dtype)</div>
<div class="line"><span class="lineno">   96</span>        <span class="keywordflow">return</span> coef</div>
<div class="line"><span class="lineno">   97</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a73116d9f130017dab7f045eab9a27dd6" name="a73116d9f130017dab7f045eab9a27dd6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a73116d9f130017dab7f045eab9a27dd6">&#9670;&#160;</a></span>l2_penalty()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.l2_penalty </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_strength</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute L2 penalty term l2_reg_strength/2 *||w||_2^2.</pre> <div class="fragment"><div class="line"><span class="lineno">  169</span>    <span class="keyword">def </span>l2_penalty(self, weights, l2_reg_strength):</div>
<div class="line"><span class="lineno">  170</span>        <span class="stringliteral">&quot;&quot;&quot;Compute L2 penalty term l2_reg_strength/2 *||w||_2^2.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  171</span>        norm2_w = weights @ weights <span class="keywordflow">if</span> weights.ndim == 1 <span class="keywordflow">else</span> squared_norm(weights)</div>
<div class="line"><span class="lineno">  172</span>        <span class="keywordflow">return</span> 0.5 * l2_reg_strength * norm2_w</div>
<div class="line"><span class="lineno">  173</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a502d1c80fd8bf5c9f2fefeb71c12d15f" name="a502d1c80fd8bf5c9f2fefeb71c12d15f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a502d1c80fd8bf5c9f2fefeb71c12d15f">&#9670;&#160;</a></span>loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coef</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_strength</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_threads</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>raw_prediction</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute the loss as sum over point-wise losses.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space). If provided, these are used. If
    None, then raw_prediction = X @ coef + intercept is calculated.

Returns
-------
loss : float
    Sum of losses per sample plus penalty.
</pre> <div class="fragment"><div class="line"><span class="lineno">  183</span>    ):</div>
<div class="line"><span class="lineno">  184</span>        <span class="stringliteral">&quot;&quot;&quot;Compute the loss as sum over point-wise losses.</span></div>
<div class="line"><span class="lineno">  185</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  186</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  187</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  188</span><span class="stringliteral">        coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)</span></div>
<div class="line"><span class="lineno">  189</span><span class="stringliteral">            Coefficients of a linear model.</span></div>
<div class="line"><span class="lineno">  190</span><span class="stringliteral">            If shape (n_classes * n_dof,), the classes of one feature are contiguous,</span></div>
<div class="line"><span class="lineno">  191</span><span class="stringliteral">            i.e. one reconstructs the 2d-array via</span></div>
<div class="line"><span class="lineno">  192</span><span class="stringliteral">            coef.reshape((n_classes, -1), order=&quot;F&quot;).</span></div>
<div class="line"><span class="lineno">  193</span><span class="stringliteral">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  194</span><span class="stringliteral">            Training data.</span></div>
<div class="line"><span class="lineno">  195</span><span class="stringliteral">        y : contiguous array of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  196</span><span class="stringliteral">            Observed, true target values.</span></div>
<div class="line"><span class="lineno">  197</span><span class="stringliteral">        sample_weight : None or contiguous array of shape (n_samples,), default=None</span></div>
<div class="line"><span class="lineno">  198</span><span class="stringliteral">            Sample weights.</span></div>
<div class="line"><span class="lineno">  199</span><span class="stringliteral">        l2_reg_strength : float, default=0.0</span></div>
<div class="line"><span class="lineno">  200</span><span class="stringliteral">            L2 regularization strength</span></div>
<div class="line"><span class="lineno">  201</span><span class="stringliteral">        n_threads : int, default=1</span></div>
<div class="line"><span class="lineno">  202</span><span class="stringliteral">            Number of OpenMP threads to use.</span></div>
<div class="line"><span class="lineno">  203</span><span class="stringliteral">        raw_prediction : C-contiguous array of shape (n_samples,) or array of \</span></div>
<div class="line"><span class="lineno">  204</span><span class="stringliteral">            shape (n_samples, n_classes)</span></div>
<div class="line"><span class="lineno">  205</span><span class="stringliteral">            Raw prediction values (in link space). If provided, these are used. If</span></div>
<div class="line"><span class="lineno">  206</span><span class="stringliteral">            None, then raw_prediction = X @ coef + intercept is calculated.</span></div>
<div class="line"><span class="lineno">  207</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  208</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  209</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  210</span><span class="stringliteral">        loss : float</span></div>
<div class="line"><span class="lineno">  211</span><span class="stringliteral">            Sum of losses per sample plus penalty.</span></div>
<div class="line"><span class="lineno">  212</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  213</span>        <span class="keywordflow">if</span> raw_prediction <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  214</span>            weights, intercept, raw_prediction = self.weight_intercept_raw(coef, X)</div>
<div class="line"><span class="lineno">  215</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  216</span>            weights, intercept = self.weight_intercept(coef)</div>
<div class="line"><span class="lineno">  217</span> </div>
<div class="line"><span class="lineno">  218</span>        loss = self.base_loss.loss(</div>
<div class="line"><span class="lineno">  219</span>            y_true=y,</div>
<div class="line"><span class="lineno">  220</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  221</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  222</span>            n_threads=n_threads,</div>
<div class="line"><span class="lineno">  223</span>        )</div>
<div class="line"><span class="lineno">  224</span>        loss = loss.sum()</div>
<div class="line"><span class="lineno">  225</span> </div>
<div class="line"><span class="lineno">  226</span>        <span class="keywordflow">return</span> loss + self.l2_penalty(weights, l2_reg_strength)</div>
<div class="line"><span class="lineno">  227</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aadedf74d59c6f362cac8adef57efc958" name="aadedf74d59c6f362cac8adef57efc958"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aadedf74d59c6f362cac8adef57efc958">&#9670;&#160;</a></span>loss_gradient()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.loss_gradient </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coef</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_strength</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_threads</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>raw_prediction</em> = <code>None</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Computes the sum of loss and gradient w.r.t. coef.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.
y : contiguous array of shape (n_samples,)
    Observed, true target values.
sample_weight : None or contiguous array of shape (n_samples,), default=None
    Sample weights.
l2_reg_strength : float, default=0.0
    L2 regularization strength
n_threads : int, default=1
    Number of OpenMP threads to use.
raw_prediction : C-contiguous array of shape (n_samples,) or array of \
    shape (n_samples, n_classes)
    Raw prediction values (in link space). If provided, these are used. If
    None, then raw_prediction = X @ coef + intercept is calculated.

Returns
-------
loss : float
    Sum of losses per sample plus penalty.

gradient : ndarray of shape coef.shape
     The gradient of the loss.
</pre> <div class="fragment"><div class="line"><span class="lineno">  237</span>    ):</div>
<div class="line"><span class="lineno">  238</span>        <span class="stringliteral">&quot;&quot;&quot;Computes the sum of loss and gradient w.r.t. coef.</span></div>
<div class="line"><span class="lineno">  239</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  240</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  241</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  242</span><span class="stringliteral">        coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)</span></div>
<div class="line"><span class="lineno">  243</span><span class="stringliteral">            Coefficients of a linear model.</span></div>
<div class="line"><span class="lineno">  244</span><span class="stringliteral">            If shape (n_classes * n_dof,), the classes of one feature are contiguous,</span></div>
<div class="line"><span class="lineno">  245</span><span class="stringliteral">            i.e. one reconstructs the 2d-array via</span></div>
<div class="line"><span class="lineno">  246</span><span class="stringliteral">            coef.reshape((n_classes, -1), order=&quot;F&quot;).</span></div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral">            Training data.</span></div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">        y : contiguous array of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral">            Observed, true target values.</span></div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral">        sample_weight : None or contiguous array of shape (n_samples,), default=None</span></div>
<div class="line"><span class="lineno">  252</span><span class="stringliteral">            Sample weights.</span></div>
<div class="line"><span class="lineno">  253</span><span class="stringliteral">        l2_reg_strength : float, default=0.0</span></div>
<div class="line"><span class="lineno">  254</span><span class="stringliteral">            L2 regularization strength</span></div>
<div class="line"><span class="lineno">  255</span><span class="stringliteral">        n_threads : int, default=1</span></div>
<div class="line"><span class="lineno">  256</span><span class="stringliteral">            Number of OpenMP threads to use.</span></div>
<div class="line"><span class="lineno">  257</span><span class="stringliteral">        raw_prediction : C-contiguous array of shape (n_samples,) or array of \</span></div>
<div class="line"><span class="lineno">  258</span><span class="stringliteral">            shape (n_samples, n_classes)</span></div>
<div class="line"><span class="lineno">  259</span><span class="stringliteral">            Raw prediction values (in link space). If provided, these are used. If</span></div>
<div class="line"><span class="lineno">  260</span><span class="stringliteral">            None, then raw_prediction = X @ coef + intercept is calculated.</span></div>
<div class="line"><span class="lineno">  261</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  262</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  263</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  264</span><span class="stringliteral">        loss : float</span></div>
<div class="line"><span class="lineno">  265</span><span class="stringliteral">            Sum of losses per sample plus penalty.</span></div>
<div class="line"><span class="lineno">  266</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  267</span><span class="stringliteral">        gradient : ndarray of shape coef.shape</span></div>
<div class="line"><span class="lineno">  268</span><span class="stringliteral">             The gradient of the loss.</span></div>
<div class="line"><span class="lineno">  269</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  270</span>        n_features, n_classes = X.shape[1], self.base_loss.n_classes</div>
<div class="line"><span class="lineno">  271</span>        n_dof = n_features + int(self.fit_intercept)</div>
<div class="line"><span class="lineno">  272</span> </div>
<div class="line"><span class="lineno">  273</span>        <span class="keywordflow">if</span> raw_prediction <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  274</span>            weights, intercept, raw_prediction = self.weight_intercept_raw(coef, X)</div>
<div class="line"><span class="lineno">  275</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  276</span>            weights, intercept = self.weight_intercept(coef)</div>
<div class="line"><span class="lineno">  277</span> </div>
<div class="line"><span class="lineno">  278</span>        loss, grad_pointwise = self.base_loss.loss_gradient(</div>
<div class="line"><span class="lineno">  279</span>            y_true=y,</div>
<div class="line"><span class="lineno">  280</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  281</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  282</span>            n_threads=n_threads,</div>
<div class="line"><span class="lineno">  283</span>        )</div>
<div class="line"><span class="lineno">  284</span>        loss = loss.sum()</div>
<div class="line"><span class="lineno">  285</span>        loss += self.l2_penalty(weights, l2_reg_strength)</div>
<div class="line"><span class="lineno">  286</span> </div>
<div class="line"><span class="lineno">  287</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  288</span>            grad = np.empty_like(coef, dtype=weights.dtype)</div>
<div class="line"><span class="lineno">  289</span>            grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights</div>
<div class="line"><span class="lineno">  290</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  291</span>                grad[-1] = grad_pointwise.sum()</div>
<div class="line"><span class="lineno">  292</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  293</span>            grad = np.empty((n_classes, n_dof), dtype=weights.dtype, order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  294</span>            <span class="comment"># grad_pointwise.shape = (n_samples, n_classes)</span></div>
<div class="line"><span class="lineno">  295</span>            grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights</div>
<div class="line"><span class="lineno">  296</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  297</span>                grad[:, -1] = grad_pointwise.sum(axis=0)</div>
<div class="line"><span class="lineno">  298</span>            <span class="keywordflow">if</span> coef.ndim == 1:</div>
<div class="line"><span class="lineno">  299</span>                grad = grad.ravel(order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  300</span> </div>
<div class="line"><span class="lineno">  301</span>        <span class="keywordflow">return</span> loss, grad</div>
<div class="line"><span class="lineno">  302</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad6aa682bba552f631193aeb838bbb4fb" name="ad6aa682bba552f631193aeb838bbb4fb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad6aa682bba552f631193aeb838bbb4fb">&#9670;&#160;</a></span>weight_intercept()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.weight_intercept </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coef</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Helper function to get coefficients and intercept.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").

Returns
-------
weights : ndarray of shape (n_features,) or (n_classes, n_features)
    Coefficients without intercept term.
intercept : float or ndarray of shape (n_classes,)
    Intercept terms.
</pre> <div class="fragment"><div class="line"><span class="lineno">   98</span>    <span class="keyword">def </span>weight_intercept(self, coef):</div>
<div class="line"><span class="lineno">   99</span>        <span class="stringliteral">&quot;&quot;&quot;Helper function to get coefficients and intercept.</span></div>
<div class="line"><span class="lineno">  100</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  101</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  102</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  103</span><span class="stringliteral">        coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)</span></div>
<div class="line"><span class="lineno">  104</span><span class="stringliteral">            Coefficients of a linear model.</span></div>
<div class="line"><span class="lineno">  105</span><span class="stringliteral">            If shape (n_classes * n_dof,), the classes of one feature are contiguous,</span></div>
<div class="line"><span class="lineno">  106</span><span class="stringliteral">            i.e. one reconstructs the 2d-array via</span></div>
<div class="line"><span class="lineno">  107</span><span class="stringliteral">            coef.reshape((n_classes, -1), order=&quot;F&quot;).</span></div>
<div class="line"><span class="lineno">  108</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  109</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  110</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  111</span><span class="stringliteral">        weights : ndarray of shape (n_features,) or (n_classes, n_features)</span></div>
<div class="line"><span class="lineno">  112</span><span class="stringliteral">            Coefficients without intercept term.</span></div>
<div class="line"><span class="lineno">  113</span><span class="stringliteral">        intercept : float or ndarray of shape (n_classes,)</span></div>
<div class="line"><span class="lineno">  114</span><span class="stringliteral">            Intercept terms.</span></div>
<div class="line"><span class="lineno">  115</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  116</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  117</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  118</span>                intercept = coef[-1]</div>
<div class="line"><span class="lineno">  119</span>                weights = coef[:-1]</div>
<div class="line"><span class="lineno">  120</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  121</span>                intercept = 0.0</div>
<div class="line"><span class="lineno">  122</span>                weights = coef</div>
<div class="line"><span class="lineno">  123</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  124</span>            <span class="comment"># reshape to (n_classes, n_dof)</span></div>
<div class="line"><span class="lineno">  125</span>            <span class="keywordflow">if</span> coef.ndim == 1:</div>
<div class="line"><span class="lineno">  126</span>                weights = coef.reshape((self.base_loss.n_classes, -1), order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  127</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  128</span>                weights = coef</div>
<div class="line"><span class="lineno">  129</span>            <span class="keywordflow">if</span> self.fit_intercept:</div>
<div class="line"><span class="lineno">  130</span>                intercept = weights[:, -1]</div>
<div class="line"><span class="lineno">  131</span>                weights = weights[:, :-1]</div>
<div class="line"><span class="lineno">  132</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  133</span>                intercept = 0.0</div>
<div class="line"><span class="lineno">  134</span> </div>
<div class="line"><span class="lineno">  135</span>        <span class="keywordflow">return</span> weights, intercept</div>
<div class="line"><span class="lineno">  136</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a2c1020b46d5e61ae8bda153835bdaa2b" name="a2c1020b46d5e61ae8bda153835bdaa2b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2c1020b46d5e61ae8bda153835bdaa2b">&#9670;&#160;</a></span>weight_intercept_raw()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.weight_intercept_raw </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>self</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coef</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Helper function to get coefficients, intercept and raw_prediction.

Parameters
----------
coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)
    Coefficients of a linear model.
    If shape (n_classes * n_dof,), the classes of one feature are contiguous,
    i.e. one reconstructs the 2d-array via
    coef.reshape((n_classes, -1), order="F").
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.

Returns
-------
weights : ndarray of shape (n_features,) or (n_classes, n_features)
    Coefficients without intercept term.
intercept : float or ndarray of shape (n_classes,)
    Intercept terms.
raw_prediction : ndarray of shape (n_samples,) or \
    (n_samples, n_classes)
</pre> <div class="fragment"><div class="line"><span class="lineno">  137</span>    <span class="keyword">def </span>weight_intercept_raw(self, coef, X):</div>
<div class="line"><span class="lineno">  138</span>        <span class="stringliteral">&quot;&quot;&quot;Helper function to get coefficients, intercept and raw_prediction.</span></div>
<div class="line"><span class="lineno">  139</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  140</span><span class="stringliteral">        Parameters</span></div>
<div class="line"><span class="lineno">  141</span><span class="stringliteral">        ----------</span></div>
<div class="line"><span class="lineno">  142</span><span class="stringliteral">        coef : ndarray of shape (n_dof,), (n_classes, n_dof) or (n_classes * n_dof,)</span></div>
<div class="line"><span class="lineno">  143</span><span class="stringliteral">            Coefficients of a linear model.</span></div>
<div class="line"><span class="lineno">  144</span><span class="stringliteral">            If shape (n_classes * n_dof,), the classes of one feature are contiguous,</span></div>
<div class="line"><span class="lineno">  145</span><span class="stringliteral">            i.e. one reconstructs the 2d-array via</span></div>
<div class="line"><span class="lineno">  146</span><span class="stringliteral">            coef.reshape((n_classes, -1), order=&quot;F&quot;).</span></div>
<div class="line"><span class="lineno">  147</span><span class="stringliteral">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  148</span><span class="stringliteral">            Training data.</span></div>
<div class="line"><span class="lineno">  149</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  150</span><span class="stringliteral">        Returns</span></div>
<div class="line"><span class="lineno">  151</span><span class="stringliteral">        -------</span></div>
<div class="line"><span class="lineno">  152</span><span class="stringliteral">        weights : ndarray of shape (n_features,) or (n_classes, n_features)</span></div>
<div class="line"><span class="lineno">  153</span><span class="stringliteral">            Coefficients without intercept term.</span></div>
<div class="line"><span class="lineno">  154</span><span class="stringliteral">        intercept : float or ndarray of shape (n_classes,)</span></div>
<div class="line"><span class="lineno">  155</span><span class="stringliteral">            Intercept terms.</span></div>
<div class="line"><span class="lineno">  156</span><span class="stringliteral">        raw_prediction : ndarray of shape (n_samples,) or \</span></div>
<div class="line"><span class="lineno">  157</span><span class="stringliteral">            (n_samples, n_classes)</span></div>
<div class="line"><span class="lineno">  158</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  159</span>        weights, intercept = self.weight_intercept(coef)</div>
<div class="line"><span class="lineno">  160</span> </div>
<div class="line"><span class="lineno">  161</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  162</span>            raw_prediction = X @ weights + intercept</div>
<div class="line"><span class="lineno">  163</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  164</span>            <span class="comment"># weights has shape (n_classes, n_dof)</span></div>
<div class="line"><span class="lineno">  165</span>            raw_prediction = X @ weights.T + intercept  <span class="comment"># ndarray, likely C-contiguous</span></div>
<div class="line"><span class="lineno">  166</span> </div>
<div class="line"><span class="lineno">  167</span>        <span class="keywordflow">return</span> weights, intercept, raw_prediction</div>
<div class="line"><span class="lineno">  168</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Member Data Documentation</h2>
<a id="ae45960a0f1e675ef15d88e7cc4d459b9" name="ae45960a0f1e675ef15d88e7cc4d459b9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae45960a0f1e675ef15d88e7cc4d459b9">&#9670;&#160;</a></span>base_loss</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.base_loss</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a9ffd044562d8ae3655156076c529a852" name="a9ffd044562d8ae3655156076c529a852"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9ffd044562d8ae3655156076c529a852">&#9670;&#160;</a></span>fit_intercept</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._linear_loss.LinearModelLoss.fit_intercept</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>/Users/rafael/Documents/GitHub/PUC-GCES-PY/Tp-GCS-Rafael-Augusto/venv/lib/python3.9/site-packages/sklearn/linear_model/<a class="el" href="__linear__loss_8py.html">_linear_loss.py</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
