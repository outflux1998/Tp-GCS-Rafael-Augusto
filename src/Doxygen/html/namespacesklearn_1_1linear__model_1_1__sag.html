<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.linear_model._sag Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model.html">linear_model</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model_1_1__sag.html">_sag</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">sklearn.linear_model._sag Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:af82a53319ebeaf50db3edb2c31aa1976" id="r_af82a53319ebeaf50db3edb2c31aa1976"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1__sag.html#af82a53319ebeaf50db3edb2c31aa1976">get_auto_step_size</a> (max_squared_sum, alpha_scaled, loss, fit_intercept, n_samples=None, is_saga=False)</td></tr>
<tr class="separator:af82a53319ebeaf50db3edb2c31aa1976"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0b6301682ade2a002a5d95303be0fa94" id="r_a0b6301682ade2a002a5d95303be0fa94"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1__sag.html#a0b6301682ade2a002a5d95303be0fa94">sag_solver</a> (X, y, sample_weight=None, loss=&quot;log&quot;, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>=1.0, beta=0.0, max_iter=1000, <a class="el" href="__lapack__subroutines_8h.html#a0357339a1a1f7b51953875ca01447445">tol</a>=0.001, verbose=0, random_state=None, check_input=True, max_squared_sum=None, warm_start_mem=None, is_saga=False)</td></tr>
<tr class="separator:a0b6301682ade2a002a5d95303be0fa94"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Solvers for Ridge and LogisticRegression using SAG algorithm</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="af82a53319ebeaf50db3edb2c31aa1976" name="af82a53319ebeaf50db3edb2c31aa1976"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af82a53319ebeaf50db3edb2c31aa1976">&#9670;&#160;</a></span>get_auto_step_size()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._sag.get_auto_step_size </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_squared_sum</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha_scaled</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>is_saga</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute automatic step size for SAG solver.

The step size is set to 1 / (alpha_scaled + L + fit_intercept) where L is
the max sum of squares for over all samples.

Parameters
----------
max_squared_sum : float
    Maximum squared sum of X over samples.

alpha_scaled : float
    Constant that multiplies the regularization term, scaled by
    1. / n_samples, the number of samples.

loss : {'log', 'squared', 'multinomial'}
    The loss function used in SAG solver.

fit_intercept : bool
    Specifies if a constant (a.k.a. bias or intercept) will be
    added to the decision function.

n_samples : int, default=None
    Number of rows in X. Useful if is_saga=True.

is_saga : bool, default=False
    Whether to return step size for the SAGA algorithm or the SAG
    algorithm.

Returns
-------
step_size : float
    Step size used in SAG solver.

References
----------
Schmidt, M., Roux, N. L., &amp; Bach, F. (2013).
Minimizing finite sums with the stochastic average gradient
https://hal.inria.fr/hal-00860051/document

:arxiv:`Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).
"SAGA: A Fast Incremental Gradient Method With Support
for Non-Strongly Convex Composite Objectives" &lt;1407.0202&gt;`
</pre> <div class="fragment"><div class="line"><span class="lineno">   21</span>):</div>
<div class="line"><span class="lineno">   22</span>    <span class="stringliteral">&quot;&quot;&quot;Compute automatic step size for SAG solver.</span></div>
<div class="line"><span class="lineno">   23</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   24</span><span class="stringliteral">    The step size is set to 1 / (alpha_scaled + L + fit_intercept) where L is</span></div>
<div class="line"><span class="lineno">   25</span><span class="stringliteral">    the max sum of squares for over all samples.</span></div>
<div class="line"><span class="lineno">   26</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   27</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   28</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   29</span><span class="stringliteral">    max_squared_sum : float</span></div>
<div class="line"><span class="lineno">   30</span><span class="stringliteral">        Maximum squared sum of X over samples.</span></div>
<div class="line"><span class="lineno">   31</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   32</span><span class="stringliteral">    alpha_scaled : float</span></div>
<div class="line"><span class="lineno">   33</span><span class="stringliteral">        Constant that multiplies the regularization term, scaled by</span></div>
<div class="line"><span class="lineno">   34</span><span class="stringliteral">        1. / n_samples, the number of samples.</span></div>
<div class="line"><span class="lineno">   35</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   36</span><span class="stringliteral">    loss : {&#39;log&#39;, &#39;squared&#39;, &#39;multinomial&#39;}</span></div>
<div class="line"><span class="lineno">   37</span><span class="stringliteral">        The loss function used in SAG solver.</span></div>
<div class="line"><span class="lineno">   38</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   39</span><span class="stringliteral">    fit_intercept : bool</span></div>
<div class="line"><span class="lineno">   40</span><span class="stringliteral">        Specifies if a constant (a.k.a. bias or intercept) will be</span></div>
<div class="line"><span class="lineno">   41</span><span class="stringliteral">        added to the decision function.</span></div>
<div class="line"><span class="lineno">   42</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   43</span><span class="stringliteral">    n_samples : int, default=None</span></div>
<div class="line"><span class="lineno">   44</span><span class="stringliteral">        Number of rows in X. Useful if is_saga=True.</span></div>
<div class="line"><span class="lineno">   45</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   46</span><span class="stringliteral">    is_saga : bool, default=False</span></div>
<div class="line"><span class="lineno">   47</span><span class="stringliteral">        Whether to return step size for the SAGA algorithm or the SAG</span></div>
<div class="line"><span class="lineno">   48</span><span class="stringliteral">        algorithm.</span></div>
<div class="line"><span class="lineno">   49</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   50</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">   51</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">   52</span><span class="stringliteral">    step_size : float</span></div>
<div class="line"><span class="lineno">   53</span><span class="stringliteral">        Step size used in SAG solver.</span></div>
<div class="line"><span class="lineno">   54</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   55</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">   56</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   57</span><span class="stringliteral">    Schmidt, M., Roux, N. L., &amp; Bach, F. (2013).</span></div>
<div class="line"><span class="lineno">   58</span><span class="stringliteral">    Minimizing finite sums with the stochastic average gradient</span></div>
<div class="line"><span class="lineno">   59</span><span class="stringliteral">    https://hal.inria.fr/hal-00860051/document</span></div>
<div class="line"><span class="lineno">   60</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   61</span><span class="stringliteral">    :arxiv:`Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).</span></div>
<div class="line"><span class="lineno">   62</span><span class="stringliteral">    &quot;SAGA: A Fast Incremental Gradient Method With Support</span></div>
<div class="line"><span class="lineno">   63</span><span class="stringliteral">    for Non-Strongly Convex Composite Objectives&quot; &lt;1407.0202&gt;`</span></div>
<div class="line"><span class="lineno">   64</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   65</span>    <span class="keywordflow">if</span> loss <span class="keywordflow">in</span> (<span class="stringliteral">&quot;log&quot;</span>, <span class="stringliteral">&quot;multinomial&quot;</span>):</div>
<div class="line"><span class="lineno">   66</span>        L = 0.25 * (max_squared_sum + int(fit_intercept)) + alpha_scaled</div>
<div class="line"><span class="lineno">   67</span>    <span class="keywordflow">elif</span> loss == <span class="stringliteral">&quot;squared&quot;</span>:</div>
<div class="line"><span class="lineno">   68</span>        <span class="comment"># inverse Lipschitz constant for squared loss</span></div>
<div class="line"><span class="lineno">   69</span>        L = max_squared_sum + int(fit_intercept) + alpha_scaled</div>
<div class="line"><span class="lineno">   70</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   71</span>        <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">   72</span>            <span class="stringliteral">&quot;Unknown loss function for SAG solver, got %s instead of &#39;log&#39; or &#39;squared&#39;&quot;</span></div>
<div class="line"><span class="lineno">   73</span>            % loss</div>
<div class="line"><span class="lineno">   74</span>        )</div>
<div class="line"><span class="lineno">   75</span>    <span class="keywordflow">if</span> is_saga:</div>
<div class="line"><span class="lineno">   76</span>        <span class="comment"># SAGA theoretical step size is 1/3L or 1 / (2 * (L + mu n))</span></div>
<div class="line"><span class="lineno">   77</span>        <span class="comment"># See Defazio et al. 2014</span></div>
<div class="line"><span class="lineno">   78</span>        mun = min(2 * n_samples * alpha_scaled, L)</div>
<div class="line"><span class="lineno">   79</span>        step = 1.0 / (2 * L + mun)</div>
<div class="line"><span class="lineno">   80</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   81</span>        <span class="comment"># SAG theoretical step size is 1/16L but it is recommended to use 1 / L</span></div>
<div class="line"><span class="lineno">   82</span>        <span class="comment"># see http://www.birs.ca//workshops//2014/14w5003/files/schmidt.pdf,</span></div>
<div class="line"><span class="lineno">   83</span>        <span class="comment"># slide 65</span></div>
<div class="line"><span class="lineno">   84</span>        step = 1.0 / L</div>
<div class="line"><span class="lineno">   85</span>    <span class="keywordflow">return</span> step</div>
<div class="line"><span class="lineno">   86</span> </div>
<div class="line"><span class="lineno">   87</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a0b6301682ade2a002a5d95303be0fa94" name="a0b6301682ade2a002a5d95303be0fa94"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0b6301682ade2a002a5d95303be0fa94">&#9670;&#160;</a></span>sag_solver()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._sag.sag_solver </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em> = <code>&quot;log&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em> = <code>1.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>beta</em> = <code>0.0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_iter</em> = <code>1000</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tol</em> = <code>0.001</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_state</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>check_input</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_squared_sum</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>warm_start_mem</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>is_saga</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">SAG solver for Ridge and LogisticRegression.

SAG stands for Stochastic Average Gradient: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a constant learning rate.

IMPORTANT NOTE: 'sag' solver converges faster on columns that are on the
same scale. You can normalize the data by using
sklearn.preprocessing.StandardScaler on your data before passing it to the
fit method.

This implementation works with data represented as dense numpy arrays or
sparse scipy arrays of floating point values for the features. It will
fit the data according to squared loss or log loss.

The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using the squared euclidean norm L2.

.. versionadded:: 0.17

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Training data.

y : ndarray of shape (n_samples,)
    Target values. With loss='multinomial', y must be label encoded
    (see preprocessing.LabelEncoder).

sample_weight : array-like of shape (n_samples,), default=None
    Weights applied to individual samples (1. for unweighted).

loss : {'log', 'squared', 'multinomial'}, default='log'
    Loss function that will be optimized:
    -'log' is the binary logistic loss, as used in LogisticRegression.
    -'squared' is the squared loss, as used in Ridge.
    -'multinomial' is the multinomial logistic loss, as used in
     LogisticRegression.

    .. versionadded:: 0.18
       *loss='multinomial'*

alpha : float, default=1.
    L2 regularization term in the objective function
    ``(0.5 * alpha * || W ||_F^2)``.

beta : float, default=0.
    L1 regularization term in the objective function
    ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True.

max_iter : int, default=1000
    The max number of passes over the training data if the stopping
    criteria is not reached.

tol : float, default=0.001
    The stopping criteria for the weights. The iterations will stop when
    max(change in weights) / max(weights) &lt; tol.

verbose : int, default=0
    The verbosity level.

random_state : int, RandomState instance or None, default=None
    Used when shuffling the data. Pass an int for reproducible output
    across multiple function calls.
    See :term:`Glossary &lt;random_state&gt;`.

check_input : bool, default=True
    If False, the input arrays X and y will not be checked.

max_squared_sum : float, default=None
    Maximum squared sum of X over samples. If None, it will be computed,
    going through all the samples. The value should be precomputed
    to speed up cross validation.

warm_start_mem : dict, default=None
    The initialization parameters used for warm starting. Warm starting is
    currently used in LogisticRegression but not in Ridge.
    It contains:
        - 'coef': the weight vector, with the intercept in last line
            if the intercept is fitted.
        - 'gradient_memory': the scalar gradient for all seen samples.
        - 'sum_gradient': the sum of gradient over all seen samples,
            for each feature.
        - 'intercept_sum_gradient': the sum of gradient over all seen
            samples, for the intercept.
        - 'seen': array of boolean describing the seen samples.
        - 'num_seen': the number of seen samples.

is_saga : bool, default=False
    Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves
    better in the first epochs, and allow for l1 regularisation.

Returns
-------
coef_ : ndarray of shape (n_features,)
    Weight vector.

n_iter_ : int
    The number of full pass on all samples.

warm_start_mem : dict
    Contains a 'coef' key with the fitted result, and possibly the
    fitted intercept at the end of the array. Contains also other keys
    used for warm starting.

Examples
--------
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn import linear_model
&gt;&gt;&gt; n_samples, n_features = 10, 5
&gt;&gt;&gt; rng = np.random.RandomState(0)
&gt;&gt;&gt; X = rng.randn(n_samples, n_features)
&gt;&gt;&gt; y = rng.randn(n_samples)
&gt;&gt;&gt; clf = linear_model.Ridge(solver='sag')
&gt;&gt;&gt; clf.fit(X, y)
Ridge(solver='sag')

&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
&gt;&gt;&gt; y = np.array([1, 1, 2, 2])
&gt;&gt;&gt; clf = linear_model.LogisticRegression(
...     solver='sag', multi_class='multinomial')
&gt;&gt;&gt; clf.fit(X, y)
LogisticRegression(multi_class='multinomial', solver='sag')

References
----------
Schmidt, M., Roux, N. L., &amp; Bach, F. (2013).
Minimizing finite sums with the stochastic average gradient
https://hal.inria.fr/hal-00860051/document

:arxiv:`Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).
"SAGA: A Fast Incremental Gradient Method With Support
for Non-Strongly Convex Composite Objectives" &lt;1407.0202&gt;`

See Also
--------
Ridge, SGDRegressor, ElasticNet, Lasso, SVR,
LogisticRegression, SGDClassifier, LinearSVC, Perceptron
</pre> <div class="fragment"><div class="line"><span class="lineno">  103</span>):</div>
<div class="line"><span class="lineno">  104</span>    <span class="stringliteral">&quot;&quot;&quot;SAG solver for Ridge and LogisticRegression.</span></div>
<div class="line"><span class="lineno">  105</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  106</span><span class="stringliteral">    SAG stands for Stochastic Average Gradient: the gradient of the loss is</span></div>
<div class="line"><span class="lineno">  107</span><span class="stringliteral">    estimated each sample at a time and the model is updated along the way with</span></div>
<div class="line"><span class="lineno">  108</span><span class="stringliteral">    a constant learning rate.</span></div>
<div class="line"><span class="lineno">  109</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  110</span><span class="stringliteral">    IMPORTANT NOTE: &#39;sag&#39; solver converges faster on columns that are on the</span></div>
<div class="line"><span class="lineno">  111</span><span class="stringliteral">    same scale. You can normalize the data by using</span></div>
<div class="line"><span class="lineno">  112</span><span class="stringliteral">    sklearn.preprocessing.StandardScaler on your data before passing it to the</span></div>
<div class="line"><span class="lineno">  113</span><span class="stringliteral">    fit method.</span></div>
<div class="line"><span class="lineno">  114</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  115</span><span class="stringliteral">    This implementation works with data represented as dense numpy arrays or</span></div>
<div class="line"><span class="lineno">  116</span><span class="stringliteral">    sparse scipy arrays of floating point values for the features. It will</span></div>
<div class="line"><span class="lineno">  117</span><span class="stringliteral">    fit the data according to squared loss or log loss.</span></div>
<div class="line"><span class="lineno">  118</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  119</span><span class="stringliteral">    The regularizer is a penalty added to the loss function that shrinks model</span></div>
<div class="line"><span class="lineno">  120</span><span class="stringliteral">    parameters towards the zero vector using the squared euclidean norm L2.</span></div>
<div class="line"><span class="lineno">  121</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  122</span><span class="stringliteral">    .. versionadded:: 0.17</span></div>
<div class="line"><span class="lineno">  123</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  124</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  125</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  126</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  127</span><span class="stringliteral">        Training data.</span></div>
<div class="line"><span class="lineno">  128</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  129</span><span class="stringliteral">    y : ndarray of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  130</span><span class="stringliteral">        Target values. With loss=&#39;multinomial&#39;, y must be label encoded</span></div>
<div class="line"><span class="lineno">  131</span><span class="stringliteral">        (see preprocessing.LabelEncoder).</span></div>
<div class="line"><span class="lineno">  132</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  133</span><span class="stringliteral">    sample_weight : array-like of shape (n_samples,), default=None</span></div>
<div class="line"><span class="lineno">  134</span><span class="stringliteral">        Weights applied to individual samples (1. for unweighted).</span></div>
<div class="line"><span class="lineno">  135</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  136</span><span class="stringliteral">    loss : {&#39;log&#39;, &#39;squared&#39;, &#39;multinomial&#39;}, default=&#39;log&#39;</span></div>
<div class="line"><span class="lineno">  137</span><span class="stringliteral">        Loss function that will be optimized:</span></div>
<div class="line"><span class="lineno">  138</span><span class="stringliteral">        -&#39;log&#39; is the binary logistic loss, as used in LogisticRegression.</span></div>
<div class="line"><span class="lineno">  139</span><span class="stringliteral">        -&#39;squared&#39; is the squared loss, as used in Ridge.</span></div>
<div class="line"><span class="lineno">  140</span><span class="stringliteral">        -&#39;multinomial&#39; is the multinomial logistic loss, as used in</span></div>
<div class="line"><span class="lineno">  141</span><span class="stringliteral">         LogisticRegression.</span></div>
<div class="line"><span class="lineno">  142</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  143</span><span class="stringliteral">        .. versionadded:: 0.18</span></div>
<div class="line"><span class="lineno">  144</span><span class="stringliteral">           *loss=&#39;multinomial&#39;*</span></div>
<div class="line"><span class="lineno">  145</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  146</span><span class="stringliteral">    alpha : float, default=1.</span></div>
<div class="line"><span class="lineno">  147</span><span class="stringliteral">        L2 regularization term in the objective function</span></div>
<div class="line"><span class="lineno">  148</span><span class="stringliteral">        ``(0.5 * alpha * || W ||_F^2)``.</span></div>
<div class="line"><span class="lineno">  149</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  150</span><span class="stringliteral">    beta : float, default=0.</span></div>
<div class="line"><span class="lineno">  151</span><span class="stringliteral">        L1 regularization term in the objective function</span></div>
<div class="line"><span class="lineno">  152</span><span class="stringliteral">        ``(beta * || W ||_1)``. Only applied if ``is_saga`` is set to True.</span></div>
<div class="line"><span class="lineno">  153</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  154</span><span class="stringliteral">    max_iter : int, default=1000</span></div>
<div class="line"><span class="lineno">  155</span><span class="stringliteral">        The max number of passes over the training data if the stopping</span></div>
<div class="line"><span class="lineno">  156</span><span class="stringliteral">        criteria is not reached.</span></div>
<div class="line"><span class="lineno">  157</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  158</span><span class="stringliteral">    tol : float, default=0.001</span></div>
<div class="line"><span class="lineno">  159</span><span class="stringliteral">        The stopping criteria for the weights. The iterations will stop when</span></div>
<div class="line"><span class="lineno">  160</span><span class="stringliteral">        max(change in weights) / max(weights) &lt; tol.</span></div>
<div class="line"><span class="lineno">  161</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  162</span><span class="stringliteral">    verbose : int, default=0</span></div>
<div class="line"><span class="lineno">  163</span><span class="stringliteral">        The verbosity level.</span></div>
<div class="line"><span class="lineno">  164</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  165</span><span class="stringliteral">    random_state : int, RandomState instance or None, default=None</span></div>
<div class="line"><span class="lineno">  166</span><span class="stringliteral">        Used when shuffling the data. Pass an int for reproducible output</span></div>
<div class="line"><span class="lineno">  167</span><span class="stringliteral">        across multiple function calls.</span></div>
<div class="line"><span class="lineno">  168</span><span class="stringliteral">        See :term:`Glossary &lt;random_state&gt;`.</span></div>
<div class="line"><span class="lineno">  169</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  170</span><span class="stringliteral">    check_input : bool, default=True</span></div>
<div class="line"><span class="lineno">  171</span><span class="stringliteral">        If False, the input arrays X and y will not be checked.</span></div>
<div class="line"><span class="lineno">  172</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  173</span><span class="stringliteral">    max_squared_sum : float, default=None</span></div>
<div class="line"><span class="lineno">  174</span><span class="stringliteral">        Maximum squared sum of X over samples. If None, it will be computed,</span></div>
<div class="line"><span class="lineno">  175</span><span class="stringliteral">        going through all the samples. The value should be precomputed</span></div>
<div class="line"><span class="lineno">  176</span><span class="stringliteral">        to speed up cross validation.</span></div>
<div class="line"><span class="lineno">  177</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  178</span><span class="stringliteral">    warm_start_mem : dict, default=None</span></div>
<div class="line"><span class="lineno">  179</span><span class="stringliteral">        The initialization parameters used for warm starting. Warm starting is</span></div>
<div class="line"><span class="lineno">  180</span><span class="stringliteral">        currently used in LogisticRegression but not in Ridge.</span></div>
<div class="line"><span class="lineno">  181</span><span class="stringliteral">        It contains:</span></div>
<div class="line"><span class="lineno">  182</span><span class="stringliteral">            - &#39;coef&#39;: the weight vector, with the intercept in last line</span></div>
<div class="line"><span class="lineno">  183</span><span class="stringliteral">                if the intercept is fitted.</span></div>
<div class="line"><span class="lineno">  184</span><span class="stringliteral">            - &#39;gradient_memory&#39;: the scalar gradient for all seen samples.</span></div>
<div class="line"><span class="lineno">  185</span><span class="stringliteral">            - &#39;sum_gradient&#39;: the sum of gradient over all seen samples,</span></div>
<div class="line"><span class="lineno">  186</span><span class="stringliteral">                for each feature.</span></div>
<div class="line"><span class="lineno">  187</span><span class="stringliteral">            - &#39;intercept_sum_gradient&#39;: the sum of gradient over all seen</span></div>
<div class="line"><span class="lineno">  188</span><span class="stringliteral">                samples, for the intercept.</span></div>
<div class="line"><span class="lineno">  189</span><span class="stringliteral">            - &#39;seen&#39;: array of boolean describing the seen samples.</span></div>
<div class="line"><span class="lineno">  190</span><span class="stringliteral">            - &#39;num_seen&#39;: the number of seen samples.</span></div>
<div class="line"><span class="lineno">  191</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  192</span><span class="stringliteral">    is_saga : bool, default=False</span></div>
<div class="line"><span class="lineno">  193</span><span class="stringliteral">        Whether to use the SAGA algorithm or the SAG algorithm. SAGA behaves</span></div>
<div class="line"><span class="lineno">  194</span><span class="stringliteral">        better in the first epochs, and allow for l1 regularisation.</span></div>
<div class="line"><span class="lineno">  195</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  196</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  197</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  198</span><span class="stringliteral">    coef_ : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  199</span><span class="stringliteral">        Weight vector.</span></div>
<div class="line"><span class="lineno">  200</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  201</span><span class="stringliteral">    n_iter_ : int</span></div>
<div class="line"><span class="lineno">  202</span><span class="stringliteral">        The number of full pass on all samples.</span></div>
<div class="line"><span class="lineno">  203</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  204</span><span class="stringliteral">    warm_start_mem : dict</span></div>
<div class="line"><span class="lineno">  205</span><span class="stringliteral">        Contains a &#39;coef&#39; key with the fitted result, and possibly the</span></div>
<div class="line"><span class="lineno">  206</span><span class="stringliteral">        fitted intercept at the end of the array. Contains also other keys</span></div>
<div class="line"><span class="lineno">  207</span><span class="stringliteral">        used for warm starting.</span></div>
<div class="line"><span class="lineno">  208</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  209</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno">  210</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  211</span><span class="stringliteral">    &gt;&gt;&gt; import numpy as np</span></div>
<div class="line"><span class="lineno">  212</span><span class="stringliteral">    &gt;&gt;&gt; from sklearn import linear_model</span></div>
<div class="line"><span class="lineno">  213</span><span class="stringliteral">    &gt;&gt;&gt; n_samples, n_features = 10, 5</span></div>
<div class="line"><span class="lineno">  214</span><span class="stringliteral">    &gt;&gt;&gt; rng = np.random.RandomState(0)</span></div>
<div class="line"><span class="lineno">  215</span><span class="stringliteral">    &gt;&gt;&gt; X = rng.randn(n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  216</span><span class="stringliteral">    &gt;&gt;&gt; y = rng.randn(n_samples)</span></div>
<div class="line"><span class="lineno">  217</span><span class="stringliteral">    &gt;&gt;&gt; clf = linear_model.Ridge(solver=&#39;sag&#39;)</span></div>
<div class="line"><span class="lineno">  218</span><span class="stringliteral">    &gt;&gt;&gt; clf.fit(X, y)</span></div>
<div class="line"><span class="lineno">  219</span><span class="stringliteral">    Ridge(solver=&#39;sag&#39;)</span></div>
<div class="line"><span class="lineno">  220</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  221</span><span class="stringliteral">    &gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])</span></div>
<div class="line"><span class="lineno">  222</span><span class="stringliteral">    &gt;&gt;&gt; y = np.array([1, 1, 2, 2])</span></div>
<div class="line"><span class="lineno">  223</span><span class="stringliteral">    &gt;&gt;&gt; clf = linear_model.LogisticRegression(</span></div>
<div class="line"><span class="lineno">  224</span><span class="stringliteral">    ...     solver=&#39;sag&#39;, multi_class=&#39;multinomial&#39;)</span></div>
<div class="line"><span class="lineno">  225</span><span class="stringliteral">    &gt;&gt;&gt; clf.fit(X, y)</span></div>
<div class="line"><span class="lineno">  226</span><span class="stringliteral">    LogisticRegression(multi_class=&#39;multinomial&#39;, solver=&#39;sag&#39;)</span></div>
<div class="line"><span class="lineno">  227</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  228</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  229</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  230</span><span class="stringliteral">    Schmidt, M., Roux, N. L., &amp; Bach, F. (2013).</span></div>
<div class="line"><span class="lineno">  231</span><span class="stringliteral">    Minimizing finite sums with the stochastic average gradient</span></div>
<div class="line"><span class="lineno">  232</span><span class="stringliteral">    https://hal.inria.fr/hal-00860051/document</span></div>
<div class="line"><span class="lineno">  233</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  234</span><span class="stringliteral">    :arxiv:`Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).</span></div>
<div class="line"><span class="lineno">  235</span><span class="stringliteral">    &quot;SAGA: A Fast Incremental Gradient Method With Support</span></div>
<div class="line"><span class="lineno">  236</span><span class="stringliteral">    for Non-Strongly Convex Composite Objectives&quot; &lt;1407.0202&gt;`</span></div>
<div class="line"><span class="lineno">  237</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  238</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  239</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  240</span><span class="stringliteral">    Ridge, SGDRegressor, ElasticNet, Lasso, SVR,</span></div>
<div class="line"><span class="lineno">  241</span><span class="stringliteral">    LogisticRegression, SGDClassifier, LinearSVC, Perceptron</span></div>
<div class="line"><span class="lineno">  242</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  243</span>    <span class="keywordflow">if</span> warm_start_mem <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  244</span>        warm_start_mem = {}</div>
<div class="line"><span class="lineno">  245</span>    <span class="comment"># Ridge default max_iter is None</span></div>
<div class="line"><span class="lineno">  246</span>    <span class="keywordflow">if</span> max_iter <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  247</span>        max_iter = 1000</div>
<div class="line"><span class="lineno">  248</span> </div>
<div class="line"><span class="lineno">  249</span>    <span class="keywordflow">if</span> check_input:</div>
<div class="line"><span class="lineno">  250</span>        _dtype = [np.float64, np.float32]</div>
<div class="line"><span class="lineno">  251</span>        X = check_array(X, dtype=_dtype, accept_sparse=<span class="stringliteral">&quot;csr&quot;</span>, order=<span class="stringliteral">&quot;C&quot;</span>)</div>
<div class="line"><span class="lineno">  252</span>        y = check_array(y, dtype=_dtype, ensure_2d=<span class="keyword">False</span>, order=<span class="stringliteral">&quot;C&quot;</span>)</div>
<div class="line"><span class="lineno">  253</span> </div>
<div class="line"><span class="lineno">  254</span>    n_samples, n_features = X.shape[0], X.shape[1]</div>
<div class="line"><span class="lineno">  255</span>    <span class="comment"># As in SGD, the alpha is scaled by n_samples.</span></div>
<div class="line"><span class="lineno">  256</span>    alpha_scaled = float(alpha) / n_samples</div>
<div class="line"><span class="lineno">  257</span>    beta_scaled = float(beta) / n_samples</div>
<div class="line"><span class="lineno">  258</span> </div>
<div class="line"><span class="lineno">  259</span>    <span class="comment"># if loss == &#39;multinomial&#39;, y should be label encoded.</span></div>
<div class="line"><span class="lineno">  260</span>    n_classes = int(y.max()) + 1 <span class="keywordflow">if</span> loss == <span class="stringliteral">&quot;multinomial&quot;</span> <span class="keywordflow">else</span> 1</div>
<div class="line"><span class="lineno">  261</span> </div>
<div class="line"><span class="lineno">  262</span>    <span class="comment"># initialization</span></div>
<div class="line"><span class="lineno">  263</span>    sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)</div>
<div class="line"><span class="lineno">  264</span> </div>
<div class="line"><span class="lineno">  265</span>    <span class="keywordflow">if</span> <span class="stringliteral">&quot;coef&quot;</span> <span class="keywordflow">in</span> warm_start_mem.keys():</div>
<div class="line"><span class="lineno">  266</span>        coef_init = warm_start_mem[<span class="stringliteral">&quot;coef&quot;</span>]</div>
<div class="line"><span class="lineno">  267</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  268</span>        <span class="comment"># assume fit_intercept is False</span></div>
<div class="line"><span class="lineno">  269</span>        coef_init = np.zeros((n_features, n_classes), dtype=X.dtype, order=<span class="stringliteral">&quot;C&quot;</span>)</div>
<div class="line"><span class="lineno">  270</span> </div>
<div class="line"><span class="lineno">  271</span>    <span class="comment"># coef_init contains possibly the intercept_init at the end.</span></div>
<div class="line"><span class="lineno">  272</span>    <span class="comment"># Note that Ridge centers the data before fitting, so fit_intercept=False.</span></div>
<div class="line"><span class="lineno">  273</span>    fit_intercept = coef_init.shape[0] == (n_features + 1)</div>
<div class="line"><span class="lineno">  274</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno">  275</span>        intercept_init = coef_init[-1, :]</div>
<div class="line"><span class="lineno">  276</span>        coef_init = coef_init[:-1, :]</div>
<div class="line"><span class="lineno">  277</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  278</span>        intercept_init = np.zeros(n_classes, dtype=X.dtype)</div>
<div class="line"><span class="lineno">  279</span> </div>
<div class="line"><span class="lineno">  280</span>    <span class="keywordflow">if</span> <span class="stringliteral">&quot;intercept_sum_gradient&quot;</span> <span class="keywordflow">in</span> warm_start_mem.keys():</div>
<div class="line"><span class="lineno">  281</span>        intercept_sum_gradient = warm_start_mem[<span class="stringliteral">&quot;intercept_sum_gradient&quot;</span>]</div>
<div class="line"><span class="lineno">  282</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  283</span>        intercept_sum_gradient = np.zeros(n_classes, dtype=X.dtype)</div>
<div class="line"><span class="lineno">  284</span> </div>
<div class="line"><span class="lineno">  285</span>    <span class="keywordflow">if</span> <span class="stringliteral">&quot;gradient_memory&quot;</span> <span class="keywordflow">in</span> warm_start_mem.keys():</div>
<div class="line"><span class="lineno">  286</span>        gradient_memory_init = warm_start_mem[<span class="stringliteral">&quot;gradient_memory&quot;</span>]</div>
<div class="line"><span class="lineno">  287</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  288</span>        gradient_memory_init = np.zeros(</div>
<div class="line"><span class="lineno">  289</span>            (n_samples, n_classes), dtype=X.dtype, order=<span class="stringliteral">&quot;C&quot;</span></div>
<div class="line"><span class="lineno">  290</span>        )</div>
<div class="line"><span class="lineno">  291</span>    <span class="keywordflow">if</span> <span class="stringliteral">&quot;sum_gradient&quot;</span> <span class="keywordflow">in</span> warm_start_mem.keys():</div>
<div class="line"><span class="lineno">  292</span>        sum_gradient_init = warm_start_mem[<span class="stringliteral">&quot;sum_gradient&quot;</span>]</div>
<div class="line"><span class="lineno">  293</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  294</span>        sum_gradient_init = np.zeros((n_features, n_classes), dtype=X.dtype, order=<span class="stringliteral">&quot;C&quot;</span>)</div>
<div class="line"><span class="lineno">  295</span> </div>
<div class="line"><span class="lineno">  296</span>    <span class="keywordflow">if</span> <span class="stringliteral">&quot;seen&quot;</span> <span class="keywordflow">in</span> warm_start_mem.keys():</div>
<div class="line"><span class="lineno">  297</span>        seen_init = warm_start_mem[<span class="stringliteral">&quot;seen&quot;</span>]</div>
<div class="line"><span class="lineno">  298</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  299</span>        seen_init = np.zeros(n_samples, dtype=np.int32, order=<span class="stringliteral">&quot;C&quot;</span>)</div>
<div class="line"><span class="lineno">  300</span> </div>
<div class="line"><span class="lineno">  301</span>    <span class="keywordflow">if</span> <span class="stringliteral">&quot;num_seen&quot;</span> <span class="keywordflow">in</span> warm_start_mem.keys():</div>
<div class="line"><span class="lineno">  302</span>        num_seen_init = warm_start_mem[<span class="stringliteral">&quot;num_seen&quot;</span>]</div>
<div class="line"><span class="lineno">  303</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  304</span>        num_seen_init = 0</div>
<div class="line"><span class="lineno">  305</span> </div>
<div class="line"><span class="lineno">  306</span>    dataset, intercept_decay = make_dataset(X, y, sample_weight, random_state)</div>
<div class="line"><span class="lineno">  307</span> </div>
<div class="line"><span class="lineno">  308</span>    <span class="keywordflow">if</span> max_squared_sum <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  309</span>        max_squared_sum = row_norms(X, squared=<span class="keyword">True</span>).max()</div>
<div class="line"><span class="lineno">  310</span>    step_size = get_auto_step_size(</div>
<div class="line"><span class="lineno">  311</span>        max_squared_sum,</div>
<div class="line"><span class="lineno">  312</span>        alpha_scaled,</div>
<div class="line"><span class="lineno">  313</span>        loss,</div>
<div class="line"><span class="lineno">  314</span>        fit_intercept,</div>
<div class="line"><span class="lineno">  315</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  316</span>        is_saga=is_saga,</div>
<div class="line"><span class="lineno">  317</span>    )</div>
<div class="line"><span class="lineno">  318</span>    <span class="keywordflow">if</span> step_size * alpha_scaled == 1:</div>
<div class="line"><span class="lineno">  319</span>        <span class="keywordflow">raise</span> ZeroDivisionError(</div>
<div class="line"><span class="lineno">  320</span>            <span class="stringliteral">&quot;Current sag implementation does not handle &quot;</span></div>
<div class="line"><span class="lineno">  321</span>            <span class="stringliteral">&quot;the case step_size * alpha_scaled == 1&quot;</span></div>
<div class="line"><span class="lineno">  322</span>        )</div>
<div class="line"><span class="lineno">  323</span> </div>
<div class="line"><span class="lineno">  324</span>    sag = sag64 <span class="keywordflow">if</span> X.dtype == np.float64 <span class="keywordflow">else</span> sag32</div>
<div class="line"><span class="lineno">  325</span>    num_seen, n_iter_ = sag(</div>
<div class="line"><span class="lineno">  326</span>        dataset,</div>
<div class="line"><span class="lineno">  327</span>        coef_init,</div>
<div class="line"><span class="lineno">  328</span>        intercept_init,</div>
<div class="line"><span class="lineno">  329</span>        n_samples,</div>
<div class="line"><span class="lineno">  330</span>        n_features,</div>
<div class="line"><span class="lineno">  331</span>        n_classes,</div>
<div class="line"><span class="lineno">  332</span>        tol,</div>
<div class="line"><span class="lineno">  333</span>        max_iter,</div>
<div class="line"><span class="lineno">  334</span>        loss,</div>
<div class="line"><span class="lineno">  335</span>        step_size,</div>
<div class="line"><span class="lineno">  336</span>        alpha_scaled,</div>
<div class="line"><span class="lineno">  337</span>        beta_scaled,</div>
<div class="line"><span class="lineno">  338</span>        sum_gradient_init,</div>
<div class="line"><span class="lineno">  339</span>        gradient_memory_init,</div>
<div class="line"><span class="lineno">  340</span>        seen_init,</div>
<div class="line"><span class="lineno">  341</span>        num_seen_init,</div>
<div class="line"><span class="lineno">  342</span>        fit_intercept,</div>
<div class="line"><span class="lineno">  343</span>        intercept_sum_gradient,</div>
<div class="line"><span class="lineno">  344</span>        intercept_decay,</div>
<div class="line"><span class="lineno">  345</span>        is_saga,</div>
<div class="line"><span class="lineno">  346</span>        verbose,</div>
<div class="line"><span class="lineno">  347</span>    )</div>
<div class="line"><span class="lineno">  348</span> </div>
<div class="line"><span class="lineno">  349</span>    <span class="keywordflow">if</span> n_iter_ == max_iter:</div>
<div class="line"><span class="lineno">  350</span>        warnings.warn(</div>
<div class="line"><span class="lineno">  351</span>            <span class="stringliteral">&quot;The max_iter was reached which means the coef_ did not converge&quot;</span>,</div>
<div class="line"><span class="lineno">  352</span>            ConvergenceWarning,</div>
<div class="line"><span class="lineno">  353</span>        )</div>
<div class="line"><span class="lineno">  354</span> </div>
<div class="line"><span class="lineno">  355</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno">  356</span>        coef_init = np.vstack((coef_init, intercept_init))</div>
<div class="line"><span class="lineno">  357</span> </div>
<div class="line"><span class="lineno">  358</span>    warm_start_mem = {</div>
<div class="line"><span class="lineno">  359</span>        <span class="stringliteral">&quot;coef&quot;</span>: coef_init,</div>
<div class="line"><span class="lineno">  360</span>        <span class="stringliteral">&quot;sum_gradient&quot;</span>: sum_gradient_init,</div>
<div class="line"><span class="lineno">  361</span>        <span class="stringliteral">&quot;intercept_sum_gradient&quot;</span>: intercept_sum_gradient,</div>
<div class="line"><span class="lineno">  362</span>        <span class="stringliteral">&quot;gradient_memory&quot;</span>: gradient_memory_init,</div>
<div class="line"><span class="lineno">  363</span>        <span class="stringliteral">&quot;seen&quot;</span>: seen_init,</div>
<div class="line"><span class="lineno">  364</span>        <span class="stringliteral">&quot;num_seen&quot;</span>: num_seen,</div>
<div class="line"><span class="lineno">  365</span>    }</div>
<div class="line"><span class="lineno">  366</span> </div>
<div class="line"><span class="lineno">  367</span>    <span class="keywordflow">if</span> loss == <span class="stringliteral">&quot;multinomial&quot;</span>:</div>
<div class="line"><span class="lineno">  368</span>        coef_ = coef_init.T</div>
<div class="line"><span class="lineno">  369</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  370</span>        coef_ = coef_init[:, 0]</div>
<div class="line"><span class="lineno">  371</span> </div>
<div class="line"><span class="lineno">  372</span>    <span class="keywordflow">return</span> coef_, n_iter_, warm_start_mem</div>
</div><!-- fragment -->
</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
