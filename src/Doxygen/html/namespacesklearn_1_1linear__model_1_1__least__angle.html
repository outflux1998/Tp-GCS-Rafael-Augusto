<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.linear_model._least_angle Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model.html">linear_model</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model_1_1__least__angle.html">_least_angle</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">sklearn.linear_model._least_angle Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__least__angle_1_1_lars.html">Lars</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Estimator classes.  <a href="classsklearn_1_1linear__model_1_1__least__angle_1_1_lars.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__least__angle_1_1_lars_c_v.html">LarsCV</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__least__angle_1_1_lasso_lars.html">LassoLars</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__least__angle_1_1_lasso_lars_c_v.html">LassoLarsCV</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1linear__model_1_1__least__angle_1_1_lasso_lars_i_c.html">LassoLarsIC</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a9db4e0637ad1d2a96c0e250f6d7e511d" id="r_a9db4e0637ad1d2a96c0e250f6d7e511d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1__least__angle.html#a9db4e0637ad1d2a96c0e250f6d7e511d">lars_path</a> (X, y, Xy=None, *Gram=None, max_iter=500, alpha_min=0, method=&quot;lar&quot;, copy_X=True, <a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>=np.finfo(float).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)</td></tr>
<tr class="separator:a9db4e0637ad1d2a96c0e250f6d7e511d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1b08ff0c162c417d94c0be0b0c869fa3" id="r_a1b08ff0c162c417d94c0be0b0c869fa3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1__least__angle.html#a1b08ff0c162c417d94c0be0b0c869fa3">lars_path_gram</a> (Xy, Gram, *n_samples, max_iter=500, alpha_min=0, method=&quot;lar&quot;, copy_X=True, <a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>=np.finfo(float).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)</td></tr>
<tr class="separator:a1b08ff0c162c417d94c0be0b0c869fa3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa62df71729db98c375f7e8e36b4eca02" id="r_aa62df71729db98c375f7e8e36b4eca02"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1__least__angle.html#aa62df71729db98c375f7e8e36b4eca02">_lars_path_solver</a> (X, y, Xy=None, Gram=None, n_samples=None, max_iter=500, alpha_min=0, method=&quot;lar&quot;, copy_X=True, <a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>=np.finfo(float).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>, copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)</td></tr>
<tr class="separator:aa62df71729db98c375f7e8e36b4eca02"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac6e06629837d3896c0ef86beb79a6f6f" id="r_ac6e06629837d3896c0ef86beb79a6f6f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1__least__angle.html#ac6e06629837d3896c0ef86beb79a6f6f">_check_copy_and_writeable</a> (array, copy=False)</td></tr>
<tr class="separator:ac6e06629837d3896c0ef86beb79a6f6f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af30e7543f1a53f5c0a08dd57d6ac94ba" id="r_af30e7543f1a53f5c0a08dd57d6ac94ba"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1__least__angle.html#af30e7543f1a53f5c0a08dd57d6ac94ba">_lars_path_residues</a> (X_train, y_train, X_test, y_test, Gram=None, copy=True, method=&quot;lars&quot;, verbose=False, fit_intercept=True, normalize=False, max_iter=500, <a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>=np.finfo(float).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>, positive=False)</td></tr>
<tr class="separator:af30e7543f1a53f5c0a08dd57d6ac94ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:adbfa3ac69f38d7b8e84c4db13058d4b4" id="r_adbfa3ac69f38d7b8e84c4db13058d4b4"><td class="memItemLeft" align="right" valign="top">dict&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1__least__angle.html#adbfa3ac69f38d7b8e84c4db13058d4b4">SOLVE_TRIANGULAR_ARGS</a> = {&quot;check_finite&quot;: False}</td></tr>
<tr class="separator:adbfa3ac69f38d7b8e84c4db13058d4b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Least Angle Regression algorithm. See the documentation on the
Generalized Linear Model for a complete discussion.
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="ac6e06629837d3896c0ef86beb79a6f6f" name="ac6e06629837d3896c0ef86beb79a6f6f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac6e06629837d3896c0ef86beb79a6f6f">&#9670;&#160;</a></span>_check_copy_and_writeable()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._least_angle._check_copy_and_writeable </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>array</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1357</span><span class="keyword">def </span>_check_copy_and_writeable(array, copy=False):</div>
<div class="line"><span class="lineno"> 1358</span>    <span class="keywordflow">if</span> copy <span class="keywordflow">or</span> <span class="keywordflow">not</span> array.flags.writeable:</div>
<div class="line"><span class="lineno"> 1359</span>        <span class="keywordflow">return</span> array.copy()</div>
<div class="line"><span class="lineno"> 1360</span>    <span class="keywordflow">return</span> array</div>
<div class="line"><span class="lineno"> 1361</span> </div>
<div class="line"><span class="lineno"> 1362</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="af30e7543f1a53f5c0a08dd57d6ac94ba" name="af30e7543f1a53f5c0a08dd57d6ac94ba"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af30e7543f1a53f5c0a08dd57d6ac94ba">&#9670;&#160;</a></span>_lars_path_residues()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._least_angle._lars_path_residues </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X_train</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_train</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X_test</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_test</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Gram</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>method</em> = <code>&quot;lars&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>normalize</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_iter</em> = <code>500</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>eps</em> = <code>np.finfo(float).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>positive</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the residues on left-out data for a full LARS path

Parameters
-----------
X_train : array-like of shape (n_samples, n_features)
    The data to fit the LARS on

y_train : array-like of shape (n_samples,)
    The target variable to fit LARS on

X_test : array-like of shape (n_samples, n_features)
    The data to compute the residues on

y_test : array-like of shape (n_samples,)
    The target variable to compute the residues on

Gram : None, 'auto' or array-like of shape (n_features, n_features), \
        default=None
    Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
    matrix is precomputed from the given X, if there are more samples
    than features

copy : bool, default=True
    Whether X_train, X_test, y_train and y_test should be copied;
    if False, they may be overwritten.

method : {'lar' , 'lasso'}, default='lar'
    Specifies the returned model. Select ``'lar'`` for Least Angle
    Regression, ``'lasso'`` for the Lasso.

verbose : bool or int, default=False
    Sets the amount of verbosity

fit_intercept : bool, default=True
    whether to calculate the intercept for this model. If set
    to false, no intercept will be used in calculations
    (i.e. data is expected to be centered).

positive : bool, default=False
    Restrict coefficients to be &gt;= 0. Be aware that you might want to
    remove fit_intercept which is set True by default.
    See reservations for using this option in combination with method
    'lasso' for expected small values of alpha in the doc of LassoLarsCV
    and LassoLarsIC.

normalize : bool, default=False
    This parameter is ignored when ``fit_intercept`` is set to False.
    If True, the regressors X will be normalized before regression by
    subtracting the mean and dividing by the l2-norm.
    If you wish to standardize, please use
    :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``
    on an estimator with ``normalize=False``.

    .. versionchanged:: 1.2
       default changed from True to False in 1.2.

    .. deprecated:: 1.2
        ``normalize`` was deprecated in version 1.2 and will be removed in 1.4.

max_iter : int, default=500
    Maximum number of iterations to perform.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

Returns
--------
alphas : array-like of shape (n_alphas,)
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
    is smaller.

active : list
    Indices of active variables at the end of the path.

coefs : array-like of shape (n_features, n_alphas)
    Coefficients along the path

residues : array-like of shape (n_alphas, n_samples)
    Residues of the prediction on the test data
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1377</span>):</div>
<div class="line"><span class="lineno"> 1378</span>    <span class="stringliteral">&quot;&quot;&quot;Compute the residues on left-out data for a full LARS path</span></div>
<div class="line"><span class="lineno"> 1379</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1380</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno"> 1381</span><span class="stringliteral">    -----------</span></div>
<div class="line"><span class="lineno"> 1382</span><span class="stringliteral">    X_train : array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 1383</span><span class="stringliteral">        The data to fit the LARS on</span></div>
<div class="line"><span class="lineno"> 1384</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1385</span><span class="stringliteral">    y_train : array-like of shape (n_samples,)</span></div>
<div class="line"><span class="lineno"> 1386</span><span class="stringliteral">        The target variable to fit LARS on</span></div>
<div class="line"><span class="lineno"> 1387</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1388</span><span class="stringliteral">    X_test : array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno"> 1389</span><span class="stringliteral">        The data to compute the residues on</span></div>
<div class="line"><span class="lineno"> 1390</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1391</span><span class="stringliteral">    y_test : array-like of shape (n_samples,)</span></div>
<div class="line"><span class="lineno"> 1392</span><span class="stringliteral">        The target variable to compute the residues on</span></div>
<div class="line"><span class="lineno"> 1393</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1394</span><span class="stringliteral">    Gram : None, &#39;auto&#39; or array-like of shape (n_features, n_features), \</span></div>
<div class="line"><span class="lineno"> 1395</span><span class="stringliteral">            default=None</span></div>
<div class="line"><span class="lineno"> 1396</span><span class="stringliteral">        Precomputed Gram matrix (X&#39; * X), if ``&#39;auto&#39;``, the Gram</span></div>
<div class="line"><span class="lineno"> 1397</span><span class="stringliteral">        matrix is precomputed from the given X, if there are more samples</span></div>
<div class="line"><span class="lineno"> 1398</span><span class="stringliteral">        than features</span></div>
<div class="line"><span class="lineno"> 1399</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1400</span><span class="stringliteral">    copy : bool, default=True</span></div>
<div class="line"><span class="lineno"> 1401</span><span class="stringliteral">        Whether X_train, X_test, y_train and y_test should be copied;</span></div>
<div class="line"><span class="lineno"> 1402</span><span class="stringliteral">        if False, they may be overwritten.</span></div>
<div class="line"><span class="lineno"> 1403</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1404</span><span class="stringliteral">    method : {&#39;lar&#39; , &#39;lasso&#39;}, default=&#39;lar&#39;</span></div>
<div class="line"><span class="lineno"> 1405</span><span class="stringliteral">        Specifies the returned model. Select ``&#39;lar&#39;`` for Least Angle</span></div>
<div class="line"><span class="lineno"> 1406</span><span class="stringliteral">        Regression, ``&#39;lasso&#39;`` for the Lasso.</span></div>
<div class="line"><span class="lineno"> 1407</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1408</span><span class="stringliteral">    verbose : bool or int, default=False</span></div>
<div class="line"><span class="lineno"> 1409</span><span class="stringliteral">        Sets the amount of verbosity</span></div>
<div class="line"><span class="lineno"> 1410</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1411</span><span class="stringliteral">    fit_intercept : bool, default=True</span></div>
<div class="line"><span class="lineno"> 1412</span><span class="stringliteral">        whether to calculate the intercept for this model. If set</span></div>
<div class="line"><span class="lineno"> 1413</span><span class="stringliteral">        to false, no intercept will be used in calculations</span></div>
<div class="line"><span class="lineno"> 1414</span><span class="stringliteral">        (i.e. data is expected to be centered).</span></div>
<div class="line"><span class="lineno"> 1415</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1416</span><span class="stringliteral">    positive : bool, default=False</span></div>
<div class="line"><span class="lineno"> 1417</span><span class="stringliteral">        Restrict coefficients to be &gt;= 0. Be aware that you might want to</span></div>
<div class="line"><span class="lineno"> 1418</span><span class="stringliteral">        remove fit_intercept which is set True by default.</span></div>
<div class="line"><span class="lineno"> 1419</span><span class="stringliteral">        See reservations for using this option in combination with method</span></div>
<div class="line"><span class="lineno"> 1420</span><span class="stringliteral">        &#39;lasso&#39; for expected small values of alpha in the doc of LassoLarsCV</span></div>
<div class="line"><span class="lineno"> 1421</span><span class="stringliteral">        and LassoLarsIC.</span></div>
<div class="line"><span class="lineno"> 1422</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1423</span><span class="stringliteral">    normalize : bool, default=False</span></div>
<div class="line"><span class="lineno"> 1424</span><span class="stringliteral">        This parameter is ignored when ``fit_intercept`` is set to False.</span></div>
<div class="line"><span class="lineno"> 1425</span><span class="stringliteral">        If True, the regressors X will be normalized before regression by</span></div>
<div class="line"><span class="lineno"> 1426</span><span class="stringliteral">        subtracting the mean and dividing by the l2-norm.</span></div>
<div class="line"><span class="lineno"> 1427</span><span class="stringliteral">        If you wish to standardize, please use</span></div>
<div class="line"><span class="lineno"> 1428</span><span class="stringliteral">        :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``</span></div>
<div class="line"><span class="lineno"> 1429</span><span class="stringliteral">        on an estimator with ``normalize=False``.</span></div>
<div class="line"><span class="lineno"> 1430</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1431</span><span class="stringliteral">        .. versionchanged:: 1.2</span></div>
<div class="line"><span class="lineno"> 1432</span><span class="stringliteral">           default changed from True to False in 1.2.</span></div>
<div class="line"><span class="lineno"> 1433</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1434</span><span class="stringliteral">        .. deprecated:: 1.2</span></div>
<div class="line"><span class="lineno"> 1435</span><span class="stringliteral">            ``normalize`` was deprecated in version 1.2 and will be removed in 1.4.</span></div>
<div class="line"><span class="lineno"> 1436</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1437</span><span class="stringliteral">    max_iter : int, default=500</span></div>
<div class="line"><span class="lineno"> 1438</span><span class="stringliteral">        Maximum number of iterations to perform.</span></div>
<div class="line"><span class="lineno"> 1439</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1440</span><span class="stringliteral">    eps : float, default=np.finfo(float).eps</span></div>
<div class="line"><span class="lineno"> 1441</span><span class="stringliteral">        The machine-precision regularization in the computation of the</span></div>
<div class="line"><span class="lineno"> 1442</span><span class="stringliteral">        Cholesky diagonal factors. Increase this for very ill-conditioned</span></div>
<div class="line"><span class="lineno"> 1443</span><span class="stringliteral">        systems. Unlike the ``tol`` parameter in some iterative</span></div>
<div class="line"><span class="lineno"> 1444</span><span class="stringliteral">        optimization-based algorithms, this parameter does not control</span></div>
<div class="line"><span class="lineno"> 1445</span><span class="stringliteral">        the tolerance of the optimization.</span></div>
<div class="line"><span class="lineno"> 1446</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1447</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno"> 1448</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno"> 1449</span><span class="stringliteral">    alphas : array-like of shape (n_alphas,)</span></div>
<div class="line"><span class="lineno"> 1450</span><span class="stringliteral">        Maximum of covariances (in absolute value) at each iteration.</span></div>
<div class="line"><span class="lineno"> 1451</span><span class="stringliteral">        ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever</span></div>
<div class="line"><span class="lineno"> 1452</span><span class="stringliteral">        is smaller.</span></div>
<div class="line"><span class="lineno"> 1453</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1454</span><span class="stringliteral">    active : list</span></div>
<div class="line"><span class="lineno"> 1455</span><span class="stringliteral">        Indices of active variables at the end of the path.</span></div>
<div class="line"><span class="lineno"> 1456</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1457</span><span class="stringliteral">    coefs : array-like of shape (n_features, n_alphas)</span></div>
<div class="line"><span class="lineno"> 1458</span><span class="stringliteral">        Coefficients along the path</span></div>
<div class="line"><span class="lineno"> 1459</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1460</span><span class="stringliteral">    residues : array-like of shape (n_alphas, n_samples)</span></div>
<div class="line"><span class="lineno"> 1461</span><span class="stringliteral">        Residues of the prediction on the test data</span></div>
<div class="line"><span class="lineno"> 1462</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1463</span>    X_train = _check_copy_and_writeable(X_train, copy)</div>
<div class="line"><span class="lineno"> 1464</span>    y_train = _check_copy_and_writeable(y_train, copy)</div>
<div class="line"><span class="lineno"> 1465</span>    X_test = _check_copy_and_writeable(X_test, copy)</div>
<div class="line"><span class="lineno"> 1466</span>    y_test = _check_copy_and_writeable(y_test, copy)</div>
<div class="line"><span class="lineno"> 1467</span> </div>
<div class="line"><span class="lineno"> 1468</span>    <span class="keywordflow">if</span> fit_intercept:</div>
<div class="line"><span class="lineno"> 1469</span>        X_mean = X_train.mean(axis=0)</div>
<div class="line"><span class="lineno"> 1470</span>        X_train -= X_mean</div>
<div class="line"><span class="lineno"> 1471</span>        X_test -= X_mean</div>
<div class="line"><span class="lineno"> 1472</span>        y_mean = y_train.mean(axis=0)</div>
<div class="line"><span class="lineno"> 1473</span>        y_train = as_float_array(y_train, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1474</span>        y_train -= y_mean</div>
<div class="line"><span class="lineno"> 1475</span>        y_test = as_float_array(y_test, copy=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno"> 1476</span>        y_test -= y_mean</div>
<div class="line"><span class="lineno"> 1477</span> </div>
<div class="line"><span class="lineno"> 1478</span>    <span class="keywordflow">if</span> normalize:</div>
<div class="line"><span class="lineno"> 1479</span>        norms = np.sqrt(np.sum(X_train**2, axis=0))</div>
<div class="line"><span class="lineno"> 1480</span>        nonzeros = np.flatnonzero(norms)</div>
<div class="line"><span class="lineno"> 1481</span>        X_train[:, nonzeros] /= norms[nonzeros]</div>
<div class="line"><span class="lineno"> 1482</span> </div>
<div class="line"><span class="lineno"> 1483</span>    alphas, active, coefs = lars_path(</div>
<div class="line"><span class="lineno"> 1484</span>        X_train,</div>
<div class="line"><span class="lineno"> 1485</span>        y_train,</div>
<div class="line"><span class="lineno"> 1486</span>        Gram=Gram,</div>
<div class="line"><span class="lineno"> 1487</span>        copy_X=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1488</span>        copy_Gram=<span class="keyword">False</span>,</div>
<div class="line"><span class="lineno"> 1489</span>        method=method,</div>
<div class="line"><span class="lineno"> 1490</span>        verbose=max(0, verbose - 1),</div>
<div class="line"><span class="lineno"> 1491</span>        max_iter=max_iter,</div>
<div class="line"><span class="lineno"> 1492</span>        eps=eps,</div>
<div class="line"><span class="lineno"> 1493</span>        positive=positive,</div>
<div class="line"><span class="lineno"> 1494</span>    )</div>
<div class="line"><span class="lineno"> 1495</span>    <span class="keywordflow">if</span> normalize:</div>
<div class="line"><span class="lineno"> 1496</span>        coefs[nonzeros] /= norms[nonzeros][:, np.newaxis]</div>
<div class="line"><span class="lineno"> 1497</span>    residues = np.dot(X_test, coefs) - y_test[:, np.newaxis]</div>
<div class="line"><span class="lineno"> 1498</span>    <span class="keywordflow">return</span> alphas, active, coefs, residues.T</div>
<div class="line"><span class="lineno"> 1499</span> </div>
<div class="line"><span class="lineno"> 1500</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aa62df71729db98c375f7e8e36b4eca02" name="aa62df71729db98c375f7e8e36b4eca02"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa62df71729db98c375f7e8e36b4eca02">&#9670;&#160;</a></span>_lars_path_solver()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._least_angle._lars_path_solver </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Xy</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Gram</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_iter</em> = <code>500</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha_min</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>method</em> = <code>&quot;lar&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy_X</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>eps</em> = <code>np.finfo(float).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy_Gram</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>return_path</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>return_n_iter</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>positive</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute Least Angle Regression or Lasso path using LARS algorithm [1]

The optimization objective for the case method='lasso' is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

in the case of method='lars', the objective function is only known in
the form of an implicit equation (see discussion in [1])

Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.

Parameters
----------
X : None or ndarray of shape (n_samples, n_features)
    Input data. Note that if X is None then Gram must be specified,
    i.e., cannot be None or False.

y : None or ndarray of shape (n_samples,)
    Input targets.

Xy : array-like of shape (n_samples,) or (n_samples, n_targets), \
        default=None
    `Xy = np.dot(X.T, y)` that can be precomputed. It is useful
    only when the Gram matrix is precomputed.

Gram : None, 'auto' or array-like of shape (n_features, n_features), \
        default=None
    Precomputed Gram matrix `(X' * X)`, if ``'auto'``, the Gram
    matrix is precomputed from the given X, if there are more samples
    than features.

n_samples : int or float, default=None
    Equivalent size of sample. If `None`, it will be `n_samples`.

max_iter : int, default=500
    Maximum number of iterations to perform, set to infinity for no limit.

alpha_min : float, default=0
    Minimum correlation along the path. It corresponds to the
    regularization parameter alpha parameter in the Lasso.

method : {'lar', 'lasso'}, default='lar'
    Specifies the returned model. Select ``'lar'`` for Least Angle
    Regression, ``'lasso'`` for the Lasso.

copy_X : bool, default=True
    If ``False``, ``X`` is overwritten.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_Gram : bool, default=True
    If ``False``, ``Gram`` is overwritten.

verbose : int, default=0
    Controls output verbosity.

return_path : bool, default=True
    If ``return_path==True`` returns the entire path, else returns only the
    last point of the path.

return_n_iter : bool, default=False
    Whether to return the number of iterations.

positive : bool, default=False
    Restrict coefficients to be &gt;= 0.
    This option is only allowed with method 'lasso'. Note that the model
    coefficients will not converge to the ordinary-least-squares solution
    for small values of alpha. Only coefficients up to the smallest alpha
    value (``alphas_[alphas_ &gt; 0.].min()`` when fit_path=True) reached by
    the stepwise Lars-Lasso algorithm are typically in congruence with the
    solution of the coordinate descent lasso_path function.

Returns
-------
alphas : array-like of shape (n_alphas + 1,)
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha &gt;= alpha_min``, whichever
    is smaller.

active : array-like of shape (n_alphas,)
    Indices of active variables at the end of the path.

coefs : array-like of shape (n_features, n_alphas + 1)
    Coefficients along the path

n_iter : int
    Number of iterations run. Returned only if return_n_iter is set
    to True.

See Also
--------
lasso_path
LassoLars
Lars
LassoLarsCV
LarsCV
sklearn.decomposition.sparse_encode

References
----------
.. [1] "Least Angle Regression", Efron et al.
       http://statweb.stanford.edu/~tibs/ftp/lars.pdf

.. [2] `Wikipedia entry on the Least-angle regression
       &lt;https://en.wikipedia.org/wiki/Least-angle_regression&gt;`_

.. [3] `Wikipedia entry on the Lasso
       &lt;https://en.wikipedia.org/wiki/Lasso_(statistics)&gt;`_</pre> <div class="fragment"><div class="line"><span class="lineno">  345</span>):</div>
<div class="line"><span class="lineno">  346</span>    <span class="stringliteral">&quot;&quot;&quot;Compute Least Angle Regression or Lasso path using LARS algorithm [1]</span></div>
<div class="line"><span class="lineno">  347</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  348</span><span class="stringliteral">    The optimization objective for the case method=&#39;lasso&#39; is::</span></div>
<div class="line"><span class="lineno">  349</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  350</span><span class="stringliteral">    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1</span></div>
<div class="line"><span class="lineno">  351</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  352</span><span class="stringliteral">    in the case of method=&#39;lars&#39;, the objective function is only known in</span></div>
<div class="line"><span class="lineno">  353</span><span class="stringliteral">    the form of an implicit equation (see discussion in [1])</span></div>
<div class="line"><span class="lineno">  354</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  355</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.</span></div>
<div class="line"><span class="lineno">  356</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  357</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  358</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  359</span><span class="stringliteral">    X : None or ndarray of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  360</span><span class="stringliteral">        Input data. Note that if X is None then Gram must be specified,</span></div>
<div class="line"><span class="lineno">  361</span><span class="stringliteral">        i.e., cannot be None or False.</span></div>
<div class="line"><span class="lineno">  362</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  363</span><span class="stringliteral">    y : None or ndarray of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  364</span><span class="stringliteral">        Input targets.</span></div>
<div class="line"><span class="lineno">  365</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  366</span><span class="stringliteral">    Xy : array-like of shape (n_samples,) or (n_samples, n_targets), \</span></div>
<div class="line"><span class="lineno">  367</span><span class="stringliteral">            default=None</span></div>
<div class="line"><span class="lineno">  368</span><span class="stringliteral">        `Xy = np.dot(X.T, y)` that can be precomputed. It is useful</span></div>
<div class="line"><span class="lineno">  369</span><span class="stringliteral">        only when the Gram matrix is precomputed.</span></div>
<div class="line"><span class="lineno">  370</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  371</span><span class="stringliteral">    Gram : None, &#39;auto&#39; or array-like of shape (n_features, n_features), \</span></div>
<div class="line"><span class="lineno">  372</span><span class="stringliteral">            default=None</span></div>
<div class="line"><span class="lineno">  373</span><span class="stringliteral">        Precomputed Gram matrix `(X&#39; * X)`, if ``&#39;auto&#39;``, the Gram</span></div>
<div class="line"><span class="lineno">  374</span><span class="stringliteral">        matrix is precomputed from the given X, if there are more samples</span></div>
<div class="line"><span class="lineno">  375</span><span class="stringliteral">        than features.</span></div>
<div class="line"><span class="lineno">  376</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  377</span><span class="stringliteral">    n_samples : int or float, default=None</span></div>
<div class="line"><span class="lineno">  378</span><span class="stringliteral">        Equivalent size of sample. If `None`, it will be `n_samples`.</span></div>
<div class="line"><span class="lineno">  379</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  380</span><span class="stringliteral">    max_iter : int, default=500</span></div>
<div class="line"><span class="lineno">  381</span><span class="stringliteral">        Maximum number of iterations to perform, set to infinity for no limit.</span></div>
<div class="line"><span class="lineno">  382</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  383</span><span class="stringliteral">    alpha_min : float, default=0</span></div>
<div class="line"><span class="lineno">  384</span><span class="stringliteral">        Minimum correlation along the path. It corresponds to the</span></div>
<div class="line"><span class="lineno">  385</span><span class="stringliteral">        regularization parameter alpha parameter in the Lasso.</span></div>
<div class="line"><span class="lineno">  386</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  387</span><span class="stringliteral">    method : {&#39;lar&#39;, &#39;lasso&#39;}, default=&#39;lar&#39;</span></div>
<div class="line"><span class="lineno">  388</span><span class="stringliteral">        Specifies the returned model. Select ``&#39;lar&#39;`` for Least Angle</span></div>
<div class="line"><span class="lineno">  389</span><span class="stringliteral">        Regression, ``&#39;lasso&#39;`` for the Lasso.</span></div>
<div class="line"><span class="lineno">  390</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  391</span><span class="stringliteral">    copy_X : bool, default=True</span></div>
<div class="line"><span class="lineno">  392</span><span class="stringliteral">        If ``False``, ``X`` is overwritten.</span></div>
<div class="line"><span class="lineno">  393</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  394</span><span class="stringliteral">    eps : float, default=np.finfo(float).eps</span></div>
<div class="line"><span class="lineno">  395</span><span class="stringliteral">        The machine-precision regularization in the computation of the</span></div>
<div class="line"><span class="lineno">  396</span><span class="stringliteral">        Cholesky diagonal factors. Increase this for very ill-conditioned</span></div>
<div class="line"><span class="lineno">  397</span><span class="stringliteral">        systems. Unlike the ``tol`` parameter in some iterative</span></div>
<div class="line"><span class="lineno">  398</span><span class="stringliteral">        optimization-based algorithms, this parameter does not control</span></div>
<div class="line"><span class="lineno">  399</span><span class="stringliteral">        the tolerance of the optimization.</span></div>
<div class="line"><span class="lineno">  400</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  401</span><span class="stringliteral">    copy_Gram : bool, default=True</span></div>
<div class="line"><span class="lineno">  402</span><span class="stringliteral">        If ``False``, ``Gram`` is overwritten.</span></div>
<div class="line"><span class="lineno">  403</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  404</span><span class="stringliteral">    verbose : int, default=0</span></div>
<div class="line"><span class="lineno">  405</span><span class="stringliteral">        Controls output verbosity.</span></div>
<div class="line"><span class="lineno">  406</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  407</span><span class="stringliteral">    return_path : bool, default=True</span></div>
<div class="line"><span class="lineno">  408</span><span class="stringliteral">        If ``return_path==True`` returns the entire path, else returns only the</span></div>
<div class="line"><span class="lineno">  409</span><span class="stringliteral">        last point of the path.</span></div>
<div class="line"><span class="lineno">  410</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  411</span><span class="stringliteral">    return_n_iter : bool, default=False</span></div>
<div class="line"><span class="lineno">  412</span><span class="stringliteral">        Whether to return the number of iterations.</span></div>
<div class="line"><span class="lineno">  413</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  414</span><span class="stringliteral">    positive : bool, default=False</span></div>
<div class="line"><span class="lineno">  415</span><span class="stringliteral">        Restrict coefficients to be &gt;= 0.</span></div>
<div class="line"><span class="lineno">  416</span><span class="stringliteral">        This option is only allowed with method &#39;lasso&#39;. Note that the model</span></div>
<div class="line"><span class="lineno">  417</span><span class="stringliteral">        coefficients will not converge to the ordinary-least-squares solution</span></div>
<div class="line"><span class="lineno">  418</span><span class="stringliteral">        for small values of alpha. Only coefficients up to the smallest alpha</span></div>
<div class="line"><span class="lineno">  419</span><span class="stringliteral">        value (``alphas_[alphas_ &gt; 0.].min()`` when fit_path=True) reached by</span></div>
<div class="line"><span class="lineno">  420</span><span class="stringliteral">        the stepwise Lars-Lasso algorithm are typically in congruence with the</span></div>
<div class="line"><span class="lineno">  421</span><span class="stringliteral">        solution of the coordinate descent lasso_path function.</span></div>
<div class="line"><span class="lineno">  422</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  423</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  424</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  425</span><span class="stringliteral">    alphas : array-like of shape (n_alphas + 1,)</span></div>
<div class="line"><span class="lineno">  426</span><span class="stringliteral">        Maximum of covariances (in absolute value) at each iteration.</span></div>
<div class="line"><span class="lineno">  427</span><span class="stringliteral">        ``n_alphas`` is either ``max_iter``, ``n_features`` or the</span></div>
<div class="line"><span class="lineno">  428</span><span class="stringliteral">        number of nodes in the path with ``alpha &gt;= alpha_min``, whichever</span></div>
<div class="line"><span class="lineno">  429</span><span class="stringliteral">        is smaller.</span></div>
<div class="line"><span class="lineno">  430</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  431</span><span class="stringliteral">    active : array-like of shape (n_alphas,)</span></div>
<div class="line"><span class="lineno">  432</span><span class="stringliteral">        Indices of active variables at the end of the path.</span></div>
<div class="line"><span class="lineno">  433</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  434</span><span class="stringliteral">    coefs : array-like of shape (n_features, n_alphas + 1)</span></div>
<div class="line"><span class="lineno">  435</span><span class="stringliteral">        Coefficients along the path</span></div>
<div class="line"><span class="lineno">  436</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  437</span><span class="stringliteral">    n_iter : int</span></div>
<div class="line"><span class="lineno">  438</span><span class="stringliteral">        Number of iterations run. Returned only if return_n_iter is set</span></div>
<div class="line"><span class="lineno">  439</span><span class="stringliteral">        to True.</span></div>
<div class="line"><span class="lineno">  440</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  441</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  442</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  443</span><span class="stringliteral">    lasso_path</span></div>
<div class="line"><span class="lineno">  444</span><span class="stringliteral">    LassoLars</span></div>
<div class="line"><span class="lineno">  445</span><span class="stringliteral">    Lars</span></div>
<div class="line"><span class="lineno">  446</span><span class="stringliteral">    LassoLarsCV</span></div>
<div class="line"><span class="lineno">  447</span><span class="stringliteral">    LarsCV</span></div>
<div class="line"><span class="lineno">  448</span><span class="stringliteral">    sklearn.decomposition.sparse_encode</span></div>
<div class="line"><span class="lineno">  449</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  450</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  451</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  452</span><span class="stringliteral">    .. [1] &quot;Least Angle Regression&quot;, Efron et al.</span></div>
<div class="line"><span class="lineno">  453</span><span class="stringliteral">           http://statweb.stanford.edu/~tibs/ftp/lars.pdf</span></div>
<div class="line"><span class="lineno">  454</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  455</span><span class="stringliteral">    .. [2] `Wikipedia entry on the Least-angle regression</span></div>
<div class="line"><span class="lineno">  456</span><span class="stringliteral">           &lt;https://en.wikipedia.org/wiki/Least-angle_regression&gt;`_</span></div>
<div class="line"><span class="lineno">  457</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  458</span><span class="stringliteral">    .. [3] `Wikipedia entry on the Lasso</span></div>
<div class="line"><span class="lineno">  459</span><span class="stringliteral">           &lt;https://en.wikipedia.org/wiki/Lasso_(statistics)&gt;`_</span></div>
<div class="line"><span class="lineno">  460</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  461</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  462</span>    <span class="keywordflow">if</span> method == <span class="stringliteral">&quot;lar&quot;</span> <span class="keywordflow">and</span> positive:</div>
<div class="line"><span class="lineno">  463</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Positive constraint not supported for &#39;lar&#39; coding method.&quot;</span>)</div>
<div class="line"><span class="lineno">  464</span> </div>
<div class="line"><span class="lineno">  465</span>    n_samples = n_samples <span class="keywordflow">if</span> n_samples <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">else</span> y.size</div>
<div class="line"><span class="lineno">  466</span> </div>
<div class="line"><span class="lineno">  467</span>    <span class="keywordflow">if</span> Xy <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  468</span>        Cov = np.dot(X.T, y)</div>
<div class="line"><span class="lineno">  469</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  470</span>        Cov = Xy.copy()</div>
<div class="line"><span class="lineno">  471</span> </div>
<div class="line"><span class="lineno">  472</span>    <span class="keywordflow">if</span> Gram <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">or</span> Gram <span class="keywordflow">is</span> <span class="keyword">False</span>:</div>
<div class="line"><span class="lineno">  473</span>        Gram = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  474</span>        <span class="keywordflow">if</span> X <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  475</span>            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;X and Gram cannot both be unspecified.&quot;</span>)</div>
<div class="line"><span class="lineno">  476</span>    <span class="keywordflow">elif</span> isinstance(Gram, str) <span class="keywordflow">and</span> Gram == <span class="stringliteral">&quot;auto&quot;</span> <span class="keywordflow">or</span> Gram <span class="keywordflow">is</span> <span class="keyword">True</span>:</div>
<div class="line"><span class="lineno">  477</span>        <span class="keywordflow">if</span> Gram <span class="keywordflow">is</span> <span class="keyword">True</span> <span class="keywordflow">or</span> X.shape[0] &gt; X.shape[1]:</div>
<div class="line"><span class="lineno">  478</span>            Gram = np.dot(X.T, X)</div>
<div class="line"><span class="lineno">  479</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  480</span>            Gram = <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  481</span>    <span class="keywordflow">elif</span> copy_Gram:</div>
<div class="line"><span class="lineno">  482</span>        Gram = Gram.copy()</div>
<div class="line"><span class="lineno">  483</span> </div>
<div class="line"><span class="lineno">  484</span>    <span class="keywordflow">if</span> Gram <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  485</span>        n_features = X.shape[1]</div>
<div class="line"><span class="lineno">  486</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  487</span>        n_features = Cov.shape[0]</div>
<div class="line"><span class="lineno">  488</span>        <span class="keywordflow">if</span> Gram.shape != (n_features, n_features):</div>
<div class="line"><span class="lineno">  489</span>            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;The shapes of the inputs Gram and Xy do not match.&quot;</span>)</div>
<div class="line"><span class="lineno">  490</span> </div>
<div class="line"><span class="lineno">  491</span>    <span class="keywordflow">if</span> copy_X <span class="keywordflow">and</span> X <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> Gram <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  492</span>        <span class="comment"># force copy. setting the array to be fortran-ordered</span></div>
<div class="line"><span class="lineno">  493</span>        <span class="comment"># speeds up the calculation of the (partial) Gram matrix</span></div>
<div class="line"><span class="lineno">  494</span>        <span class="comment"># and allows to easily swap columns</span></div>
<div class="line"><span class="lineno">  495</span>        X = X.copy(<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  496</span> </div>
<div class="line"><span class="lineno">  497</span>    max_features = min(max_iter, n_features)</div>
<div class="line"><span class="lineno">  498</span> </div>
<div class="line"><span class="lineno">  499</span>    dtypes = set(a.dtype <span class="keywordflow">for</span> a <span class="keywordflow">in</span> (X, y, Xy, Gram) <span class="keywordflow">if</span> a <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">  500</span>    <span class="keywordflow">if</span> len(dtypes) == 1:</div>
<div class="line"><span class="lineno">  501</span>        <span class="comment"># use the precision level of input data if it is consistent</span></div>
<div class="line"><span class="lineno">  502</span>        return_dtype = next(<a class="code hl_variable" href="__lapack__subroutines_8h.html#a60590d91febfcb54d88443940cd5f23e">iter</a>(dtypes))</div>
<div class="line"><span class="lineno">  503</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  504</span>        <span class="comment"># fallback to double precision otherwise</span></div>
<div class="line"><span class="lineno">  505</span>        return_dtype = np.float64</div>
<div class="line"><span class="lineno">  506</span> </div>
<div class="line"><span class="lineno">  507</span>    <span class="keywordflow">if</span> return_path:</div>
<div class="line"><span class="lineno">  508</span>        coefs = np.zeros((max_features + 1, n_features), dtype=return_dtype)</div>
<div class="line"><span class="lineno">  509</span>        alphas = np.zeros(max_features + 1, dtype=return_dtype)</div>
<div class="line"><span class="lineno">  510</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  511</span>        coef, prev_coef = (</div>
<div class="line"><span class="lineno">  512</span>            np.zeros(n_features, dtype=return_dtype),</div>
<div class="line"><span class="lineno">  513</span>            np.zeros(n_features, dtype=return_dtype),</div>
<div class="line"><span class="lineno">  514</span>        )</div>
<div class="line"><span class="lineno">  515</span>        alpha, prev_alpha = (</div>
<div class="line"><span class="lineno">  516</span>            np.array([0.0], dtype=return_dtype),</div>
<div class="line"><span class="lineno">  517</span>            np.array([0.0], dtype=return_dtype),</div>
<div class="line"><span class="lineno">  518</span>        )</div>
<div class="line"><span class="lineno">  519</span>        <span class="comment"># above better ideas?</span></div>
<div class="line"><span class="lineno">  520</span> </div>
<div class="line"><span class="lineno">  521</span>    n_iter, n_active = 0, 0</div>
<div class="line"><span class="lineno">  522</span>    active, indices = list(), np.arange(n_features)</div>
<div class="line"><span class="lineno">  523</span>    <span class="comment"># holds the sign of covariance</span></div>
<div class="line"><span class="lineno">  524</span>    sign_active = np.empty(max_features, dtype=np.int8)</div>
<div class="line"><span class="lineno">  525</span>    drop = <span class="keyword">False</span></div>
<div class="line"><span class="lineno">  526</span> </div>
<div class="line"><span class="lineno">  527</span>    <span class="comment"># will hold the cholesky factorization. Only lower part is</span></div>
<div class="line"><span class="lineno">  528</span>    <span class="comment"># referenced.</span></div>
<div class="line"><span class="lineno">  529</span>    <span class="keywordflow">if</span> Gram <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  530</span>        L = np.empty((max_features, max_features), dtype=X.dtype)</div>
<div class="line"><span class="lineno">  531</span>        swap, nrm2 = linalg.get_blas_funcs((<span class="stringliteral">&quot;swap&quot;</span>, <span class="stringliteral">&quot;nrm2&quot;</span>), (X,))</div>
<div class="line"><span class="lineno">  532</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  533</span>        L = np.empty((max_features, max_features), dtype=Gram.dtype)</div>
<div class="line"><span class="lineno">  534</span>        swap, nrm2 = linalg.get_blas_funcs((<span class="stringliteral">&quot;swap&quot;</span>, <span class="stringliteral">&quot;nrm2&quot;</span>), (Cov,))</div>
<div class="line"><span class="lineno">  535</span>    (solve_cholesky,) = get_lapack_funcs((<span class="stringliteral">&quot;potrs&quot;</span>,), (L,))</div>
<div class="line"><span class="lineno">  536</span> </div>
<div class="line"><span class="lineno">  537</span>    <span class="keywordflow">if</span> verbose:</div>
<div class="line"><span class="lineno">  538</span>        <span class="keywordflow">if</span> verbose &gt; 1:</div>
<div class="line"><span class="lineno">  539</span>            print(<span class="stringliteral">&quot;Step\t\tAdded\t\tDropped\t\tActive set size\t\tC&quot;</span>)</div>
<div class="line"><span class="lineno">  540</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  541</span>            sys.stdout.write(<span class="stringliteral">&quot;.&quot;</span>)</div>
<div class="line"><span class="lineno">  542</span>            sys.stdout.flush()</div>
<div class="line"><span class="lineno">  543</span> </div>
<div class="line"><span class="lineno">  544</span>    tiny32 = np.finfo(np.float32).tiny  <span class="comment"># to avoid division by 0 warning</span></div>
<div class="line"><span class="lineno">  545</span>    cov_precision = np.finfo(Cov.dtype).precision</div>
<div class="line"><span class="lineno">  546</span>    equality_tolerance = np.finfo(np.float32).eps</div>
<div class="line"><span class="lineno">  547</span> </div>
<div class="line"><span class="lineno">  548</span>    <span class="keywordflow">if</span> Gram <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  549</span>        Gram_copy = Gram.copy()</div>
<div class="line"><span class="lineno">  550</span>        Cov_copy = Cov.copy()</div>
<div class="line"><span class="lineno">  551</span> </div>
<div class="line"><span class="lineno">  552</span>    <span class="keywordflow">while</span> <span class="keyword">True</span>:</div>
<div class="line"><span class="lineno">  553</span>        <span class="keywordflow">if</span> Cov.size:</div>
<div class="line"><span class="lineno">  554</span>            <span class="keywordflow">if</span> positive:</div>
<div class="line"><span class="lineno">  555</span>                C_idx = np.argmax(Cov)</div>
<div class="line"><span class="lineno">  556</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  557</span>                C_idx = np.argmax(np.abs(Cov))</div>
<div class="line"><span class="lineno">  558</span> </div>
<div class="line"><span class="lineno">  559</span>            C_ = Cov[C_idx]</div>
<div class="line"><span class="lineno">  560</span> </div>
<div class="line"><span class="lineno">  561</span>            <span class="keywordflow">if</span> positive:</div>
<div class="line"><span class="lineno">  562</span>                C = C_</div>
<div class="line"><span class="lineno">  563</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  564</span>                C = np.fabs(C_)</div>
<div class="line"><span class="lineno">  565</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  566</span>            C = 0.0</div>
<div class="line"><span class="lineno">  567</span> </div>
<div class="line"><span class="lineno">  568</span>        <span class="keywordflow">if</span> return_path:</div>
<div class="line"><span class="lineno">  569</span>            alpha = alphas[n_iter, np.newaxis]</div>
<div class="line"><span class="lineno">  570</span>            coef = coefs[n_iter]</div>
<div class="line"><span class="lineno">  571</span>            prev_alpha = alphas[n_iter - 1, np.newaxis]</div>
<div class="line"><span class="lineno">  572</span>            prev_coef = coefs[n_iter - 1]</div>
<div class="line"><span class="lineno">  573</span> </div>
<div class="line"><span class="lineno">  574</span>        alpha[0] = C / n_samples</div>
<div class="line"><span class="lineno">  575</span>        <span class="keywordflow">if</span> alpha[0] &lt;= alpha_min + equality_tolerance:  <span class="comment"># early stopping</span></div>
<div class="line"><span class="lineno">  576</span>            <span class="keywordflow">if</span> abs(alpha[0] - alpha_min) &gt; equality_tolerance:</div>
<div class="line"><span class="lineno">  577</span>                <span class="comment"># interpolation factor 0 &lt;= ss &lt; 1</span></div>
<div class="line"><span class="lineno">  578</span>                <span class="keywordflow">if</span> n_iter &gt; 0:</div>
<div class="line"><span class="lineno">  579</span>                    <span class="comment"># In the first iteration, all alphas are zero, the formula</span></div>
<div class="line"><span class="lineno">  580</span>                    <span class="comment"># below would make ss a NaN</span></div>
<div class="line"><span class="lineno">  581</span>                    ss = (prev_alpha[0] - alpha_min) / (prev_alpha[0] - alpha[0])</div>
<div class="line"><span class="lineno">  582</span>                    coef[:] = prev_coef + ss * (coef - prev_coef)</div>
<div class="line"><span class="lineno">  583</span>                alpha[0] = alpha_min</div>
<div class="line"><span class="lineno">  584</span>            <span class="keywordflow">if</span> return_path:</div>
<div class="line"><span class="lineno">  585</span>                coefs[n_iter] = coef</div>
<div class="line"><span class="lineno">  586</span>            <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  587</span> </div>
<div class="line"><span class="lineno">  588</span>        <span class="keywordflow">if</span> n_iter &gt;= max_iter <span class="keywordflow">or</span> n_active &gt;= n_features:</div>
<div class="line"><span class="lineno">  589</span>            <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  590</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> drop:</div>
<div class="line"><span class="lineno">  591</span> </div>
<div class="line"><span class="lineno">  592</span>            </div>
<div class="line"><span class="lineno">  600</span> </div>
<div class="line"><span class="lineno">  601</span>            <span class="keywordflow">if</span> positive:</div>
<div class="line"><span class="lineno">  602</span>                sign_active[n_active] = np.ones_like(C_)</div>
<div class="line"><span class="lineno">  603</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  604</span>                sign_active[n_active] = np.sign(C_)</div>
<div class="line"><span class="lineno">  605</span>            m, n = n_active, C_idx + n_active</div>
<div class="line"><span class="lineno">  606</span> </div>
<div class="line"><span class="lineno">  607</span>            Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])</div>
<div class="line"><span class="lineno">  608</span>            indices[n], indices[m] = indices[m], indices[n]</div>
<div class="line"><span class="lineno">  609</span>            Cov_not_shortened = Cov</div>
<div class="line"><span class="lineno">  610</span>            Cov = Cov[1:]  <span class="comment"># remove Cov[0]</span></div>
<div class="line"><span class="lineno">  611</span> </div>
<div class="line"><span class="lineno">  612</span>            <span class="keywordflow">if</span> Gram <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  613</span>                X.T[n], X.T[m] = swap(X.T[n], X.T[m])</div>
<div class="line"><span class="lineno">  614</span>                c = nrm2(X.T[n_active]) ** 2</div>
<div class="line"><span class="lineno">  615</span>                L[n_active, :n_active] = np.dot(X.T[n_active], X.T[:n_active].T)</div>
<div class="line"><span class="lineno">  616</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  617</span>                <span class="comment"># swap does only work inplace if matrix is fortran</span></div>
<div class="line"><span class="lineno">  618</span>                <span class="comment"># contiguous ...</span></div>
<div class="line"><span class="lineno">  619</span>                Gram[m], Gram[n] = swap(Gram[m], Gram[n])</div>
<div class="line"><span class="lineno">  620</span>                Gram[:, m], Gram[:, n] = swap(Gram[:, m], Gram[:, n])</div>
<div class="line"><span class="lineno">  621</span>                c = Gram[n_active, n_active]</div>
<div class="line"><span class="lineno">  622</span>                L[n_active, :n_active] = Gram[n_active, :n_active]</div>
<div class="line"><span class="lineno">  623</span> </div>
<div class="line"><span class="lineno">  624</span>            <span class="comment"># Update the cholesky decomposition for the Gram matrix</span></div>
<div class="line"><span class="lineno">  625</span>            <span class="keywordflow">if</span> n_active:</div>
<div class="line"><span class="lineno">  626</span>                linalg.solve_triangular(</div>
<div class="line"><span class="lineno">  627</span>                    L[:n_active, :n_active],</div>
<div class="line"><span class="lineno">  628</span>                    L[n_active, :n_active],</div>
<div class="line"><span class="lineno">  629</span>                    trans=0,</div>
<div class="line"><span class="lineno">  630</span>                    lower=1,</div>
<div class="line"><span class="lineno">  631</span>                    overwrite_b=<span class="keyword">True</span>,</div>
<div class="line"><span class="lineno">  632</span>                    **SOLVE_TRIANGULAR_ARGS,</div>
<div class="line"><span class="lineno">  633</span>                )</div>
<div class="line"><span class="lineno">  634</span> </div>
<div class="line"><span class="lineno">  635</span>            v = np.dot(L[n_active, :n_active], L[n_active, :n_active])</div>
<div class="line"><span class="lineno">  636</span>            diag = max(np.sqrt(np.abs(c - v)), eps)</div>
<div class="line"><span class="lineno">  637</span>            L[n_active, n_active] = diag</div>
<div class="line"><span class="lineno">  638</span> </div>
<div class="line"><span class="lineno">  639</span>            <span class="keywordflow">if</span> diag &lt; 1e-7:</div>
<div class="line"><span class="lineno">  640</span>                <span class="comment"># The system is becoming too ill-conditioned.</span></div>
<div class="line"><span class="lineno">  641</span>                <span class="comment"># We have degenerate vectors in our active set.</span></div>
<div class="line"><span class="lineno">  642</span>                <span class="comment"># We&#39;ll &#39;drop for good&#39; the last regressor added.</span></div>
<div class="line"><span class="lineno">  643</span> </div>
<div class="line"><span class="lineno">  644</span>                <span class="comment"># Note: this case is very rare. It is no longer triggered by</span></div>
<div class="line"><span class="lineno">  645</span>                <span class="comment"># the test suite. The `equality_tolerance` margin added in 0.16</span></div>
<div class="line"><span class="lineno">  646</span>                <span class="comment"># to get early stopping to work consistently on all versions of</span></div>
<div class="line"><span class="lineno">  647</span>                <span class="comment"># Python including 32 bit Python under Windows seems to make it</span></div>
<div class="line"><span class="lineno">  648</span>                <span class="comment"># very difficult to trigger the &#39;drop for good&#39; strategy.</span></div>
<div class="line"><span class="lineno">  649</span>                warnings.warn(</div>
<div class="line"><span class="lineno">  650</span>                    <span class="stringliteral">&quot;Regressors in active set degenerate. &quot;</span></div>
<div class="line"><span class="lineno">  651</span>                    <span class="stringliteral">&quot;Dropping a regressor, after %i iterations, &quot;</span></div>
<div class="line"><span class="lineno">  652</span>                    <span class="stringliteral">&quot;i.e. alpha=%.3e, &quot;</span></div>
<div class="line"><span class="lineno">  653</span>                    <span class="stringliteral">&quot;with an active set of %i regressors, and &quot;</span></div>
<div class="line"><span class="lineno">  654</span>                    <span class="stringliteral">&quot;the smallest cholesky pivot element being %.3e.&quot;</span></div>
<div class="line"><span class="lineno">  655</span>                    <span class="stringliteral">&quot; Reduce max_iter or increase eps parameters.&quot;</span></div>
<div class="line"><span class="lineno">  656</span>                    % (n_iter, alpha, n_active, diag),</div>
<div class="line"><span class="lineno">  657</span>                    ConvergenceWarning,</div>
<div class="line"><span class="lineno">  658</span>                )</div>
<div class="line"><span class="lineno">  659</span> </div>
<div class="line"><span class="lineno">  660</span>                <span class="comment"># XXX: need to figure a &#39;drop for good&#39; way</span></div>
<div class="line"><span class="lineno">  661</span>                Cov = Cov_not_shortened</div>
<div class="line"><span class="lineno">  662</span>                Cov[0] = 0</div>
<div class="line"><span class="lineno">  663</span>                Cov[C_idx], Cov[0] = swap(Cov[C_idx], Cov[0])</div>
<div class="line"><span class="lineno">  664</span>                <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno">  665</span> </div>
<div class="line"><span class="lineno">  666</span>            active.append(indices[n_active])</div>
<div class="line"><span class="lineno">  667</span>            n_active += 1</div>
<div class="line"><span class="lineno">  668</span> </div>
<div class="line"><span class="lineno">  669</span>            <span class="keywordflow">if</span> verbose &gt; 1:</div>
<div class="line"><span class="lineno">  670</span>                print(</div>
<div class="line"><span class="lineno">  671</span>                    <span class="stringliteral">&quot;%s\t\t%s\t\t%s\t\t%s\t\t%s&quot;</span> % (n_iter, active[-1], <span class="stringliteral">&quot;&quot;</span>, n_active, C)</div>
<div class="line"><span class="lineno">  672</span>                )</div>
<div class="line"><span class="lineno">  673</span> </div>
<div class="line"><span class="lineno">  674</span>        <span class="keywordflow">if</span> method == <span class="stringliteral">&quot;lasso&quot;</span> <span class="keywordflow">and</span> n_iter &gt; 0 <span class="keywordflow">and</span> prev_alpha[0] &lt; alpha[0]:</div>
<div class="line"><span class="lineno">  675</span>            <span class="comment"># alpha is increasing. This is because the updates of Cov are</span></div>
<div class="line"><span class="lineno">  676</span>            <span class="comment"># bringing in too much numerical error that is greater than</span></div>
<div class="line"><span class="lineno">  677</span>            <span class="comment"># than the remaining correlation with the</span></div>
<div class="line"><span class="lineno">  678</span>            <span class="comment"># regressors. Time to bail out</span></div>
<div class="line"><span class="lineno">  679</span>            warnings.warn(</div>
<div class="line"><span class="lineno">  680</span>                <span class="stringliteral">&quot;Early stopping the lars path, as the residues &quot;</span></div>
<div class="line"><span class="lineno">  681</span>                <span class="stringliteral">&quot;are small and the current value of alpha is no &quot;</span></div>
<div class="line"><span class="lineno">  682</span>                <span class="stringliteral">&quot;longer well controlled. %i iterations, alpha=%.3e, &quot;</span></div>
<div class="line"><span class="lineno">  683</span>                <span class="stringliteral">&quot;previous alpha=%.3e, with an active set of %i &quot;</span></div>
<div class="line"><span class="lineno">  684</span>                <span class="stringliteral">&quot;regressors.&quot;</span> % (n_iter, alpha, prev_alpha, n_active),</div>
<div class="line"><span class="lineno">  685</span>                ConvergenceWarning,</div>
<div class="line"><span class="lineno">  686</span>            )</div>
<div class="line"><span class="lineno">  687</span>            <span class="keywordflow">break</span></div>
<div class="line"><span class="lineno">  688</span> </div>
<div class="line"><span class="lineno">  689</span>        <span class="comment"># least squares solution</span></div>
<div class="line"><span class="lineno">  690</span>        least_squares, _ = solve_cholesky(</div>
<div class="line"><span class="lineno">  691</span>            L[:n_active, :n_active], sign_active[:n_active], lower=<span class="keyword">True</span></div>
<div class="line"><span class="lineno">  692</span>        )</div>
<div class="line"><span class="lineno">  693</span> </div>
<div class="line"><span class="lineno">  694</span>        <span class="keywordflow">if</span> least_squares.size == 1 <span class="keywordflow">and</span> least_squares == 0:</div>
<div class="line"><span class="lineno">  695</span>            <span class="comment"># This happens because sign_active[:n_active] = 0</span></div>
<div class="line"><span class="lineno">  696</span>            least_squares[...] = 1</div>
<div class="line"><span class="lineno">  697</span>            AA = 1.0</div>
<div class="line"><span class="lineno">  698</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  699</span>            <span class="comment"># is this really needed ?</span></div>
<div class="line"><span class="lineno">  700</span>            AA = 1.0 / np.sqrt(np.sum(least_squares * sign_active[:n_active]))</div>
<div class="line"><span class="lineno">  701</span> </div>
<div class="line"><span class="lineno">  702</span>            <span class="keywordflow">if</span> <span class="keywordflow">not</span> np.isfinite(AA):</div>
<div class="line"><span class="lineno">  703</span>                <span class="comment"># L is too ill-conditioned</span></div>
<div class="line"><span class="lineno">  704</span>                i = 0</div>
<div class="line"><span class="lineno">  705</span>                L_ = L[:n_active, :n_active].copy()</div>
<div class="line"><span class="lineno">  706</span>                <span class="keywordflow">while</span> <span class="keywordflow">not</span> np.isfinite(AA):</div>
<div class="line"><span class="lineno">  707</span>                    L_.flat[:: n_active + 1] += (2**i) * eps</div>
<div class="line"><span class="lineno">  708</span>                    least_squares, _ = solve_cholesky(</div>
<div class="line"><span class="lineno">  709</span>                        L_, sign_active[:n_active], lower=<span class="keyword">True</span></div>
<div class="line"><span class="lineno">  710</span>                    )</div>
<div class="line"><span class="lineno">  711</span>                    tmp = max(np.sum(least_squares * sign_active[:n_active]), eps)</div>
<div class="line"><span class="lineno">  712</span>                    AA = 1.0 / np.sqrt(tmp)</div>
<div class="line"><span class="lineno">  713</span>                    i += 1</div>
<div class="line"><span class="lineno">  714</span>            least_squares *= AA</div>
<div class="line"><span class="lineno">  715</span> </div>
<div class="line"><span class="lineno">  716</span>        <span class="keywordflow">if</span> Gram <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  717</span>            <span class="comment"># equiangular direction of variables in the active set</span></div>
<div class="line"><span class="lineno">  718</span>            eq_dir = np.dot(X.T[:n_active].T, least_squares)</div>
<div class="line"><span class="lineno">  719</span>            <span class="comment"># correlation between each unactive variables and</span></div>
<div class="line"><span class="lineno">  720</span>            <span class="comment"># eqiangular vector</span></div>
<div class="line"><span class="lineno">  721</span>            corr_eq_dir = np.dot(X.T[n_active:], eq_dir)</div>
<div class="line"><span class="lineno">  722</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  723</span>            <span class="comment"># if huge number of features, this takes 50% of time, I</span></div>
<div class="line"><span class="lineno">  724</span>            <span class="comment"># think could be avoided if we just update it using an</span></div>
<div class="line"><span class="lineno">  725</span>            <span class="comment"># orthogonal (QR) decomposition of X</span></div>
<div class="line"><span class="lineno">  726</span>            corr_eq_dir = np.dot(Gram[:n_active, n_active:].T, least_squares)</div>
<div class="line"><span class="lineno">  727</span> </div>
<div class="line"><span class="lineno">  728</span>        <span class="comment"># Explicit rounding can be necessary to avoid `np.argmax(Cov)` yielding</span></div>
<div class="line"><span class="lineno">  729</span>        <span class="comment"># unstable results because of rounding errors.</span></div>
<div class="line"><span class="lineno">  730</span>        np.around(corr_eq_dir, decimals=cov_precision, out=corr_eq_dir)</div>
<div class="line"><span class="lineno">  731</span> </div>
<div class="line"><span class="lineno">  732</span>        g1 = arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))</div>
<div class="line"><span class="lineno">  733</span>        <span class="keywordflow">if</span> positive:</div>
<div class="line"><span class="lineno">  734</span>            gamma_ = min(g1, C / AA)</div>
<div class="line"><span class="lineno">  735</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  736</span>            g2 = arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))</div>
<div class="line"><span class="lineno">  737</span>            gamma_ = min(g1, g2, C / AA)</div>
<div class="line"><span class="lineno">  738</span> </div>
<div class="line"><span class="lineno">  739</span>        <span class="comment"># TODO: better names for these variables: z</span></div>
<div class="line"><span class="lineno">  740</span>        drop = <span class="keyword">False</span></div>
<div class="line"><span class="lineno">  741</span>        z = -coef[active] / (least_squares + tiny32)</div>
<div class="line"><span class="lineno">  742</span>        z_pos = arrayfuncs.min_pos(z)</div>
<div class="line"><span class="lineno">  743</span>        <span class="keywordflow">if</span> z_pos &lt; gamma_:</div>
<div class="line"><span class="lineno">  744</span>            <span class="comment"># some coefficients have changed sign</span></div>
<div class="line"><span class="lineno">  745</span>            idx = np.where(z == z_pos)[0][::-1]</div>
<div class="line"><span class="lineno">  746</span> </div>
<div class="line"><span class="lineno">  747</span>            <span class="comment"># update the sign, important for LAR</span></div>
<div class="line"><span class="lineno">  748</span>            sign_active[idx] = -sign_active[idx]</div>
<div class="line"><span class="lineno">  749</span> </div>
<div class="line"><span class="lineno">  750</span>            <span class="keywordflow">if</span> method == <span class="stringliteral">&quot;lasso&quot;</span>:</div>
<div class="line"><span class="lineno">  751</span>                gamma_ = z_pos</div>
<div class="line"><span class="lineno">  752</span>            drop = <span class="keyword">True</span></div>
<div class="line"><span class="lineno">  753</span> </div>
<div class="line"><span class="lineno">  754</span>        n_iter += 1</div>
<div class="line"><span class="lineno">  755</span> </div>
<div class="line"><span class="lineno">  756</span>        <span class="keywordflow">if</span> return_path:</div>
<div class="line"><span class="lineno">  757</span>            <span class="keywordflow">if</span> n_iter &gt;= coefs.shape[0]:</div>
<div class="line"><span class="lineno">  758</span>                del coef, alpha, prev_alpha, prev_coef</div>
<div class="line"><span class="lineno">  759</span>                <span class="comment"># resize the coefs and alphas array</span></div>
<div class="line"><span class="lineno">  760</span>                add_features = 2 * max(1, (max_features - n_active))</div>
<div class="line"><span class="lineno">  761</span>                coefs = np.resize(coefs, (n_iter + add_features, n_features))</div>
<div class="line"><span class="lineno">  762</span>                coefs[-add_features:] = 0</div>
<div class="line"><span class="lineno">  763</span>                alphas = np.resize(alphas, n_iter + add_features)</div>
<div class="line"><span class="lineno">  764</span>                alphas[-add_features:] = 0</div>
<div class="line"><span class="lineno">  765</span>            coef = coefs[n_iter]</div>
<div class="line"><span class="lineno">  766</span>            prev_coef = coefs[n_iter - 1]</div>
<div class="line"><span class="lineno">  767</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  768</span>            <span class="comment"># mimic the effect of incrementing n_iter on the array references</span></div>
<div class="line"><span class="lineno">  769</span>            prev_coef = coef</div>
<div class="line"><span class="lineno">  770</span>            prev_alpha[0] = alpha[0]</div>
<div class="line"><span class="lineno">  771</span>            coef = np.zeros_like(coef)</div>
<div class="line"><span class="lineno">  772</span> </div>
<div class="line"><span class="lineno">  773</span>        coef[active] = prev_coef[active] + gamma_ * least_squares</div>
<div class="line"><span class="lineno">  774</span> </div>
<div class="line"><span class="lineno">  775</span>        <span class="comment"># update correlations</span></div>
<div class="line"><span class="lineno">  776</span>        Cov -= gamma_ * corr_eq_dir</div>
<div class="line"><span class="lineno">  777</span> </div>
<div class="line"><span class="lineno">  778</span>        <span class="comment"># See if any coefficient has changed sign</span></div>
<div class="line"><span class="lineno">  779</span>        <span class="keywordflow">if</span> drop <span class="keywordflow">and</span> method == <span class="stringliteral">&quot;lasso&quot;</span>:</div>
<div class="line"><span class="lineno">  780</span> </div>
<div class="line"><span class="lineno">  781</span>            <span class="comment"># handle the case when idx is not length of 1</span></div>
<div class="line"><span class="lineno">  782</span>            <span class="keywordflow">for</span> ii <span class="keywordflow">in</span> idx:</div>
<div class="line"><span class="lineno">  783</span>                arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii)</div>
<div class="line"><span class="lineno">  784</span> </div>
<div class="line"><span class="lineno">  785</span>            n_active -= 1</div>
<div class="line"><span class="lineno">  786</span>            <span class="comment"># handle the case when idx is not length of 1</span></div>
<div class="line"><span class="lineno">  787</span>            drop_idx = [active.pop(ii) <span class="keywordflow">for</span> ii <span class="keywordflow">in</span> idx]</div>
<div class="line"><span class="lineno">  788</span> </div>
<div class="line"><span class="lineno">  789</span>            <span class="keywordflow">if</span> Gram <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  790</span>                <span class="comment"># propagate dropped variable</span></div>
<div class="line"><span class="lineno">  791</span>                <span class="keywordflow">for</span> ii <span class="keywordflow">in</span> idx:</div>
<div class="line"><span class="lineno">  792</span>                    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(ii, n_active):</div>
<div class="line"><span class="lineno">  793</span>                        X.T[i], X.T[i + 1] = swap(X.T[i], X.T[i + 1])</div>
<div class="line"><span class="lineno">  794</span>                        <span class="comment"># yeah this is stupid</span></div>
<div class="line"><span class="lineno">  795</span>                        indices[i], indices[i + 1] = indices[i + 1], indices[i]</div>
<div class="line"><span class="lineno">  796</span> </div>
<div class="line"><span class="lineno">  797</span>                <span class="comment"># TODO: this could be updated</span></div>
<div class="line"><span class="lineno">  798</span>                residual = y - np.dot(X[:, :n_active], coef[active])</div>
<div class="line"><span class="lineno">  799</span>                temp = np.dot(X.T[n_active], residual)</div>
<div class="line"><span class="lineno">  800</span> </div>
<div class="line"><span class="lineno">  801</span>                Cov = np.r_[temp, Cov]</div>
<div class="line"><span class="lineno">  802</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  803</span>                <span class="keywordflow">for</span> ii <span class="keywordflow">in</span> idx:</div>
<div class="line"><span class="lineno">  804</span>                    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(ii, n_active):</div>
<div class="line"><span class="lineno">  805</span>                        indices[i], indices[i + 1] = indices[i + 1], indices[i]</div>
<div class="line"><span class="lineno">  806</span>                        Gram[i], Gram[i + 1] = swap(Gram[i], Gram[i + 1])</div>
<div class="line"><span class="lineno">  807</span>                        Gram[:, i], Gram[:, i + 1] = swap(Gram[:, i], Gram[:, i + 1])</div>
<div class="line"><span class="lineno">  808</span> </div>
<div class="line"><span class="lineno">  809</span>                <span class="comment"># Cov_n = Cov_j + x_j * X + increment(betas) TODO:</span></div>
<div class="line"><span class="lineno">  810</span>                <span class="comment"># will this still work with multiple drops ?</span></div>
<div class="line"><span class="lineno">  811</span> </div>
<div class="line"><span class="lineno">  812</span>                <span class="comment"># recompute covariance. Probably could be done better</span></div>
<div class="line"><span class="lineno">  813</span>                <span class="comment"># wrong as Xy is not swapped with the rest of variables</span></div>
<div class="line"><span class="lineno">  814</span> </div>
<div class="line"><span class="lineno">  815</span>                <span class="comment"># TODO: this could be updated</span></div>
<div class="line"><span class="lineno">  816</span>                temp = Cov_copy[drop_idx] - np.dot(Gram_copy[drop_idx], coef)</div>
<div class="line"><span class="lineno">  817</span>                Cov = np.r_[temp, Cov]</div>
<div class="line"><span class="lineno">  818</span> </div>
<div class="line"><span class="lineno">  819</span>            sign_active = np.delete(sign_active, idx)</div>
<div class="line"><span class="lineno">  820</span>            sign_active = np.append(sign_active, 0.0)  <span class="comment"># just to maintain size</span></div>
<div class="line"><span class="lineno">  821</span>            <span class="keywordflow">if</span> verbose &gt; 1:</div>
<div class="line"><span class="lineno">  822</span>                print(</div>
<div class="line"><span class="lineno">  823</span>                    <span class="stringliteral">&quot;%s\t\t%s\t\t%s\t\t%s\t\t%s&quot;</span></div>
<div class="line"><span class="lineno">  824</span>                    % (n_iter, <span class="stringliteral">&quot;&quot;</span>, drop_idx, n_active, abs(temp))</div>
<div class="line"><span class="lineno">  825</span>                )</div>
<div class="line"><span class="lineno">  826</span> </div>
<div class="line"><span class="lineno">  827</span>    <span class="keywordflow">if</span> return_path:</div>
<div class="line"><span class="lineno">  828</span>        <span class="comment"># resize coefs in case of early stop</span></div>
<div class="line"><span class="lineno">  829</span>        alphas = alphas[: n_iter + 1]</div>
<div class="line"><span class="lineno">  830</span>        coefs = coefs[: n_iter + 1]</div>
<div class="line"><span class="lineno">  831</span> </div>
<div class="line"><span class="lineno">  832</span>        <span class="keywordflow">if</span> return_n_iter:</div>
<div class="line"><span class="lineno">  833</span>            <span class="keywordflow">return</span> alphas, active, coefs.T, n_iter</div>
<div class="line"><span class="lineno">  834</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  835</span>            <span class="keywordflow">return</span> alphas, active, coefs.T</div>
<div class="line"><span class="lineno">  836</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  837</span>        <span class="keywordflow">if</span> return_n_iter:</div>
<div class="line"><span class="lineno">  838</span>            <span class="keywordflow">return</span> alpha, active, coef, n_iter</div>
<div class="line"><span class="lineno">  839</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  840</span>            <span class="keywordflow">return</span> alpha, active, coef</div>
<div class="line"><span class="lineno">  841</span> </div>
<div class="line"><span class="lineno">  842</span> </div>
<div class="ttc" id="a__lapack__subroutines_8h_html_a60590d91febfcb54d88443940cd5f23e"><div class="ttname"><a href="__lapack__subroutines_8h.html#a60590d91febfcb54d88443940cd5f23e">iter</a></div><div class="ttdeci">void int double int double double double double int int * iter</div><div class="ttdef"><b>Definition</b> _lapack_subroutines.h:623</div></div>
</div><!-- fragment -->
</div>
</div>
<a id="a9db4e0637ad1d2a96c0e250f6d7e511d" name="a9db4e0637ad1d2a96c0e250f6d7e511d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9db4e0637ad1d2a96c0e250f6d7e511d">&#9670;&#160;</a></span>lars_path()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._least_angle.lars_path </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Xy</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>Gram</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_iter</em> = <code>500</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha_min</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>method</em> = <code>&quot;lar&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy_X</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>eps</em> = <code>np.finfo(float).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy_Gram</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>return_path</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>return_n_iter</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>positive</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute Least Angle Regression or Lasso path using LARS algorithm [1].

The optimization objective for the case method='lasso' is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

in the case of method='lars', the objective function is only known in
the form of an implicit equation (see discussion in [1])

Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.

Parameters
----------
X : None or array-like of shape (n_samples, n_features)
    Input data. Note that if X is None then the Gram matrix must be
    specified, i.e., cannot be None or False.

y : None or array-like of shape (n_samples,)
    Input targets.

Xy : array-like of shape (n_samples,) or (n_samples, n_targets), \
        default=None
    Xy = np.dot(X.T, y) that can be precomputed. It is useful
    only when the Gram matrix is precomputed.

Gram : None, 'auto', array-like of shape (n_features, n_features), \
        default=None
    Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
    matrix is precomputed from the given X, if there are more samples
    than features.

max_iter : int, default=500
    Maximum number of iterations to perform, set to infinity for no limit.

alpha_min : float, default=0
    Minimum correlation along the path. It corresponds to the
    regularization parameter alpha parameter in the Lasso.

method : {'lar', 'lasso'}, default='lar'
    Specifies the returned model. Select ``'lar'`` for Least Angle
    Regression, ``'lasso'`` for the Lasso.

copy_X : bool, default=True
    If ``False``, ``X`` is overwritten.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_Gram : bool, default=True
    If ``False``, ``Gram`` is overwritten.

verbose : int, default=0
    Controls output verbosity.

return_path : bool, default=True
    If ``return_path==True`` returns the entire path, else returns only the
    last point of the path.

return_n_iter : bool, default=False
    Whether to return the number of iterations.

positive : bool, default=False
    Restrict coefficients to be &gt;= 0.
    This option is only allowed with method 'lasso'. Note that the model
    coefficients will not converge to the ordinary-least-squares solution
    for small values of alpha. Only coefficients up to the smallest alpha
    value (``alphas_[alphas_ &gt; 0.].min()`` when fit_path=True) reached by
    the stepwise Lars-Lasso algorithm are typically in congruence with the
    solution of the coordinate descent lasso_path function.

Returns
-------
alphas : array-like of shape (n_alphas + 1,)
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha &gt;= alpha_min``, whichever
    is smaller.

active : array-like of shape (n_alphas,)
    Indices of active variables at the end of the path.

coefs : array-like of shape (n_features, n_alphas + 1)
    Coefficients along the path.

n_iter : int
    Number of iterations run. Returned only if return_n_iter is set
    to True.

See Also
--------
lars_path_gram : Compute LARS path in the sufficient stats mode.
lasso_path : Compute Lasso path with coordinate descent.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
Lars : Least Angle Regression model a.k.a. LAR.
LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.
LarsCV : Cross-validated Least Angle Regression model.
sklearn.decomposition.sparse_encode : Sparse coding.

References
----------
.. [1] "Least Angle Regression", Efron et al.
       http://statweb.stanford.edu/~tibs/ftp/lars.pdf

.. [2] `Wikipedia entry on the Least-angle regression
       &lt;https://en.wikipedia.org/wiki/Least-angle_regression&gt;`_

.. [3] `Wikipedia entry on the Lasso
       &lt;https://en.wikipedia.org/wiki/Lasso_(statistics)&gt;`_
</pre> <div class="fragment"><div class="line"><span class="lineno">   52</span>):</div>
<div class="line"><span class="lineno">   53</span>    <span class="stringliteral">&quot;&quot;&quot;Compute Least Angle Regression or Lasso path using LARS algorithm [1].</span></div>
<div class="line"><span class="lineno">   54</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   55</span><span class="stringliteral">    The optimization objective for the case method=&#39;lasso&#39; is::</span></div>
<div class="line"><span class="lineno">   56</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   57</span><span class="stringliteral">    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1</span></div>
<div class="line"><span class="lineno">   58</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   59</span><span class="stringliteral">    in the case of method=&#39;lars&#39;, the objective function is only known in</span></div>
<div class="line"><span class="lineno">   60</span><span class="stringliteral">    the form of an implicit equation (see discussion in [1])</span></div>
<div class="line"><span class="lineno">   61</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   62</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.</span></div>
<div class="line"><span class="lineno">   63</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   64</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   65</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   66</span><span class="stringliteral">    X : None or array-like of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">   67</span><span class="stringliteral">        Input data. Note that if X is None then the Gram matrix must be</span></div>
<div class="line"><span class="lineno">   68</span><span class="stringliteral">        specified, i.e., cannot be None or False.</span></div>
<div class="line"><span class="lineno">   69</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   70</span><span class="stringliteral">    y : None or array-like of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">   71</span><span class="stringliteral">        Input targets.</span></div>
<div class="line"><span class="lineno">   72</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   73</span><span class="stringliteral">    Xy : array-like of shape (n_samples,) or (n_samples, n_targets), \</span></div>
<div class="line"><span class="lineno">   74</span><span class="stringliteral">            default=None</span></div>
<div class="line"><span class="lineno">   75</span><span class="stringliteral">        Xy = np.dot(X.T, y) that can be precomputed. It is useful</span></div>
<div class="line"><span class="lineno">   76</span><span class="stringliteral">        only when the Gram matrix is precomputed.</span></div>
<div class="line"><span class="lineno">   77</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   78</span><span class="stringliteral">    Gram : None, &#39;auto&#39;, array-like of shape (n_features, n_features), \</span></div>
<div class="line"><span class="lineno">   79</span><span class="stringliteral">            default=None</span></div>
<div class="line"><span class="lineno">   80</span><span class="stringliteral">        Precomputed Gram matrix (X&#39; * X), if ``&#39;auto&#39;``, the Gram</span></div>
<div class="line"><span class="lineno">   81</span><span class="stringliteral">        matrix is precomputed from the given X, if there are more samples</span></div>
<div class="line"><span class="lineno">   82</span><span class="stringliteral">        than features.</span></div>
<div class="line"><span class="lineno">   83</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   84</span><span class="stringliteral">    max_iter : int, default=500</span></div>
<div class="line"><span class="lineno">   85</span><span class="stringliteral">        Maximum number of iterations to perform, set to infinity for no limit.</span></div>
<div class="line"><span class="lineno">   86</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   87</span><span class="stringliteral">    alpha_min : float, default=0</span></div>
<div class="line"><span class="lineno">   88</span><span class="stringliteral">        Minimum correlation along the path. It corresponds to the</span></div>
<div class="line"><span class="lineno">   89</span><span class="stringliteral">        regularization parameter alpha parameter in the Lasso.</span></div>
<div class="line"><span class="lineno">   90</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   91</span><span class="stringliteral">    method : {&#39;lar&#39;, &#39;lasso&#39;}, default=&#39;lar&#39;</span></div>
<div class="line"><span class="lineno">   92</span><span class="stringliteral">        Specifies the returned model. Select ``&#39;lar&#39;`` for Least Angle</span></div>
<div class="line"><span class="lineno">   93</span><span class="stringliteral">        Regression, ``&#39;lasso&#39;`` for the Lasso.</span></div>
<div class="line"><span class="lineno">   94</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   95</span><span class="stringliteral">    copy_X : bool, default=True</span></div>
<div class="line"><span class="lineno">   96</span><span class="stringliteral">        If ``False``, ``X`` is overwritten.</span></div>
<div class="line"><span class="lineno">   97</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   98</span><span class="stringliteral">    eps : float, default=np.finfo(float).eps</span></div>
<div class="line"><span class="lineno">   99</span><span class="stringliteral">        The machine-precision regularization in the computation of the</span></div>
<div class="line"><span class="lineno">  100</span><span class="stringliteral">        Cholesky diagonal factors. Increase this for very ill-conditioned</span></div>
<div class="line"><span class="lineno">  101</span><span class="stringliteral">        systems. Unlike the ``tol`` parameter in some iterative</span></div>
<div class="line"><span class="lineno">  102</span><span class="stringliteral">        optimization-based algorithms, this parameter does not control</span></div>
<div class="line"><span class="lineno">  103</span><span class="stringliteral">        the tolerance of the optimization.</span></div>
<div class="line"><span class="lineno">  104</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  105</span><span class="stringliteral">    copy_Gram : bool, default=True</span></div>
<div class="line"><span class="lineno">  106</span><span class="stringliteral">        If ``False``, ``Gram`` is overwritten.</span></div>
<div class="line"><span class="lineno">  107</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  108</span><span class="stringliteral">    verbose : int, default=0</span></div>
<div class="line"><span class="lineno">  109</span><span class="stringliteral">        Controls output verbosity.</span></div>
<div class="line"><span class="lineno">  110</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  111</span><span class="stringliteral">    return_path : bool, default=True</span></div>
<div class="line"><span class="lineno">  112</span><span class="stringliteral">        If ``return_path==True`` returns the entire path, else returns only the</span></div>
<div class="line"><span class="lineno">  113</span><span class="stringliteral">        last point of the path.</span></div>
<div class="line"><span class="lineno">  114</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  115</span><span class="stringliteral">    return_n_iter : bool, default=False</span></div>
<div class="line"><span class="lineno">  116</span><span class="stringliteral">        Whether to return the number of iterations.</span></div>
<div class="line"><span class="lineno">  117</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  118</span><span class="stringliteral">    positive : bool, default=False</span></div>
<div class="line"><span class="lineno">  119</span><span class="stringliteral">        Restrict coefficients to be &gt;= 0.</span></div>
<div class="line"><span class="lineno">  120</span><span class="stringliteral">        This option is only allowed with method &#39;lasso&#39;. Note that the model</span></div>
<div class="line"><span class="lineno">  121</span><span class="stringliteral">        coefficients will not converge to the ordinary-least-squares solution</span></div>
<div class="line"><span class="lineno">  122</span><span class="stringliteral">        for small values of alpha. Only coefficients up to the smallest alpha</span></div>
<div class="line"><span class="lineno">  123</span><span class="stringliteral">        value (``alphas_[alphas_ &gt; 0.].min()`` when fit_path=True) reached by</span></div>
<div class="line"><span class="lineno">  124</span><span class="stringliteral">        the stepwise Lars-Lasso algorithm are typically in congruence with the</span></div>
<div class="line"><span class="lineno">  125</span><span class="stringliteral">        solution of the coordinate descent lasso_path function.</span></div>
<div class="line"><span class="lineno">  126</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  127</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  128</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  129</span><span class="stringliteral">    alphas : array-like of shape (n_alphas + 1,)</span></div>
<div class="line"><span class="lineno">  130</span><span class="stringliteral">        Maximum of covariances (in absolute value) at each iteration.</span></div>
<div class="line"><span class="lineno">  131</span><span class="stringliteral">        ``n_alphas`` is either ``max_iter``, ``n_features`` or the</span></div>
<div class="line"><span class="lineno">  132</span><span class="stringliteral">        number of nodes in the path with ``alpha &gt;= alpha_min``, whichever</span></div>
<div class="line"><span class="lineno">  133</span><span class="stringliteral">        is smaller.</span></div>
<div class="line"><span class="lineno">  134</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  135</span><span class="stringliteral">    active : array-like of shape (n_alphas,)</span></div>
<div class="line"><span class="lineno">  136</span><span class="stringliteral">        Indices of active variables at the end of the path.</span></div>
<div class="line"><span class="lineno">  137</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  138</span><span class="stringliteral">    coefs : array-like of shape (n_features, n_alphas + 1)</span></div>
<div class="line"><span class="lineno">  139</span><span class="stringliteral">        Coefficients along the path.</span></div>
<div class="line"><span class="lineno">  140</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  141</span><span class="stringliteral">    n_iter : int</span></div>
<div class="line"><span class="lineno">  142</span><span class="stringliteral">        Number of iterations run. Returned only if return_n_iter is set</span></div>
<div class="line"><span class="lineno">  143</span><span class="stringliteral">        to True.</span></div>
<div class="line"><span class="lineno">  144</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  145</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  146</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  147</span><span class="stringliteral">    lars_path_gram : Compute LARS path in the sufficient stats mode.</span></div>
<div class="line"><span class="lineno">  148</span><span class="stringliteral">    lasso_path : Compute Lasso path with coordinate descent.</span></div>
<div class="line"><span class="lineno">  149</span><span class="stringliteral">    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.</span></div>
<div class="line"><span class="lineno">  150</span><span class="stringliteral">    Lars : Least Angle Regression model a.k.a. LAR.</span></div>
<div class="line"><span class="lineno">  151</span><span class="stringliteral">    LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.</span></div>
<div class="line"><span class="lineno">  152</span><span class="stringliteral">    LarsCV : Cross-validated Least Angle Regression model.</span></div>
<div class="line"><span class="lineno">  153</span><span class="stringliteral">    sklearn.decomposition.sparse_encode : Sparse coding.</span></div>
<div class="line"><span class="lineno">  154</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  155</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  156</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  157</span><span class="stringliteral">    .. [1] &quot;Least Angle Regression&quot;, Efron et al.</span></div>
<div class="line"><span class="lineno">  158</span><span class="stringliteral">           http://statweb.stanford.edu/~tibs/ftp/lars.pdf</span></div>
<div class="line"><span class="lineno">  159</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  160</span><span class="stringliteral">    .. [2] `Wikipedia entry on the Least-angle regression</span></div>
<div class="line"><span class="lineno">  161</span><span class="stringliteral">           &lt;https://en.wikipedia.org/wiki/Least-angle_regression&gt;`_</span></div>
<div class="line"><span class="lineno">  162</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  163</span><span class="stringliteral">    .. [3] `Wikipedia entry on the Lasso</span></div>
<div class="line"><span class="lineno">  164</span><span class="stringliteral">           &lt;https://en.wikipedia.org/wiki/Lasso_(statistics)&gt;`_</span></div>
<div class="line"><span class="lineno">  165</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  166</span>    <span class="keywordflow">if</span> X <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> Gram <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  167</span>        <span class="keywordflow">raise</span> ValueError(</div>
<div class="line"><span class="lineno">  168</span>            <span class="stringliteral">&quot;X cannot be None if Gram is not None&quot;</span></div>
<div class="line"><span class="lineno">  169</span>            <span class="stringliteral">&quot;Use lars_path_gram to avoid passing X and y.&quot;</span></div>
<div class="line"><span class="lineno">  170</span>        )</div>
<div class="line"><span class="lineno">  171</span>    <span class="keywordflow">return</span> _lars_path_solver(</div>
<div class="line"><span class="lineno">  172</span>        X=X,</div>
<div class="line"><span class="lineno">  173</span>        y=y,</div>
<div class="line"><span class="lineno">  174</span>        Xy=Xy,</div>
<div class="line"><span class="lineno">  175</span>        Gram=Gram,</div>
<div class="line"><span class="lineno">  176</span>        n_samples=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno">  177</span>        max_iter=max_iter,</div>
<div class="line"><span class="lineno">  178</span>        alpha_min=alpha_min,</div>
<div class="line"><span class="lineno">  179</span>        method=method,</div>
<div class="line"><span class="lineno">  180</span>        copy_X=copy_X,</div>
<div class="line"><span class="lineno">  181</span>        eps=eps,</div>
<div class="line"><span class="lineno">  182</span>        copy_Gram=copy_Gram,</div>
<div class="line"><span class="lineno">  183</span>        verbose=verbose,</div>
<div class="line"><span class="lineno">  184</span>        return_path=return_path,</div>
<div class="line"><span class="lineno">  185</span>        return_n_iter=return_n_iter,</div>
<div class="line"><span class="lineno">  186</span>        positive=positive,</div>
<div class="line"><span class="lineno">  187</span>    )</div>
<div class="line"><span class="lineno">  188</span> </div>
<div class="line"><span class="lineno">  189</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a1b08ff0c162c417d94c0be0b0c869fa3" name="a1b08ff0c162c417d94c0be0b0c869fa3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1b08ff0c162c417d94c0be0b0c869fa3">&#9670;&#160;</a></span>lars_path_gram()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model._least_angle.lars_path_gram </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Xy</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Gram</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>n_samples</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>max_iter</em> = <code>500</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha_min</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>method</em> = <code>&quot;lar&quot;</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy_X</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>eps</em> = <code>np.finfo(float).<a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a></code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>copy_Gram</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>return_path</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>return_n_iter</em> = <code>False</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>positive</em> = <code>False</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">The lars_path in the sufficient stats mode [1].

The optimization objective for the case method='lasso' is::

(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

in the case of method='lars', the objective function is only known in
the form of an implicit equation (see discussion in [1])

Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.

Parameters
----------
Xy : array-like of shape (n_samples,) or (n_samples, n_targets)
    Xy = np.dot(X.T, y).

Gram : array-like of shape (n_features, n_features)
    Gram = np.dot(X.T * X).

n_samples : int or float
    Equivalent size of sample.

max_iter : int, default=500
    Maximum number of iterations to perform, set to infinity for no limit.

alpha_min : float, default=0
    Minimum correlation along the path. It corresponds to the
    regularization parameter alpha parameter in the Lasso.

method : {'lar', 'lasso'}, default='lar'
    Specifies the returned model. Select ``'lar'`` for Least Angle
    Regression, ``'lasso'`` for the Lasso.

copy_X : bool, default=True
    If ``False``, ``X`` is overwritten.

eps : float, default=np.finfo(float).eps
    The machine-precision regularization in the computation of the
    Cholesky diagonal factors. Increase this for very ill-conditioned
    systems. Unlike the ``tol`` parameter in some iterative
    optimization-based algorithms, this parameter does not control
    the tolerance of the optimization.

copy_Gram : bool, default=True
    If ``False``, ``Gram`` is overwritten.

verbose : int, default=0
    Controls output verbosity.

return_path : bool, default=True
    If ``return_path==True`` returns the entire path, else returns only the
    last point of the path.

return_n_iter : bool, default=False
    Whether to return the number of iterations.

positive : bool, default=False
    Restrict coefficients to be &gt;= 0.
    This option is only allowed with method 'lasso'. Note that the model
    coefficients will not converge to the ordinary-least-squares solution
    for small values of alpha. Only coefficients up to the smallest alpha
    value (``alphas_[alphas_ &gt; 0.].min()`` when fit_path=True) reached by
    the stepwise Lars-Lasso algorithm are typically in congruence with the
    solution of the coordinate descent lasso_path function.

Returns
-------
alphas : array-like of shape (n_alphas + 1,)
    Maximum of covariances (in absolute value) at each iteration.
    ``n_alphas`` is either ``max_iter``, ``n_features`` or the
    number of nodes in the path with ``alpha &gt;= alpha_min``, whichever
    is smaller.

active : array-like of shape (n_alphas,)
    Indices of active variables at the end of the path.

coefs : array-like of shape (n_features, n_alphas + 1)
    Coefficients along the path.

n_iter : int
    Number of iterations run. Returned only if return_n_iter is set
    to True.

See Also
--------
lars_path_gram : Compute LARS path.
lasso_path : Compute Lasso path with coordinate descent.
LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.
Lars : Least Angle Regression model a.k.a. LAR.
LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.
LarsCV : Cross-validated Least Angle Regression model.
sklearn.decomposition.sparse_encode : Sparse coding.

References
----------
.. [1] "Least Angle Regression", Efron et al.
       http://statweb.stanford.edu/~tibs/ftp/lars.pdf

.. [2] `Wikipedia entry on the Least-angle regression
       &lt;https://en.wikipedia.org/wiki/Least-angle_regression&gt;`_

.. [3] `Wikipedia entry on the Lasso
       &lt;https://en.wikipedia.org/wiki/Lasso_(statistics)&gt;`_
</pre> <div class="fragment"><div class="line"><span class="lineno">  205</span>):</div>
<div class="line"><span class="lineno">  206</span>    <span class="stringliteral">&quot;&quot;&quot;The lars_path in the sufficient stats mode [1].</span></div>
<div class="line"><span class="lineno">  207</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  208</span><span class="stringliteral">    The optimization objective for the case method=&#39;lasso&#39; is::</span></div>
<div class="line"><span class="lineno">  209</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  210</span><span class="stringliteral">    (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1</span></div>
<div class="line"><span class="lineno">  211</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  212</span><span class="stringliteral">    in the case of method=&#39;lars&#39;, the objective function is only known in</span></div>
<div class="line"><span class="lineno">  213</span><span class="stringliteral">    the form of an implicit equation (see discussion in [1])</span></div>
<div class="line"><span class="lineno">  214</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  215</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;least_angle_regression&gt;`.</span></div>
<div class="line"><span class="lineno">  216</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  217</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  218</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  219</span><span class="stringliteral">    Xy : array-like of shape (n_samples,) or (n_samples, n_targets)</span></div>
<div class="line"><span class="lineno">  220</span><span class="stringliteral">        Xy = np.dot(X.T, y).</span></div>
<div class="line"><span class="lineno">  221</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  222</span><span class="stringliteral">    Gram : array-like of shape (n_features, n_features)</span></div>
<div class="line"><span class="lineno">  223</span><span class="stringliteral">        Gram = np.dot(X.T * X).</span></div>
<div class="line"><span class="lineno">  224</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  225</span><span class="stringliteral">    n_samples : int or float</span></div>
<div class="line"><span class="lineno">  226</span><span class="stringliteral">        Equivalent size of sample.</span></div>
<div class="line"><span class="lineno">  227</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  228</span><span class="stringliteral">    max_iter : int, default=500</span></div>
<div class="line"><span class="lineno">  229</span><span class="stringliteral">        Maximum number of iterations to perform, set to infinity for no limit.</span></div>
<div class="line"><span class="lineno">  230</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  231</span><span class="stringliteral">    alpha_min : float, default=0</span></div>
<div class="line"><span class="lineno">  232</span><span class="stringliteral">        Minimum correlation along the path. It corresponds to the</span></div>
<div class="line"><span class="lineno">  233</span><span class="stringliteral">        regularization parameter alpha parameter in the Lasso.</span></div>
<div class="line"><span class="lineno">  234</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  235</span><span class="stringliteral">    method : {&#39;lar&#39;, &#39;lasso&#39;}, default=&#39;lar&#39;</span></div>
<div class="line"><span class="lineno">  236</span><span class="stringliteral">        Specifies the returned model. Select ``&#39;lar&#39;`` for Least Angle</span></div>
<div class="line"><span class="lineno">  237</span><span class="stringliteral">        Regression, ``&#39;lasso&#39;`` for the Lasso.</span></div>
<div class="line"><span class="lineno">  238</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  239</span><span class="stringliteral">    copy_X : bool, default=True</span></div>
<div class="line"><span class="lineno">  240</span><span class="stringliteral">        If ``False``, ``X`` is overwritten.</span></div>
<div class="line"><span class="lineno">  241</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  242</span><span class="stringliteral">    eps : float, default=np.finfo(float).eps</span></div>
<div class="line"><span class="lineno">  243</span><span class="stringliteral">        The machine-precision regularization in the computation of the</span></div>
<div class="line"><span class="lineno">  244</span><span class="stringliteral">        Cholesky diagonal factors. Increase this for very ill-conditioned</span></div>
<div class="line"><span class="lineno">  245</span><span class="stringliteral">        systems. Unlike the ``tol`` parameter in some iterative</span></div>
<div class="line"><span class="lineno">  246</span><span class="stringliteral">        optimization-based algorithms, this parameter does not control</span></div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral">        the tolerance of the optimization.</span></div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">    copy_Gram : bool, default=True</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral">        If ``False``, ``Gram`` is overwritten.</span></div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  252</span><span class="stringliteral">    verbose : int, default=0</span></div>
<div class="line"><span class="lineno">  253</span><span class="stringliteral">        Controls output verbosity.</span></div>
<div class="line"><span class="lineno">  254</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  255</span><span class="stringliteral">    return_path : bool, default=True</span></div>
<div class="line"><span class="lineno">  256</span><span class="stringliteral">        If ``return_path==True`` returns the entire path, else returns only the</span></div>
<div class="line"><span class="lineno">  257</span><span class="stringliteral">        last point of the path.</span></div>
<div class="line"><span class="lineno">  258</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  259</span><span class="stringliteral">    return_n_iter : bool, default=False</span></div>
<div class="line"><span class="lineno">  260</span><span class="stringliteral">        Whether to return the number of iterations.</span></div>
<div class="line"><span class="lineno">  261</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  262</span><span class="stringliteral">    positive : bool, default=False</span></div>
<div class="line"><span class="lineno">  263</span><span class="stringliteral">        Restrict coefficients to be &gt;= 0.</span></div>
<div class="line"><span class="lineno">  264</span><span class="stringliteral">        This option is only allowed with method &#39;lasso&#39;. Note that the model</span></div>
<div class="line"><span class="lineno">  265</span><span class="stringliteral">        coefficients will not converge to the ordinary-least-squares solution</span></div>
<div class="line"><span class="lineno">  266</span><span class="stringliteral">        for small values of alpha. Only coefficients up to the smallest alpha</span></div>
<div class="line"><span class="lineno">  267</span><span class="stringliteral">        value (``alphas_[alphas_ &gt; 0.].min()`` when fit_path=True) reached by</span></div>
<div class="line"><span class="lineno">  268</span><span class="stringliteral">        the stepwise Lars-Lasso algorithm are typically in congruence with the</span></div>
<div class="line"><span class="lineno">  269</span><span class="stringliteral">        solution of the coordinate descent lasso_path function.</span></div>
<div class="line"><span class="lineno">  270</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  271</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  272</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  273</span><span class="stringliteral">    alphas : array-like of shape (n_alphas + 1,)</span></div>
<div class="line"><span class="lineno">  274</span><span class="stringliteral">        Maximum of covariances (in absolute value) at each iteration.</span></div>
<div class="line"><span class="lineno">  275</span><span class="stringliteral">        ``n_alphas`` is either ``max_iter``, ``n_features`` or the</span></div>
<div class="line"><span class="lineno">  276</span><span class="stringliteral">        number of nodes in the path with ``alpha &gt;= alpha_min``, whichever</span></div>
<div class="line"><span class="lineno">  277</span><span class="stringliteral">        is smaller.</span></div>
<div class="line"><span class="lineno">  278</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  279</span><span class="stringliteral">    active : array-like of shape (n_alphas,)</span></div>
<div class="line"><span class="lineno">  280</span><span class="stringliteral">        Indices of active variables at the end of the path.</span></div>
<div class="line"><span class="lineno">  281</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  282</span><span class="stringliteral">    coefs : array-like of shape (n_features, n_alphas + 1)</span></div>
<div class="line"><span class="lineno">  283</span><span class="stringliteral">        Coefficients along the path.</span></div>
<div class="line"><span class="lineno">  284</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  285</span><span class="stringliteral">    n_iter : int</span></div>
<div class="line"><span class="lineno">  286</span><span class="stringliteral">        Number of iterations run. Returned only if return_n_iter is set</span></div>
<div class="line"><span class="lineno">  287</span><span class="stringliteral">        to True.</span></div>
<div class="line"><span class="lineno">  288</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  289</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  290</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  291</span><span class="stringliteral">    lars_path_gram : Compute LARS path.</span></div>
<div class="line"><span class="lineno">  292</span><span class="stringliteral">    lasso_path : Compute Lasso path with coordinate descent.</span></div>
<div class="line"><span class="lineno">  293</span><span class="stringliteral">    LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.</span></div>
<div class="line"><span class="lineno">  294</span><span class="stringliteral">    Lars : Least Angle Regression model a.k.a. LAR.</span></div>
<div class="line"><span class="lineno">  295</span><span class="stringliteral">    LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.</span></div>
<div class="line"><span class="lineno">  296</span><span class="stringliteral">    LarsCV : Cross-validated Least Angle Regression model.</span></div>
<div class="line"><span class="lineno">  297</span><span class="stringliteral">    sklearn.decomposition.sparse_encode : Sparse coding.</span></div>
<div class="line"><span class="lineno">  298</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  299</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  300</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  301</span><span class="stringliteral">    .. [1] &quot;Least Angle Regression&quot;, Efron et al.</span></div>
<div class="line"><span class="lineno">  302</span><span class="stringliteral">           http://statweb.stanford.edu/~tibs/ftp/lars.pdf</span></div>
<div class="line"><span class="lineno">  303</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  304</span><span class="stringliteral">    .. [2] `Wikipedia entry on the Least-angle regression</span></div>
<div class="line"><span class="lineno">  305</span><span class="stringliteral">           &lt;https://en.wikipedia.org/wiki/Least-angle_regression&gt;`_</span></div>
<div class="line"><span class="lineno">  306</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  307</span><span class="stringliteral">    .. [3] `Wikipedia entry on the Lasso</span></div>
<div class="line"><span class="lineno">  308</span><span class="stringliteral">           &lt;https://en.wikipedia.org/wiki/Lasso_(statistics)&gt;`_</span></div>
<div class="line"><span class="lineno">  309</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  310</span>    <span class="keywordflow">return</span> _lars_path_solver(</div>
<div class="line"><span class="lineno">  311</span>        X=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno">  312</span>        y=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno">  313</span>        Xy=Xy,</div>
<div class="line"><span class="lineno">  314</span>        Gram=Gram,</div>
<div class="line"><span class="lineno">  315</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  316</span>        max_iter=max_iter,</div>
<div class="line"><span class="lineno">  317</span>        alpha_min=alpha_min,</div>
<div class="line"><span class="lineno">  318</span>        method=method,</div>
<div class="line"><span class="lineno">  319</span>        copy_X=copy_X,</div>
<div class="line"><span class="lineno">  320</span>        eps=eps,</div>
<div class="line"><span class="lineno">  321</span>        copy_Gram=copy_Gram,</div>
<div class="line"><span class="lineno">  322</span>        verbose=verbose,</div>
<div class="line"><span class="lineno">  323</span>        return_path=return_path,</div>
<div class="line"><span class="lineno">  324</span>        return_n_iter=return_n_iter,</div>
<div class="line"><span class="lineno">  325</span>        positive=positive,</div>
<div class="line"><span class="lineno">  326</span>    )</div>
<div class="line"><span class="lineno">  327</span> </div>
<div class="line"><span class="lineno">  328</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="adbfa3ac69f38d7b8e84c4db13058d4b4" name="adbfa3ac69f38d7b8e84c4db13058d4b4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adbfa3ac69f38d7b8e84c4db13058d4b4">&#9670;&#160;</a></span>SOLVE_TRIANGULAR_ARGS</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">dict sklearn.linear_model._least_angle.SOLVE_TRIANGULAR_ARGS = {&quot;check_finite&quot;: False}</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
