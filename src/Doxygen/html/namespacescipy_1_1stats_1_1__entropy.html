<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: scipy.stats._entropy Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacescipy.html">scipy</a></li><li class="navelem"><a class="el" href="namespacescipy_1_1stats.html">stats</a></li><li class="navelem"><a class="el" href="namespacescipy_1_1stats_1_1__entropy.html">_entropy</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">scipy.stats._entropy Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:adda40496c374a928e072c08fbb643ee0" id="r_adda40496c374a928e072c08fbb643ee0"><td class="memItemLeft" align="right" valign="top">Union[np.number, np.ndarray]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1stats_1_1__entropy.html#adda40496c374a928e072c08fbb643ee0">entropy</a> (np.typing.ArrayLike pk, Optional[np.typing.ArrayLike] qk=None, Optional[float] base=None, int axis=0)</td></tr>
<tr class="separator:adda40496c374a928e072c08fbb643ee0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3bbf47006bd31ab1d2ff6406dfaee1af" id="r_a3bbf47006bd31ab1d2ff6406dfaee1af"><td class="memItemLeft" align="right" valign="top">Union[np.number, np.ndarray]&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1stats_1_1__entropy.html#a3bbf47006bd31ab1d2ff6406dfaee1af">differential_entropy</a> (np.typing.ArrayLike values, *Optional[int] window_length=None, Optional[float] base=None, int axis=0, str <a class="el" href="namespacescipy_1_1stats_1_1__multivariate.html#ad5850caf3faceef835b3baacc93038ee">method</a>=&quot;auto&quot;)</td></tr>
<tr class="separator:a3bbf47006bd31ab1d2ff6406dfaee1af"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8da8253d95c4150183c20f4d9f9d95ea" id="r_a8da8253d95c4150183c20f4d9f9d95ea"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1stats_1_1__entropy.html#a8da8253d95c4150183c20f4d9f9d95ea">_pad_along_last_axis</a> (X, m)</td></tr>
<tr class="separator:a8da8253d95c4150183c20f4d9f9d95ea"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2a01b018776843abfb9261305cd30c68" id="r_a2a01b018776843abfb9261305cd30c68"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1stats_1_1__entropy.html#a2a01b018776843abfb9261305cd30c68">_vasicek_entropy</a> (X, m)</td></tr>
<tr class="separator:a2a01b018776843abfb9261305cd30c68"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad777cd154d4b45a745d1ccd812dd3af2" id="r_ad777cd154d4b45a745d1ccd812dd3af2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1stats_1_1__entropy.html#ad777cd154d4b45a745d1ccd812dd3af2">_van_es_entropy</a> (X, m)</td></tr>
<tr class="separator:ad777cd154d4b45a745d1ccd812dd3af2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac9b0d2377dc302465e2da02678b76846" id="r_ac9b0d2377dc302465e2da02678b76846"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1stats_1_1__entropy.html#ac9b0d2377dc302465e2da02678b76846">_ebrahimi_entropy</a> (X, m)</td></tr>
<tr class="separator:ac9b0d2377dc302465e2da02678b76846"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae2d17de2a6387d3fa0b41294c50f91be" id="r_ae2d17de2a6387d3fa0b41294c50f91be"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacescipy_1_1stats_1_1__entropy.html#ae2d17de2a6387d3fa0b41294c50f91be">_correa_entropy</a> (X, m)</td></tr>
<tr class="separator:ae2d17de2a6387d3fa0b41294c50f91be"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Created on Fri Apr  2 09:06:05 2021

@author: matth
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="ae2d17de2a6387d3fa0b41294c50f91be" name="ae2d17de2a6387d3fa0b41294c50f91be"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae2d17de2a6387d3fa0b41294c50f91be">&#9670;&#160;</a></span>_correa_entropy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._correa_entropy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>m</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the Correa estimator as described in [6].</pre> <div class="fragment"><div class="line"><span class="lineno">  325</span><span class="keyword">def </span>_correa_entropy(X, m):</div>
<div class="line"><span class="lineno">  326</span>    <span class="stringliteral">&quot;&quot;&quot;Compute the Correa estimator as described in [6].&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  327</span>    <span class="comment"># No equation number, but referred to as HC_mn</span></div>
<div class="line"><span class="lineno">  328</span>    n = X.shape[-1]</div>
<div class="line"><span class="lineno">  329</span>    X = _pad_along_last_axis(X, m)</div>
<div class="line"><span class="lineno">  330</span> </div>
<div class="line"><span class="lineno">  331</span>    i = np.arange(1, n+1)</div>
<div class="line"><span class="lineno">  332</span>    dj = np.arange(-m, m+1)[:, <span class="keywordtype">None</span>]</div>
<div class="line"><span class="lineno">  333</span>    j = i + dj</div>
<div class="line"><span class="lineno">  334</span>    j0 = j + m - 1  <span class="comment"># 0-indexed version of j</span></div>
<div class="line"><span class="lineno">  335</span> </div>
<div class="line"><span class="lineno">  336</span>    Xibar = np.mean(X[..., j0], axis=-2, keepdims=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  337</span>    difference = X[..., j0] - Xibar</div>
<div class="line"><span class="lineno">  338</span>    num = np.sum(difference*dj, axis=-2)  <span class="comment"># dj is d-i</span></div>
<div class="line"><span class="lineno">  339</span>    den = n*np.sum(difference**2, axis=-2)</div>
<div class="line"><span class="lineno">  340</span>    <span class="keywordflow">return</span> -np.mean(np.log(num/den), axis=-1)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ac9b0d2377dc302465e2da02678b76846" name="ac9b0d2377dc302465e2da02678b76846"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac9b0d2377dc302465e2da02678b76846">&#9670;&#160;</a></span>_ebrahimi_entropy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._ebrahimi_entropy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>m</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the Ebrahimi estimator as described in [6].</pre> <div class="fragment"><div class="line"><span class="lineno">  308</span><span class="keyword">def </span>_ebrahimi_entropy(X, m):</div>
<div class="line"><span class="lineno">  309</span>    <span class="stringliteral">&quot;&quot;&quot;Compute the Ebrahimi estimator as described in [6].&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  310</span>    <span class="comment"># No equation number, but referred to as HE_mn</span></div>
<div class="line"><span class="lineno">  311</span>    n = X.shape[-1]</div>
<div class="line"><span class="lineno">  312</span>    X = _pad_along_last_axis(X, m)</div>
<div class="line"><span class="lineno">  313</span> </div>
<div class="line"><span class="lineno">  314</span>    differences = X[..., 2 * m:] - X[..., : -2 * m:]</div>
<div class="line"><span class="lineno">  315</span> </div>
<div class="line"><span class="lineno">  316</span>    i = np.arange(1, n+1).astype(float)</div>
<div class="line"><span class="lineno">  317</span>    ci = np.ones_like(i)*2</div>
<div class="line"><span class="lineno">  318</span>    ci[i &lt;= m] = 1 + (i[i &lt;= m] - 1)/m</div>
<div class="line"><span class="lineno">  319</span>    ci[i &gt;= n - m + 1] = 1 + (n - i[i &gt;= n-m+1])/m</div>
<div class="line"><span class="lineno">  320</span> </div>
<div class="line"><span class="lineno">  321</span>    logs = np.log(n * differences / (ci * m))</div>
<div class="line"><span class="lineno">  322</span>    <span class="keywordflow">return</span> np.mean(logs, axis=-1)</div>
<div class="line"><span class="lineno">  323</span> </div>
<div class="line"><span class="lineno">  324</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a8da8253d95c4150183c20f4d9f9d95ea" name="a8da8253d95c4150183c20f4d9f9d95ea"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8da8253d95c4150183c20f4d9f9d95ea">&#9670;&#160;</a></span>_pad_along_last_axis()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._pad_along_last_axis </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>m</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Pad the data for computing the rolling window difference.</pre> <div class="fragment"><div class="line"><span class="lineno">  278</span><span class="keyword">def </span>_pad_along_last_axis(X, m):</div>
<div class="line"><span class="lineno">  279</span>    <span class="stringliteral">&quot;&quot;&quot;Pad the data for computing the rolling window difference.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  280</span>    <span class="comment"># scales a  bit better than method in _vasicek_like_entropy</span></div>
<div class="line"><span class="lineno">  281</span>    shape = np.array(X.shape)</div>
<div class="line"><span class="lineno">  282</span>    shape[-1] = m</div>
<div class="line"><span class="lineno">  283</span>    Xl = np.broadcast_to(X[..., [0]], shape)  <span class="comment"># [0] vs 0 to maintain shape</span></div>
<div class="line"><span class="lineno">  284</span>    Xr = np.broadcast_to(X[..., [-1]], shape)</div>
<div class="line"><span class="lineno">  285</span>    <span class="keywordflow">return</span> np.concatenate((Xl, X, Xr), axis=-1)</div>
<div class="line"><span class="lineno">  286</span> </div>
<div class="line"><span class="lineno">  287</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad777cd154d4b45a745d1ccd812dd3af2" name="ad777cd154d4b45a745d1ccd812dd3af2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad777cd154d4b45a745d1ccd812dd3af2">&#9670;&#160;</a></span>_van_es_entropy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._van_es_entropy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>m</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the van Es estimator as described in [6].</pre> <div class="fragment"><div class="line"><span class="lineno">  297</span><span class="keyword">def </span>_van_es_entropy(X, m):</div>
<div class="line"><span class="lineno">  298</span>    <span class="stringliteral">&quot;&quot;&quot;Compute the van Es estimator as described in [6].&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  299</span>    <span class="comment"># No equation number, but referred to as HVE_mn.</span></div>
<div class="line"><span class="lineno">  300</span>    <span class="comment"># Typo: there should be a log within the summation.</span></div>
<div class="line"><span class="lineno">  301</span>    n = X.shape[-1]</div>
<div class="line"><span class="lineno">  302</span>    difference = X[..., m:] - X[..., :-m]</div>
<div class="line"><span class="lineno">  303</span>    term1 = 1/(n-m) * np.sum(np.log((n+1)/m * difference), axis=-1)</div>
<div class="line"><span class="lineno">  304</span>    k = np.arange(m, n+1)</div>
<div class="line"><span class="lineno">  305</span>    <span class="keywordflow">return</span> term1 + np.sum(1/k) + np.log(m) - np.log(n+1)</div>
<div class="line"><span class="lineno">  306</span> </div>
<div class="line"><span class="lineno">  307</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a2a01b018776843abfb9261305cd30c68" name="a2a01b018776843abfb9261305cd30c68"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2a01b018776843abfb9261305cd30c68">&#9670;&#160;</a></span>_vasicek_entropy()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">scipy.stats._entropy._vasicek_entropy </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>m</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Compute the Vasicek estimator as described in [6] Eq. 1.3.</pre> <div class="fragment"><div class="line"><span class="lineno">  288</span><span class="keyword">def </span>_vasicek_entropy(X, m):</div>
<div class="line"><span class="lineno">  289</span>    <span class="stringliteral">&quot;&quot;&quot;Compute the Vasicek estimator as described in [6] Eq. 1.3.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  290</span>    n = X.shape[-1]</div>
<div class="line"><span class="lineno">  291</span>    X = _pad_along_last_axis(X, m)</div>
<div class="line"><span class="lineno">  292</span>    differences = X[..., 2 * m:] - X[..., : -2 * m:]</div>
<div class="line"><span class="lineno">  293</span>    logs = np.log(n/(2*m) * differences)</div>
<div class="line"><span class="lineno">  294</span>    <span class="keywordflow">return</span> np.mean(logs, axis=-1)</div>
<div class="line"><span class="lineno">  295</span> </div>
<div class="line"><span class="lineno">  296</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a3bbf47006bd31ab1d2ff6406dfaee1af" name="a3bbf47006bd31ab1d2ff6406dfaee1af"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3bbf47006bd31ab1d2ff6406dfaee1af">&#9670;&#160;</a></span>differential_entropy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Union[np.number, np.ndarray] scipy.stats._entropy.differential_entropy </td>
          <td>(</td>
          <td class="paramtype">np.typing.ArrayLike&#160;</td>
          <td class="paramname"><em>values</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*Optional[int] &#160;</td>
          <td class="paramname"><em>window_length</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>base</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>axis</em> = <code>0</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">str &#160;</td>
          <td class="paramname"><em>method</em> = <code>&quot;auto&quot;</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Given a sample of a distribution, estimate the differential entropy.

Several estimation methods are available using the `method` parameter. By
default, a method is selected based the size of the sample.

Parameters
----------
values : sequence
    Sample from a continuous distribution.
window_length : int, optional
    Window length for computing Vasicek estimate. Must be an integer
    between 1 and half of the sample size. If ``None`` (the default), it
    uses the heuristic value

    .. math::
        \left \lfloor \sqrt{n} + 0.5 \right \rfloor

    where :math:`n` is the sample size. This heuristic was originally
    proposed in [2]_ and has become common in the literature.
base : float, optional
    The logarithmic base to use, defaults to ``e`` (natural logarithm).
axis : int, optional
    The axis along which the differential entropy is calculated.
    Default is 0.
method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional
    The method used to estimate the differential entropy from the sample.
    Default is ``'auto'``.  See Notes for more information.

Returns
-------
entropy : float
    The calculated differential entropy.

Notes
-----
This function will converge to the true differential entropy in the limit

.. math::
    n \to \infty, \quad m \to \infty, \quad \frac{m}{n} \to 0

The optimal choice of ``window_length`` for a given sample size depends on
the (unknown) distribution. Typically, the smoother the density of the
distribution, the larger the optimal value of ``window_length`` [1]_.

The following options are available for the `method` parameter.

* ``'vasicek'`` uses the estimator presented in [1]_. This is
  one of the first and most influential estimators of differential entropy.
* ``'van es'`` uses the bias-corrected estimator presented in [3]_, which
  is not only consistent but, under some conditions, asymptotically normal.
* ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown
  in simulation to have smaller bias and mean squared error than
  the Vasicek estimator.
* ``'correa'`` uses the estimator presented in [5]_ based on local linear
  regression. In a simulation study, it had consistently smaller mean
  square error than the Vasiceck estimator, but it is more expensive to
  compute.
* ``'auto'`` selects the method automatically (default). Currently,
  this selects ``'van es'`` for very small samples (&lt;10), ``'ebrahimi'``
  for moderate sample sizes (11-1000), and ``'vasicek'`` for larger
  samples, but this behavior is subject to change in future versions.

All estimators are implemented as described in [6]_.

References
----------
.. [1] Vasicek, O. (1976). A test for normality based on sample entropy.
       Journal of the Royal Statistical Society:
       Series B (Methodological), 38(1), 54-59.
.. [2] Crzcgorzewski, P., &amp; Wirczorkowski, R. (1999). Entropy-based
       goodness-of-fit test for exponentiality. Communications in
       Statistics-Theory and Methods, 28(5), 1183-1202.
.. [3] Van Es, B. (1992). Estimating functionals related to a density by a
       class of statistics based on spacings. Scandinavian Journal of
       Statistics, 61-72.
.. [4] Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures
       of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234.
.. [5] Correa, J. C. (1995). A new estimator of entropy. Communications
       in Statistics-Theory and Methods, 24(10), 2439-2449.
.. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.
       Annals of Data Science, 2(2), 231-241.
       https://link.springer.com/article/10.1007/s40745-015-0045-9

Examples
--------
&gt;&gt;&gt; from scipy.stats import differential_entropy, norm

Entropy of a standard normal distribution:

&gt;&gt;&gt; rng = np.random.default_rng()
&gt;&gt;&gt; values = rng.standard_normal(100)
&gt;&gt;&gt; differential_entropy(values)
1.3407817436640392

Compare with the true entropy:

&gt;&gt;&gt; float(norm.entropy())
1.4189385332046727

For several sample sizes between 5 and 1000, compare the accuracy of
the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,
compare the root mean squared error (over 1000 trials) between the estimate
and the true differential entropy of the distribution.

&gt;&gt;&gt; from scipy import stats
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; def rmse(res, expected):
...     '''Root mean squared error'''
...     return np.sqrt(np.mean((res - expected)**2))
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; a, b = np.log10(5), np.log10(1000)
&gt;&gt;&gt; ns = np.round(np.logspace(a, b, 10)).astype(int)
&gt;&gt;&gt; reps = 1000  # number of repetitions for each sample size
&gt;&gt;&gt; expected = stats.expon.entropy()
&gt;&gt;&gt;
&gt;&gt;&gt; method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}
&gt;&gt;&gt; for method in method_errors:
...     for n in ns:
...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)
...        res = stats.differential_entropy(rvs, method=method, axis=-1)
...        error = rmse(res, expected)
...        method_errors[method].append(error)
&gt;&gt;&gt;
&gt;&gt;&gt; for method, errors in method_errors.items():
...     plt.loglog(ns, errors, label=method)
&gt;&gt;&gt;
&gt;&gt;&gt; plt.legend()
&gt;&gt;&gt; plt.xlabel('sample size')
&gt;&gt;&gt; plt.ylabel('RMSE (1000 trials)')
&gt;&gt;&gt; plt.title('Entropy Estimator Error (Exponential Distribution)')</pre> <div class="fragment"><div class="line"><span class="lineno">   98</span>) -&gt; Union[np.number, np.ndarray]:</div>
<div class="line"><span class="lineno">   99</span>    <span class="stringliteral">r&quot;&quot;&quot;Given a sample of a distribution, estimate the differential entropy.</span></div>
<div class="line"><span class="lineno">  100</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  101</span><span class="stringliteral">    Several estimation methods are available using the `method` parameter. By</span></div>
<div class="line"><span class="lineno">  102</span><span class="stringliteral">    default, a method is selected based the size of the sample.</span></div>
<div class="line"><span class="lineno">  103</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  104</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  105</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  106</span><span class="stringliteral">    values : sequence</span></div>
<div class="line"><span class="lineno">  107</span><span class="stringliteral">        Sample from a continuous distribution.</span></div>
<div class="line"><span class="lineno">  108</span><span class="stringliteral">    window_length : int, optional</span></div>
<div class="line"><span class="lineno">  109</span><span class="stringliteral">        Window length for computing Vasicek estimate. Must be an integer</span></div>
<div class="line"><span class="lineno">  110</span><span class="stringliteral">        between 1 and half of the sample size. If ``None`` (the default), it</span></div>
<div class="line"><span class="lineno">  111</span><span class="stringliteral">        uses the heuristic value</span></div>
<div class="line"><span class="lineno">  112</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  113</span><span class="stringliteral">        .. math::</span></div>
<div class="line"><span class="lineno">  114</span><span class="stringliteral">            \left \lfloor \sqrt{n} + 0.5 \right \rfloor</span></div>
<div class="line"><span class="lineno">  115</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  116</span><span class="stringliteral">        where :math:`n` is the sample size. This heuristic was originally</span></div>
<div class="line"><span class="lineno">  117</span><span class="stringliteral">        proposed in [2]_ and has become common in the literature.</span></div>
<div class="line"><span class="lineno">  118</span><span class="stringliteral">    base : float, optional</span></div>
<div class="line"><span class="lineno">  119</span><span class="stringliteral">        The logarithmic base to use, defaults to ``e`` (natural logarithm).</span></div>
<div class="line"><span class="lineno">  120</span><span class="stringliteral">    axis : int, optional</span></div>
<div class="line"><span class="lineno">  121</span><span class="stringliteral">        The axis along which the differential entropy is calculated.</span></div>
<div class="line"><span class="lineno">  122</span><span class="stringliteral">        Default is 0.</span></div>
<div class="line"><span class="lineno">  123</span><span class="stringliteral">    method : {&#39;vasicek&#39;, &#39;van es&#39;, &#39;ebrahimi&#39;, &#39;correa&#39;, &#39;auto&#39;}, optional</span></div>
<div class="line"><span class="lineno">  124</span><span class="stringliteral">        The method used to estimate the differential entropy from the sample.</span></div>
<div class="line"><span class="lineno">  125</span><span class="stringliteral">        Default is ``&#39;auto&#39;``.  See Notes for more information.</span></div>
<div class="line"><span class="lineno">  126</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  127</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  128</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  129</span><span class="stringliteral">    entropy : float</span></div>
<div class="line"><span class="lineno">  130</span><span class="stringliteral">        The calculated differential entropy.</span></div>
<div class="line"><span class="lineno">  131</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  132</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  133</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  134</span><span class="stringliteral">    This function will converge to the true differential entropy in the limit</span></div>
<div class="line"><span class="lineno">  135</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  136</span><span class="stringliteral">    .. math::</span></div>
<div class="line"><span class="lineno">  137</span><span class="stringliteral">        n \to \infty, \quad m \to \infty, \quad \frac{m}{n} \to 0</span></div>
<div class="line"><span class="lineno">  138</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  139</span><span class="stringliteral">    The optimal choice of ``window_length`` for a given sample size depends on</span></div>
<div class="line"><span class="lineno">  140</span><span class="stringliteral">    the (unknown) distribution. Typically, the smoother the density of the</span></div>
<div class="line"><span class="lineno">  141</span><span class="stringliteral">    distribution, the larger the optimal value of ``window_length`` [1]_.</span></div>
<div class="line"><span class="lineno">  142</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  143</span><span class="stringliteral">    The following options are available for the `method` parameter.</span></div>
<div class="line"><span class="lineno">  144</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  145</span><span class="stringliteral">    * ``&#39;vasicek&#39;`` uses the estimator presented in [1]_. This is</span></div>
<div class="line"><span class="lineno">  146</span><span class="stringliteral">      one of the first and most influential estimators of differential entropy.</span></div>
<div class="line"><span class="lineno">  147</span><span class="stringliteral">    * ``&#39;van es&#39;`` uses the bias-corrected estimator presented in [3]_, which</span></div>
<div class="line"><span class="lineno">  148</span><span class="stringliteral">      is not only consistent but, under some conditions, asymptotically normal.</span></div>
<div class="line"><span class="lineno">  149</span><span class="stringliteral">    * ``&#39;ebrahimi&#39;`` uses an estimator presented in [4]_, which was shown</span></div>
<div class="line"><span class="lineno">  150</span><span class="stringliteral">      in simulation to have smaller bias and mean squared error than</span></div>
<div class="line"><span class="lineno">  151</span><span class="stringliteral">      the Vasicek estimator.</span></div>
<div class="line"><span class="lineno">  152</span><span class="stringliteral">    * ``&#39;correa&#39;`` uses the estimator presented in [5]_ based on local linear</span></div>
<div class="line"><span class="lineno">  153</span><span class="stringliteral">      regression. In a simulation study, it had consistently smaller mean</span></div>
<div class="line"><span class="lineno">  154</span><span class="stringliteral">      square error than the Vasiceck estimator, but it is more expensive to</span></div>
<div class="line"><span class="lineno">  155</span><span class="stringliteral">      compute.</span></div>
<div class="line"><span class="lineno">  156</span><span class="stringliteral">    * ``&#39;auto&#39;`` selects the method automatically (default). Currently,</span></div>
<div class="line"><span class="lineno">  157</span><span class="stringliteral">      this selects ``&#39;van es&#39;`` for very small samples (&lt;10), ``&#39;ebrahimi&#39;``</span></div>
<div class="line"><span class="lineno">  158</span><span class="stringliteral">      for moderate sample sizes (11-1000), and ``&#39;vasicek&#39;`` for larger</span></div>
<div class="line"><span class="lineno">  159</span><span class="stringliteral">      samples, but this behavior is subject to change in future versions.</span></div>
<div class="line"><span class="lineno">  160</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  161</span><span class="stringliteral">    All estimators are implemented as described in [6]_.</span></div>
<div class="line"><span class="lineno">  162</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  163</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">  164</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  165</span><span class="stringliteral">    .. [1] Vasicek, O. (1976). A test for normality based on sample entropy.</span></div>
<div class="line"><span class="lineno">  166</span><span class="stringliteral">           Journal of the Royal Statistical Society:</span></div>
<div class="line"><span class="lineno">  167</span><span class="stringliteral">           Series B (Methodological), 38(1), 54-59.</span></div>
<div class="line"><span class="lineno">  168</span><span class="stringliteral">    .. [2] Crzcgorzewski, P., &amp; Wirczorkowski, R. (1999). Entropy-based</span></div>
<div class="line"><span class="lineno">  169</span><span class="stringliteral">           goodness-of-fit test for exponentiality. Communications in</span></div>
<div class="line"><span class="lineno">  170</span><span class="stringliteral">           Statistics-Theory and Methods, 28(5), 1183-1202.</span></div>
<div class="line"><span class="lineno">  171</span><span class="stringliteral">    .. [3] Van Es, B. (1992). Estimating functionals related to a density by a</span></div>
<div class="line"><span class="lineno">  172</span><span class="stringliteral">           class of statistics based on spacings. Scandinavian Journal of</span></div>
<div class="line"><span class="lineno">  173</span><span class="stringliteral">           Statistics, 61-72.</span></div>
<div class="line"><span class="lineno">  174</span><span class="stringliteral">    .. [4] Ebrahimi, N., Pflughoeft, K., &amp; Soofi, E. S. (1994). Two measures</span></div>
<div class="line"><span class="lineno">  175</span><span class="stringliteral">           of sample entropy. Statistics &amp; Probability Letters, 20(3), 225-234.</span></div>
<div class="line"><span class="lineno">  176</span><span class="stringliteral">    .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications</span></div>
<div class="line"><span class="lineno">  177</span><span class="stringliteral">           in Statistics-Theory and Methods, 24(10), 2439-2449.</span></div>
<div class="line"><span class="lineno">  178</span><span class="stringliteral">    .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.</span></div>
<div class="line"><span class="lineno">  179</span><span class="stringliteral">           Annals of Data Science, 2(2), 231-241.</span></div>
<div class="line"><span class="lineno">  180</span><span class="stringliteral">           https://link.springer.com/article/10.1007/s40745-015-0045-9</span></div>
<div class="line"><span class="lineno">  181</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  182</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno">  183</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  184</span><span class="stringliteral">    &gt;&gt;&gt; from scipy.stats import differential_entropy, norm</span></div>
<div class="line"><span class="lineno">  185</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  186</span><span class="stringliteral">    Entropy of a standard normal distribution:</span></div>
<div class="line"><span class="lineno">  187</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  188</span><span class="stringliteral">    &gt;&gt;&gt; rng = np.random.default_rng()</span></div>
<div class="line"><span class="lineno">  189</span><span class="stringliteral">    &gt;&gt;&gt; values = rng.standard_normal(100)</span></div>
<div class="line"><span class="lineno">  190</span><span class="stringliteral">    &gt;&gt;&gt; differential_entropy(values)</span></div>
<div class="line"><span class="lineno">  191</span><span class="stringliteral">    1.3407817436640392</span></div>
<div class="line"><span class="lineno">  192</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  193</span><span class="stringliteral">    Compare with the true entropy:</span></div>
<div class="line"><span class="lineno">  194</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  195</span><span class="stringliteral">    &gt;&gt;&gt; float(norm.entropy())</span></div>
<div class="line"><span class="lineno">  196</span><span class="stringliteral">    1.4189385332046727</span></div>
<div class="line"><span class="lineno">  197</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  198</span><span class="stringliteral">    For several sample sizes between 5 and 1000, compare the accuracy of</span></div>
<div class="line"><span class="lineno">  199</span><span class="stringliteral">    the ``&#39;vasicek&#39;``, ``&#39;van es&#39;``, and ``&#39;ebrahimi&#39;`` methods. Specifically,</span></div>
<div class="line"><span class="lineno">  200</span><span class="stringliteral">    compare the root mean squared error (over 1000 trials) between the estimate</span></div>
<div class="line"><span class="lineno">  201</span><span class="stringliteral">    and the true differential entropy of the distribution.</span></div>
<div class="line"><span class="lineno">  202</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  203</span><span class="stringliteral">    &gt;&gt;&gt; from scipy import stats</span></div>
<div class="line"><span class="lineno">  204</span><span class="stringliteral">    &gt;&gt;&gt; import matplotlib.pyplot as plt</span></div>
<div class="line"><span class="lineno">  205</span><span class="stringliteral">    &gt;&gt;&gt;</span></div>
<div class="line"><span class="lineno">  206</span><span class="stringliteral">    &gt;&gt;&gt;</span></div>
<div class="line"><span class="lineno">  207</span><span class="stringliteral">    &gt;&gt;&gt; def rmse(res, expected):</span></div>
<div class="line"><span class="lineno">  208</span><span class="stringliteral">    ...     &#39;&#39;&#39;Root mean squared error&#39;&#39;&#39;</span></div>
<div class="line"><span class="lineno">  209</span><span class="stringliteral">    ...     return np.sqrt(np.mean((res - expected)**2))</span></div>
<div class="line"><span class="lineno">  210</span><span class="stringliteral">    &gt;&gt;&gt;</span></div>
<div class="line"><span class="lineno">  211</span><span class="stringliteral">    &gt;&gt;&gt;</span></div>
<div class="line"><span class="lineno">  212</span><span class="stringliteral">    &gt;&gt;&gt; a, b = np.log10(5), np.log10(1000)</span></div>
<div class="line"><span class="lineno">  213</span><span class="stringliteral">    &gt;&gt;&gt; ns = np.round(np.logspace(a, b, 10)).astype(int)</span></div>
<div class="line"><span class="lineno">  214</span><span class="stringliteral">    &gt;&gt;&gt; reps = 1000  # number of repetitions for each sample size</span></div>
<div class="line"><span class="lineno">  215</span><span class="stringliteral">    &gt;&gt;&gt; expected = stats.expon.entropy()</span></div>
<div class="line"><span class="lineno">  216</span><span class="stringliteral">    &gt;&gt;&gt;</span></div>
<div class="line"><span class="lineno">  217</span><span class="stringliteral">    &gt;&gt;&gt; method_errors = {&#39;vasicek&#39;: [], &#39;van es&#39;: [], &#39;ebrahimi&#39;: []}</span></div>
<div class="line"><span class="lineno">  218</span><span class="stringliteral">    &gt;&gt;&gt; for method in method_errors:</span></div>
<div class="line"><span class="lineno">  219</span><span class="stringliteral">    ...     for n in ns:</span></div>
<div class="line"><span class="lineno">  220</span><span class="stringliteral">    ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)</span></div>
<div class="line"><span class="lineno">  221</span><span class="stringliteral">    ...        res = stats.differential_entropy(rvs, method=method, axis=-1)</span></div>
<div class="line"><span class="lineno">  222</span><span class="stringliteral">    ...        error = rmse(res, expected)</span></div>
<div class="line"><span class="lineno">  223</span><span class="stringliteral">    ...        method_errors[method].append(error)</span></div>
<div class="line"><span class="lineno">  224</span><span class="stringliteral">    &gt;&gt;&gt;</span></div>
<div class="line"><span class="lineno">  225</span><span class="stringliteral">    &gt;&gt;&gt; for method, errors in method_errors.items():</span></div>
<div class="line"><span class="lineno">  226</span><span class="stringliteral">    ...     plt.loglog(ns, errors, label=method)</span></div>
<div class="line"><span class="lineno">  227</span><span class="stringliteral">    &gt;&gt;&gt;</span></div>
<div class="line"><span class="lineno">  228</span><span class="stringliteral">    &gt;&gt;&gt; plt.legend()</span></div>
<div class="line"><span class="lineno">  229</span><span class="stringliteral">    &gt;&gt;&gt; plt.xlabel(&#39;sample size&#39;)</span></div>
<div class="line"><span class="lineno">  230</span><span class="stringliteral">    &gt;&gt;&gt; plt.ylabel(&#39;RMSE (1000 trials)&#39;)</span></div>
<div class="line"><span class="lineno">  231</span><span class="stringliteral">    &gt;&gt;&gt; plt.title(&#39;Entropy Estimator Error (Exponential Distribution)&#39;)</span></div>
<div class="line"><span class="lineno">  232</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  233</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  234</span><span class="stringliteral">    values = np.asarray(values)</span></div>
<div class="line"><span class="lineno">  235</span><span class="stringliteral">    values = np.moveaxis(values, axis, -1)</span></div>
<div class="line"><span class="lineno">  236</span><span class="stringliteral">    n = values.shape[-1]  # number of observations</span></div>
<div class="line"><span class="lineno">  237</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  238</span><span class="stringliteral">    if window_length is None:</span></div>
<div class="line"><span class="lineno">  239</span><span class="stringliteral">        window_length = math.floor(math.sqrt(n) + 0.5)</span></div>
<div class="line"><span class="lineno">  240</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  241</span><span class="stringliteral">    if not 2 &lt;= 2 * window_length &lt; n:</span></div>
<div class="line"><span class="lineno">  242</span><span class="stringliteral">        raise ValueError(</span></div>
<div class="line"><span class="lineno">  243</span><span class="stringliteral">            f&quot;Window length ({window_length}) must be positive and less &quot;</span></div>
<div class="line"><span class="lineno">  244</span><span class="stringliteral">            f&quot;than half the sample size ({n}).&quot;,</span></div>
<div class="line"><span class="lineno">  245</span><span class="stringliteral">        )</span></div>
<div class="line"><span class="lineno">  246</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral">    if base is not None and base &lt;= 0:</span></div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral">        raise ValueError(&quot;`base` must be a positive number or `None`.&quot;)</span></div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral">    sorted_data = np.sort(values, axis=-1)</span></div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  252</span><span class="stringliteral">    methods = {&quot;vasicek&quot;: _vasicek_entropy,</span></div>
<div class="line"><span class="lineno">  253</span><span class="stringliteral">               &quot;van es&quot;: _van_es_entropy,</span></div>
<div class="line"><span class="lineno">  254</span><span class="stringliteral">               &quot;correa&quot;: _correa_entropy,</span></div>
<div class="line"><span class="lineno">  255</span><span class="stringliteral">               &quot;ebrahimi&quot;: _ebrahimi_entropy,</span></div>
<div class="line"><span class="lineno">  256</span><span class="stringliteral">               &quot;auto&quot;: _vasicek_entropy}</span></div>
<div class="line"><span class="lineno">  257</span><span class="stringliteral">    method = method.lower()</span></div>
<div class="line"><span class="lineno">  258</span><span class="stringliteral">    if method not in methods:</span></div>
<div class="line"><span class="lineno">  259</span><span class="stringliteral">        message = f&quot;`method` must be one of {set(methods)}&quot;</span></div>
<div class="line"><span class="lineno">  260</span><span class="stringliteral">        raise ValueError(message)</span></div>
<div class="line"><span class="lineno">  261</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  262</span><span class="stringliteral">    if method == &quot;auto&quot;:</span></div>
<div class="line"><span class="lineno">  263</span><span class="stringliteral">        if n &lt;= 10:</span></div>
<div class="line"><span class="lineno">  264</span><span class="stringliteral">            method = &#39;van es&#39;</span></div>
<div class="line"><span class="lineno">  265</span><span class="stringliteral">        elif n &lt;= 1000:</span></div>
<div class="line"><span class="lineno">  266</span><span class="stringliteral">            method = &#39;ebrahimi&#39;</span></div>
<div class="line"><span class="lineno">  267</span><span class="stringliteral">        else:</span></div>
<div class="line"><span class="lineno">  268</span><span class="stringliteral">            method = &#39;vasicek&#39;</span></div>
<div class="line"><span class="lineno">  269</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  270</span><span class="stringliteral">    res = methods[method](sorted_data, window_length)</span></div>
<div class="line"><span class="lineno">  271</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  272</span><span class="stringliteral">    if base is not None:</span></div>
<div class="line"><span class="lineno">  273</span><span class="stringliteral">        res /= np.log(base)</span></div>
<div class="line"><span class="lineno">  274</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  275</span><span class="stringliteral">    return res</span></div>
<div class="line"><span class="lineno">  276</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  277</span><span class="stringliteral"></span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="adda40496c374a928e072c08fbb643ee0" name="adda40496c374a928e072c08fbb643ee0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adda40496c374a928e072c08fbb643ee0">&#9670;&#160;</a></span>entropy()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"> Union[np.number, np.ndarray] scipy.stats._entropy.entropy </td>
          <td>(</td>
          <td class="paramtype">np.typing.ArrayLike&#160;</td>
          <td class="paramname"><em>pk</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[np.typing.ArrayLike] &#160;</td>
          <td class="paramname"><em>qk</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Optional[float] &#160;</td>
          <td class="paramname"><em>base</em> = <code>None</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int &#160;</td>
          <td class="paramname"><em>axis</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Calculate the entropy of a distribution for given probability values.

If only probabilities `pk` are given, the entropy is calculated as
``S = -sum(pk * log(pk), axis=axis)``.

If `qk` is not None, then compute the Kullback-Leibler divergence
``S = sum(pk * log(pk / qk), axis=axis)``.

This routine will normalize `pk` and `qk` if they don't sum to 1.

Parameters
----------
pk : array_like
    Defines the (discrete) distribution. Along each axis-slice of ``pk``,
    element ``i`` is the  (possibly unnormalized) probability of event
    ``i``.
qk : array_like, optional
    Sequence against which the relative entropy is computed. Should be in
    the same format as `pk`.
base : float, optional
    The logarithmic base to use, defaults to ``e`` (natural logarithm).
axis : int, optional
    The axis along which the entropy is calculated. Default is 0.

Returns
-------
S : {float, array_like}
    The calculated entropy.

Examples
--------

&gt;&gt;&gt; from scipy.stats import entropy

Bernoulli trial with different p.
The outcome of a fair coin is the most uncertain:

&gt;&gt;&gt; entropy([1/2, 1/2], base=2)
1.0

The outcome of a biased coin is less uncertain:

&gt;&gt;&gt; entropy([9/10, 1/10], base=2)
0.46899559358928117

Relative entropy:

&gt;&gt;&gt; entropy([1/2, 1/2], qk=[9/10, 1/10])
0.5108256237659907</pre> <div class="fragment"><div class="line"><span class="lineno">   21</span>            ) -&gt; Union[np.number, np.ndarray]:</div>
<div class="line"><span class="lineno">   22</span>    <span class="stringliteral">&quot;&quot;&quot;Calculate the entropy of a distribution for given probability values.</span></div>
<div class="line"><span class="lineno">   23</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   24</span><span class="stringliteral">    If only probabilities `pk` are given, the entropy is calculated as</span></div>
<div class="line"><span class="lineno">   25</span><span class="stringliteral">    ``S = -sum(pk * log(pk), axis=axis)``.</span></div>
<div class="line"><span class="lineno">   26</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   27</span><span class="stringliteral">    If `qk` is not None, then compute the Kullback-Leibler divergence</span></div>
<div class="line"><span class="lineno">   28</span><span class="stringliteral">    ``S = sum(pk * log(pk / qk), axis=axis)``.</span></div>
<div class="line"><span class="lineno">   29</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   30</span><span class="stringliteral">    This routine will normalize `pk` and `qk` if they don&#39;t sum to 1.</span></div>
<div class="line"><span class="lineno">   31</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   32</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   33</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   34</span><span class="stringliteral">    pk : array_like</span></div>
<div class="line"><span class="lineno">   35</span><span class="stringliteral">        Defines the (discrete) distribution. Along each axis-slice of ``pk``,</span></div>
<div class="line"><span class="lineno">   36</span><span class="stringliteral">        element ``i`` is the  (possibly unnormalized) probability of event</span></div>
<div class="line"><span class="lineno">   37</span><span class="stringliteral">        ``i``.</span></div>
<div class="line"><span class="lineno">   38</span><span class="stringliteral">    qk : array_like, optional</span></div>
<div class="line"><span class="lineno">   39</span><span class="stringliteral">        Sequence against which the relative entropy is computed. Should be in</span></div>
<div class="line"><span class="lineno">   40</span><span class="stringliteral">        the same format as `pk`.</span></div>
<div class="line"><span class="lineno">   41</span><span class="stringliteral">    base : float, optional</span></div>
<div class="line"><span class="lineno">   42</span><span class="stringliteral">        The logarithmic base to use, defaults to ``e`` (natural logarithm).</span></div>
<div class="line"><span class="lineno">   43</span><span class="stringliteral">    axis : int, optional</span></div>
<div class="line"><span class="lineno">   44</span><span class="stringliteral">        The axis along which the entropy is calculated. Default is 0.</span></div>
<div class="line"><span class="lineno">   45</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   46</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">   47</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">   48</span><span class="stringliteral">    S : {float, array_like}</span></div>
<div class="line"><span class="lineno">   49</span><span class="stringliteral">        The calculated entropy.</span></div>
<div class="line"><span class="lineno">   50</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   51</span><span class="stringliteral">    Examples</span></div>
<div class="line"><span class="lineno">   52</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">   53</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   54</span><span class="stringliteral">    &gt;&gt;&gt; from scipy.stats import entropy</span></div>
<div class="line"><span class="lineno">   55</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   56</span><span class="stringliteral">    Bernoulli trial with different p.</span></div>
<div class="line"><span class="lineno">   57</span><span class="stringliteral">    The outcome of a fair coin is the most uncertain:</span></div>
<div class="line"><span class="lineno">   58</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   59</span><span class="stringliteral">    &gt;&gt;&gt; entropy([1/2, 1/2], base=2)</span></div>
<div class="line"><span class="lineno">   60</span><span class="stringliteral">    1.0</span></div>
<div class="line"><span class="lineno">   61</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   62</span><span class="stringliteral">    The outcome of a biased coin is less uncertain:</span></div>
<div class="line"><span class="lineno">   63</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   64</span><span class="stringliteral">    &gt;&gt;&gt; entropy([9/10, 1/10], base=2)</span></div>
<div class="line"><span class="lineno">   65</span><span class="stringliteral">    0.46899559358928117</span></div>
<div class="line"><span class="lineno">   66</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   67</span><span class="stringliteral">    Relative entropy:</span></div>
<div class="line"><span class="lineno">   68</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   69</span><span class="stringliteral">    &gt;&gt;&gt; entropy([1/2, 1/2], qk=[9/10, 1/10])</span></div>
<div class="line"><span class="lineno">   70</span><span class="stringliteral">    0.5108256237659907</span></div>
<div class="line"><span class="lineno">   71</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   72</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   73</span>    <span class="keywordflow">if</span> base <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> base &lt;= 0:</div>
<div class="line"><span class="lineno">   74</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;`base` must be a positive number or `None`.&quot;</span>)</div>
<div class="line"><span class="lineno">   75</span> </div>
<div class="line"><span class="lineno">   76</span>    pk = np.asarray(pk)</div>
<div class="line"><span class="lineno">   77</span>    pk = 1.0*pk / np.sum(pk, axis=axis, keepdims=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">   78</span>    <span class="keywordflow">if</span> qk <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">   79</span>        vec = special.entr(pk)</div>
<div class="line"><span class="lineno">   80</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   81</span>        qk = np.asarray(qk)</div>
<div class="line"><span class="lineno">   82</span>        pk, qk = np.broadcast_arrays(pk, qk)</div>
<div class="line"><span class="lineno">   83</span>        qk = 1.0*qk / np.sum(qk, axis=axis, keepdims=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">   84</span>        vec = special.rel_entr(pk, qk)</div>
<div class="line"><span class="lineno">   85</span>    S = np.sum(vec, axis=axis)</div>
<div class="line"><span class="lineno">   86</span>    <span class="keywordflow">if</span> base <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">   87</span>        S /= np.log(base)</div>
<div class="line"><span class="lineno">   88</span>    <span class="keywordflow">return</span> S</div>
<div class="line"><span class="lineno">   89</span> </div>
<div class="line"><span class="lineno">   90</span> </div>
</div><!-- fragment -->
</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
