<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.ensemble.tests.test_gradient_boosting_loss_functions Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1ensemble.html">ensemble</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests.html">tests</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html">test_gradient_boosting_loss_functions</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">sklearn.ensemble.tests.test_gradient_boosting_loss_functions Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ad912eb991744edf9c17050f7ee30f52a" id="r_ad912eb991744edf9c17050f7ee30f52a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#ad912eb991744edf9c17050f7ee30f52a">test_binomial_deviance</a> ()</td></tr>
<tr class="separator:ad912eb991744edf9c17050f7ee30f52a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a109fe5017d8dce1c9526827f53b68024" id="r_a109fe5017d8dce1c9526827f53b68024"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#a109fe5017d8dce1c9526827f53b68024">test_sample_weight_smoke</a> ()</td></tr>
<tr class="separator:a109fe5017d8dce1c9526827f53b68024"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9ec9e89f04f03b17a4d05fbd5f702155" id="r_a9ec9e89f04f03b17a4d05fbd5f702155"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#a9ec9e89f04f03b17a4d05fbd5f702155">test_sample_weight_init_estimators</a> ()</td></tr>
<tr class="separator:a9ec9e89f04f03b17a4d05fbd5f702155"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a15dfc6cfc20b093a63d9e10b1c1d8454" id="r_a15dfc6cfc20b093a63d9e10b1c1d8454"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#a15dfc6cfc20b093a63d9e10b1c1d8454">test_quantile_loss_function</a> ()</td></tr>
<tr class="separator:a15dfc6cfc20b093a63d9e10b1c1d8454"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae2de621de8b8b0127d28985b14c43163" id="r_ae2de621de8b8b0127d28985b14c43163"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#ae2de621de8b8b0127d28985b14c43163">test_sample_weight_deviance</a> ()</td></tr>
<tr class="separator:ae2de621de8b8b0127d28985b14c43163"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4d1ef009a2356e29fd5c04f3b7730282" id="r_a4d1ef009a2356e29fd5c04f3b7730282"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#a4d1ef009a2356e29fd5c04f3b7730282">test_multinomial_deviance</a> (n_classes, n_samples, global_random_seed)</td></tr>
<tr class="separator:a4d1ef009a2356e29fd5c04f3b7730282"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a12583274aabcbc14b49cda33d878f05c" id="r_a12583274aabcbc14b49cda33d878f05c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#a12583274aabcbc14b49cda33d878f05c">test_mdl_computation_weighted</a> ()</td></tr>
<tr class="separator:a12583274aabcbc14b49cda33d878f05c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7eaeb4832c1ffa5a238f737ed0c5de83" id="r_a7eaeb4832c1ffa5a238f737ed0c5de83"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#a7eaeb4832c1ffa5a238f737ed0c5de83">test_mdl_exception</a> (<a class="el" href="__blas__subroutines_8h.html#a25eafceb38c8e75bc60701fea6623f71">n</a>)</td></tr>
<tr class="separator:a7eaeb4832c1ffa5a238f737ed0c5de83"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04123da7115196ab1b5ae9e795a1bffd" id="r_a04123da7115196ab1b5ae9e795a1bffd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#a04123da7115196ab1b5ae9e795a1bffd">test_init_raw_predictions_shapes</a> ()</td></tr>
<tr class="separator:a04123da7115196ab1b5ae9e795a1bffd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad0638a8d3f2c41da5307577589c47a3e" id="r_ad0638a8d3f2c41da5307577589c47a3e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#ad0638a8d3f2c41da5307577589c47a3e">test_init_raw_predictions_values</a> (global_random_seed)</td></tr>
<tr class="separator:ad0638a8d3f2c41da5307577589c47a3e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad30934e9932befd038702c962c6751cf" id="r_ad30934e9932befd038702c962c6751cf"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#ad30934e9932befd038702c962c6751cf">test_lad_equals_quantiles</a> (global_random_seed, <a class="el" href="__blas__subroutines_8h.html#a29dda7d0819a860e921db821deb590c9">alpha</a>)</td></tr>
<tr class="separator:ad30934e9932befd038702c962c6751cf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1841396e4ccb7bc7b3ab26a7071e8bdb" id="r_a1841396e4ccb7bc7b3ab26a7071e8bdb"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1tests_1_1test__gradient__boosting__loss__functions.html#a1841396e4ccb7bc7b3ab26a7071e8bdb">test_exponential_loss</a> ()</td></tr>
<tr class="separator:a1841396e4ccb7bc7b3ab26a7071e8bdb"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Testing for the gradient boosting loss functions and initial estimators.
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="ad912eb991744edf9c17050f7ee30f52a" name="ad912eb991744edf9c17050f7ee30f52a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad912eb991744edf9c17050f7ee30f52a">&#9670;&#160;</a></span>test_binomial_deviance()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_binomial_deviance </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   23</span><span class="keyword">def </span>test_binomial_deviance():</div>
<div class="line"><span class="lineno">   24</span>    <span class="comment"># Check binomial deviance loss.</span></div>
<div class="line"><span class="lineno">   25</span>    <span class="comment"># Check against alternative definitions in ESLII.</span></div>
<div class="line"><span class="lineno">   26</span>    bd = BinomialDeviance(2)</div>
<div class="line"><span class="lineno">   27</span> </div>
<div class="line"><span class="lineno">   28</span>    <span class="comment"># pred has the same BD for y in {0, 1}</span></div>
<div class="line"><span class="lineno">   29</span>    <span class="keyword">assert</span> bd(np.array([0.0]), np.array([0.0])) == bd(np.array([1.0]), np.array([0.0]))</div>
<div class="line"><span class="lineno">   30</span> </div>
<div class="line"><span class="lineno">   31</span>    <span class="keyword">assert</span> bd(np.array([1.0, 1, 1]), np.array([100.0, 100, 100])) == approx(0)</div>
<div class="line"><span class="lineno">   32</span>    <span class="keyword">assert</span> bd(np.array([1.0, 0, 0]), np.array([100.0, -100, -100])) == approx(0)</div>
<div class="line"><span class="lineno">   33</span> </div>
<div class="line"><span class="lineno">   34</span>    <span class="comment"># check if same results as alternative definition of deviance, from ESLII</span></div>
<div class="line"><span class="lineno">   35</span>    <span class="comment"># Eq. (10.18): -loglike = log(1 + exp(-2*z*f))</span></div>
<div class="line"><span class="lineno">   36</span>    <span class="comment"># Note:</span></div>
<div class="line"><span class="lineno">   37</span>    <span class="comment"># - We use y = {0, 1}, ESL (10.18) uses z in {-1, 1}, hence y=2*y-1</span></div>
<div class="line"><span class="lineno">   38</span>    <span class="comment"># - ESL 2*f = pred_raw, hence the factor 2 of ESL disappears.</span></div>
<div class="line"><span class="lineno">   39</span>    <span class="comment"># - Deviance = -2*loglike + .., hence a factor of 2 in front.</span></div>
<div class="line"><span class="lineno">   40</span>    <span class="keyword">def </span>alt_dev(y, raw_pred):</div>
<div class="line"><span class="lineno">   41</span>        z = 2 * y - 1</div>
<div class="line"><span class="lineno">   42</span>        <span class="keywordflow">return</span> 2 * np.mean(np.log(1 + np.exp(-z * raw_pred)))</div>
<div class="line"><span class="lineno">   43</span> </div>
<div class="line"><span class="lineno">   44</span>    test_data = product(</div>
<div class="line"><span class="lineno">   45</span>        (np.array([0.0, 0, 0]), np.array([1.0, 1, 1])),</div>
<div class="line"><span class="lineno">   46</span>        (np.array([-5.0, -5, -5]), np.array([3.0, 3, 3])),</div>
<div class="line"><span class="lineno">   47</span>    )</div>
<div class="line"><span class="lineno">   48</span> </div>
<div class="line"><span class="lineno">   49</span>    <span class="keywordflow">for</span> datum <span class="keywordflow">in</span> test_data:</div>
<div class="line"><span class="lineno">   50</span>        <span class="keyword">assert</span> bd(*datum) == approx(alt_dev(*datum))</div>
<div class="line"><span class="lineno">   51</span> </div>
<div class="line"><span class="lineno">   52</span>    <span class="comment"># check the negative gradient against alternative formula from ESLII</span></div>
<div class="line"><span class="lineno">   53</span>    <span class="comment"># Note: negative_gradient is half the negative gradient.</span></div>
<div class="line"><span class="lineno">   54</span>    <span class="keyword">def </span>alt_ng(y, raw_pred):</div>
<div class="line"><span class="lineno">   55</span>        z = 2 * y - 1</div>
<div class="line"><span class="lineno">   56</span>        <span class="keywordflow">return</span> z / (1 + np.exp(z * raw_pred))</div>
<div class="line"><span class="lineno">   57</span> </div>
<div class="line"><span class="lineno">   58</span>    <span class="keywordflow">for</span> datum <span class="keywordflow">in</span> test_data:</div>
<div class="line"><span class="lineno">   59</span>        <span class="keyword">assert</span> bd.negative_gradient(*datum) == approx(alt_ng(*datum))</div>
<div class="line"><span class="lineno">   60</span> </div>
<div class="line"><span class="lineno">   61</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a1841396e4ccb7bc7b3ab26a7071e8bdb" name="a1841396e4ccb7bc7b3ab26a7071e8bdb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1841396e4ccb7bc7b3ab26a7071e8bdb">&#9670;&#160;</a></span>test_exponential_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_exponential_loss </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check that we compute the negative gradient of the exponential loss.

Non-regression test for:
https://github.com/scikit-learn/scikit-learn/issues/9666
</pre> <div class="fragment"><div class="line"><span class="lineno">  324</span><span class="keyword">def </span>test_exponential_loss():</div>
<div class="line"><span class="lineno">  325</span>    <span class="stringliteral">&quot;&quot;&quot;Check that we compute the negative gradient of the exponential loss.</span></div>
<div class="line"><span class="lineno">  326</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  327</span><span class="stringliteral">    Non-regression test for:</span></div>
<div class="line"><span class="lineno">  328</span><span class="stringliteral">    https://github.com/scikit-learn/scikit-learn/issues/9666</span></div>
<div class="line"><span class="lineno">  329</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  330</span>    loss = ExponentialLoss(n_classes=2)</div>
<div class="line"><span class="lineno">  331</span>    y_true = np.array([0])</div>
<div class="line"><span class="lineno">  332</span>    y_pred = np.array([0])</div>
<div class="line"><span class="lineno">  333</span>    <span class="comment"># we expect to have loss = exp(0) = 1</span></div>
<div class="line"><span class="lineno">  334</span>    <span class="keyword">assert</span> loss(y_true, y_pred) == pytest.approx(1)</div>
<div class="line"><span class="lineno">  335</span>    <span class="comment"># we expect to have negative gradient = -1 * (1 * exp(0)) = -1</span></div>
<div class="line"><span class="lineno">  336</span>    assert_allclose(loss.negative_gradient(y_true, y_pred), -1)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a04123da7115196ab1b5ae9e795a1bffd" name="a04123da7115196ab1b5ae9e795a1bffd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a04123da7115196ab1b5ae9e795a1bffd">&#9670;&#160;</a></span>test_init_raw_predictions_shapes()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_init_raw_predictions_shapes </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  197</span><span class="keyword">def </span>test_init_raw_predictions_shapes():</div>
<div class="line"><span class="lineno">  198</span>    <span class="comment"># Make sure get_init_raw_predictions returns float64 arrays with shape</span></div>
<div class="line"><span class="lineno">  199</span>    <span class="comment"># (n_samples, K) where K is 1 for binary classification and regression, and</span></div>
<div class="line"><span class="lineno">  200</span>    <span class="comment"># K = n_classes for multiclass classification</span></div>
<div class="line"><span class="lineno">  201</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  202</span> </div>
<div class="line"><span class="lineno">  203</span>    n_samples = 100</div>
<div class="line"><span class="lineno">  204</span>    X = rng.normal(size=(n_samples, 5))</div>
<div class="line"><span class="lineno">  205</span>    y = rng.normal(size=n_samples)</div>
<div class="line"><span class="lineno">  206</span>    <span class="keywordflow">for</span> loss <span class="keywordflow">in</span> (</div>
<div class="line"><span class="lineno">  207</span>        LeastSquaresError(),</div>
<div class="line"><span class="lineno">  208</span>        LeastAbsoluteError(),</div>
<div class="line"><span class="lineno">  209</span>        QuantileLossFunction(),</div>
<div class="line"><span class="lineno">  210</span>        HuberLossFunction(),</div>
<div class="line"><span class="lineno">  211</span>    ):</div>
<div class="line"><span class="lineno">  212</span>        init_estimator = loss.init_estimator().fit(X, y)</div>
<div class="line"><span class="lineno">  213</span>        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)</div>
<div class="line"><span class="lineno">  214</span>        <span class="keyword">assert</span> raw_predictions.shape == (n_samples, 1)</div>
<div class="line"><span class="lineno">  215</span>        <span class="keyword">assert</span> raw_predictions.dtype == np.float64</div>
<div class="line"><span class="lineno">  216</span> </div>
<div class="line"><span class="lineno">  217</span>    y = rng.randint(0, 2, size=n_samples)</div>
<div class="line"><span class="lineno">  218</span>    <span class="keywordflow">for</span> loss <span class="keywordflow">in</span> (BinomialDeviance(n_classes=2), ExponentialLoss(n_classes=2)):</div>
<div class="line"><span class="lineno">  219</span>        init_estimator = loss.init_estimator().fit(X, y)</div>
<div class="line"><span class="lineno">  220</span>        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)</div>
<div class="line"><span class="lineno">  221</span>        <span class="keyword">assert</span> raw_predictions.shape == (n_samples, 1)</div>
<div class="line"><span class="lineno">  222</span>        <span class="keyword">assert</span> raw_predictions.dtype == np.float64</div>
<div class="line"><span class="lineno">  223</span> </div>
<div class="line"><span class="lineno">  224</span>    <span class="keywordflow">for</span> n_classes <span class="keywordflow">in</span> range(3, 5):</div>
<div class="line"><span class="lineno">  225</span>        y = rng.randint(0, n_classes, size=n_samples)</div>
<div class="line"><span class="lineno">  226</span>        loss = MultinomialDeviance(n_classes=n_classes)</div>
<div class="line"><span class="lineno">  227</span>        init_estimator = loss.init_estimator().fit(X, y)</div>
<div class="line"><span class="lineno">  228</span>        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)</div>
<div class="line"><span class="lineno">  229</span>        <span class="keyword">assert</span> raw_predictions.shape == (n_samples, n_classes)</div>
<div class="line"><span class="lineno">  230</span>        <span class="keyword">assert</span> raw_predictions.dtype == np.float64</div>
<div class="line"><span class="lineno">  231</span> </div>
<div class="line"><span class="lineno">  232</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad0638a8d3f2c41da5307577589c47a3e" name="ad0638a8d3f2c41da5307577589c47a3e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad0638a8d3f2c41da5307577589c47a3e">&#9670;&#160;</a></span>test_init_raw_predictions_values()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_init_raw_predictions_values </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  233</span><span class="keyword">def </span>test_init_raw_predictions_values(global_random_seed):</div>
<div class="line"><span class="lineno">  234</span>    <span class="comment"># Make sure the get_init_raw_predictions() returns the expected values for</span></div>
<div class="line"><span class="lineno">  235</span>    <span class="comment"># each loss.</span></div>
<div class="line"><span class="lineno">  236</span>    rng = np.random.RandomState(global_random_seed)</div>
<div class="line"><span class="lineno">  237</span> </div>
<div class="line"><span class="lineno">  238</span>    n_samples = 100</div>
<div class="line"><span class="lineno">  239</span>    X = rng.normal(size=(n_samples, 5))</div>
<div class="line"><span class="lineno">  240</span>    y = rng.normal(size=n_samples)</div>
<div class="line"><span class="lineno">  241</span> </div>
<div class="line"><span class="lineno">  242</span>    <span class="comment"># Least squares loss</span></div>
<div class="line"><span class="lineno">  243</span>    loss = LeastSquaresError()</div>
<div class="line"><span class="lineno">  244</span>    init_estimator = loss.init_estimator().fit(X, y)</div>
<div class="line"><span class="lineno">  245</span>    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)</div>
<div class="line"><span class="lineno">  246</span>    <span class="comment"># Make sure baseline prediction is the mean of all targets</span></div>
<div class="line"><span class="lineno">  247</span>    assert_allclose(raw_predictions, y.mean())</div>
<div class="line"><span class="lineno">  248</span> </div>
<div class="line"><span class="lineno">  249</span>    <span class="comment"># Least absolute and huber loss</span></div>
<div class="line"><span class="lineno">  250</span>    <span class="keywordflow">for</span> Loss <span class="keywordflow">in</span> (LeastAbsoluteError, HuberLossFunction):</div>
<div class="line"><span class="lineno">  251</span>        loss = Loss()</div>
<div class="line"><span class="lineno">  252</span>        init_estimator = loss.init_estimator().fit(X, y)</div>
<div class="line"><span class="lineno">  253</span>        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)</div>
<div class="line"><span class="lineno">  254</span>        <span class="comment"># Make sure baseline prediction is the median of all targets</span></div>
<div class="line"><span class="lineno">  255</span>        assert_allclose(raw_predictions, np.median(y))</div>
<div class="line"><span class="lineno">  256</span> </div>
<div class="line"><span class="lineno">  257</span>    <span class="comment"># Quantile loss</span></div>
<div class="line"><span class="lineno">  258</span>    <span class="keywordflow">for</span> alpha <span class="keywordflow">in</span> (0.1, 0.5, 0.9):</div>
<div class="line"><span class="lineno">  259</span>        loss = QuantileLossFunction(alpha=alpha)</div>
<div class="line"><span class="lineno">  260</span>        init_estimator = loss.init_estimator().fit(X, y)</div>
<div class="line"><span class="lineno">  261</span>        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)</div>
<div class="line"><span class="lineno">  262</span>        <span class="comment"># Make sure baseline prediction is the alpha-quantile of all targets</span></div>
<div class="line"><span class="lineno">  263</span>        assert_allclose(raw_predictions, np.percentile(y, alpha * 100))</div>
<div class="line"><span class="lineno">  264</span> </div>
<div class="line"><span class="lineno">  265</span>    y = rng.randint(0, 2, size=n_samples)</div>
<div class="line"><span class="lineno">  266</span> </div>
<div class="line"><span class="lineno">  267</span>    <span class="comment"># Binomial deviance</span></div>
<div class="line"><span class="lineno">  268</span>    loss = BinomialDeviance(n_classes=2)</div>
<div class="line"><span class="lineno">  269</span>    init_estimator = loss.init_estimator().fit(X, y)</div>
<div class="line"><span class="lineno">  270</span>    <span class="comment"># Make sure baseline prediction is equal to link_function(p), where p</span></div>
<div class="line"><span class="lineno">  271</span>    <span class="comment"># is the proba of the positive class. We want predict_proba() to return p,</span></div>
<div class="line"><span class="lineno">  272</span>    <span class="comment"># and by definition</span></div>
<div class="line"><span class="lineno">  273</span>    <span class="comment"># p = inverse_link_function(raw_prediction) = sigmoid(raw_prediction)</span></div>
<div class="line"><span class="lineno">  274</span>    <span class="comment"># So we want raw_prediction = link_function(p) = log(p / (1 - p))</span></div>
<div class="line"><span class="lineno">  275</span>    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)</div>
<div class="line"><span class="lineno">  276</span>    p = y.mean()</div>
<div class="line"><span class="lineno">  277</span>    assert_allclose(raw_predictions, np.log(p / (1 - p)))</div>
<div class="line"><span class="lineno">  278</span> </div>
<div class="line"><span class="lineno">  279</span>    <span class="comment"># Exponential loss</span></div>
<div class="line"><span class="lineno">  280</span>    loss = ExponentialLoss(n_classes=2)</div>
<div class="line"><span class="lineno">  281</span>    init_estimator = loss.init_estimator().fit(X, y)</div>
<div class="line"><span class="lineno">  282</span>    raw_predictions = loss.get_init_raw_predictions(y, init_estimator)</div>
<div class="line"><span class="lineno">  283</span>    p = y.mean()</div>
<div class="line"><span class="lineno">  284</span>    assert_allclose(raw_predictions, 0.5 * np.log(p / (1 - p)))</div>
<div class="line"><span class="lineno">  285</span> </div>
<div class="line"><span class="lineno">  286</span>    <span class="comment"># Multinomial deviance loss</span></div>
<div class="line"><span class="lineno">  287</span>    <span class="keywordflow">for</span> n_classes <span class="keywordflow">in</span> range(3, 5):</div>
<div class="line"><span class="lineno">  288</span>        y = rng.randint(0, n_classes, size=n_samples)</div>
<div class="line"><span class="lineno">  289</span>        loss = MultinomialDeviance(n_classes=n_classes)</div>
<div class="line"><span class="lineno">  290</span>        init_estimator = loss.init_estimator().fit(X, y)</div>
<div class="line"><span class="lineno">  291</span>        raw_predictions = loss.get_init_raw_predictions(y, init_estimator)</div>
<div class="line"><span class="lineno">  292</span>        <span class="keywordflow">for</span> k <span class="keywordflow">in</span> range(n_classes):</div>
<div class="line"><span class="lineno">  293</span>            p = (y == k).mean()</div>
<div class="line"><span class="lineno">  294</span>            assert_allclose(raw_predictions[:, k], np.log(p))</div>
<div class="line"><span class="lineno">  295</span> </div>
<div class="line"><span class="lineno">  296</span> </div>
<div class="line"><span class="lineno">  297</span><span class="preprocessor">@pytest.mark.parametrize(&quot;alpha&quot;, [0.4, 0.5, 0.6])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="ad30934e9932befd038702c962c6751cf" name="ad30934e9932befd038702c962c6751cf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad30934e9932befd038702c962c6751cf">&#9670;&#160;</a></span>test_lad_equals_quantiles()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_lad_equals_quantiles </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>alpha</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  298</span><span class="keyword">def </span>test_lad_equals_quantiles(global_random_seed, alpha):</div>
<div class="line"><span class="lineno">  299</span>    <span class="comment"># Make sure quantile loss with alpha = .5 is equivalent to LAD</span></div>
<div class="line"><span class="lineno">  300</span>    lad = LeastAbsoluteError()</div>
<div class="line"><span class="lineno">  301</span>    ql = QuantileLossFunction(alpha=alpha)</div>
<div class="line"><span class="lineno">  302</span> </div>
<div class="line"><span class="lineno">  303</span>    n_samples = 50</div>
<div class="line"><span class="lineno">  304</span>    rng = np.random.RandomState(global_random_seed)</div>
<div class="line"><span class="lineno">  305</span>    raw_predictions = rng.normal(size=(n_samples))</div>
<div class="line"><span class="lineno">  306</span>    y_true = rng.normal(size=(n_samples))</div>
<div class="line"><span class="lineno">  307</span> </div>
<div class="line"><span class="lineno">  308</span>    lad_loss = lad(y_true, raw_predictions)</div>
<div class="line"><span class="lineno">  309</span>    ql_loss = ql(y_true, raw_predictions)</div>
<div class="line"><span class="lineno">  310</span>    <span class="keywordflow">if</span> alpha == 0.5:</div>
<div class="line"><span class="lineno">  311</span>        <span class="keyword">assert</span> lad_loss == approx(2 * ql_loss)</div>
<div class="line"><span class="lineno">  312</span> </div>
<div class="line"><span class="lineno">  313</span>    weights = np.linspace(0, 1, n_samples) ** 2</div>
<div class="line"><span class="lineno">  314</span>    lad_weighted_loss = lad(y_true, raw_predictions, sample_weight=weights)</div>
<div class="line"><span class="lineno">  315</span>    ql_weighted_loss = ql(y_true, raw_predictions, sample_weight=weights)</div>
<div class="line"><span class="lineno">  316</span>    <span class="keywordflow">if</span> alpha == 0.5:</div>
<div class="line"><span class="lineno">  317</span>        <span class="keyword">assert</span> lad_weighted_loss == approx(2 * ql_weighted_loss)</div>
<div class="line"><span class="lineno">  318</span>    pbl_weighted_loss = mean_pinball_loss(</div>
<div class="line"><span class="lineno">  319</span>        y_true, raw_predictions, sample_weight=weights, alpha=alpha</div>
<div class="line"><span class="lineno">  320</span>    )</div>
<div class="line"><span class="lineno">  321</span>    <span class="keyword">assert</span> pbl_weighted_loss == approx(ql_weighted_loss)</div>
<div class="line"><span class="lineno">  322</span> </div>
<div class="line"><span class="lineno">  323</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a12583274aabcbc14b49cda33d878f05c" name="a12583274aabcbc14b49cda33d878f05c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a12583274aabcbc14b49cda33d878f05c">&#9670;&#160;</a></span>test_mdl_computation_weighted()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_mdl_computation_weighted </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  179</span><span class="keyword">def </span>test_mdl_computation_weighted():</div>
<div class="line"><span class="lineno">  180</span>    raw_predictions = np.array([[1.0, -1.0, -0.1], [-2.0, 1.0, 2.0]])</div>
<div class="line"><span class="lineno">  181</span>    y_true = np.array([0, 1])</div>
<div class="line"><span class="lineno">  182</span>    weights = np.array([1, 3])</div>
<div class="line"><span class="lineno">  183</span>    expected_loss = 1.0909323</div>
<div class="line"><span class="lineno">  184</span>    <span class="comment"># MultinomialDeviance loss computation with weights.</span></div>
<div class="line"><span class="lineno">  185</span>    loss = MultinomialDeviance(3)</div>
<div class="line"><span class="lineno">  186</span>    <span class="keyword">assert</span> loss(y_true, raw_predictions, weights) == approx(expected_loss)</div>
<div class="line"><span class="lineno">  187</span> </div>
<div class="line"><span class="lineno">  188</span> </div>
<div class="line"><span class="lineno">  189</span><span class="preprocessor">@pytest.mark.parametrize(&quot;n&quot;, [0, 1, 2])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a7eaeb4832c1ffa5a238f737ed0c5de83" name="a7eaeb4832c1ffa5a238f737ed0c5de83"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7eaeb4832c1ffa5a238f737ed0c5de83">&#9670;&#160;</a></span>test_mdl_exception()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_mdl_exception </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  190</span><span class="keyword">def </span>test_mdl_exception(n):</div>
<div class="line"><span class="lineno">  191</span>    <span class="comment"># Check that MultinomialDeviance throws an exception when n_classes &lt;= 2</span></div>
<div class="line"><span class="lineno">  192</span>    err_msg = <span class="stringliteral">&quot;MultinomialDeviance requires more than 2 classes.&quot;</span></div>
<div class="line"><span class="lineno">  193</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=err_msg):</div>
<div class="line"><span class="lineno">  194</span>        MultinomialDeviance(n)</div>
<div class="line"><span class="lineno">  195</span> </div>
<div class="line"><span class="lineno">  196</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a4d1ef009a2356e29fd5c04f3b7730282" name="a4d1ef009a2356e29fd5c04f3b7730282"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4d1ef009a2356e29fd5c04f3b7730282">&#9670;&#160;</a></span>test_multinomial_deviance()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_multinomial_deviance </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_classes</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  157</span><span class="keyword">def </span>test_multinomial_deviance(n_classes, n_samples, global_random_seed):</div>
<div class="line"><span class="lineno">  158</span>    <span class="comment"># Check multinomial deviance with and without sample weights.</span></div>
<div class="line"><span class="lineno">  159</span>    rng = np.random.RandomState(global_random_seed)</div>
<div class="line"><span class="lineno">  160</span>    sample_weight = np.ones(n_samples)</div>
<div class="line"><span class="lineno">  161</span>    y_true = rng.randint(0, n_classes, size=n_samples)</div>
<div class="line"><span class="lineno">  162</span>    y_pred = np.zeros((n_samples, n_classes), dtype=np.float64)</div>
<div class="line"><span class="lineno">  163</span>    <span class="keywordflow">for</span> klass <span class="keywordflow">in</span> range(y_pred.shape[1]):</div>
<div class="line"><span class="lineno">  164</span>        y_pred[:, klass] = y_true == klass</div>
<div class="line"><span class="lineno">  165</span> </div>
<div class="line"><span class="lineno">  166</span>    loss = MultinomialDeviance(n_classes)</div>
<div class="line"><span class="lineno">  167</span>    loss_wo_sw = loss(y_true, y_pred)</div>
<div class="line"><span class="lineno">  168</span>    <span class="keyword">assert</span> loss_wo_sw &gt; 0</div>
<div class="line"><span class="lineno">  169</span>    loss_w_sw = loss(y_true, y_pred, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  170</span>    <span class="keyword">assert</span> loss_wo_sw == approx(loss_w_sw)</div>
<div class="line"><span class="lineno">  171</span> </div>
<div class="line"><span class="lineno">  172</span>    <span class="comment"># Multinomial deviance uses weighted average loss rather than</span></div>
<div class="line"><span class="lineno">  173</span>    <span class="comment"># weighted sum loss, so we make sure that the value remains the same</span></div>
<div class="line"><span class="lineno">  174</span>    <span class="comment"># when we device the weight by 2.</span></div>
<div class="line"><span class="lineno">  175</span>    loss_w_sw = loss(y_true, y_pred, sample_weight=0.5 * sample_weight)</div>
<div class="line"><span class="lineno">  176</span>    <span class="keyword">assert</span> loss_wo_sw == approx(loss_w_sw)</div>
<div class="line"><span class="lineno">  177</span> </div>
<div class="line"><span class="lineno">  178</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a15dfc6cfc20b093a63d9e10b1c1d8454" name="a15dfc6cfc20b093a63d9e10b1c1d8454"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a15dfc6cfc20b093a63d9e10b1c1d8454">&#9670;&#160;</a></span>test_quantile_loss_function()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_quantile_loss_function </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  111</span><span class="keyword">def </span>test_quantile_loss_function():</div>
<div class="line"><span class="lineno">  112</span>    <span class="comment"># Non regression test for the QuantileLossFunction object</span></div>
<div class="line"><span class="lineno">  113</span>    <span class="comment"># There was a sign problem when evaluating the function</span></div>
<div class="line"><span class="lineno">  114</span>    <span class="comment"># for negative values of &#39;ytrue - ypred&#39;</span></div>
<div class="line"><span class="lineno">  115</span>    x = np.asarray([-1.0, 0.0, 1.0])</div>
<div class="line"><span class="lineno">  116</span>    y_found = QuantileLossFunction(0.9)(x, np.zeros_like(x))</div>
<div class="line"><span class="lineno">  117</span>    y_expected = np.asarray([0.1, 0.0, 0.9]).mean()</div>
<div class="line"><span class="lineno">  118</span>    np.testing.assert_allclose(y_found, y_expected)</div>
<div class="line"><span class="lineno">  119</span>    y_found_p = mean_pinball_loss(x, np.zeros_like(x), alpha=0.9)</div>
<div class="line"><span class="lineno">  120</span>    np.testing.assert_allclose(y_found, y_found_p)</div>
<div class="line"><span class="lineno">  121</span> </div>
<div class="line"><span class="lineno">  122</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ae2de621de8b8b0127d28985b14c43163" name="ae2de621de8b8b0127d28985b14c43163"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae2de621de8b8b0127d28985b14c43163">&#9670;&#160;</a></span>test_sample_weight_deviance()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_sample_weight_deviance </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  123</span><span class="keyword">def </span>test_sample_weight_deviance():</div>
<div class="line"><span class="lineno">  124</span>    <span class="comment"># Test if deviance supports sample weights.</span></div>
<div class="line"><span class="lineno">  125</span>    rng = check_random_state(13)</div>
<div class="line"><span class="lineno">  126</span>    sample_weight = np.ones(100)</div>
<div class="line"><span class="lineno">  127</span>    reg_y = rng.rand(100)</div>
<div class="line"><span class="lineno">  128</span>    clf_y = rng.randint(0, 2, size=100)</div>
<div class="line"><span class="lineno">  129</span>    mclf_y = rng.randint(0, 3, size=100)</div>
<div class="line"><span class="lineno">  130</span> </div>
<div class="line"><span class="lineno">  131</span>    <span class="keywordflow">for</span> Loss <span class="keywordflow">in</span> LOSS_FUNCTIONS.values():</div>
<div class="line"><span class="lineno">  132</span>        <span class="keywordflow">if</span> Loss <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  133</span>            <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno">  134</span>        <span class="keywordflow">if</span> issubclass(Loss, RegressionLossFunction):</div>
<div class="line"><span class="lineno">  135</span>            y = reg_y</div>
<div class="line"><span class="lineno">  136</span>            p = reg_y</div>
<div class="line"><span class="lineno">  137</span>            loss = Loss()</div>
<div class="line"><span class="lineno">  138</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  139</span>            k = 2</div>
<div class="line"><span class="lineno">  140</span>            y = clf_y</div>
<div class="line"><span class="lineno">  141</span>            p = clf_y</div>
<div class="line"><span class="lineno">  142</span>            <span class="keywordflow">if</span> Loss.is_multi_class:</div>
<div class="line"><span class="lineno">  143</span>                k = 3</div>
<div class="line"><span class="lineno">  144</span>                y = mclf_y</div>
<div class="line"><span class="lineno">  145</span>                <span class="comment"># one-hot encoding</span></div>
<div class="line"><span class="lineno">  146</span>                p = np.zeros((y.shape[0], k), dtype=np.float64)</div>
<div class="line"><span class="lineno">  147</span>                <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(k):</div>
<div class="line"><span class="lineno">  148</span>                    p[:, i] = y == i</div>
<div class="line"><span class="lineno">  149</span>            loss = Loss(k)</div>
<div class="line"><span class="lineno">  150</span> </div>
<div class="line"><span class="lineno">  151</span>        deviance_w_w = loss(y, p, sample_weight)</div>
<div class="line"><span class="lineno">  152</span>        deviance_wo_w = loss(y, p)</div>
<div class="line"><span class="lineno">  153</span>        assert_allclose(deviance_wo_w, deviance_w_w)</div>
<div class="line"><span class="lineno">  154</span> </div>
<div class="line"><span class="lineno">  155</span> </div>
<div class="line"><span class="lineno">  156</span><span class="preprocessor">@pytest.mark.parametrize(&quot;n_classes, n_samples&quot;, [(3, 100)</span>, (5, 57), (7, 13)])</div>
</div><!-- fragment -->
</div>
</div>
<a id="a9ec9e89f04f03b17a4d05fbd5f702155" name="a9ec9e89f04f03b17a4d05fbd5f702155"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9ec9e89f04f03b17a4d05fbd5f702155">&#9670;&#160;</a></span>test_sample_weight_init_estimators()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_sample_weight_init_estimators </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   74</span><span class="keyword">def </span>test_sample_weight_init_estimators():</div>
<div class="line"><span class="lineno">   75</span>    <span class="comment"># Smoke test for init estimators with sample weights.</span></div>
<div class="line"><span class="lineno">   76</span>    rng = check_random_state(13)</div>
<div class="line"><span class="lineno">   77</span>    X = rng.rand(100, 2)</div>
<div class="line"><span class="lineno">   78</span>    sample_weight = np.ones(100)</div>
<div class="line"><span class="lineno">   79</span>    reg_y = rng.rand(100)</div>
<div class="line"><span class="lineno">   80</span> </div>
<div class="line"><span class="lineno">   81</span>    clf_y = rng.randint(0, 2, size=100)</div>
<div class="line"><span class="lineno">   82</span> </div>
<div class="line"><span class="lineno">   83</span>    <span class="keywordflow">for</span> Loss <span class="keywordflow">in</span> LOSS_FUNCTIONS.values():</div>
<div class="line"><span class="lineno">   84</span>        <span class="keywordflow">if</span> Loss <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">   85</span>            <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno">   86</span>        <span class="keywordflow">if</span> issubclass(Loss, RegressionLossFunction):</div>
<div class="line"><span class="lineno">   87</span>            y = reg_y</div>
<div class="line"><span class="lineno">   88</span>            loss = Loss()</div>
<div class="line"><span class="lineno">   89</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   90</span>            k = 2</div>
<div class="line"><span class="lineno">   91</span>            y = clf_y</div>
<div class="line"><span class="lineno">   92</span>            <span class="keywordflow">if</span> Loss.is_multi_class:</div>
<div class="line"><span class="lineno">   93</span>                <span class="comment"># skip multiclass</span></div>
<div class="line"><span class="lineno">   94</span>                <span class="keywordflow">continue</span></div>
<div class="line"><span class="lineno">   95</span>            loss = Loss(k)</div>
<div class="line"><span class="lineno">   96</span> </div>
<div class="line"><span class="lineno">   97</span>        init_est = loss.init_estimator()</div>
<div class="line"><span class="lineno">   98</span>        init_est.fit(X, y)</div>
<div class="line"><span class="lineno">   99</span>        out = loss.get_init_raw_predictions(X, init_est)</div>
<div class="line"><span class="lineno">  100</span>        <span class="keyword">assert</span> out.shape == (y.shape[0], 1)</div>
<div class="line"><span class="lineno">  101</span> </div>
<div class="line"><span class="lineno">  102</span>        sw_init_est = loss.init_estimator()</div>
<div class="line"><span class="lineno">  103</span>        sw_init_est.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  104</span>        sw_out = loss.get_init_raw_predictions(X, sw_init_est)</div>
<div class="line"><span class="lineno">  105</span>        <span class="keyword">assert</span> sw_out.shape == (y.shape[0], 1)</div>
<div class="line"><span class="lineno">  106</span> </div>
<div class="line"><span class="lineno">  107</span>        <span class="comment"># check if predictions match</span></div>
<div class="line"><span class="lineno">  108</span>        assert_allclose(out, sw_out, rtol=1e-2)</div>
<div class="line"><span class="lineno">  109</span> </div>
<div class="line"><span class="lineno">  110</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a109fe5017d8dce1c9526827f53b68024" name="a109fe5017d8dce1c9526827f53b68024"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a109fe5017d8dce1c9526827f53b68024">&#9670;&#160;</a></span>test_sample_weight_smoke()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble.tests.test_gradient_boosting_loss_functions.test_sample_weight_smoke </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   62</span><span class="keyword">def </span>test_sample_weight_smoke():</div>
<div class="line"><span class="lineno">   63</span>    rng = check_random_state(13)</div>
<div class="line"><span class="lineno">   64</span>    y = rng.rand(100)</div>
<div class="line"><span class="lineno">   65</span>    pred = rng.rand(100)</div>
<div class="line"><span class="lineno">   66</span> </div>
<div class="line"><span class="lineno">   67</span>    <span class="comment"># least squares</span></div>
<div class="line"><span class="lineno">   68</span>    loss = LeastSquaresError()</div>
<div class="line"><span class="lineno">   69</span>    loss_wo_sw = loss(y, pred)</div>
<div class="line"><span class="lineno">   70</span>    loss_w_sw = loss(y, pred, np.ones(pred.shape[0], dtype=np.float32))</div>
<div class="line"><span class="lineno">   71</span>    <span class="keyword">assert</span> loss_wo_sw == approx(loss_w_sw)</div>
<div class="line"><span class="lineno">   72</span> </div>
<div class="line"><span class="lineno">   73</span> </div>
</div><!-- fragment -->
</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
