<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1ensemble.html">ensemble</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting.html">_hist_gradient_boosting</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests.html">tests</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html">test_gradient_boosting</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a3cb0417328a4806f2391c495dd0a1d7d" id="r_a3cb0417328a4806f2391c495dd0a1d7d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a3cb0417328a4806f2391c495dd0a1d7d">_make_dumb_dataset</a> (n_samples)</td></tr>
<tr class="separator:a3cb0417328a4806f2391c495dd0a1d7d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6f21b56f9a5b98fc0dd76756ebe0efe7" id="r_a6f21b56f9a5b98fc0dd76756ebe0efe7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a6f21b56f9a5b98fc0dd76756ebe0efe7">test_init_parameters_validation</a> (GradientBoosting, X, y, params, err_msg)</td></tr>
<tr class="separator:a6f21b56f9a5b98fc0dd76756ebe0efe7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a00e8ad7affd51f3ff468e68cf2d80d25" id="r_a00e8ad7affd51f3ff468e68cf2d80d25"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a00e8ad7affd51f3ff468e68cf2d80d25">test_invalid_classification_loss</a> ()</td></tr>
<tr class="separator:a00e8ad7affd51f3ff468e68cf2d80d25"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a016eb99729cb3cd57f0fbcbc0ed2e843" id="r_a016eb99729cb3cd57f0fbcbc0ed2e843"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a016eb99729cb3cd57f0fbcbc0ed2e843">test_early_stopping_regression</a> (scoring, validation_fraction, early_stopping, n_iter_no_change, <a class="el" href="__lapack__subroutines_8h.html#a0357339a1a1f7b51953875ca01447445">tol</a>)</td></tr>
<tr class="separator:a016eb99729cb3cd57f0fbcbc0ed2e843"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a140c2d8adde1479922bc20ccd17284f2" id="r_a140c2d8adde1479922bc20ccd17284f2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a140c2d8adde1479922bc20ccd17284f2">test_early_stopping_classification</a> (data, scoring, validation_fraction, early_stopping, n_iter_no_change, <a class="el" href="__lapack__subroutines_8h.html#a0357339a1a1f7b51953875ca01447445">tol</a>)</td></tr>
<tr class="separator:a140c2d8adde1479922bc20ccd17284f2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8459a3e19f33b7c5c737eea853217165" id="r_a8459a3e19f33b7c5c737eea853217165"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a8459a3e19f33b7c5c737eea853217165">test_early_stopping_default</a> (GradientBoosting, X, y)</td></tr>
<tr class="separator:a8459a3e19f33b7c5c737eea853217165"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a71608b821a528f4a0e7addc343a6faf2" id="r_a71608b821a528f4a0e7addc343a6faf2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a71608b821a528f4a0e7addc343a6faf2">test_should_stop</a> (scores, n_iter_no_change, <a class="el" href="__lapack__subroutines_8h.html#a0357339a1a1f7b51953875ca01447445">tol</a>, stopping)</td></tr>
<tr class="separator:a71608b821a528f4a0e7addc343a6faf2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18cd37dfca1203c6620b77b81fd17773" id="r_a18cd37dfca1203c6620b77b81fd17773"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a18cd37dfca1203c6620b77b81fd17773">test_absolute_error</a> ()</td></tr>
<tr class="separator:a18cd37dfca1203c6620b77b81fd17773"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a07802ac5175658b75071cb4511617a32" id="r_a07802ac5175658b75071cb4511617a32"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a07802ac5175658b75071cb4511617a32">test_absolute_error_sample_weight</a> ()</td></tr>
<tr class="separator:a07802ac5175658b75071cb4511617a32"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aeedc7d4d8f253b37d8d480846a3922fe" id="r_aeedc7d4d8f253b37d8d480846a3922fe"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#aeedc7d4d8f253b37d8d480846a3922fe">test_asymmetric_error</a> (quantile)</td></tr>
<tr class="separator:aeedc7d4d8f253b37d8d480846a3922fe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1ca254ed0aeec61714fe1c104fc56f1b" id="r_a1ca254ed0aeec61714fe1c104fc56f1b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a1ca254ed0aeec61714fe1c104fc56f1b">test_poisson_y_positive</a> (y)</td></tr>
<tr class="separator:a1ca254ed0aeec61714fe1c104fc56f1b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad1eec4d622764adbc5e7c7f556843700" id="r_ad1eec4d622764adbc5e7c7f556843700"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#ad1eec4d622764adbc5e7c7f556843700">test_poisson</a> ()</td></tr>
<tr class="separator:ad1eec4d622764adbc5e7c7f556843700"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae5976cdf21aa520af142204aa453bba3" id="r_ae5976cdf21aa520af142204aa453bba3"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#ae5976cdf21aa520af142204aa453bba3">test_binning_train_validation_are_separated</a> ()</td></tr>
<tr class="separator:ae5976cdf21aa520af142204aa453bba3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acb9d774447896ad1daf647c80d490d85" id="r_acb9d774447896ad1daf647c80d490d85"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#acb9d774447896ad1daf647c80d490d85">test_missing_values_trivial</a> ()</td></tr>
<tr class="separator:acb9d774447896ad1daf647c80d490d85"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a62337de7919430e3a4f86839fcf9f9f4" id="r_a62337de7919430e3a4f86839fcf9f9f4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a62337de7919430e3a4f86839fcf9f9f4">test_missing_values_resilience</a> (problem, missing_proportion, expected_min_score_classification, expected_min_score_regression)</td></tr>
<tr class="separator:a62337de7919430e3a4f86839fcf9f9f4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af98154dd121ef172cb649b7721329366" id="r_af98154dd121ef172cb649b7721329366"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#af98154dd121ef172cb649b7721329366">test_zero_division_hessians</a> (data)</td></tr>
<tr class="separator:af98154dd121ef172cb649b7721329366"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5d544161b88105da941f5e573cf46b92" id="r_a5d544161b88105da941f5e573cf46b92"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a5d544161b88105da941f5e573cf46b92">test_small_trainset</a> ()</td></tr>
<tr class="separator:a5d544161b88105da941f5e573cf46b92"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8d1c27c8cd23d18f669d498771f553c0" id="r_a8d1c27c8cd23d18f669d498771f553c0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a8d1c27c8cd23d18f669d498771f553c0">test_missing_values_minmax_imputation</a> ()</td></tr>
<tr class="separator:a8d1c27c8cd23d18f669d498771f553c0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a06b9730a2d982f66a52e6cb980e4a791" id="r_a06b9730a2d982f66a52e6cb980e4a791"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a06b9730a2d982f66a52e6cb980e4a791">test_infinite_values</a> ()</td></tr>
<tr class="separator:a06b9730a2d982f66a52e6cb980e4a791"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5cbd0681ae1a2f009e32794b4fecb9b2" id="r_a5cbd0681ae1a2f009e32794b4fecb9b2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a5cbd0681ae1a2f009e32794b4fecb9b2">test_consistent_lengths</a> ()</td></tr>
<tr class="separator:a5cbd0681ae1a2f009e32794b4fecb9b2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aef66043329bdf4f76fc09b2a1d1528a5" id="r_aef66043329bdf4f76fc09b2a1d1528a5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#aef66043329bdf4f76fc09b2a1d1528a5">test_infinite_values_missing_values</a> ()</td></tr>
<tr class="separator:aef66043329bdf4f76fc09b2a1d1528a5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab82934bcc0e719c139f74e3fcc2a76df" id="r_ab82934bcc0e719c139f74e3fcc2a76df"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#ab82934bcc0e719c139f74e3fcc2a76df">test_crossentropy_binary_problem</a> ()</td></tr>
<tr class="separator:ab82934bcc0e719c139f74e3fcc2a76df"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ad1d0cb9b87956d4359cfb0ab36a525" id="r_a2ad1d0cb9b87956d4359cfb0ab36a525"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a2ad1d0cb9b87956d4359cfb0ab36a525">test_string_target_early_stopping</a> (scoring)</td></tr>
<tr class="separator:a2ad1d0cb9b87956d4359cfb0ab36a525"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2e3011584396e3739af6aa29337c97fc" id="r_a2e3011584396e3739af6aa29337c97fc"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a2e3011584396e3739af6aa29337c97fc">test_zero_sample_weights_regression</a> ()</td></tr>
<tr class="separator:a2e3011584396e3739af6aa29337c97fc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af655eec50eaa5cfa28efd6c5b1e127be" id="r_af655eec50eaa5cfa28efd6c5b1e127be"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#af655eec50eaa5cfa28efd6c5b1e127be">test_zero_sample_weights_classification</a> ()</td></tr>
<tr class="separator:af655eec50eaa5cfa28efd6c5b1e127be"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3667ed8a538d1a730f6723864d553955" id="r_a3667ed8a538d1a730f6723864d553955"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a3667ed8a538d1a730f6723864d553955">test_sample_weight_effect</a> (problem, duplication)</td></tr>
<tr class="separator:a3667ed8a538d1a730f6723864d553955"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0238eda2a532e8bccf6b40679c22fa6" id="r_aa0238eda2a532e8bccf6b40679c22fa6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#aa0238eda2a532e8bccf6b40679c22fa6">test_sum_hessians_are_sample_weight</a> (Loss)</td></tr>
<tr class="separator:aa0238eda2a532e8bccf6b40679c22fa6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a66910d2da73db4ffd930410fc979a81f" id="r_a66910d2da73db4ffd930410fc979a81f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a66910d2da73db4ffd930410fc979a81f">test_max_depth_max_leaf_nodes</a> ()</td></tr>
<tr class="separator:a66910d2da73db4ffd930410fc979a81f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f62e32b39fa3a35bd60873b609a1456" id="r_a4f62e32b39fa3a35bd60873b609a1456"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a4f62e32b39fa3a35bd60873b609a1456">test_early_stopping_on_test_set_with_warm_start</a> ()</td></tr>
<tr class="separator:a4f62e32b39fa3a35bd60873b609a1456"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a636081db2cbbe8364936e3bc8e46adaf" id="r_a636081db2cbbe8364936e3bc8e46adaf"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a636081db2cbbe8364936e3bc8e46adaf">test_single_node_trees</a> (Est)</td></tr>
<tr class="separator:a636081db2cbbe8364936e3bc8e46adaf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0589a4b5b95a18ea8e439b1fb3c38505" id="r_a0589a4b5b95a18ea8e439b1fb3c38505"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a0589a4b5b95a18ea8e439b1fb3c38505">test_custom_loss</a> (Est, loss, X, y)</td></tr>
<tr class="separator:a0589a4b5b95a18ea8e439b1fb3c38505"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8d8340f954b874290fb03bff29a4d123" id="r_a8d8340f954b874290fb03bff29a4d123"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a8d8340f954b874290fb03bff29a4d123">test_staged_predict</a> (HistGradientBoosting, X, y)</td></tr>
<tr class="separator:a8d8340f954b874290fb03bff29a4d123"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad2ef12f655c29b8826305a455bdc2993" id="r_ad2ef12f655c29b8826305a455bdc2993"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#ad2ef12f655c29b8826305a455bdc2993">test_unknown_categories_nan</a> (insert_missing, Est, bool_categorical_parameter)</td></tr>
<tr class="separator:ad2ef12f655c29b8826305a455bdc2993"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5a284403352e0b389598266c28b9c681" id="r_a5a284403352e0b389598266c28b9c681"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a5a284403352e0b389598266c28b9c681">test_categorical_encoding_strategies</a> ()</td></tr>
<tr class="separator:a5a284403352e0b389598266c28b9c681"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a596a7bb4671910ab2a92808f498f6ed5" id="r_a596a7bb4671910ab2a92808f498f6ed5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a596a7bb4671910ab2a92808f498f6ed5">test_categorical_spec_errors</a> (Est, categorical_features, monotonic_cst, expected_msg)</td></tr>
<tr class="separator:a596a7bb4671910ab2a92808f498f6ed5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac69ab6aa96ede1a2da4e0c908012ebfb" id="r_ac69ab6aa96ede1a2da4e0c908012ebfb"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#ac69ab6aa96ede1a2da4e0c908012ebfb">test_categorical_spec_errors_with_feature_names</a> (Est)</td></tr>
<tr class="separator:ac69ab6aa96ede1a2da4e0c908012ebfb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afe8af4050e25a6f98829132f88ef5295" id="r_afe8af4050e25a6f98829132f88ef5295"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#afe8af4050e25a6f98829132f88ef5295">test_categorical_spec_no_categories</a> (Est, categorical_features, as_array)</td></tr>
<tr class="separator:afe8af4050e25a6f98829132f88ef5295"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a221fee47f05d374ae6fb9c16f6913146" id="r_a221fee47f05d374ae6fb9c16f6913146"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a221fee47f05d374ae6fb9c16f6913146">test_categorical_bad_encoding_errors</a> (Est, use_pandas, feature_name)</td></tr>
<tr class="separator:a221fee47f05d374ae6fb9c16f6913146"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ade78a35bb512556c552f74e5771952e0" id="r_ade78a35bb512556c552f74e5771952e0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#ade78a35bb512556c552f74e5771952e0">test_uint8_predict</a> (Est)</td></tr>
<tr class="separator:ade78a35bb512556c552f74e5771952e0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6899a98ba2ca7069b673af35aa91d40d" id="r_a6899a98ba2ca7069b673af35aa91d40d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a6899a98ba2ca7069b673af35aa91d40d">test_check_interaction_cst</a> (interaction_cst, n_features, result)</td></tr>
<tr class="separator:a6899a98ba2ca7069b673af35aa91d40d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d5029f1dcb1cd33faf267a057ef3e82" id="r_a7d5029f1dcb1cd33faf267a057ef3e82"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a7d5029f1dcb1cd33faf267a057ef3e82">test_interaction_cst_numerically</a> ()</td></tr>
<tr class="separator:a7d5029f1dcb1cd33faf267a057ef3e82"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aab3a23e46f1a8237f993e35afefaa37d" id="r_aab3a23e46f1a8237f993e35afefaa37d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#aab3a23e46f1a8237f993e35afefaa37d">test_loss_deprecated</a> (old_loss, new_loss, Estimator)</td></tr>
<tr class="separator:aab3a23e46f1a8237f993e35afefaa37d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a962466734911480f7433bcd355c21ae4" id="r_a962466734911480f7433bcd355c21ae4"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a962466734911480f7433bcd355c21ae4">test_no_user_warning_with_scoring</a> ()</td></tr>
<tr class="separator:a962466734911480f7433bcd355c21ae4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a124b83ee297d5edcadfa012d71d1d826" id="r_a124b83ee297d5edcadfa012d71d1d826"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a124b83ee297d5edcadfa012d71d1d826">test_class_weights</a> ()</td></tr>
<tr class="separator:a124b83ee297d5edcadfa012d71d1d826"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a021d57b8ad64db46dc18a12ca277d99b" id="r_a021d57b8ad64db46dc18a12ca277d99b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a021d57b8ad64db46dc18a12ca277d99b">test_unknown_category_that_are_negative</a> ()</td></tr>
<tr class="separator:a021d57b8ad64db46dc18a12ca277d99b"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a66ae9539a1d8c5532a3a3afafce7b0b5" id="r_a66ae9539a1d8c5532a3a3afafce7b0b5"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a66ae9539a1d8c5532a3a3afafce7b0b5">n_threads</a> = _openmp_effective_n_threads()</td></tr>
<tr class="separator:a66ae9539a1d8c5532a3a3afafce7b0b5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1fc4d72d69604589b032f9a2bfecfce8" id="r_a1fc4d72d69604589b032f9a2bfecfce8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a1fc4d72d69604589b032f9a2bfecfce8">X_classification</a></td></tr>
<tr class="separator:a1fc4d72d69604589b032f9a2bfecfce8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a768b0cf666f4a3541ab4bb014871f89b" id="r_a768b0cf666f4a3541ab4bb014871f89b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a768b0cf666f4a3541ab4bb014871f89b">y_classification</a></td></tr>
<tr class="separator:a768b0cf666f4a3541ab4bb014871f89b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a12c321c4cccfc2c4ab5486c01b7e824f" id="r_a12c321c4cccfc2c4ab5486c01b7e824f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a12c321c4cccfc2c4ab5486c01b7e824f">random_state</a></td></tr>
<tr class="separator:a12c321c4cccfc2c4ab5486c01b7e824f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad7b4f1981157cf99aa73d929a21c7558" id="r_ad7b4f1981157cf99aa73d929a21c7558"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#ad7b4f1981157cf99aa73d929a21c7558">X_regression</a></td></tr>
<tr class="separator:ad7b4f1981157cf99aa73d929a21c7558"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1d7dfef02e15041ac27c5865809e63d0" id="r_a1d7dfef02e15041ac27c5865809e63d0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a1d7dfef02e15041ac27c5865809e63d0">y_regression</a></td></tr>
<tr class="separator:a1d7dfef02e15041ac27c5865809e63d0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5a0de99b8fbcc43b09704857ae574a91" id="r_a5a0de99b8fbcc43b09704857ae574a91"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a5a0de99b8fbcc43b09704857ae574a91">X_multi_classification</a></td></tr>
<tr class="separator:a5a0de99b8fbcc43b09704857ae574a91"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a82d9e0f19d5e973342b805041caa122f" id="r_a82d9e0f19d5e973342b805041caa122f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a82d9e0f19d5e973342b805041caa122f">y_multi_classification</a></td></tr>
<tr class="separator:a82d9e0f19d5e973342b805041caa122f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3231ca533f46d5f35be1669f1f81e67c" id="r_a3231ca533f46d5f35be1669f1f81e67c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a3231ca533f46d5f35be1669f1f81e67c">n_classes</a></td></tr>
<tr class="separator:a3231ca533f46d5f35be1669f1f81e67c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a672b351a8ca5f4e702a2bb14e756af87" id="r_a672b351a8ca5f4e702a2bb14e756af87"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a672b351a8ca5f4e702a2bb14e756af87">n_informative</a></td></tr>
<tr class="separator:a672b351a8ca5f4e702a2bb14e756af87"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2d399df1ab632341043d8db61fb5a21c" id="r_a2d399df1ab632341043d8db61fb5a21c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a2d399df1ab632341043d8db61fb5a21c">data_min_</a></td></tr>
<tr class="separator:a2d399df1ab632341043d8db61fb5a21c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a08352f5c493029cd45a0ff1b73355fff" id="r_a08352f5c493029cd45a0ff1b73355fff"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1ensemble_1_1__hist__gradient__boosting_1_1tests_1_1test__gradient__boosting.html#a08352f5c493029cd45a0ff1b73355fff">data_max_</a></td></tr>
<tr class="separator:a08352f5c493029cd45a0ff1b73355fff"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Function Documentation</h2>
<a id="a3cb0417328a4806f2391c495dd0a1d7d" name="a3cb0417328a4806f2391c495dd0a1d7d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3cb0417328a4806f2391c495dd0a1d7d">&#9670;&#160;</a></span>_make_dumb_dataset()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting._make_dumb_dataset </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Make a dumb dataset to test early stopping.</pre> <div class="fragment"><div class="line"><span class="lineno">   43</span><span class="keyword">def </span>_make_dumb_dataset(n_samples):</div>
<div class="line"><span class="lineno">   44</span>    <span class="stringliteral">&quot;&quot;&quot;Make a dumb dataset to test early stopping.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   45</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno">   46</span>    X_dumb = rng.randn(n_samples, 1)</div>
<div class="line"><span class="lineno">   47</span>    y_dumb = (X_dumb[:, 0] &gt; 0).astype(<span class="stringliteral">&quot;int64&quot;</span>)</div>
<div class="line"><span class="lineno">   48</span>    <span class="keywordflow">return</span> X_dumb, y_dumb</div>
<div class="line"><span class="lineno">   49</span> </div>
<div class="line"><span class="lineno">   50</span> </div>
<div class="line"><span class="lineno">   51</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">   52</span>    <span class="stringliteral">&quot;GradientBoosting, X, y&quot;</span>,</div>
<div class="line"><span class="lineno">   53</span>    [</div>
<div class="line"><span class="lineno">   54</span>        (HistGradientBoostingClassifier, X_classification, y_classification),</div>
<div class="line"><span class="lineno">   55</span>        (HistGradientBoostingRegressor, X_regression, y_regression),</div>
<div class="line"><span class="lineno">   56</span>    ],</div>
<div class="line"><span class="lineno">   57</span>)</div>
<div class="line"><span class="lineno">   58</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">   59</span>    <span class="stringliteral">&quot;params, err_msg&quot;</span>,</div>
<div class="line"><span class="lineno">   60</span>    [</div>
<div class="line"><span class="lineno">   61</span>        (</div>
<div class="line"><span class="lineno">   62</span>            {<span class="stringliteral">&quot;interaction_cst&quot;</span>: [0, 1]},</div>
<div class="line"><span class="lineno">   63</span>            <span class="stringliteral">&quot;Interaction constraints must be a sequence of tuples or lists&quot;</span>,</div>
<div class="line"><span class="lineno">   64</span>        ),</div>
<div class="line"><span class="lineno">   65</span>        (</div>
<div class="line"><span class="lineno">   66</span>            {<span class="stringliteral">&quot;interaction_cst&quot;</span>: [{0, 9999}]},</div>
<div class="line"><span class="lineno">   67</span>            <span class="stringliteral">r&quot;Interaction constraints must consist of integer indices in \[0,&quot;</span></div>
<div class="line"><span class="lineno">   68</span>            <span class="stringliteral">r&quot; n_features - 1\] = \[.*\], specifying the position of features,&quot;</span>,</div>
<div class="line"><span class="lineno">   69</span>        ),</div>
<div class="line"><span class="lineno">   70</span>        (</div>
<div class="line"><span class="lineno">   71</span>            {<span class="stringliteral">&quot;interaction_cst&quot;</span>: [{-1, 0}]},</div>
<div class="line"><span class="lineno">   72</span>            <span class="stringliteral">r&quot;Interaction constraints must consist of integer indices in \[0,&quot;</span></div>
<div class="line"><span class="lineno">   73</span>            <span class="stringliteral">r&quot; n_features - 1\] = \[.*\], specifying the position of features,&quot;</span>,</div>
<div class="line"><span class="lineno">   74</span>        ),</div>
<div class="line"><span class="lineno">   75</span>        (</div>
<div class="line"><span class="lineno">   76</span>            {<span class="stringliteral">&quot;interaction_cst&quot;</span>: [{0.5}]},</div>
<div class="line"><span class="lineno">   77</span>            <span class="stringliteral">r&quot;Interaction constraints must consist of integer indices in \[0,&quot;</span></div>
<div class="line"><span class="lineno">   78</span>            <span class="stringliteral">r&quot; n_features - 1\] = \[.*\], specifying the position of features,&quot;</span>,</div>
<div class="line"><span class="lineno">   79</span>        ),</div>
<div class="line"><span class="lineno">   80</span>    ],</div>
<div class="line"><span class="lineno">   81</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a18cd37dfca1203c6620b77b81fd17773" name="a18cd37dfca1203c6620b77b81fd17773"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a18cd37dfca1203c6620b77b81fd17773">&#9670;&#160;</a></span>test_absolute_error()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_absolute_error </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  229</span><span class="keyword">def </span>test_absolute_error():</div>
<div class="line"><span class="lineno">  230</span>    <span class="comment"># For coverage only.</span></div>
<div class="line"><span class="lineno">  231</span>    X, y = make_regression(n_samples=500, random_state=0)</div>
<div class="line"><span class="lineno">  232</span>    gbdt = HistGradientBoostingRegressor(loss=<span class="stringliteral">&quot;absolute_error&quot;</span>, random_state=0)</div>
<div class="line"><span class="lineno">  233</span>    gbdt.fit(X, y)</div>
<div class="line"><span class="lineno">  234</span>    <span class="keyword">assert</span> gbdt.score(X, y) &gt; 0.9</div>
<div class="line"><span class="lineno">  235</span> </div>
<div class="line"><span class="lineno">  236</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a07802ac5175658b75071cb4511617a32" name="a07802ac5175658b75071cb4511617a32"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a07802ac5175658b75071cb4511617a32">&#9670;&#160;</a></span>test_absolute_error_sample_weight()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_absolute_error_sample_weight </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  237</span><span class="keyword">def </span>test_absolute_error_sample_weight():</div>
<div class="line"><span class="lineno">  238</span>    <span class="comment"># non regression test for issue #19400</span></div>
<div class="line"><span class="lineno">  239</span>    <span class="comment"># make sure no error is thrown during fit of</span></div>
<div class="line"><span class="lineno">  240</span>    <span class="comment"># HistGradientBoostingRegressor with absolute_error loss function</span></div>
<div class="line"><span class="lineno">  241</span>    <span class="comment"># and passing sample_weight</span></div>
<div class="line"><span class="lineno">  242</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  243</span>    n_samples = 100</div>
<div class="line"><span class="lineno">  244</span>    X = rng.uniform(-1, 1, size=(n_samples, 2))</div>
<div class="line"><span class="lineno">  245</span>    y = rng.uniform(-1, 1, size=n_samples)</div>
<div class="line"><span class="lineno">  246</span>    sample_weight = rng.uniform(0, 1, size=n_samples)</div>
<div class="line"><span class="lineno">  247</span>    gbdt = HistGradientBoostingRegressor(loss=<span class="stringliteral">&quot;absolute_error&quot;</span>)</div>
<div class="line"><span class="lineno">  248</span>    gbdt.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  249</span> </div>
<div class="line"><span class="lineno">  250</span> </div>
<div class="line"><span class="lineno">  251</span><span class="preprocessor">@pytest.mark.parametrize(&quot;quantile&quot;, [0.2, 0.5, 0.8])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="aeedc7d4d8f253b37d8d480846a3922fe" name="aeedc7d4d8f253b37d8d480846a3922fe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aeedc7d4d8f253b37d8d480846a3922fe">&#9670;&#160;</a></span>test_asymmetric_error()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_asymmetric_error </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>quantile</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test quantile regression for asymmetric distributed targets.</pre> <div class="fragment"><div class="line"><span class="lineno">  252</span><span class="keyword">def </span>test_asymmetric_error(quantile):</div>
<div class="line"><span class="lineno">  253</span>    <span class="stringliteral">&quot;&quot;&quot;Test quantile regression for asymmetric distributed targets.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  254</span>    n_samples = 10_000</div>
<div class="line"><span class="lineno">  255</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno">  256</span>    <span class="comment"># take care that X @ coef + intercept &gt; 0</span></div>
<div class="line"><span class="lineno">  257</span>    X = np.concatenate(</div>
<div class="line"><span class="lineno">  258</span>        (</div>
<div class="line"><span class="lineno">  259</span>            np.abs(rng.randn(n_samples)[:, <span class="keywordtype">None</span>]),</div>
<div class="line"><span class="lineno">  260</span>            -rng.randint(2, size=(n_samples, 1)),</div>
<div class="line"><span class="lineno">  261</span>        ),</div>
<div class="line"><span class="lineno">  262</span>        axis=1,</div>
<div class="line"><span class="lineno">  263</span>    )</div>
<div class="line"><span class="lineno">  264</span>    intercept = 1.23</div>
<div class="line"><span class="lineno">  265</span>    coef = np.array([0.5, -2])</div>
<div class="line"><span class="lineno">  266</span>    <span class="comment"># For an exponential distribution with rate lambda, e.g. exp(-lambda * x),</span></div>
<div class="line"><span class="lineno">  267</span>    <span class="comment"># the quantile at level q is:</span></div>
<div class="line"><span class="lineno">  268</span>    <span class="comment">#   quantile(q) = - log(1 - q) / lambda</span></div>
<div class="line"><span class="lineno">  269</span>    <span class="comment">#   scale = 1/lambda = -quantile(q) / log(1-q)</span></div>
<div class="line"><span class="lineno">  270</span>    y = rng.exponential(</div>
<div class="line"><span class="lineno">  271</span>        scale=-(X @ coef + intercept) / np.log(1 - quantile), size=n_samples</div>
<div class="line"><span class="lineno">  272</span>    )</div>
<div class="line"><span class="lineno">  273</span>    model = HistGradientBoostingRegressor(</div>
<div class="line"><span class="lineno">  274</span>        loss=<span class="stringliteral">&quot;quantile&quot;</span>,</div>
<div class="line"><span class="lineno">  275</span>        quantile=quantile,</div>
<div class="line"><span class="lineno">  276</span>        max_iter=25,</div>
<div class="line"><span class="lineno">  277</span>        random_state=0,</div>
<div class="line"><span class="lineno">  278</span>        max_leaf_nodes=10,</div>
<div class="line"><span class="lineno">  279</span>    ).fit(X, y)</div>
<div class="line"><span class="lineno">  280</span>    assert_allclose(np.mean(model.predict(X) &gt; y), quantile, rtol=1e-2)</div>
<div class="line"><span class="lineno">  281</span> </div>
<div class="line"><span class="lineno">  282</span>    pinball_loss = PinballLoss(quantile=quantile)</div>
<div class="line"><span class="lineno">  283</span>    loss_true_quantile = pinball_loss(y, X @ coef + intercept)</div>
<div class="line"><span class="lineno">  284</span>    loss_pred_quantile = pinball_loss(y, model.predict(X))</div>
<div class="line"><span class="lineno">  285</span>    <span class="comment"># we are overfitting</span></div>
<div class="line"><span class="lineno">  286</span>    <span class="keyword">assert</span> loss_pred_quantile &lt;= loss_true_quantile</div>
<div class="line"><span class="lineno">  287</span> </div>
<div class="line"><span class="lineno">  288</span> </div>
<div class="line"><span class="lineno">  289</span><span class="preprocessor">@pytest.mark.parametrize(&quot;y&quot;, [([1.0, -2.0, 0.0])</span>, ([0.0, 0.0, 0.0])])</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae5976cdf21aa520af142204aa453bba3" name="ae5976cdf21aa520af142204aa453bba3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae5976cdf21aa520af142204aa453bba3">&#9670;&#160;</a></span>test_binning_train_validation_are_separated()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_binning_train_validation_are_separated </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  328</span><span class="keyword">def </span>test_binning_train_validation_are_separated():</div>
<div class="line"><span class="lineno">  329</span>    <span class="comment"># Make sure training and validation data are binned separately.</span></div>
<div class="line"><span class="lineno">  330</span>    <span class="comment"># See issue 13926</span></div>
<div class="line"><span class="lineno">  331</span> </div>
<div class="line"><span class="lineno">  332</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  333</span>    validation_fraction = 0.2</div>
<div class="line"><span class="lineno">  334</span>    gb = HistGradientBoostingClassifier(</div>
<div class="line"><span class="lineno">  335</span>        early_stopping=<span class="keyword">True</span>, validation_fraction=validation_fraction, random_state=rng</div>
<div class="line"><span class="lineno">  336</span>    )</div>
<div class="line"><span class="lineno">  337</span>    gb.fit(X_classification, y_classification)</div>
<div class="line"><span class="lineno">  338</span>    mapper_training_data = gb._bin_mapper</div>
<div class="line"><span class="lineno">  339</span> </div>
<div class="line"><span class="lineno">  340</span>    <span class="comment"># Note that since the data is small there is no subsampling and the</span></div>
<div class="line"><span class="lineno">  341</span>    <span class="comment"># random_state doesn&#39;t matter</span></div>
<div class="line"><span class="lineno">  342</span>    mapper_whole_data = _BinMapper(random_state=0)</div>
<div class="line"><span class="lineno">  343</span>    mapper_whole_data.fit(X_classification)</div>
<div class="line"><span class="lineno">  344</span> </div>
<div class="line"><span class="lineno">  345</span>    n_samples = X_classification.shape[0]</div>
<div class="line"><span class="lineno">  346</span>    <span class="keyword">assert</span> np.all(</div>
<div class="line"><span class="lineno">  347</span>        mapper_training_data.n_bins_non_missing_</div>
<div class="line"><span class="lineno">  348</span>        == int((1 - validation_fraction) * n_samples)</div>
<div class="line"><span class="lineno">  349</span>    )</div>
<div class="line"><span class="lineno">  350</span>    <span class="keyword">assert</span> np.all(</div>
<div class="line"><span class="lineno">  351</span>        mapper_training_data.n_bins_non_missing_</div>
<div class="line"><span class="lineno">  352</span>        != mapper_whole_data.n_bins_non_missing_</div>
<div class="line"><span class="lineno">  353</span>    )</div>
<div class="line"><span class="lineno">  354</span> </div>
<div class="line"><span class="lineno">  355</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a221fee47f05d374ae6fb9c16f6913146" name="a221fee47f05d374ae6fb9c16f6913146"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a221fee47f05d374ae6fb9c16f6913146">&#9670;&#160;</a></span>test_categorical_bad_encoding_errors()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_bad_encoding_errors </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Est</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>use_pandas</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>feature_name</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1143</span><span class="keyword">def </span>test_categorical_bad_encoding_errors(Est, use_pandas, feature_name):</div>
<div class="line"><span class="lineno"> 1144</span>    <span class="comment"># Test errors when categories are encoded incorrectly</span></div>
<div class="line"><span class="lineno"> 1145</span> </div>
<div class="line"><span class="lineno"> 1146</span>    gb = Est(categorical_features=[<span class="keyword">True</span>], max_bins=2)</div>
<div class="line"><span class="lineno"> 1147</span> </div>
<div class="line"><span class="lineno"> 1148</span>    <span class="keywordflow">if</span> use_pandas:</div>
<div class="line"><span class="lineno"> 1149</span>        pd = pytest.importorskip(<span class="stringliteral">&quot;pandas&quot;</span>)</div>
<div class="line"><span class="lineno"> 1150</span>        X = pd.DataFrame({<span class="stringliteral">&quot;f0&quot;</span>: [0, 1, 2]})</div>
<div class="line"><span class="lineno"> 1151</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1152</span>        X = np.array([[0, 1, 2]]).T</div>
<div class="line"><span class="lineno"> 1153</span>    y = np.arange(3)</div>
<div class="line"><span class="lineno"> 1154</span>    msg = f<span class="stringliteral">&quot;Categorical feature {feature_name} is expected to have a cardinality &lt;= 2&quot;</span></div>
<div class="line"><span class="lineno"> 1155</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=msg):</div>
<div class="line"><span class="lineno"> 1156</span>        gb.fit(X, y)</div>
<div class="line"><span class="lineno"> 1157</span> </div>
<div class="line"><span class="lineno"> 1158</span>    <span class="keywordflow">if</span> use_pandas:</div>
<div class="line"><span class="lineno"> 1159</span>        X = pd.DataFrame({<span class="stringliteral">&quot;f0&quot;</span>: [0, 2]})</div>
<div class="line"><span class="lineno"> 1160</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1161</span>        X = np.array([[0, 2]]).T</div>
<div class="line"><span class="lineno"> 1162</span>    y = np.arange(2)</div>
<div class="line"><span class="lineno"> 1163</span>    msg = (</div>
<div class="line"><span class="lineno"> 1164</span>        f<span class="stringliteral">&quot;Categorical feature {feature_name} is expected to be encoded with values &lt; 2&quot;</span></div>
<div class="line"><span class="lineno"> 1165</span>    )</div>
<div class="line"><span class="lineno"> 1166</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=msg):</div>
<div class="line"><span class="lineno"> 1167</span>        gb.fit(X, y)</div>
<div class="line"><span class="lineno"> 1168</span> </div>
<div class="line"><span class="lineno"> 1169</span>    <span class="comment"># nans are ignored in the counts</span></div>
<div class="line"><span class="lineno"> 1170</span>    X = np.array([[0, 1, np.nan]]).T</div>
<div class="line"><span class="lineno"> 1171</span>    y = np.arange(3)</div>
<div class="line"><span class="lineno"> 1172</span>    gb.fit(X, y)</div>
<div class="line"><span class="lineno"> 1173</span> </div>
<div class="line"><span class="lineno"> 1174</span> </div>
<div class="line"><span class="lineno"> 1175</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1176</span>    <span class="stringliteral">&quot;Est&quot;</span>, (HistGradientBoostingClassifier, HistGradientBoostingRegressor)</div>
<div class="line"><span class="lineno"> 1177</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a5a284403352e0b389598266c28b9c681" name="a5a284403352e0b389598266c28b9c681"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5a284403352e0b389598266c28b9c681">&#9670;&#160;</a></span>test_categorical_encoding_strategies()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_encoding_strategies </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  957</span><span class="keyword">def </span>test_categorical_encoding_strategies():</div>
<div class="line"><span class="lineno">  958</span>    <span class="comment"># Check native categorical handling vs different encoding strategies. We</span></div>
<div class="line"><span class="lineno">  959</span>    <span class="comment"># make sure that native encoding needs only 1 split to achieve a perfect</span></div>
<div class="line"><span class="lineno">  960</span>    <span class="comment"># prediction on a simple dataset. In contrast, OneHotEncoded data needs</span></div>
<div class="line"><span class="lineno">  961</span>    <span class="comment"># more depth / splits, and treating categories as ordered (just using</span></div>
<div class="line"><span class="lineno">  962</span>    <span class="comment"># OrdinalEncoder) requires even more depth.</span></div>
<div class="line"><span class="lineno">  963</span> </div>
<div class="line"><span class="lineno">  964</span>    <span class="comment"># dataset with one random continuous feature, and one categorical feature</span></div>
<div class="line"><span class="lineno">  965</span>    <span class="comment"># with values in [0, 5], e.g. from an OrdinalEncoder.</span></div>
<div class="line"><span class="lineno">  966</span>    <span class="comment"># class == 1 iff categorical value in {0, 2, 4}</span></div>
<div class="line"><span class="lineno">  967</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  968</span>    n_samples = 10_000</div>
<div class="line"><span class="lineno">  969</span>    f1 = rng.rand(n_samples)</div>
<div class="line"><span class="lineno">  970</span>    f2 = rng.randint(6, size=n_samples)</div>
<div class="line"><span class="lineno">  971</span>    X = np.c_[f1, f2]</div>
<div class="line"><span class="lineno">  972</span>    y = np.zeros(shape=n_samples)</div>
<div class="line"><span class="lineno">  973</span>    y[X[:, 1] % 2 == 0] = 1</div>
<div class="line"><span class="lineno">  974</span> </div>
<div class="line"><span class="lineno">  975</span>    <span class="comment"># make sure dataset is balanced so that the baseline_prediction doesn&#39;t</span></div>
<div class="line"><span class="lineno">  976</span>    <span class="comment"># influence predictions too much with max_iter = 1</span></div>
<div class="line"><span class="lineno">  977</span>    <span class="keyword">assert</span> 0.49 &lt; y.mean() &lt; 0.51</div>
<div class="line"><span class="lineno">  978</span> </div>
<div class="line"><span class="lineno">  979</span>    native_cat_specs = [</div>
<div class="line"><span class="lineno">  980</span>        [<span class="keyword">False</span>, <span class="keyword">True</span>],</div>
<div class="line"><span class="lineno">  981</span>        [1],</div>
<div class="line"><span class="lineno">  982</span>    ]</div>
<div class="line"><span class="lineno">  983</span>    <span class="keywordflow">try</span>:</div>
<div class="line"><span class="lineno">  984</span>        <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div>
<div class="line"><span class="lineno">  985</span> </div>
<div class="line"><span class="lineno">  986</span>        X = pd.DataFrame(X, columns=[<span class="stringliteral">&quot;f_0&quot;</span>, <span class="stringliteral">&quot;f_1&quot;</span>])</div>
<div class="line"><span class="lineno">  987</span>        native_cat_specs.append([<span class="stringliteral">&quot;f_1&quot;</span>])</div>
<div class="line"><span class="lineno">  988</span>    <span class="keywordflow">except</span> ImportError:</div>
<div class="line"><span class="lineno">  989</span>        <span class="keywordflow">pass</span></div>
<div class="line"><span class="lineno">  990</span> </div>
<div class="line"><span class="lineno">  991</span>    <span class="keywordflow">for</span> native_cat_spec <span class="keywordflow">in</span> native_cat_specs:</div>
<div class="line"><span class="lineno">  992</span>        clf_cat = HistGradientBoostingClassifier(</div>
<div class="line"><span class="lineno">  993</span>            max_iter=1, max_depth=1, categorical_features=native_cat_spec</div>
<div class="line"><span class="lineno">  994</span>        )</div>
<div class="line"><span class="lineno">  995</span> </div>
<div class="line"><span class="lineno">  996</span>        <span class="comment"># Using native categorical encoding, we get perfect predictions with just</span></div>
<div class="line"><span class="lineno">  997</span>        <span class="comment"># one split</span></div>
<div class="line"><span class="lineno">  998</span>        <span class="keyword">assert</span> cross_val_score(clf_cat, X, y).mean() == 1</div>
<div class="line"><span class="lineno">  999</span> </div>
<div class="line"><span class="lineno"> 1000</span>    <span class="comment"># quick sanity check for the bitset: 0, 2, 4 = 2**0 + 2**2 + 2**4 = 21</span></div>
<div class="line"><span class="lineno"> 1001</span>    expected_left_bitset = [21, 0, 0, 0, 0, 0, 0, 0]</div>
<div class="line"><span class="lineno"> 1002</span>    left_bitset = clf_cat.fit(X, y)._predictors[0][0].raw_left_cat_bitsets[0]</div>
<div class="line"><span class="lineno"> 1003</span>    assert_array_equal(left_bitset, expected_left_bitset)</div>
<div class="line"><span class="lineno"> 1004</span> </div>
<div class="line"><span class="lineno"> 1005</span>    <span class="comment"># Treating categories as ordered, we need more depth / more splits to get</span></div>
<div class="line"><span class="lineno"> 1006</span>    <span class="comment"># the same predictions</span></div>
<div class="line"><span class="lineno"> 1007</span>    clf_no_cat = HistGradientBoostingClassifier(</div>
<div class="line"><span class="lineno"> 1008</span>        max_iter=1, max_depth=4, categorical_features=<span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1009</span>    )</div>
<div class="line"><span class="lineno"> 1010</span>    <span class="keyword">assert</span> cross_val_score(clf_no_cat, X, y).mean() &lt; 0.9</div>
<div class="line"><span class="lineno"> 1011</span> </div>
<div class="line"><span class="lineno"> 1012</span>    clf_no_cat.set_params(max_depth=5)</div>
<div class="line"><span class="lineno"> 1013</span>    <span class="keyword">assert</span> cross_val_score(clf_no_cat, X, y).mean() == 1</div>
<div class="line"><span class="lineno"> 1014</span> </div>
<div class="line"><span class="lineno"> 1015</span>    <span class="comment"># Using OHEd data, we need less splits than with pure OEd data, but we</span></div>
<div class="line"><span class="lineno"> 1016</span>    <span class="comment"># still need more splits than with the native categorical splits</span></div>
<div class="line"><span class="lineno"> 1017</span>    ct = make_column_transformer(</div>
<div class="line"><span class="lineno"> 1018</span>        (OneHotEncoder(sparse_output=<span class="keyword">False</span>), [1]), remainder=<span class="stringliteral">&quot;passthrough&quot;</span></div>
<div class="line"><span class="lineno"> 1019</span>    )</div>
<div class="line"><span class="lineno"> 1020</span>    X_ohe = ct.fit_transform(X)</div>
<div class="line"><span class="lineno"> 1021</span>    clf_no_cat.set_params(max_depth=2)</div>
<div class="line"><span class="lineno"> 1022</span>    <span class="keyword">assert</span> cross_val_score(clf_no_cat, X_ohe, y).mean() &lt; 0.9</div>
<div class="line"><span class="lineno"> 1023</span> </div>
<div class="line"><span class="lineno"> 1024</span>    clf_no_cat.set_params(max_depth=3)</div>
<div class="line"><span class="lineno"> 1025</span>    <span class="keyword">assert</span> cross_val_score(clf_no_cat, X_ohe, y).mean() == 1</div>
<div class="line"><span class="lineno"> 1026</span> </div>
<div class="line"><span class="lineno"> 1027</span> </div>
<div class="line"><span class="lineno"> 1028</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1029</span>    <span class="stringliteral">&quot;Est&quot;</span>, (HistGradientBoostingClassifier, HistGradientBoostingRegressor)</div>
<div class="line"><span class="lineno"> 1030</span>)</div>
<div class="line"><span class="lineno"> 1031</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1032</span>    <span class="stringliteral">&quot;categorical_features, monotonic_cst, expected_msg&quot;</span>,</div>
<div class="line"><span class="lineno"> 1033</span>    [</div>
<div class="line"><span class="lineno"> 1034</span>        (</div>
<div class="line"><span class="lineno"> 1035</span>            [b<span class="stringliteral">&quot;hello&quot;</span>, b<span class="stringliteral">&quot;world&quot;</span>],</div>
<div class="line"><span class="lineno"> 1036</span>            <span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno"> 1037</span>            re.escape(</div>
<div class="line"><span class="lineno"> 1038</span>                <span class="stringliteral">&quot;categorical_features must be an array-like of bool, int or str, &quot;</span></div>
<div class="line"><span class="lineno"> 1039</span>                <span class="stringliteral">&quot;got: bytes40.&quot;</span></div>
<div class="line"><span class="lineno"> 1040</span>            ),</div>
<div class="line"><span class="lineno"> 1041</span>        ),</div>
<div class="line"><span class="lineno"> 1042</span>        (</div>
<div class="line"><span class="lineno"> 1043</span>            np.array([b<span class="stringliteral">&quot;hello&quot;</span>, 1.3], dtype=object),</div>
<div class="line"><span class="lineno"> 1044</span>            <span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno"> 1045</span>            re.escape(</div>
<div class="line"><span class="lineno"> 1046</span>                <span class="stringliteral">&quot;categorical_features must be an array-like of bool, int or str, &quot;</span></div>
<div class="line"><span class="lineno"> 1047</span>                <span class="stringliteral">&quot;got: bytes, float.&quot;</span></div>
<div class="line"><span class="lineno"> 1048</span>            ),</div>
<div class="line"><span class="lineno"> 1049</span>        ),</div>
<div class="line"><span class="lineno"> 1050</span>        (</div>
<div class="line"><span class="lineno"> 1051</span>            [0, -1],</div>
<div class="line"><span class="lineno"> 1052</span>            <span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno"> 1053</span>            re.escape(</div>
<div class="line"><span class="lineno"> 1054</span>                <span class="stringliteral">&quot;categorical_features set as integer indices must be in &quot;</span></div>
<div class="line"><span class="lineno"> 1055</span>                <span class="stringliteral">&quot;[0, n_features - 1]&quot;</span></div>
<div class="line"><span class="lineno"> 1056</span>            ),</div>
<div class="line"><span class="lineno"> 1057</span>        ),</div>
<div class="line"><span class="lineno"> 1058</span>        (</div>
<div class="line"><span class="lineno"> 1059</span>            [<span class="keyword">True</span>, <span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">True</span>],</div>
<div class="line"><span class="lineno"> 1060</span>            <span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno"> 1061</span>            re.escape(</div>
<div class="line"><span class="lineno"> 1062</span>                <span class="stringliteral">&quot;categorical_features set as a boolean mask must have shape &quot;</span></div>
<div class="line"><span class="lineno"> 1063</span>                <span class="stringliteral">&quot;(n_features,)&quot;</span></div>
<div class="line"><span class="lineno"> 1064</span>            ),</div>
<div class="line"><span class="lineno"> 1065</span>        ),</div>
<div class="line"><span class="lineno"> 1066</span>        (</div>
<div class="line"><span class="lineno"> 1067</span>            [<span class="keyword">True</span>, <span class="keyword">True</span>, <span class="keyword">False</span>, <span class="keyword">False</span>],</div>
<div class="line"><span class="lineno"> 1068</span>            [0, -1, 0, 1],</div>
<div class="line"><span class="lineno"> 1069</span>            <span class="stringliteral">&quot;Categorical features cannot have monotonic constraints&quot;</span>,</div>
<div class="line"><span class="lineno"> 1070</span>        ),</div>
<div class="line"><span class="lineno"> 1071</span>    ],</div>
<div class="line"><span class="lineno"> 1072</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a596a7bb4671910ab2a92808f498f6ed5" name="a596a7bb4671910ab2a92808f498f6ed5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a596a7bb4671910ab2a92808f498f6ed5">&#9670;&#160;</a></span>test_categorical_spec_errors()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_spec_errors </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Est</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>categorical_features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>monotonic_cst</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>expected_msg</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1075</span>):</div>
<div class="line"><span class="lineno"> 1076</span>    <span class="comment"># Test errors when categories are specified incorrectly</span></div>
<div class="line"><span class="lineno"> 1077</span>    n_samples = 100</div>
<div class="line"><span class="lineno"> 1078</span>    X, y = make_classification(random_state=0, n_features=4, n_samples=n_samples)</div>
<div class="line"><span class="lineno"> 1079</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno"> 1080</span>    X[:, 0] = rng.randint(0, 10, size=n_samples)</div>
<div class="line"><span class="lineno"> 1081</span>    X[:, 1] = rng.randint(0, 10, size=n_samples)</div>
<div class="line"><span class="lineno"> 1082</span>    est = Est(categorical_features=categorical_features, monotonic_cst=monotonic_cst)</div>
<div class="line"><span class="lineno"> 1083</span> </div>
<div class="line"><span class="lineno"> 1084</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=expected_msg):</div>
<div class="line"><span class="lineno"> 1085</span>        est.fit(X, y)</div>
<div class="line"><span class="lineno"> 1086</span> </div>
<div class="line"><span class="lineno"> 1087</span> </div>
<div class="line"><span class="lineno"> 1088</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1089</span>    <span class="stringliteral">&quot;Est&quot;</span>, (HistGradientBoostingClassifier, HistGradientBoostingRegressor)</div>
<div class="line"><span class="lineno"> 1090</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ac69ab6aa96ede1a2da4e0c908012ebfb" name="ac69ab6aa96ede1a2da4e0c908012ebfb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac69ab6aa96ede1a2da4e0c908012ebfb">&#9670;&#160;</a></span>test_categorical_spec_errors_with_feature_names()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_spec_errors_with_feature_names </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Est</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1091</span><span class="keyword">def </span>test_categorical_spec_errors_with_feature_names(Est):</div>
<div class="line"><span class="lineno"> 1092</span>    pd = pytest.importorskip(<span class="stringliteral">&quot;pandas&quot;</span>)</div>
<div class="line"><span class="lineno"> 1093</span>    n_samples = 10</div>
<div class="line"><span class="lineno"> 1094</span>    X = pd.DataFrame(</div>
<div class="line"><span class="lineno"> 1095</span>        {</div>
<div class="line"><span class="lineno"> 1096</span>            <span class="stringliteral">&quot;f0&quot;</span>: range(n_samples),</div>
<div class="line"><span class="lineno"> 1097</span>            <span class="stringliteral">&quot;f1&quot;</span>: range(n_samples),</div>
<div class="line"><span class="lineno"> 1098</span>            <span class="stringliteral">&quot;f2&quot;</span>: [1.0] * n_samples,</div>
<div class="line"><span class="lineno"> 1099</span>        }</div>
<div class="line"><span class="lineno"> 1100</span>    )</div>
<div class="line"><span class="lineno"> 1101</span>    y = [0, 1] * (n_samples // 2)</div>
<div class="line"><span class="lineno"> 1102</span> </div>
<div class="line"><span class="lineno"> 1103</span>    est = Est(categorical_features=[<span class="stringliteral">&quot;f0&quot;</span>, <span class="stringliteral">&quot;f1&quot;</span>, <span class="stringliteral">&quot;f3&quot;</span>])</div>
<div class="line"><span class="lineno"> 1104</span>    expected_msg = re.escape(</div>
<div class="line"><span class="lineno"> 1105</span>        <span class="stringliteral">&quot;categorical_features has a item value &#39;f3&#39; which is not a valid &quot;</span></div>
<div class="line"><span class="lineno"> 1106</span>        <span class="stringliteral">&quot;feature name of the training data.&quot;</span></div>
<div class="line"><span class="lineno"> 1107</span>    )</div>
<div class="line"><span class="lineno"> 1108</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=expected_msg):</div>
<div class="line"><span class="lineno"> 1109</span>        est.fit(X, y)</div>
<div class="line"><span class="lineno"> 1110</span> </div>
<div class="line"><span class="lineno"> 1111</span>    est = Est(categorical_features=[<span class="stringliteral">&quot;f0&quot;</span>, <span class="stringliteral">&quot;f1&quot;</span>])</div>
<div class="line"><span class="lineno"> 1112</span>    expected_msg = re.escape(</div>
<div class="line"><span class="lineno"> 1113</span>        <span class="stringliteral">&quot;categorical_features should be passed as an array of integers or &quot;</span></div>
<div class="line"><span class="lineno"> 1114</span>        <span class="stringliteral">&quot;as a boolean mask when the model is fitted on data without feature &quot;</span></div>
<div class="line"><span class="lineno"> 1115</span>        <span class="stringliteral">&quot;names.&quot;</span></div>
<div class="line"><span class="lineno"> 1116</span>    )</div>
<div class="line"><span class="lineno"> 1117</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=expected_msg):</div>
<div class="line"><span class="lineno"> 1118</span>        est.fit(X.to_numpy(), y)</div>
<div class="line"><span class="lineno"> 1119</span> </div>
<div class="line"><span class="lineno"> 1120</span> </div>
<div class="line"><span class="lineno"> 1121</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1122</span>    <span class="stringliteral">&quot;Est&quot;</span>, (HistGradientBoostingClassifier, HistGradientBoostingRegressor)</div>
<div class="line"><span class="lineno"> 1123</span>)</div>
<div class="line"><span class="lineno"> 1124</span><span class="preprocessor">@pytest.mark.parametrize(&quot;categorical_features&quot;, ([False, False], [])</span>)</div>
<div class="line"><span class="lineno"> 1125</span><span class="preprocessor">@pytest.mark.parametrize(&quot;as_array&quot;, (True, False)</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="afe8af4050e25a6f98829132f88ef5295" name="afe8af4050e25a6f98829132f88ef5295"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afe8af4050e25a6f98829132f88ef5295">&#9670;&#160;</a></span>test_categorical_spec_no_categories()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_spec_no_categories </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Est</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>categorical_features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>as_array</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1126</span><span class="keyword">def </span>test_categorical_spec_no_categories(Est, categorical_features, as_array):</div>
<div class="line"><span class="lineno"> 1127</span>    <span class="comment"># Make sure we can properly detect that no categorical features are present</span></div>
<div class="line"><span class="lineno"> 1128</span>    <span class="comment"># even if the categorical_features parameter is not None</span></div>
<div class="line"><span class="lineno"> 1129</span>    X = np.arange(10).reshape(5, 2)</div>
<div class="line"><span class="lineno"> 1130</span>    y = np.arange(5)</div>
<div class="line"><span class="lineno"> 1131</span>    <span class="keywordflow">if</span> as_array:</div>
<div class="line"><span class="lineno"> 1132</span>        categorical_features = np.asarray(categorical_features)</div>
<div class="line"><span class="lineno"> 1133</span>    est = Est(categorical_features=categorical_features).fit(X, y)</div>
<div class="line"><span class="lineno"> 1134</span>    <span class="keyword">assert</span> est.is_categorical_ <span class="keywordflow">is</span> <span class="keywordtype">None</span></div>
<div class="line"><span class="lineno"> 1135</span> </div>
<div class="line"><span class="lineno"> 1136</span> </div>
<div class="line"><span class="lineno"> 1137</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1138</span>    <span class="stringliteral">&quot;Est&quot;</span>, (HistGradientBoostingClassifier, HistGradientBoostingRegressor)</div>
<div class="line"><span class="lineno"> 1139</span>)</div>
<div class="line"><span class="lineno"> 1140</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1141</span>    <span class="stringliteral">&quot;use_pandas, feature_name&quot;</span>, [(<span class="keyword">False</span>, <span class="stringliteral">&quot;at index 0&quot;</span>), (<span class="keyword">True</span>, <span class="stringliteral">&quot;&#39;f0&#39;&quot;</span>)]</div>
<div class="line"><span class="lineno"> 1142</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a6899a98ba2ca7069b673af35aa91d40d" name="a6899a98ba2ca7069b673af35aa91d40d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6899a98ba2ca7069b673af35aa91d40d">&#9670;&#160;</a></span>test_check_interaction_cst()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_check_interaction_cst </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>interaction_cst</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>result</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check that _check_interaction_cst returns the expected list of sets</pre> <div class="fragment"><div class="line"><span class="lineno"> 1205</span><span class="keyword">def </span>test_check_interaction_cst(interaction_cst, n_features, result):</div>
<div class="line"><span class="lineno"> 1206</span>    <span class="stringliteral">&quot;&quot;&quot;Check that _check_interaction_cst returns the expected list of sets&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1207</span>    est = HistGradientBoostingRegressor()</div>
<div class="line"><span class="lineno"> 1208</span>    est.set_params(interaction_cst=interaction_cst)</div>
<div class="line"><span class="lineno"> 1209</span>    <span class="keyword">assert</span> est._check_interaction_cst(n_features) == result</div>
<div class="line"><span class="lineno"> 1210</span> </div>
<div class="line"><span class="lineno"> 1211</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a124b83ee297d5edcadfa012d71d1d826" name="a124b83ee297d5edcadfa012d71d1d826"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a124b83ee297d5edcadfa012d71d1d826">&#9670;&#160;</a></span>test_class_weights()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_class_weights </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">High level test to check class_weights.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1306</span><span class="keyword">def </span>test_class_weights():</div>
<div class="line"><span class="lineno"> 1307</span>    <span class="stringliteral">&quot;&quot;&quot;High level test to check class_weights.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1308</span>    n_samples = 255</div>
<div class="line"><span class="lineno"> 1309</span>    n_features = 2</div>
<div class="line"><span class="lineno"> 1310</span> </div>
<div class="line"><span class="lineno"> 1311</span>    X, y = make_classification(</div>
<div class="line"><span class="lineno"> 1312</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno"> 1313</span>        n_features=n_features,</div>
<div class="line"><span class="lineno"> 1314</span>        n_informative=n_features,</div>
<div class="line"><span class="lineno"> 1315</span>        n_redundant=0,</div>
<div class="line"><span class="lineno"> 1316</span>        n_clusters_per_class=1,</div>
<div class="line"><span class="lineno"> 1317</span>        n_classes=2,</div>
<div class="line"><span class="lineno"> 1318</span>        random_state=0,</div>
<div class="line"><span class="lineno"> 1319</span>    )</div>
<div class="line"><span class="lineno"> 1320</span>    y_is_1 = y == 1</div>
<div class="line"><span class="lineno"> 1321</span> </div>
<div class="line"><span class="lineno"> 1322</span>    <span class="comment"># class_weight is the same as sample weights with the corresponding class</span></div>
<div class="line"><span class="lineno"> 1323</span>    clf = HistGradientBoostingClassifier(</div>
<div class="line"><span class="lineno"> 1324</span>        min_samples_leaf=2, random_state=0, max_depth=2</div>
<div class="line"><span class="lineno"> 1325</span>    )</div>
<div class="line"><span class="lineno"> 1326</span>    sample_weight = np.ones(shape=(n_samples))</div>
<div class="line"><span class="lineno"> 1327</span>    sample_weight[y_is_1] = 3.0</div>
<div class="line"><span class="lineno"> 1328</span>    clf.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1329</span> </div>
<div class="line"><span class="lineno"> 1330</span>    class_weight = {0: 1.0, 1: 3.0}</div>
<div class="line"><span class="lineno"> 1331</span>    clf_class_weighted = clone(clf).set_params(class_weight=class_weight)</div>
<div class="line"><span class="lineno"> 1332</span>    clf_class_weighted.fit(X, y)</div>
<div class="line"><span class="lineno"> 1333</span> </div>
<div class="line"><span class="lineno"> 1334</span>    assert_allclose(clf.decision_function(X), clf_class_weighted.decision_function(X))</div>
<div class="line"><span class="lineno"> 1335</span> </div>
<div class="line"><span class="lineno"> 1336</span>    <span class="comment"># Check that sample_weight and class_weight are multiplicative</span></div>
<div class="line"><span class="lineno"> 1337</span>    clf.fit(X, y, sample_weight=sample_weight**2)</div>
<div class="line"><span class="lineno"> 1338</span>    clf_class_weighted.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1339</span>    assert_allclose(clf.decision_function(X), clf_class_weighted.decision_function(X))</div>
<div class="line"><span class="lineno"> 1340</span> </div>
<div class="line"><span class="lineno"> 1341</span>    <span class="comment"># Make imbalanced dataset</span></div>
<div class="line"><span class="lineno"> 1342</span>    X_imb = np.concatenate((X[~y_is_1], X[y_is_1][:10]))</div>
<div class="line"><span class="lineno"> 1343</span>    y_imb = np.concatenate((y[~y_is_1], y[y_is_1][:10]))</div>
<div class="line"><span class="lineno"> 1344</span> </div>
<div class="line"><span class="lineno"> 1345</span>    <span class="comment"># class_weight=&quot;balanced&quot; is the same as sample_weights to be</span></div>
<div class="line"><span class="lineno"> 1346</span>    <span class="comment"># inversely proportional to n_samples / (n_classes * np.bincount(y))</span></div>
<div class="line"><span class="lineno"> 1347</span>    clf_balanced = clone(clf).set_params(class_weight=<span class="stringliteral">&quot;balanced&quot;</span>)</div>
<div class="line"><span class="lineno"> 1348</span>    clf_balanced.fit(X_imb, y_imb)</div>
<div class="line"><span class="lineno"> 1349</span> </div>
<div class="line"><span class="lineno"> 1350</span>    class_weight = y_imb.shape[0] / (2 * np.bincount(y_imb))</div>
<div class="line"><span class="lineno"> 1351</span>    sample_weight = class_weight[y_imb]</div>
<div class="line"><span class="lineno"> 1352</span>    clf_sample_weight = clone(clf).set_params(class_weight=<span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno"> 1353</span>    clf_sample_weight.fit(X_imb, y_imb, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1354</span> </div>
<div class="line"><span class="lineno"> 1355</span>    assert_allclose(</div>
<div class="line"><span class="lineno"> 1356</span>        clf_balanced.decision_function(X_imb),</div>
<div class="line"><span class="lineno"> 1357</span>        clf_sample_weight.decision_function(X_imb),</div>
<div class="line"><span class="lineno"> 1358</span>    )</div>
<div class="line"><span class="lineno"> 1359</span> </div>
<div class="line"><span class="lineno"> 1360</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a5cbd0681ae1a2f009e32794b4fecb9b2" name="a5cbd0681ae1a2f009e32794b4fecb9b2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5cbd0681ae1a2f009e32794b4fecb9b2">&#9670;&#160;</a></span>test_consistent_lengths()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_consistent_lengths </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  587</span><span class="keyword">def </span>test_consistent_lengths():</div>
<div class="line"><span class="lineno">  588</span>    X = np.array([-np.inf, 0, 1, np.inf]).reshape(-1, 1)</div>
<div class="line"><span class="lineno">  589</span>    y = np.array([0, 0, 1, 1])</div>
<div class="line"><span class="lineno">  590</span>    sample_weight = np.array([0.1, 0.3, 0.1])</div>
<div class="line"><span class="lineno">  591</span>    gbdt = HistGradientBoostingRegressor()</div>
<div class="line"><span class="lineno">  592</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=<span class="stringliteral">r&quot;sample_weight.shape == \&zwj;(3,\&zwj;), expected&quot;</span>):</div>
<div class="line"><span class="lineno">  593</span>        gbdt.fit(X, y, sample_weight)</div>
<div class="line"><span class="lineno">  594</span> </div>
<div class="line"><span class="lineno">  595</span>    <span class="keyword">with</span> pytest.raises(</div>
<div class="line"><span class="lineno">  596</span>        ValueError, match=<span class="stringliteral">&quot;Found input variables with inconsistent number&quot;</span></div>
<div class="line"><span class="lineno">  597</span>    ):</div>
<div class="line"><span class="lineno">  598</span>        gbdt.fit(X, y[1:])</div>
<div class="line"><span class="lineno">  599</span> </div>
<div class="line"><span class="lineno">  600</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ab82934bcc0e719c139f74e3fcc2a76df" name="ab82934bcc0e719c139f74e3fcc2a76df"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab82934bcc0e719c139f74e3fcc2a76df">&#9670;&#160;</a></span>test_crossentropy_binary_problem()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_crossentropy_binary_problem </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  621</span><span class="keyword">def </span>test_crossentropy_binary_problem():</div>
<div class="line"><span class="lineno">  622</span>    <span class="comment"># categorical_crossentropy should only be used if there are more than two</span></div>
<div class="line"><span class="lineno">  623</span>    <span class="comment"># classes present. PR #14869</span></div>
<div class="line"><span class="lineno">  624</span>    X = [[1], [0]]</div>
<div class="line"><span class="lineno">  625</span>    y = [0, 1]</div>
<div class="line"><span class="lineno">  626</span>    gbrt = HistGradientBoostingClassifier(loss=<span class="stringliteral">&quot;categorical_crossentropy&quot;</span>)</div>
<div class="line"><span class="lineno">  627</span>    <span class="keyword">with</span> pytest.raises(</div>
<div class="line"><span class="lineno">  628</span>        ValueError, match=<span class="stringliteral">&quot;loss=&#39;categorical_crossentropy&#39; is not suitable for&quot;</span></div>
<div class="line"><span class="lineno">  629</span>    ):</div>
<div class="line"><span class="lineno">  630</span>        gbrt.fit(X, y)</div>
<div class="line"><span class="lineno">  631</span> </div>
<div class="line"><span class="lineno">  632</span> </div>
<div class="line"><span class="lineno">  633</span><span class="preprocessor">@pytest.mark.parametrize(&quot;scoring&quot;, [None, &quot;loss&quot;])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a0589a4b5b95a18ea8e439b1fb3c38505" name="a0589a4b5b95a18ea8e439b1fb3c38505"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0589a4b5b95a18ea8e439b1fb3c38505">&#9670;&#160;</a></span>test_custom_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_custom_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Est</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  864</span><span class="keyword">def </span>test_custom_loss(Est, loss, X, y):</div>
<div class="line"><span class="lineno">  865</span>    est = Est(loss=loss, max_iter=20)</div>
<div class="line"><span class="lineno">  866</span>    est.fit(X, y)</div>
<div class="line"><span class="lineno">  867</span> </div>
<div class="line"><span class="lineno">  868</span> </div>
<div class="line"><span class="lineno">  869</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  870</span>    <span class="stringliteral">&quot;HistGradientBoosting, X, y&quot;</span>,</div>
<div class="line"><span class="lineno">  871</span>    [</div>
<div class="line"><span class="lineno">  872</span>        (HistGradientBoostingClassifier, X_classification, y_classification),</div>
<div class="line"><span class="lineno">  873</span>        (HistGradientBoostingRegressor, X_regression, y_regression),</div>
<div class="line"><span class="lineno">  874</span>        (</div>
<div class="line"><span class="lineno">  875</span>            HistGradientBoostingClassifier,</div>
<div class="line"><span class="lineno">  876</span>            X_multi_classification,</div>
<div class="line"><span class="lineno">  877</span>            y_multi_classification,</div>
<div class="line"><span class="lineno">  878</span>        ),</div>
<div class="line"><span class="lineno">  879</span>    ],</div>
<div class="line"><span class="lineno">  880</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a140c2d8adde1479922bc20ccd17284f2" name="a140c2d8adde1479922bc20ccd17284f2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a140c2d8adde1479922bc20ccd17284f2">&#9670;&#160;</a></span>test_early_stopping_classification()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_early_stopping_classification </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>scoring</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>validation_fraction</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>early_stopping</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_iter_no_change</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tol</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  163</span>):</div>
<div class="line"><span class="lineno">  164</span> </div>
<div class="line"><span class="lineno">  165</span>    max_iter = 50</div>
<div class="line"><span class="lineno">  166</span> </div>
<div class="line"><span class="lineno">  167</span>    X, y = data</div>
<div class="line"><span class="lineno">  168</span> </div>
<div class="line"><span class="lineno">  169</span>    gb = HistGradientBoostingClassifier(</div>
<div class="line"><span class="lineno">  170</span>        verbose=1,  <span class="comment"># just for coverage</span></div>
<div class="line"><span class="lineno">  171</span>        min_samples_leaf=5,  <span class="comment"># easier to overfit fast</span></div>
<div class="line"><span class="lineno">  172</span>        scoring=scoring,</div>
<div class="line"><span class="lineno">  173</span>        tol=tol,</div>
<div class="line"><span class="lineno">  174</span>        early_stopping=early_stopping,</div>
<div class="line"><span class="lineno">  175</span>        validation_fraction=validation_fraction,</div>
<div class="line"><span class="lineno">  176</span>        max_iter=max_iter,</div>
<div class="line"><span class="lineno">  177</span>        n_iter_no_change=n_iter_no_change,</div>
<div class="line"><span class="lineno">  178</span>        random_state=0,</div>
<div class="line"><span class="lineno">  179</span>    )</div>
<div class="line"><span class="lineno">  180</span>    gb.fit(X, y)</div>
<div class="line"><span class="lineno">  181</span> </div>
<div class="line"><span class="lineno">  182</span>    <span class="keywordflow">if</span> early_stopping <span class="keywordflow">is</span> <span class="keyword">True</span>:</div>
<div class="line"><span class="lineno">  183</span>        <span class="keyword">assert</span> n_iter_no_change &lt;= gb.n_iter_ &lt; max_iter</div>
<div class="line"><span class="lineno">  184</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  185</span>        <span class="keyword">assert</span> gb.n_iter_ == max_iter</div>
<div class="line"><span class="lineno">  186</span> </div>
<div class="line"><span class="lineno">  187</span> </div>
<div class="line"><span class="lineno">  188</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  189</span>    <span class="stringliteral">&quot;GradientBoosting, X, y&quot;</span>,</div>
<div class="line"><span class="lineno">  190</span>    [</div>
<div class="line"><span class="lineno">  191</span>        (HistGradientBoostingClassifier, *_make_dumb_dataset(10000)),</div>
<div class="line"><span class="lineno">  192</span>        (HistGradientBoostingClassifier, *_make_dumb_dataset(10001)),</div>
<div class="line"><span class="lineno">  193</span>        (HistGradientBoostingRegressor, *_make_dumb_dataset(10000)),</div>
<div class="line"><span class="lineno">  194</span>        (HistGradientBoostingRegressor, *_make_dumb_dataset(10001)),</div>
<div class="line"><span class="lineno">  195</span>    ],</div>
<div class="line"><span class="lineno">  196</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a8459a3e19f33b7c5c737eea853217165" name="a8459a3e19f33b7c5c737eea853217165"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8459a3e19f33b7c5c737eea853217165">&#9670;&#160;</a></span>test_early_stopping_default()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_early_stopping_default </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>GradientBoosting</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  197</span><span class="keyword">def </span>test_early_stopping_default(GradientBoosting, X, y):</div>
<div class="line"><span class="lineno">  198</span>    <span class="comment"># Test that early stopping is enabled by default if and only if there</span></div>
<div class="line"><span class="lineno">  199</span>    <span class="comment"># are more than 10000 samples</span></div>
<div class="line"><span class="lineno">  200</span>    gb = GradientBoosting(max_iter=10, n_iter_no_change=2, tol=1e-1)</div>
<div class="line"><span class="lineno">  201</span>    gb.fit(X, y)</div>
<div class="line"><span class="lineno">  202</span>    <span class="keywordflow">if</span> X.shape[0] &gt; 10000:</div>
<div class="line"><span class="lineno">  203</span>        <span class="keyword">assert</span> gb.n_iter_ &lt; gb.max_iter</div>
<div class="line"><span class="lineno">  204</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  205</span>        <span class="keyword">assert</span> gb.n_iter_ == gb.max_iter</div>
<div class="line"><span class="lineno">  206</span> </div>
<div class="line"><span class="lineno">  207</span> </div>
<div class="line"><span class="lineno">  208</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  209</span>    <span class="stringliteral">&quot;scores, n_iter_no_change, tol, stopping&quot;</span>,</div>
<div class="line"><span class="lineno">  210</span>    [</div>
<div class="line"><span class="lineno">  211</span>        ([], 1, 0.001, <span class="keyword">False</span>),  <span class="comment"># not enough iterations</span></div>
<div class="line"><span class="lineno">  212</span>        ([1, 1, 1], 5, 0.001, <span class="keyword">False</span>),  <span class="comment"># not enough iterations</span></div>
<div class="line"><span class="lineno">  213</span>        ([1, 1, 1, 1, 1], 5, 0.001, <span class="keyword">False</span>),  <span class="comment"># not enough iterations</span></div>
<div class="line"><span class="lineno">  214</span>        ([1, 2, 3, 4, 5, 6], 5, 0.001, <span class="keyword">False</span>),  <span class="comment"># significant improvement</span></div>
<div class="line"><span class="lineno">  215</span>        ([1, 2, 3, 4, 5, 6], 5, 0.0, <span class="keyword">False</span>),  <span class="comment"># significant improvement</span></div>
<div class="line"><span class="lineno">  216</span>        ([1, 2, 3, 4, 5, 6], 5, 0.999, <span class="keyword">False</span>),  <span class="comment"># significant improvement</span></div>
<div class="line"><span class="lineno">  217</span>        ([1, 2, 3, 4, 5, 6], 5, 5 - 1e-5, <span class="keyword">False</span>),  <span class="comment"># significant improvement</span></div>
<div class="line"><span class="lineno">  218</span>        ([1] * 6, 5, 0.0, <span class="keyword">True</span>),  <span class="comment"># no significant improvement</span></div>
<div class="line"><span class="lineno">  219</span>        ([1] * 6, 5, 0.001, <span class="keyword">True</span>),  <span class="comment"># no significant improvement</span></div>
<div class="line"><span class="lineno">  220</span>        ([1] * 6, 5, 5, <span class="keyword">True</span>),  <span class="comment"># no significant improvement</span></div>
<div class="line"><span class="lineno">  221</span>    ],</div>
<div class="line"><span class="lineno">  222</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a4f62e32b39fa3a35bd60873b609a1456" name="a4f62e32b39fa3a35bd60873b609a1456"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4f62e32b39fa3a35bd60873b609a1456">&#9670;&#160;</a></span>test_early_stopping_on_test_set_with_warm_start()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_early_stopping_on_test_set_with_warm_start </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  806</span><span class="keyword">def </span>test_early_stopping_on_test_set_with_warm_start():</div>
<div class="line"><span class="lineno">  807</span>    <span class="comment"># Non regression test for #16661 where second fit fails with</span></div>
<div class="line"><span class="lineno">  808</span>    <span class="comment"># warm_start=True, early_stopping is on, and no validation set</span></div>
<div class="line"><span class="lineno">  809</span>    X, y = make_classification(random_state=0)</div>
<div class="line"><span class="lineno">  810</span>    gb = HistGradientBoostingClassifier(</div>
<div class="line"><span class="lineno">  811</span>        max_iter=1,</div>
<div class="line"><span class="lineno">  812</span>        scoring=<span class="stringliteral">&quot;loss&quot;</span>,</div>
<div class="line"><span class="lineno">  813</span>        warm_start=<span class="keyword">True</span>,</div>
<div class="line"><span class="lineno">  814</span>        early_stopping=<span class="keyword">True</span>,</div>
<div class="line"><span class="lineno">  815</span>        n_iter_no_change=1,</div>
<div class="line"><span class="lineno">  816</span>        validation_fraction=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno">  817</span>    )</div>
<div class="line"><span class="lineno">  818</span> </div>
<div class="line"><span class="lineno">  819</span>    gb.fit(X, y)</div>
<div class="line"><span class="lineno">  820</span>    <span class="comment"># does not raise on second call</span></div>
<div class="line"><span class="lineno">  821</span>    gb.set_params(max_iter=2)</div>
<div class="line"><span class="lineno">  822</span>    gb.fit(X, y)</div>
<div class="line"><span class="lineno">  823</span> </div>
<div class="line"><span class="lineno">  824</span> </div>
<div class="line"><span class="lineno">  825</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  826</span>    <span class="stringliteral">&quot;Est&quot;</span>, (HistGradientBoostingClassifier, HistGradientBoostingRegressor)</div>
<div class="line"><span class="lineno">  827</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a016eb99729cb3cd57f0fbcbc0ed2e843" name="a016eb99729cb3cd57f0fbcbc0ed2e843"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a016eb99729cb3cd57f0fbcbc0ed2e843">&#9670;&#160;</a></span>test_early_stopping_regression()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_early_stopping_regression </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>scoring</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>validation_fraction</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>early_stopping</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_iter_no_change</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tol</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  115</span>):</div>
<div class="line"><span class="lineno">  116</span> </div>
<div class="line"><span class="lineno">  117</span>    max_iter = 200</div>
<div class="line"><span class="lineno">  118</span> </div>
<div class="line"><span class="lineno">  119</span>    X, y = make_regression(n_samples=50, random_state=0)</div>
<div class="line"><span class="lineno">  120</span> </div>
<div class="line"><span class="lineno">  121</span>    gb = HistGradientBoostingRegressor(</div>
<div class="line"><span class="lineno">  122</span>        verbose=1,  <span class="comment"># just for coverage</span></div>
<div class="line"><span class="lineno">  123</span>        min_samples_leaf=5,  <span class="comment"># easier to overfit fast</span></div>
<div class="line"><span class="lineno">  124</span>        scoring=scoring,</div>
<div class="line"><span class="lineno">  125</span>        tol=tol,</div>
<div class="line"><span class="lineno">  126</span>        early_stopping=early_stopping,</div>
<div class="line"><span class="lineno">  127</span>        validation_fraction=validation_fraction,</div>
<div class="line"><span class="lineno">  128</span>        max_iter=max_iter,</div>
<div class="line"><span class="lineno">  129</span>        n_iter_no_change=n_iter_no_change,</div>
<div class="line"><span class="lineno">  130</span>        random_state=0,</div>
<div class="line"><span class="lineno">  131</span>    )</div>
<div class="line"><span class="lineno">  132</span>    gb.fit(X, y)</div>
<div class="line"><span class="lineno">  133</span> </div>
<div class="line"><span class="lineno">  134</span>    <span class="keywordflow">if</span> early_stopping:</div>
<div class="line"><span class="lineno">  135</span>        <span class="keyword">assert</span> n_iter_no_change &lt;= gb.n_iter_ &lt; max_iter</div>
<div class="line"><span class="lineno">  136</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  137</span>        <span class="keyword">assert</span> gb.n_iter_ == max_iter</div>
<div class="line"><span class="lineno">  138</span> </div>
<div class="line"><span class="lineno">  139</span> </div>
<div class="line"><span class="lineno">  140</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  141</span>    <span class="stringliteral">&quot;data&quot;</span>,</div>
<div class="line"><span class="lineno">  142</span>    (</div>
<div class="line"><span class="lineno">  143</span>        make_classification(n_samples=30, random_state=0),</div>
<div class="line"><span class="lineno">  144</span>        make_classification(</div>
<div class="line"><span class="lineno">  145</span>            n_samples=30, n_classes=3, n_clusters_per_class=1, random_state=0</div>
<div class="line"><span class="lineno">  146</span>        ),</div>
<div class="line"><span class="lineno">  147</span>    ),</div>
<div class="line"><span class="lineno">  148</span>)</div>
<div class="line"><span class="lineno">  149</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  150</span>    <span class="stringliteral">&quot;scoring, validation_fraction, early_stopping, n_iter_no_change, tol&quot;</span>,</div>
<div class="line"><span class="lineno">  151</span>    [</div>
<div class="line"><span class="lineno">  152</span>        (<span class="stringliteral">&quot;accuracy&quot;</span>, 0.1, <span class="keyword">True</span>, 5, 1e-7),  <span class="comment"># use scorer</span></div>
<div class="line"><span class="lineno">  153</span>        (<span class="stringliteral">&quot;accuracy&quot;</span>, <span class="keywordtype">None</span>, <span class="keyword">True</span>, 5, 1e-1),  <span class="comment"># use scorer on training data</span></div>
<div class="line"><span class="lineno">  154</span>        (<span class="keywordtype">None</span>, 0.1, <span class="keyword">True</span>, 5, 1e-7),  <span class="comment"># same with default scorer</span></div>
<div class="line"><span class="lineno">  155</span>        (<span class="keywordtype">None</span>, <span class="keywordtype">None</span>, <span class="keyword">True</span>, 5, 1e-1),</div>
<div class="line"><span class="lineno">  156</span>        (<span class="stringliteral">&quot;loss&quot;</span>, 0.1, <span class="keyword">True</span>, 5, 1e-7),  <span class="comment"># use loss</span></div>
<div class="line"><span class="lineno">  157</span>        (<span class="stringliteral">&quot;loss&quot;</span>, <span class="keywordtype">None</span>, <span class="keyword">True</span>, 5, 1e-1),  <span class="comment"># use loss on training data</span></div>
<div class="line"><span class="lineno">  158</span>        (<span class="keywordtype">None</span>, <span class="keywordtype">None</span>, <span class="keyword">False</span>, 5, 0.0),  <span class="comment"># no early stopping</span></div>
<div class="line"><span class="lineno">  159</span>    ],</div>
<div class="line"><span class="lineno">  160</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a06b9730a2d982f66a52e6cb980e4a791" name="a06b9730a2d982f66a52e6cb980e4a791"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a06b9730a2d982f66a52e6cb980e4a791">&#9670;&#160;</a></span>test_infinite_values()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_infinite_values </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  576</span><span class="keyword">def </span>test_infinite_values():</div>
<div class="line"><span class="lineno">  577</span>    <span class="comment"># Basic test for infinite values</span></div>
<div class="line"><span class="lineno">  578</span> </div>
<div class="line"><span class="lineno">  579</span>    X = np.array([-np.inf, 0, 1, np.inf]).reshape(-1, 1)</div>
<div class="line"><span class="lineno">  580</span>    y = np.array([0, 0, 1, 1])</div>
<div class="line"><span class="lineno">  581</span> </div>
<div class="line"><span class="lineno">  582</span>    gbdt = HistGradientBoostingRegressor(min_samples_leaf=1)</div>
<div class="line"><span class="lineno">  583</span>    gbdt.fit(X, y)</div>
<div class="line"><span class="lineno">  584</span>    np.testing.assert_allclose(gbdt.predict(X), y, atol=1e-4)</div>
<div class="line"><span class="lineno">  585</span> </div>
<div class="line"><span class="lineno">  586</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aef66043329bdf4f76fc09b2a1d1528a5" name="aef66043329bdf4f76fc09b2a1d1528a5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aef66043329bdf4f76fc09b2a1d1528a5">&#9670;&#160;</a></span>test_infinite_values_missing_values()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_infinite_values_missing_values </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  601</span><span class="keyword">def </span>test_infinite_values_missing_values():</div>
<div class="line"><span class="lineno">  602</span>    <span class="comment"># High level test making sure that inf and nan values are properly handled</span></div>
<div class="line"><span class="lineno">  603</span>    <span class="comment"># when both are present. This is similar to</span></div>
<div class="line"><span class="lineno">  604</span>    <span class="comment"># test_split_on_nan_with_infinite_values() in test_grower.py, though we</span></div>
<div class="line"><span class="lineno">  605</span>    <span class="comment"># cannot check the predictions for binned values here.</span></div>
<div class="line"><span class="lineno">  606</span> </div>
<div class="line"><span class="lineno">  607</span>    X = np.asarray([-np.inf, 0, 1, np.inf, np.nan]).reshape(-1, 1)</div>
<div class="line"><span class="lineno">  608</span>    y_isnan = np.isnan(X.ravel())</div>
<div class="line"><span class="lineno">  609</span>    y_isinf = X.ravel() == np.inf</div>
<div class="line"><span class="lineno">  610</span> </div>
<div class="line"><span class="lineno">  611</span>    stump_clf = HistGradientBoostingClassifier(</div>
<div class="line"><span class="lineno">  612</span>        min_samples_leaf=1, max_iter=1, learning_rate=1, max_depth=2</div>
<div class="line"><span class="lineno">  613</span>    )</div>
<div class="line"><span class="lineno">  614</span> </div>
<div class="line"><span class="lineno">  615</span>    <span class="keyword">assert</span> stump_clf.fit(X, y_isinf).score(X, y_isinf) == 1</div>
<div class="line"><span class="lineno">  616</span>    <span class="keyword">assert</span> stump_clf.fit(X, y_isnan).score(X, y_isnan) == 1</div>
<div class="line"><span class="lineno">  617</span> </div>
<div class="line"><span class="lineno">  618</span> </div>
<div class="line"><span class="lineno">  619</span><span class="comment"># TODO(1.3): remove</span></div>
<div class="line"><span class="lineno">  620</span><span class="preprocessor">@pytest.mark.filterwarnings(&quot;ignore::FutureWarning&quot;)</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a6f21b56f9a5b98fc0dd76756ebe0efe7" name="a6f21b56f9a5b98fc0dd76756ebe0efe7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6f21b56f9a5b98fc0dd76756ebe0efe7">&#9670;&#160;</a></span>test_init_parameters_validation()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_init_parameters_validation </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>GradientBoosting</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>err_msg</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   82</span><span class="keyword">def </span>test_init_parameters_validation(GradientBoosting, X, y, params, err_msg):</div>
<div class="line"><span class="lineno">   83</span> </div>
<div class="line"><span class="lineno">   84</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=err_msg):</div>
<div class="line"><span class="lineno">   85</span>        GradientBoosting(**params).fit(X, y)</div>
<div class="line"><span class="lineno">   86</span> </div>
<div class="line"><span class="lineno">   87</span> </div>
<div class="line"><span class="lineno">   88</span><span class="comment"># TODO(1.3): remove</span></div>
<div class="line"><span class="lineno">   89</span><span class="preprocessor">@pytest.mark.filterwarnings(&quot;ignore::FutureWarning&quot;)</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a7d5029f1dcb1cd33faf267a057ef3e82" name="a7d5029f1dcb1cd33faf267a057ef3e82"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7d5029f1dcb1cd33faf267a057ef3e82">&#9670;&#160;</a></span>test_interaction_cst_numerically()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_interaction_cst_numerically </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check that interaction constraints have no forbidden interactions.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1212</span><span class="keyword">def </span>test_interaction_cst_numerically():</div>
<div class="line"><span class="lineno"> 1213</span>    <span class="stringliteral">&quot;&quot;&quot;Check that interaction constraints have no forbidden interactions.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1214</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno"> 1215</span>    n_samples = 1000</div>
<div class="line"><span class="lineno"> 1216</span>    X = rng.uniform(size=(n_samples, 2))</div>
<div class="line"><span class="lineno"> 1217</span>    <span class="comment"># Construct y with a strong interaction term</span></div>
<div class="line"><span class="lineno"> 1218</span>    <span class="comment"># y = x0 + x1 + 5 * x0 * x1</span></div>
<div class="line"><span class="lineno"> 1219</span>    y = np.hstack((X, 5 * X[:, [0]] * X[:, [1]])).sum(axis=1)</div>
<div class="line"><span class="lineno"> 1220</span> </div>
<div class="line"><span class="lineno"> 1221</span>    est = HistGradientBoostingRegressor(random_state=42)</div>
<div class="line"><span class="lineno"> 1222</span>    est.fit(X, y)</div>
<div class="line"><span class="lineno"> 1223</span>    est_no_interactions = HistGradientBoostingRegressor(</div>
<div class="line"><span class="lineno"> 1224</span>        interaction_cst=[{0}, {1}], random_state=42</div>
<div class="line"><span class="lineno"> 1225</span>    )</div>
<div class="line"><span class="lineno"> 1226</span>    est_no_interactions.fit(X, y)</div>
<div class="line"><span class="lineno"> 1227</span> </div>
<div class="line"><span class="lineno"> 1228</span>    delta = 0.25</div>
<div class="line"><span class="lineno"> 1229</span>    <span class="comment"># Make sure we do not extrapolate out of the training set as tree-based estimators</span></div>
<div class="line"><span class="lineno"> 1230</span>    <span class="comment"># are very bad in doing so.</span></div>
<div class="line"><span class="lineno"> 1231</span>    X_test = X[(X[:, 0] &lt; 1 - delta) &amp; (X[:, 1] &lt; 1 - delta)]</div>
<div class="line"><span class="lineno"> 1232</span>    X_delta_d_0 = X_test + [delta, 0]</div>
<div class="line"><span class="lineno"> 1233</span>    X_delta_0_d = X_test + [0, delta]</div>
<div class="line"><span class="lineno"> 1234</span>    X_delta_d_d = X_test + [delta, delta]</div>
<div class="line"><span class="lineno"> 1235</span> </div>
<div class="line"><span class="lineno"> 1236</span>    <span class="comment"># Note: For the y from above as a function of x0 and x1, we have</span></div>
<div class="line"><span class="lineno"> 1237</span>    <span class="comment"># y(x0+d, x1+d) = y(x0, x1) + 5 * d * (2/5 + x0 + x1) + 5 * d**2</span></div>
<div class="line"><span class="lineno"> 1238</span>    <span class="comment"># y(x0+d, x1)   = y(x0, x1) + 5 * d * (1/5 + x1)</span></div>
<div class="line"><span class="lineno"> 1239</span>    <span class="comment"># y(x0,   x1+d) = y(x0, x1) + 5 * d * (1/5 + x0)</span></div>
<div class="line"><span class="lineno"> 1240</span>    <span class="comment"># Without interaction constraints, we would expect a result of 5 * d**2 for the</span></div>
<div class="line"><span class="lineno"> 1241</span>    <span class="comment"># following expression, but zero with constraints in place.</span></div>
<div class="line"><span class="lineno"> 1242</span>    assert_allclose(</div>
<div class="line"><span class="lineno"> 1243</span>        est_no_interactions.predict(X_delta_d_d)</div>
<div class="line"><span class="lineno"> 1244</span>        + est_no_interactions.predict(X_test)</div>
<div class="line"><span class="lineno"> 1245</span>        - est_no_interactions.predict(X_delta_d_0)</div>
<div class="line"><span class="lineno"> 1246</span>        - est_no_interactions.predict(X_delta_0_d),</div>
<div class="line"><span class="lineno"> 1247</span>        0,</div>
<div class="line"><span class="lineno"> 1248</span>        atol=1e-12,</div>
<div class="line"><span class="lineno"> 1249</span>    )</div>
<div class="line"><span class="lineno"> 1250</span> </div>
<div class="line"><span class="lineno"> 1251</span>    <span class="comment"># Correct result of the expressions is 5 * delta**2. But this is hard to achieve by</span></div>
<div class="line"><span class="lineno"> 1252</span>    <span class="comment"># a fitted tree-based model. However, with 100 iterations the expression should</span></div>
<div class="line"><span class="lineno"> 1253</span>    <span class="comment"># at least be positive!</span></div>
<div class="line"><span class="lineno"> 1254</span>    <span class="keyword">assert</span> np.all(</div>
<div class="line"><span class="lineno"> 1255</span>        est.predict(X_delta_d_d)</div>
<div class="line"><span class="lineno"> 1256</span>        + est.predict(X_test)</div>
<div class="line"><span class="lineno"> 1257</span>        - est.predict(X_delta_d_0)</div>
<div class="line"><span class="lineno"> 1258</span>        - est.predict(X_delta_0_d)</div>
<div class="line"><span class="lineno"> 1259</span>        &gt; 0.01</div>
<div class="line"><span class="lineno"> 1260</span>    )</div>
<div class="line"><span class="lineno"> 1261</span> </div>
<div class="line"><span class="lineno"> 1262</span> </div>
<div class="line"><span class="lineno"> 1263</span><span class="comment"># TODO(1.3): Remove</span></div>
<div class="line"><span class="lineno"> 1264</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1265</span>    <span class="stringliteral">&quot;old_loss, new_loss, Estimator&quot;</span>,</div>
<div class="line"><span class="lineno"> 1266</span>    [</div>
<div class="line"><span class="lineno"> 1267</span>        (<span class="stringliteral">&quot;auto&quot;</span>, <span class="stringliteral">&quot;log_loss&quot;</span>, HistGradientBoostingClassifier),</div>
<div class="line"><span class="lineno"> 1268</span>        (<span class="stringliteral">&quot;binary_crossentropy&quot;</span>, <span class="stringliteral">&quot;log_loss&quot;</span>, HistGradientBoostingClassifier),</div>
<div class="line"><span class="lineno"> 1269</span>        (<span class="stringliteral">&quot;categorical_crossentropy&quot;</span>, <span class="stringliteral">&quot;log_loss&quot;</span>, HistGradientBoostingClassifier),</div>
<div class="line"><span class="lineno"> 1270</span>    ],</div>
<div class="line"><span class="lineno"> 1271</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a00e8ad7affd51f3ff468e68cf2d80d25" name="a00e8ad7affd51f3ff468e68cf2d80d25"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00e8ad7affd51f3ff468e68cf2d80d25">&#9670;&#160;</a></span>test_invalid_classification_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_invalid_classification_loss </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   90</span><span class="keyword">def </span>test_invalid_classification_loss():</div>
<div class="line"><span class="lineno">   91</span>    binary_clf = HistGradientBoostingClassifier(loss=<span class="stringliteral">&quot;binary_crossentropy&quot;</span>)</div>
<div class="line"><span class="lineno">   92</span>    err_msg = (</div>
<div class="line"><span class="lineno">   93</span>        <span class="stringliteral">&quot;loss=&#39;binary_crossentropy&#39; is not defined for multiclass &quot;</span></div>
<div class="line"><span class="lineno">   94</span>        <span class="stringliteral">&quot;classification with n_classes=3, use &quot;</span></div>
<div class="line"><span class="lineno">   95</span>        <span class="stringliteral">&quot;loss=&#39;log_loss&#39; instead&quot;</span></div>
<div class="line"><span class="lineno">   96</span>    )</div>
<div class="line"><span class="lineno">   97</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=err_msg):</div>
<div class="line"><span class="lineno">   98</span>        binary_clf.fit(np.zeros(shape=(3, 2)), np.arange(3))</div>
<div class="line"><span class="lineno">   99</span> </div>
<div class="line"><span class="lineno">  100</span> </div>
<div class="line"><span class="lineno">  101</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  102</span>    <span class="stringliteral">&quot;scoring, validation_fraction, early_stopping, n_iter_no_change, tol&quot;</span>,</div>
<div class="line"><span class="lineno">  103</span>    [</div>
<div class="line"><span class="lineno">  104</span>        (<span class="stringliteral">&quot;neg_mean_squared_error&quot;</span>, 0.1, <span class="keyword">True</span>, 5, 1e-7),  <span class="comment"># use scorer</span></div>
<div class="line"><span class="lineno">  105</span>        (<span class="stringliteral">&quot;neg_mean_squared_error&quot;</span>, <span class="keywordtype">None</span>, <span class="keyword">True</span>, 5, 1e-1),  <span class="comment"># use scorer on train</span></div>
<div class="line"><span class="lineno">  106</span>        (<span class="keywordtype">None</span>, 0.1, <span class="keyword">True</span>, 5, 1e-7),  <span class="comment"># same with default scorer</span></div>
<div class="line"><span class="lineno">  107</span>        (<span class="keywordtype">None</span>, <span class="keywordtype">None</span>, <span class="keyword">True</span>, 5, 1e-1),</div>
<div class="line"><span class="lineno">  108</span>        (<span class="stringliteral">&quot;loss&quot;</span>, 0.1, <span class="keyword">True</span>, 5, 1e-7),  <span class="comment"># use loss</span></div>
<div class="line"><span class="lineno">  109</span>        (<span class="stringliteral">&quot;loss&quot;</span>, <span class="keywordtype">None</span>, <span class="keyword">True</span>, 5, 1e-1),  <span class="comment"># use loss on training data</span></div>
<div class="line"><span class="lineno">  110</span>        (<span class="keywordtype">None</span>, <span class="keywordtype">None</span>, <span class="keyword">False</span>, 5, 0.0),  <span class="comment"># no early stopping</span></div>
<div class="line"><span class="lineno">  111</span>    ],</div>
<div class="line"><span class="lineno">  112</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="aab3a23e46f1a8237f993e35afefaa37d" name="aab3a23e46f1a8237f993e35afefaa37d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aab3a23e46f1a8237f993e35afefaa37d">&#9670;&#160;</a></span>test_loss_deprecated()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_loss_deprecated </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>old_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>new_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Estimator</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1272</span><span class="keyword">def </span>test_loss_deprecated(old_loss, new_loss, Estimator):</div>
<div class="line"><span class="lineno"> 1273</span>    <span class="keywordflow">if</span> old_loss == <span class="stringliteral">&quot;categorical_crossentropy&quot;</span>:</div>
<div class="line"><span class="lineno"> 1274</span>        X, y = X_multi_classification[:10], y_multi_classification[:10]</div>
<div class="line"><span class="lineno"> 1275</span>        <span class="keyword">assert</span> len(np.unique(y)) &gt; 2</div>
<div class="line"><span class="lineno"> 1276</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1277</span>        X, y = X_classification[:10], y_classification[:10]</div>
<div class="line"><span class="lineno"> 1278</span> </div>
<div class="line"><span class="lineno"> 1279</span>    est1 = Estimator(loss=old_loss, random_state=0)</div>
<div class="line"><span class="lineno"> 1280</span> </div>
<div class="line"><span class="lineno"> 1281</span>    <span class="keyword">with</span> pytest.warns(FutureWarning, match=f<span class="stringliteral">&quot;The loss &#39;{old_loss}&#39; was deprecated&quot;</span>):</div>
<div class="line"><span class="lineno"> 1282</span>        est1.fit(X, y)</div>
<div class="line"><span class="lineno"> 1283</span> </div>
<div class="line"><span class="lineno"> 1284</span>    est2 = Estimator(loss=new_loss, random_state=0)</div>
<div class="line"><span class="lineno"> 1285</span>    est2.fit(X, y)</div>
<div class="line"><span class="lineno"> 1286</span>    assert_allclose(est1.predict(X), est2.predict(X))</div>
<div class="line"><span class="lineno"> 1287</span> </div>
<div class="line"><span class="lineno"> 1288</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a66910d2da73db4ffd930410fc979a81f" name="a66910d2da73db4ffd930410fc979a81f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a66910d2da73db4ffd930410fc979a81f">&#9670;&#160;</a></span>test_max_depth_max_leaf_nodes()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_max_depth_max_leaf_nodes </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  791</span><span class="keyword">def </span>test_max_depth_max_leaf_nodes():</div>
<div class="line"><span class="lineno">  792</span>    <span class="comment"># Non regression test for</span></div>
<div class="line"><span class="lineno">  793</span>    <span class="comment"># https://github.com/scikit-learn/scikit-learn/issues/16179</span></div>
<div class="line"><span class="lineno">  794</span>    <span class="comment"># there was a bug when the max_depth and the max_leaf_nodes criteria were</span></div>
<div class="line"><span class="lineno">  795</span>    <span class="comment"># met at the same time, which would lead to max_leaf_nodes not being</span></div>
<div class="line"><span class="lineno">  796</span>    <span class="comment"># respected.</span></div>
<div class="line"><span class="lineno">  797</span>    X, y = make_classification(random_state=0)</div>
<div class="line"><span class="lineno">  798</span>    est = HistGradientBoostingClassifier(max_depth=2, max_leaf_nodes=3, max_iter=1).fit(</div>
<div class="line"><span class="lineno">  799</span>        X, y</div>
<div class="line"><span class="lineno">  800</span>    )</div>
<div class="line"><span class="lineno">  801</span>    tree = est._predictors[0][0]</div>
<div class="line"><span class="lineno">  802</span>    <span class="keyword">assert</span> tree.get_max_depth() == 2</div>
<div class="line"><span class="lineno">  803</span>    <span class="keyword">assert</span> tree.get_n_leaf_nodes() == 3  <span class="comment"># would be 4 prior to bug fix</span></div>
<div class="line"><span class="lineno">  804</span> </div>
<div class="line"><span class="lineno">  805</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a8d1c27c8cd23d18f669d498771f553c0" name="a8d1c27c8cd23d18f669d498771f553c0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8d1c27c8cd23d18f669d498771f553c0">&#9670;&#160;</a></span>test_missing_values_minmax_imputation()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_missing_values_minmax_imputation </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  474</span><span class="keyword">def </span>test_missing_values_minmax_imputation():</div>
<div class="line"><span class="lineno">  475</span>    <span class="comment"># Compare the buit-in missing value handling of Histogram GBC with an</span></div>
<div class="line"><span class="lineno">  476</span>    <span class="comment"># a-priori missing value imputation strategy that should yield the same</span></div>
<div class="line"><span class="lineno">  477</span>    <span class="comment"># results in terms of decision function.</span></div>
<div class="line"><span class="lineno">  478</span>    <span class="comment">#</span></div>
<div class="line"><span class="lineno">  479</span>    <span class="comment"># Each feature (containing NaNs) is replaced by 2 features:</span></div>
<div class="line"><span class="lineno">  480</span>    <span class="comment"># - one where the nans are replaced by min(feature) - 1</span></div>
<div class="line"><span class="lineno">  481</span>    <span class="comment"># - one where the nans are replaced by max(feature) + 1</span></div>
<div class="line"><span class="lineno">  482</span>    <span class="comment"># A split where nans go to the left has an equivalent split in the</span></div>
<div class="line"><span class="lineno">  483</span>    <span class="comment"># first (min) feature, and a split where nans go to the right has an</span></div>
<div class="line"><span class="lineno">  484</span>    <span class="comment"># equivalent split in the second (max) feature.</span></div>
<div class="line"><span class="lineno">  485</span>    <span class="comment">#</span></div>
<div class="line"><span class="lineno">  486</span>    <span class="comment"># Assuming the data is such that there is never a tie to select the best</span></div>
<div class="line"><span class="lineno">  487</span>    <span class="comment"># feature to split on during training, the learned decision trees should be</span></div>
<div class="line"><span class="lineno">  488</span>    <span class="comment"># strictly equivalent (learn a sequence of splits that encode the same</span></div>
<div class="line"><span class="lineno">  489</span>    <span class="comment"># decision function).</span></div>
<div class="line"><span class="lineno">  490</span>    <span class="comment">#</span></div>
<div class="line"><span class="lineno">  491</span>    <span class="comment"># The MinMaxImputer transformer is meant to be a toy implementation of the</span></div>
<div class="line"><span class="lineno">  492</span>    <span class="comment"># &quot;Missing In Attributes&quot; (MIA) missing value handling for decision trees</span></div>
<div class="line"><span class="lineno">  493</span>    <span class="comment"># https://www.sciencedirect.com/science/article/abs/pii/S0167865508000305</span></div>
<div class="line"><span class="lineno">  494</span>    <span class="comment"># The implementation of MIA as an imputation transformer was suggested by</span></div>
<div class="line"><span class="lineno">  495</span>    <span class="comment"># &quot;Remark 3&quot; in :arxiv:&#39;&lt;1902.06931&gt;`</span></div>
<div class="line"><span class="lineno">  496</span> </div>
<div class="line"><span class="lineno">  497</span>    <span class="keyword">class </span>MinMaxImputer(TransformerMixin, BaseEstimator):</div>
<div class="line"><span class="lineno">  498</span>        <span class="keyword">def </span>fit(self, X, y=None):</div>
<div class="line"><span class="lineno">  499</span>            mm = MinMaxScaler().fit(X)</div>
<div class="line"><span class="lineno">  500</span>            self.data_min_ = mm.data_min_</div>
<div class="line"><span class="lineno">  501</span>            self.data_max_ = mm.data_max_</div>
<div class="line"><span class="lineno">  502</span>            <span class="keywordflow">return</span> self</div>
<div class="line"><span class="lineno">  503</span> </div>
<div class="line"><span class="lineno">  504</span>        <span class="keyword">def </span>transform(self, X):</div>
<div class="line"><span class="lineno">  505</span>            X_min, X_max = X.copy(), X.copy()</div>
<div class="line"><span class="lineno">  506</span> </div>
<div class="line"><span class="lineno">  507</span>            <span class="keywordflow">for</span> feature_idx <span class="keywordflow">in</span> range(X.shape[1]):</div>
<div class="line"><span class="lineno">  508</span>                nan_mask = np.isnan(X[:, feature_idx])</div>
<div class="line"><span class="lineno">  509</span>                X_min[nan_mask, feature_idx] = self.data_min_[feature_idx] - 1</div>
<div class="line"><span class="lineno">  510</span>                X_max[nan_mask, feature_idx] = self.data_max_[feature_idx] + 1</div>
<div class="line"><span class="lineno">  511</span> </div>
<div class="line"><span class="lineno">  512</span>            <span class="keywordflow">return</span> np.concatenate([X_min, X_max], axis=1)</div>
<div class="line"><span class="lineno">  513</span> </div>
<div class="line"><span class="lineno">  514</span>    <span class="keyword">def </span>make_missing_value_data(n_samples=int(1e4), seed=0):</div>
<div class="line"><span class="lineno">  515</span>        rng = np.random.RandomState(seed)</div>
<div class="line"><span class="lineno">  516</span>        X, y = make_regression(n_samples=n_samples, n_features=4, random_state=rng)</div>
<div class="line"><span class="lineno">  517</span> </div>
<div class="line"><span class="lineno">  518</span>        <span class="comment"># Pre-bin the data to ensure a deterministic handling by the 2</span></div>
<div class="line"><span class="lineno">  519</span>        <span class="comment"># strategies and also make it easier to insert np.nan in a structured</span></div>
<div class="line"><span class="lineno">  520</span>        <span class="comment"># way:</span></div>
<div class="line"><span class="lineno">  521</span>        X = KBinsDiscretizer(n_bins=42, encode=<span class="stringliteral">&quot;ordinal&quot;</span>).fit_transform(X)</div>
<div class="line"><span class="lineno">  522</span> </div>
<div class="line"><span class="lineno">  523</span>        <span class="comment"># First feature has missing values completely at random:</span></div>
<div class="line"><span class="lineno">  524</span>        rnd_mask = rng.rand(X.shape[0]) &gt; 0.9</div>
<div class="line"><span class="lineno">  525</span>        X[rnd_mask, 0] = np.nan</div>
<div class="line"><span class="lineno">  526</span> </div>
<div class="line"><span class="lineno">  527</span>        <span class="comment"># Second and third features have missing values for extreme values</span></div>
<div class="line"><span class="lineno">  528</span>        <span class="comment"># (censoring missingness):</span></div>
<div class="line"><span class="lineno">  529</span>        low_mask = X[:, 1] == 0</div>
<div class="line"><span class="lineno">  530</span>        X[low_mask, 1] = np.nan</div>
<div class="line"><span class="lineno">  531</span> </div>
<div class="line"><span class="lineno">  532</span>        high_mask = X[:, 2] == X[:, 2].max()</div>
<div class="line"><span class="lineno">  533</span>        X[high_mask, 2] = np.nan</div>
<div class="line"><span class="lineno">  534</span> </div>
<div class="line"><span class="lineno">  535</span>        <span class="comment"># Make the last feature nan pattern very informative:</span></div>
<div class="line"><span class="lineno">  536</span>        y_max = np.percentile(y, 70)</div>
<div class="line"><span class="lineno">  537</span>        y_max_mask = y &gt;= y_max</div>
<div class="line"><span class="lineno">  538</span>        y[y_max_mask] = y_max</div>
<div class="line"><span class="lineno">  539</span>        X[y_max_mask, 3] = np.nan</div>
<div class="line"><span class="lineno">  540</span> </div>
<div class="line"><span class="lineno">  541</span>        <span class="comment"># Check that there is at least one missing value in each feature:</span></div>
<div class="line"><span class="lineno">  542</span>        <span class="keywordflow">for</span> feature_idx <span class="keywordflow">in</span> range(X.shape[1]):</div>
<div class="line"><span class="lineno">  543</span>            <span class="keyword">assert</span> any(np.isnan(X[:, feature_idx]))</div>
<div class="line"><span class="lineno">  544</span> </div>
<div class="line"><span class="lineno">  545</span>        <span class="comment"># Let&#39;s use a test set to check that the learned decision function is</span></div>
<div class="line"><span class="lineno">  546</span>        <span class="comment"># the same as evaluated on unseen data. Otherwise it could just be the</span></div>
<div class="line"><span class="lineno">  547</span>        <span class="comment"># case that we find two independent ways to overfit the training set.</span></div>
<div class="line"><span class="lineno">  548</span>        <span class="keywordflow">return</span> train_test_split(X, y, random_state=rng)</div>
<div class="line"><span class="lineno">  549</span> </div>
<div class="line"><span class="lineno">  550</span>    <span class="comment"># n_samples need to be large enough to minimize the likelihood of having</span></div>
<div class="line"><span class="lineno">  551</span>    <span class="comment"># several candidate splits with the same gain value in a given tree.</span></div>
<div class="line"><span class="lineno">  552</span>    X_train, X_test, y_train, y_test = make_missing_value_data(</div>
<div class="line"><span class="lineno">  553</span>        n_samples=int(1e4), seed=0</div>
<div class="line"><span class="lineno">  554</span>    )</div>
<div class="line"><span class="lineno">  555</span> </div>
<div class="line"><span class="lineno">  556</span>    <span class="comment"># Use a small number of leaf nodes and iterations so as to keep</span></div>
<div class="line"><span class="lineno">  557</span>    <span class="comment"># under-fitting models to minimize the likelihood of ties when training the</span></div>
<div class="line"><span class="lineno">  558</span>    <span class="comment"># model.</span></div>
<div class="line"><span class="lineno">  559</span>    gbm1 = HistGradientBoostingRegressor(max_iter=100, max_leaf_nodes=5, random_state=0)</div>
<div class="line"><span class="lineno">  560</span>    gbm1.fit(X_train, y_train)</div>
<div class="line"><span class="lineno">  561</span> </div>
<div class="line"><span class="lineno">  562</span>    gbm2 = make_pipeline(MinMaxImputer(), clone(gbm1))</div>
<div class="line"><span class="lineno">  563</span>    gbm2.fit(X_train, y_train)</div>
<div class="line"><span class="lineno">  564</span> </div>
<div class="line"><span class="lineno">  565</span>    <span class="comment"># Check that the model reach the same score:</span></div>
<div class="line"><span class="lineno">  566</span>    <span class="keyword">assert</span> gbm1.score(X_train, y_train) == pytest.approx(gbm2.score(X_train, y_train))</div>
<div class="line"><span class="lineno">  567</span> </div>
<div class="line"><span class="lineno">  568</span>    <span class="keyword">assert</span> gbm1.score(X_test, y_test) == pytest.approx(gbm2.score(X_test, y_test))</div>
<div class="line"><span class="lineno">  569</span> </div>
<div class="line"><span class="lineno">  570</span>    <span class="comment"># Check the individual prediction match as a finer grained</span></div>
<div class="line"><span class="lineno">  571</span>    <span class="comment"># decision function check.</span></div>
<div class="line"><span class="lineno">  572</span>    assert_allclose(gbm1.predict(X_train), gbm2.predict(X_train))</div>
<div class="line"><span class="lineno">  573</span>    assert_allclose(gbm1.predict(X_test), gbm2.predict(X_test))</div>
<div class="line"><span class="lineno">  574</span> </div>
<div class="line"><span class="lineno">  575</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a62337de7919430e3a4f86839fcf9f9f4" name="a62337de7919430e3a4f86839fcf9f9f4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a62337de7919430e3a4f86839fcf9f9f4">&#9670;&#160;</a></span>test_missing_values_resilience()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_missing_values_resilience </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>problem</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>missing_proportion</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>expected_min_score_classification</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>expected_min_score_regression</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  386</span>):</div>
<div class="line"><span class="lineno">  387</span>    <span class="comment"># Make sure the estimators can deal with missing values and still yield</span></div>
<div class="line"><span class="lineno">  388</span>    <span class="comment"># decent predictions</span></div>
<div class="line"><span class="lineno">  389</span> </div>
<div class="line"><span class="lineno">  390</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  391</span>    n_samples = 1000</div>
<div class="line"><span class="lineno">  392</span>    n_features = 2</div>
<div class="line"><span class="lineno">  393</span>    <span class="keywordflow">if</span> problem == <span class="stringliteral">&quot;regression&quot;</span>:</div>
<div class="line"><span class="lineno">  394</span>        X, y = make_regression(</div>
<div class="line"><span class="lineno">  395</span>            n_samples=n_samples,</div>
<div class="line"><span class="lineno">  396</span>            n_features=n_features,</div>
<div class="line"><span class="lineno">  397</span>            n_informative=n_features,</div>
<div class="line"><span class="lineno">  398</span>            random_state=rng,</div>
<div class="line"><span class="lineno">  399</span>        )</div>
<div class="line"><span class="lineno">  400</span>        gb = HistGradientBoostingRegressor()</div>
<div class="line"><span class="lineno">  401</span>        expected_min_score = expected_min_score_regression</div>
<div class="line"><span class="lineno">  402</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  403</span>        X, y = make_classification(</div>
<div class="line"><span class="lineno">  404</span>            n_samples=n_samples,</div>
<div class="line"><span class="lineno">  405</span>            n_features=n_features,</div>
<div class="line"><span class="lineno">  406</span>            n_informative=n_features,</div>
<div class="line"><span class="lineno">  407</span>            n_redundant=0,</div>
<div class="line"><span class="lineno">  408</span>            n_repeated=0,</div>
<div class="line"><span class="lineno">  409</span>            random_state=rng,</div>
<div class="line"><span class="lineno">  410</span>        )</div>
<div class="line"><span class="lineno">  411</span>        gb = HistGradientBoostingClassifier()</div>
<div class="line"><span class="lineno">  412</span>        expected_min_score = expected_min_score_classification</div>
<div class="line"><span class="lineno">  413</span> </div>
<div class="line"><span class="lineno">  414</span>    mask = rng.binomial(1, missing_proportion, size=X.shape).astype(bool)</div>
<div class="line"><span class="lineno">  415</span>    X[mask] = np.nan</div>
<div class="line"><span class="lineno">  416</span> </div>
<div class="line"><span class="lineno">  417</span>    gb.fit(X, y)</div>
<div class="line"><span class="lineno">  418</span> </div>
<div class="line"><span class="lineno">  419</span>    <span class="keyword">assert</span> gb.score(X, y) &gt; expected_min_score</div>
<div class="line"><span class="lineno">  420</span> </div>
<div class="line"><span class="lineno">  421</span> </div>
<div class="line"><span class="lineno">  422</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  423</span>    <span class="stringliteral">&quot;data&quot;</span>,</div>
<div class="line"><span class="lineno">  424</span>    [</div>
<div class="line"><span class="lineno">  425</span>        make_classification(random_state=0, n_classes=2),</div>
<div class="line"><span class="lineno">  426</span>        make_classification(random_state=0, n_classes=3, n_informative=3),</div>
<div class="line"><span class="lineno">  427</span>    ],</div>
<div class="line"><span class="lineno">  428</span>    ids=[<span class="stringliteral">&quot;binary_log_loss&quot;</span>, <span class="stringliteral">&quot;multiclass_log_loss&quot;</span>],</div>
<div class="line"><span class="lineno">  429</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="acb9d774447896ad1daf647c80d490d85" name="acb9d774447896ad1daf647c80d490d85"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acb9d774447896ad1daf647c80d490d85">&#9670;&#160;</a></span>test_missing_values_trivial()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_missing_values_trivial </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  356</span><span class="keyword">def </span>test_missing_values_trivial():</div>
<div class="line"><span class="lineno">  357</span>    <span class="comment"># sanity check for missing values support. With only one feature and</span></div>
<div class="line"><span class="lineno">  358</span>    <span class="comment"># y == isnan(X), the gbdt is supposed to reach perfect accuracy on the</span></div>
<div class="line"><span class="lineno">  359</span>    <span class="comment"># training set.</span></div>
<div class="line"><span class="lineno">  360</span> </div>
<div class="line"><span class="lineno">  361</span>    n_samples = 100</div>
<div class="line"><span class="lineno">  362</span>    n_features = 1</div>
<div class="line"><span class="lineno">  363</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  364</span> </div>
<div class="line"><span class="lineno">  365</span>    X = rng.normal(size=(n_samples, n_features))</div>
<div class="line"><span class="lineno">  366</span>    mask = rng.binomial(1, 0.5, size=X.shape).astype(bool)</div>
<div class="line"><span class="lineno">  367</span>    X[mask] = np.nan</div>
<div class="line"><span class="lineno">  368</span>    y = mask.ravel()</div>
<div class="line"><span class="lineno">  369</span>    gb = HistGradientBoostingClassifier()</div>
<div class="line"><span class="lineno">  370</span>    gb.fit(X, y)</div>
<div class="line"><span class="lineno">  371</span> </div>
<div class="line"><span class="lineno">  372</span>    <span class="keyword">assert</span> gb.score(X, y) == pytest.approx(1)</div>
<div class="line"><span class="lineno">  373</span> </div>
<div class="line"><span class="lineno">  374</span> </div>
<div class="line"><span class="lineno">  375</span><span class="preprocessor">@pytest.mark.parametrize(&quot;problem&quot;, (&quot;classification&quot;, &quot;regression&quot;)</span>)</div>
<div class="line"><span class="lineno">  376</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  377</span>    <span class="stringliteral">&quot;missing_proportion, expected_min_score_classification, &quot;</span></div>
<div class="line"><span class="lineno">  378</span>    <span class="stringliteral">&quot;expected_min_score_regression&quot;</span>,</div>
<div class="line"><span class="lineno">  379</span>    [(0.1, 0.97, 0.89), (0.2, 0.93, 0.81), (0.5, 0.79, 0.52)],</div>
<div class="line"><span class="lineno">  380</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a962466734911480f7433bcd355c21ae4" name="a962466734911480f7433bcd355c21ae4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a962466734911480f7433bcd355c21ae4">&#9670;&#160;</a></span>test_no_user_warning_with_scoring()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_no_user_warning_with_scoring </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check that no UserWarning is raised when scoring is set.

Non-regression test for #22907.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1289</span><span class="keyword">def </span>test_no_user_warning_with_scoring():</div>
<div class="line"><span class="lineno"> 1290</span>    <span class="stringliteral">&quot;&quot;&quot;Check that no UserWarning is raised when scoring is set.</span></div>
<div class="line"><span class="lineno"> 1291</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1292</span><span class="stringliteral">    Non-regression test for #22907.</span></div>
<div class="line"><span class="lineno"> 1293</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1294</span>    pd = pytest.importorskip(<span class="stringliteral">&quot;pandas&quot;</span>)</div>
<div class="line"><span class="lineno"> 1295</span>    X, y = make_regression(n_samples=50, random_state=0)</div>
<div class="line"><span class="lineno"> 1296</span>    X_df = pd.DataFrame(X, columns=[f<span class="stringliteral">&quot;col{i}&quot;</span> <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(X.shape[1])])</div>
<div class="line"><span class="lineno"> 1297</span> </div>
<div class="line"><span class="lineno"> 1298</span>    est = HistGradientBoostingRegressor(</div>
<div class="line"><span class="lineno"> 1299</span>        random_state=0, scoring=<span class="stringliteral">&quot;neg_mean_absolute_error&quot;</span>, early_stopping=<span class="keyword">True</span></div>
<div class="line"><span class="lineno"> 1300</span>    )</div>
<div class="line"><span class="lineno"> 1301</span>    <span class="keyword">with</span> warnings.catch_warnings():</div>
<div class="line"><span class="lineno"> 1302</span>        warnings.simplefilter(<span class="stringliteral">&quot;error&quot;</span>, UserWarning)</div>
<div class="line"><span class="lineno"> 1303</span>        est.fit(X_df, y)</div>
<div class="line"><span class="lineno"> 1304</span> </div>
<div class="line"><span class="lineno"> 1305</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ad1eec4d622764adbc5e7c7f556843700" name="ad1eec4d622764adbc5e7c7f556843700"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad1eec4d622764adbc5e7c7f556843700">&#9670;&#160;</a></span>test_poisson()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_poisson </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  298</span><span class="keyword">def </span>test_poisson():</div>
<div class="line"><span class="lineno">  299</span>    <span class="comment"># For Poisson distributed target, Poisson loss should give better results</span></div>
<div class="line"><span class="lineno">  300</span>    <span class="comment"># than least squares measured in Poisson deviance as metric.</span></div>
<div class="line"><span class="lineno">  301</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno">  302</span>    n_train, n_test, n_features = 500, 100, 100</div>
<div class="line"><span class="lineno">  303</span>    X = make_low_rank_matrix(</div>
<div class="line"><span class="lineno">  304</span>        n_samples=n_train + n_test, n_features=n_features, random_state=rng</div>
<div class="line"><span class="lineno">  305</span>    )</div>
<div class="line"><span class="lineno">  306</span>    <span class="comment"># We create a log-linear Poisson model and downscale coef as it will get</span></div>
<div class="line"><span class="lineno">  307</span>    <span class="comment"># exponentiated.</span></div>
<div class="line"><span class="lineno">  308</span>    coef = rng.uniform(low=-2, high=2, size=n_features) / np.max(X, axis=0)</div>
<div class="line"><span class="lineno">  309</span>    y = rng.poisson(lam=np.exp(X @ coef))</div>
<div class="line"><span class="lineno">  310</span>    X_train, X_test, y_train, y_test = train_test_split(</div>
<div class="line"><span class="lineno">  311</span>        X, y, test_size=n_test, random_state=rng</div>
<div class="line"><span class="lineno">  312</span>    )</div>
<div class="line"><span class="lineno">  313</span>    gbdt_pois = HistGradientBoostingRegressor(loss=<span class="stringliteral">&quot;poisson&quot;</span>, random_state=rng)</div>
<div class="line"><span class="lineno">  314</span>    gbdt_ls = HistGradientBoostingRegressor(loss=<span class="stringliteral">&quot;squared_error&quot;</span>, random_state=rng)</div>
<div class="line"><span class="lineno">  315</span>    gbdt_pois.fit(X_train, y_train)</div>
<div class="line"><span class="lineno">  316</span>    gbdt_ls.fit(X_train, y_train)</div>
<div class="line"><span class="lineno">  317</span>    dummy = DummyRegressor(strategy=<span class="stringliteral">&quot;mean&quot;</span>).fit(X_train, y_train)</div>
<div class="line"><span class="lineno">  318</span> </div>
<div class="line"><span class="lineno">  319</span>    <span class="keywordflow">for</span> X, y <span class="keywordflow">in</span> [(X_train, y_train), (X_test, y_test)]:</div>
<div class="line"><span class="lineno">  320</span>        metric_pois = mean_poisson_deviance(y, gbdt_pois.predict(X))</div>
<div class="line"><span class="lineno">  321</span>        <span class="comment"># squared_error might produce non-positive predictions =&gt; clip</span></div>
<div class="line"><span class="lineno">  322</span>        metric_ls = mean_poisson_deviance(y, np.clip(gbdt_ls.predict(X), 1e-15, <span class="keywordtype">None</span>))</div>
<div class="line"><span class="lineno">  323</span>        metric_dummy = mean_poisson_deviance(y, dummy.predict(X))</div>
<div class="line"><span class="lineno">  324</span>        <span class="keyword">assert</span> metric_pois &lt; metric_ls</div>
<div class="line"><span class="lineno">  325</span>        <span class="keyword">assert</span> metric_pois &lt; metric_dummy</div>
<div class="line"><span class="lineno">  326</span> </div>
<div class="line"><span class="lineno">  327</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a1ca254ed0aeec61714fe1c104fc56f1b" name="a1ca254ed0aeec61714fe1c104fc56f1b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1ca254ed0aeec61714fe1c104fc56f1b">&#9670;&#160;</a></span>test_poisson_y_positive()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_poisson_y_positive </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  290</span><span class="keyword">def </span>test_poisson_y_positive(y):</div>
<div class="line"><span class="lineno">  291</span>    <span class="comment"># Test that ValueError is raised if either one y_i &lt; 0 or sum(y_i) &lt;= 0.</span></div>
<div class="line"><span class="lineno">  292</span>    err_msg = <span class="stringliteral">r&quot;loss=&#39;poisson&#39; requires non-negative y and sum\&zwj;(y\&zwj;) &gt; 0.&quot;</span></div>
<div class="line"><span class="lineno">  293</span>    gbdt = HistGradientBoostingRegressor(loss=<span class="stringliteral">&quot;poisson&quot;</span>, random_state=0)</div>
<div class="line"><span class="lineno">  294</span>    <span class="keyword">with</span> pytest.raises(ValueError, match=err_msg):</div>
<div class="line"><span class="lineno">  295</span>        gbdt.fit(np.zeros(shape=(len(y), 1)), y)</div>
<div class="line"><span class="lineno">  296</span> </div>
<div class="line"><span class="lineno">  297</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a3667ed8a538d1a730f6723864d553955" name="a3667ed8a538d1a730f6723864d553955"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3667ed8a538d1a730f6723864d553955">&#9670;&#160;</a></span>test_sample_weight_effect()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_sample_weight_effect </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>problem</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>duplication</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  682</span><span class="keyword">def </span>test_sample_weight_effect(problem, duplication):</div>
<div class="line"><span class="lineno">  683</span>    <span class="comment"># High level test to make sure that duplicating a sample is equivalent to</span></div>
<div class="line"><span class="lineno">  684</span>    <span class="comment"># giving it weight of 2.</span></div>
<div class="line"><span class="lineno">  685</span> </div>
<div class="line"><span class="lineno">  686</span>    <span class="comment"># fails for n_samples &gt; 255 because binning does not take sample weights</span></div>
<div class="line"><span class="lineno">  687</span>    <span class="comment"># into account. Keeping n_samples &lt;= 255 makes</span></div>
<div class="line"><span class="lineno">  688</span>    <span class="comment"># sure only unique values are used so SW have no effect on binning.</span></div>
<div class="line"><span class="lineno">  689</span>    n_samples = 255</div>
<div class="line"><span class="lineno">  690</span>    n_features = 2</div>
<div class="line"><span class="lineno">  691</span>    <span class="keywordflow">if</span> problem == <span class="stringliteral">&quot;regression&quot;</span>:</div>
<div class="line"><span class="lineno">  692</span>        X, y = make_regression(</div>
<div class="line"><span class="lineno">  693</span>            n_samples=n_samples,</div>
<div class="line"><span class="lineno">  694</span>            n_features=n_features,</div>
<div class="line"><span class="lineno">  695</span>            n_informative=n_features,</div>
<div class="line"><span class="lineno">  696</span>            random_state=0,</div>
<div class="line"><span class="lineno">  697</span>        )</div>
<div class="line"><span class="lineno">  698</span>        Klass = HistGradientBoostingRegressor</div>
<div class="line"><span class="lineno">  699</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  700</span>        n_classes = 2 <span class="keywordflow">if</span> problem == <span class="stringliteral">&quot;binary_classification&quot;</span> <span class="keywordflow">else</span> 3</div>
<div class="line"><span class="lineno">  701</span>        X, y = make_classification(</div>
<div class="line"><span class="lineno">  702</span>            n_samples=n_samples,</div>
<div class="line"><span class="lineno">  703</span>            n_features=n_features,</div>
<div class="line"><span class="lineno">  704</span>            n_informative=n_features,</div>
<div class="line"><span class="lineno">  705</span>            n_redundant=0,</div>
<div class="line"><span class="lineno">  706</span>            n_clusters_per_class=1,</div>
<div class="line"><span class="lineno">  707</span>            n_classes=n_classes,</div>
<div class="line"><span class="lineno">  708</span>            random_state=0,</div>
<div class="line"><span class="lineno">  709</span>        )</div>
<div class="line"><span class="lineno">  710</span>        Klass = HistGradientBoostingClassifier</div>
<div class="line"><span class="lineno">  711</span> </div>
<div class="line"><span class="lineno">  712</span>    <span class="comment"># This test can&#39;t pass if min_samples_leaf &gt; 1 because that would force 2</span></div>
<div class="line"><span class="lineno">  713</span>    <span class="comment"># samples to be in the same node in est_sw, while these samples would be</span></div>
<div class="line"><span class="lineno">  714</span>    <span class="comment"># free to be separate in est_dup: est_dup would just group together the</span></div>
<div class="line"><span class="lineno">  715</span>    <span class="comment"># duplicated samples.</span></div>
<div class="line"><span class="lineno">  716</span>    est = Klass(min_samples_leaf=1)</div>
<div class="line"><span class="lineno">  717</span> </div>
<div class="line"><span class="lineno">  718</span>    <span class="comment"># Create dataset with duplicate and corresponding sample weights</span></div>
<div class="line"><span class="lineno">  719</span>    <span class="keywordflow">if</span> duplication == <span class="stringliteral">&quot;half&quot;</span>:</div>
<div class="line"><span class="lineno">  720</span>        lim = n_samples // 2</div>
<div class="line"><span class="lineno">  721</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  722</span>        lim = n_samples</div>
<div class="line"><span class="lineno">  723</span>    X_dup = np.r_[X, X[:lim]]</div>
<div class="line"><span class="lineno">  724</span>    y_dup = np.r_[y, y[:lim]]</div>
<div class="line"><span class="lineno">  725</span>    sample_weight = np.ones(shape=(n_samples))</div>
<div class="line"><span class="lineno">  726</span>    sample_weight[:lim] = 2</div>
<div class="line"><span class="lineno">  727</span> </div>
<div class="line"><span class="lineno">  728</span>    est_sw = clone(est).fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  729</span>    est_dup = clone(est).fit(X_dup, y_dup)</div>
<div class="line"><span class="lineno">  730</span> </div>
<div class="line"><span class="lineno">  731</span>    <span class="comment"># checking raw_predict is stricter than just predict for classification</span></div>
<div class="line"><span class="lineno">  732</span>    <span class="keyword">assert</span> np.allclose(est_sw._raw_predict(X_dup), est_dup._raw_predict(X_dup))</div>
<div class="line"><span class="lineno">  733</span> </div>
<div class="line"><span class="lineno">  734</span> </div>
<div class="line"><span class="lineno">  735</span><span class="preprocessor">@pytest.mark.parametrize(&quot;Loss&quot;, (HalfSquaredError, AbsoluteError)</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a71608b821a528f4a0e7addc343a6faf2" name="a71608b821a528f4a0e7addc343a6faf2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a71608b821a528f4a0e7addc343a6faf2">&#9670;&#160;</a></span>test_should_stop()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_should_stop </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>scores</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_iter_no_change</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>tol</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>stopping</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  223</span><span class="keyword">def </span>test_should_stop(scores, n_iter_no_change, tol, stopping):</div>
<div class="line"><span class="lineno">  224</span> </div>
<div class="line"><span class="lineno">  225</span>    gbdt = HistGradientBoostingClassifier(n_iter_no_change=n_iter_no_change, tol=tol)</div>
<div class="line"><span class="lineno">  226</span>    <span class="keyword">assert</span> gbdt._should_stop(scores) == stopping</div>
<div class="line"><span class="lineno">  227</span> </div>
<div class="line"><span class="lineno">  228</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a636081db2cbbe8364936e3bc8e46adaf" name="a636081db2cbbe8364936e3bc8e46adaf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a636081db2cbbe8364936e3bc8e46adaf">&#9670;&#160;</a></span>test_single_node_trees()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_single_node_trees </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Est</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  828</span><span class="keyword">def </span>test_single_node_trees(Est):</div>
<div class="line"><span class="lineno">  829</span>    <span class="comment"># Make sure it&#39;s still possible to build single-node trees. In that case</span></div>
<div class="line"><span class="lineno">  830</span>    <span class="comment"># the value of the root is set to 0. That&#39;s a correct value: if the tree is</span></div>
<div class="line"><span class="lineno">  831</span>    <span class="comment"># single-node that&#39;s because min_gain_to_split is not respected right from</span></div>
<div class="line"><span class="lineno">  832</span>    <span class="comment"># the root, so we don&#39;t want the tree to have any impact on the</span></div>
<div class="line"><span class="lineno">  833</span>    <span class="comment"># predictions.</span></div>
<div class="line"><span class="lineno">  834</span> </div>
<div class="line"><span class="lineno">  835</span>    X, y = make_classification(random_state=0)</div>
<div class="line"><span class="lineno">  836</span>    y[:] = 1  <span class="comment"># constant target will lead to a single root node</span></div>
<div class="line"><span class="lineno">  837</span> </div>
<div class="line"><span class="lineno">  838</span>    est = Est(max_iter=20)</div>
<div class="line"><span class="lineno">  839</span>    est.fit(X, y)</div>
<div class="line"><span class="lineno">  840</span> </div>
<div class="line"><span class="lineno">  841</span>    <span class="keyword">assert</span> all(len(predictor[0].nodes) == 1 <span class="keywordflow">for</span> predictor <span class="keywordflow">in</span> est._predictors)</div>
<div class="line"><span class="lineno">  842</span>    <span class="keyword">assert</span> all(predictor[0].nodes[0][<span class="stringliteral">&quot;value&quot;</span>] == 0 <span class="keywordflow">for</span> predictor <span class="keywordflow">in</span> est._predictors)</div>
<div class="line"><span class="lineno">  843</span>    <span class="comment"># Still gives correct predictions thanks to the baseline prediction</span></div>
<div class="line"><span class="lineno">  844</span>    assert_allclose(est.predict(X), y)</div>
<div class="line"><span class="lineno">  845</span> </div>
<div class="line"><span class="lineno">  846</span> </div>
<div class="line"><span class="lineno">  847</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  848</span>    <span class="stringliteral">&quot;Est, loss, X, y&quot;</span>,</div>
<div class="line"><span class="lineno">  849</span>    [</div>
<div class="line"><span class="lineno">  850</span>        (</div>
<div class="line"><span class="lineno">  851</span>            HistGradientBoostingClassifier,</div>
<div class="line"><span class="lineno">  852</span>            HalfBinomialLoss(sample_weight=<span class="keywordtype">None</span>),</div>
<div class="line"><span class="lineno">  853</span>            X_classification,</div>
<div class="line"><span class="lineno">  854</span>            y_classification,</div>
<div class="line"><span class="lineno">  855</span>        ),</div>
<div class="line"><span class="lineno">  856</span>        (</div>
<div class="line"><span class="lineno">  857</span>            HistGradientBoostingRegressor,</div>
<div class="line"><span class="lineno">  858</span>            HalfSquaredError(sample_weight=<span class="keywordtype">None</span>),</div>
<div class="line"><span class="lineno">  859</span>            X_regression,</div>
<div class="line"><span class="lineno">  860</span>            y_regression,</div>
<div class="line"><span class="lineno">  861</span>        ),</div>
<div class="line"><span class="lineno">  862</span>    ],</div>
<div class="line"><span class="lineno">  863</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a5d544161b88105da941f5e573cf46b92" name="a5d544161b88105da941f5e573cf46b92"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5d544161b88105da941f5e573cf46b92">&#9670;&#160;</a></span>test_small_trainset()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_small_trainset </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  443</span><span class="keyword">def </span>test_small_trainset():</div>
<div class="line"><span class="lineno">  444</span>    <span class="comment"># Make sure that the small trainset is stratified and has the expected</span></div>
<div class="line"><span class="lineno">  445</span>    <span class="comment"># length (10k samples)</span></div>
<div class="line"><span class="lineno">  446</span>    n_samples = 20000</div>
<div class="line"><span class="lineno">  447</span>    original_distrib = {0: 0.1, 1: 0.2, 2: 0.3, 3: 0.4}</div>
<div class="line"><span class="lineno">  448</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno">  449</span>    X = rng.randn(n_samples).reshape(n_samples, 1)</div>
<div class="line"><span class="lineno">  450</span>    y = [</div>
<div class="line"><span class="lineno">  451</span>        [class_] * int(prop * n_samples) <span class="keywordflow">for</span> (class_, prop) <span class="keywordflow">in</span> original_distrib.items()</div>
<div class="line"><span class="lineno">  452</span>    ]</div>
<div class="line"><span class="lineno">  453</span>    y = shuffle(np.concatenate(y))</div>
<div class="line"><span class="lineno">  454</span>    gb = HistGradientBoostingClassifier()</div>
<div class="line"><span class="lineno">  455</span> </div>
<div class="line"><span class="lineno">  456</span>    <span class="comment"># Compute the small training set</span></div>
<div class="line"><span class="lineno">  457</span>    X_small, y_small, _ = gb._get_small_trainset(</div>
<div class="line"><span class="lineno">  458</span>        X, y, seed=42, sample_weight_train=<span class="keywordtype">None</span></div>
<div class="line"><span class="lineno">  459</span>    )</div>
<div class="line"><span class="lineno">  460</span> </div>
<div class="line"><span class="lineno">  461</span>    <span class="comment"># Compute the class distribution in the small training set</span></div>
<div class="line"><span class="lineno">  462</span>    unique, counts = np.unique(y_small, return_counts=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  463</span>    small_distrib = {class_: count / 10000 <span class="keywordflow">for</span> (class_, count) <span class="keywordflow">in</span> zip(unique, counts)}</div>
<div class="line"><span class="lineno">  464</span> </div>
<div class="line"><span class="lineno">  465</span>    <span class="comment"># Test that the small training set has the expected length</span></div>
<div class="line"><span class="lineno">  466</span>    <span class="keyword">assert</span> X_small.shape[0] == 10000</div>
<div class="line"><span class="lineno">  467</span>    <span class="keyword">assert</span> y_small.shape[0] == 10000</div>
<div class="line"><span class="lineno">  468</span> </div>
<div class="line"><span class="lineno">  469</span>    <span class="comment"># Test that the class distributions in the whole dataset and in the small</span></div>
<div class="line"><span class="lineno">  470</span>    <span class="comment"># training set are identical</span></div>
<div class="line"><span class="lineno">  471</span>    <span class="keyword">assert</span> small_distrib == pytest.approx(original_distrib)</div>
<div class="line"><span class="lineno">  472</span> </div>
<div class="line"><span class="lineno">  473</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a8d8340f954b874290fb03bff29a4d123" name="a8d8340f954b874290fb03bff29a4d123"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8d8340f954b874290fb03bff29a4d123">&#9670;&#160;</a></span>test_staged_predict()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_staged_predict </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>HistGradientBoosting</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  881</span><span class="keyword">def </span>test_staged_predict(HistGradientBoosting, X, y):</div>
<div class="line"><span class="lineno">  882</span> </div>
<div class="line"><span class="lineno">  883</span>    <span class="comment"># Test whether staged predictor eventually gives</span></div>
<div class="line"><span class="lineno">  884</span>    <span class="comment"># the same prediction.</span></div>
<div class="line"><span class="lineno">  885</span>    X_train, X_test, y_train, y_test = train_test_split(</div>
<div class="line"><span class="lineno">  886</span>        X, y, test_size=0.5, random_state=0</div>
<div class="line"><span class="lineno">  887</span>    )</div>
<div class="line"><span class="lineno">  888</span>    gb = HistGradientBoosting(max_iter=10)</div>
<div class="line"><span class="lineno">  889</span> </div>
<div class="line"><span class="lineno">  890</span>    <span class="comment"># test raise NotFittedError if not fitted</span></div>
<div class="line"><span class="lineno">  891</span>    <span class="keyword">with</span> pytest.raises(NotFittedError):</div>
<div class="line"><span class="lineno">  892</span>        next(gb.staged_predict(X_test))</div>
<div class="line"><span class="lineno">  893</span> </div>
<div class="line"><span class="lineno">  894</span>    gb.fit(X_train, y_train)</div>
<div class="line"><span class="lineno">  895</span> </div>
<div class="line"><span class="lineno">  896</span>    <span class="comment"># test if the staged predictions of each iteration</span></div>
<div class="line"><span class="lineno">  897</span>    <span class="comment"># are equal to the corresponding predictions of the same estimator</span></div>
<div class="line"><span class="lineno">  898</span>    <span class="comment"># trained from scratch.</span></div>
<div class="line"><span class="lineno">  899</span>    <span class="comment"># this also test limit case when max_iter = 1</span></div>
<div class="line"><span class="lineno">  900</span>    method_names = (</div>
<div class="line"><span class="lineno">  901</span>        [<span class="stringliteral">&quot;predict&quot;</span>]</div>
<div class="line"><span class="lineno">  902</span>        <span class="keywordflow">if</span> is_regressor(gb)</div>
<div class="line"><span class="lineno">  903</span>        <span class="keywordflow">else</span> [<span class="stringliteral">&quot;predict&quot;</span>, <span class="stringliteral">&quot;predict_proba&quot;</span>, <span class="stringliteral">&quot;decision_function&quot;</span>]</div>
<div class="line"><span class="lineno">  904</span>    )</div>
<div class="line"><span class="lineno">  905</span>    <span class="keywordflow">for</span> method_name <span class="keywordflow">in</span> method_names:</div>
<div class="line"><span class="lineno">  906</span> </div>
<div class="line"><span class="lineno">  907</span>        staged_method = getattr(gb, <span class="stringliteral">&quot;staged_&quot;</span> + method_name)</div>
<div class="line"><span class="lineno">  908</span>        staged_predictions = list(staged_method(X_test))</div>
<div class="line"><span class="lineno">  909</span>        <span class="keyword">assert</span> len(staged_predictions) == gb.n_iter_</div>
<div class="line"><span class="lineno">  910</span>        <span class="keywordflow">for</span> n_iter, staged_predictions <span class="keywordflow">in</span> enumerate(staged_method(X_test), 1):</div>
<div class="line"><span class="lineno">  911</span>            aux = HistGradientBoosting(max_iter=n_iter)</div>
<div class="line"><span class="lineno">  912</span>            aux.fit(X_train, y_train)</div>
<div class="line"><span class="lineno">  913</span>            pred_aux = getattr(aux, method_name)(X_test)</div>
<div class="line"><span class="lineno">  914</span> </div>
<div class="line"><span class="lineno">  915</span>            assert_allclose(staged_predictions, pred_aux)</div>
<div class="line"><span class="lineno">  916</span>            <span class="keyword">assert</span> staged_predictions.shape == pred_aux.shape</div>
<div class="line"><span class="lineno">  917</span> </div>
<div class="line"><span class="lineno">  918</span> </div>
<div class="line"><span class="lineno">  919</span><span class="preprocessor">@pytest.mark.parametrize(&quot;insert_missing&quot;, [False, True])</span></div>
<div class="line"><span class="lineno">  920</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  921</span>    <span class="stringliteral">&quot;Est&quot;</span>, (HistGradientBoostingRegressor, HistGradientBoostingClassifier)</div>
<div class="line"><span class="lineno">  922</span>)</div>
<div class="line"><span class="lineno">  923</span><span class="preprocessor">@pytest.mark.parametrize(&quot;bool_categorical_parameter&quot;, [True, False])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a2ad1d0cb9b87956d4359cfb0ab36a525" name="a2ad1d0cb9b87956d4359cfb0ab36a525"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2ad1d0cb9b87956d4359cfb0ab36a525">&#9670;&#160;</a></span>test_string_target_early_stopping()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_string_target_early_stopping </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>scoring</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  634</span><span class="keyword">def </span>test_string_target_early_stopping(scoring):</div>
<div class="line"><span class="lineno">  635</span>    <span class="comment"># Regression tests for #14709 where the targets need to be encoded before</span></div>
<div class="line"><span class="lineno">  636</span>    <span class="comment"># to compute the score</span></div>
<div class="line"><span class="lineno">  637</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno">  638</span>    X = rng.randn(100, 10)</div>
<div class="line"><span class="lineno">  639</span>    y = np.array([<span class="stringliteral">&quot;x&quot;</span>] * 50 + [<span class="stringliteral">&quot;y&quot;</span>] * 50, dtype=object)</div>
<div class="line"><span class="lineno">  640</span>    gbrt = HistGradientBoostingClassifier(n_iter_no_change=10, scoring=scoring)</div>
<div class="line"><span class="lineno">  641</span>    gbrt.fit(X, y)</div>
<div class="line"><span class="lineno">  642</span> </div>
<div class="line"><span class="lineno">  643</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aa0238eda2a532e8bccf6b40679c22fa6" name="aa0238eda2a532e8bccf6b40679c22fa6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa0238eda2a532e8bccf6b40679c22fa6">&#9670;&#160;</a></span>test_sum_hessians_are_sample_weight()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_sum_hessians_are_sample_weight </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Loss</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  736</span><span class="keyword">def </span>test_sum_hessians_are_sample_weight(Loss):</div>
<div class="line"><span class="lineno">  737</span>    <span class="comment"># For losses with constant hessians, the sum_hessians field of the</span></div>
<div class="line"><span class="lineno">  738</span>    <span class="comment"># histograms must be equal to the sum of the sample weight of samples at</span></div>
<div class="line"><span class="lineno">  739</span>    <span class="comment"># the corresponding bin.</span></div>
<div class="line"><span class="lineno">  740</span> </div>
<div class="line"><span class="lineno">  741</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  742</span>    n_samples = 1000</div>
<div class="line"><span class="lineno">  743</span>    n_features = 2</div>
<div class="line"><span class="lineno">  744</span>    X, y = make_regression(n_samples=n_samples, n_features=n_features, random_state=rng)</div>
<div class="line"><span class="lineno">  745</span>    bin_mapper = _BinMapper()</div>
<div class="line"><span class="lineno">  746</span>    X_binned = bin_mapper.fit_transform(X)</div>
<div class="line"><span class="lineno">  747</span> </div>
<div class="line"><span class="lineno">  748</span>    <span class="comment"># While sample weights are supposed to be positive, this still works.</span></div>
<div class="line"><span class="lineno">  749</span>    sample_weight = rng.normal(size=n_samples)</div>
<div class="line"><span class="lineno">  750</span> </div>
<div class="line"><span class="lineno">  751</span>    loss = Loss(sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  752</span>    gradients, hessians = loss.init_gradient_and_hessian(</div>
<div class="line"><span class="lineno">  753</span>        n_samples=n_samples, dtype=G_H_DTYPE</div>
<div class="line"><span class="lineno">  754</span>    )</div>
<div class="line"><span class="lineno">  755</span>    gradients, hessians = gradients.reshape((-1, 1)), hessians.reshape((-1, 1))</div>
<div class="line"><span class="lineno">  756</span>    raw_predictions = rng.normal(size=(n_samples, 1))</div>
<div class="line"><span class="lineno">  757</span>    loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  758</span>        y_true=y,</div>
<div class="line"><span class="lineno">  759</span>        raw_prediction=raw_predictions,</div>
<div class="line"><span class="lineno">  760</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  761</span>        gradient_out=gradients,</div>
<div class="line"><span class="lineno">  762</span>        hessian_out=hessians,</div>
<div class="line"><span class="lineno">  763</span>        n_threads=n_threads,</div>
<div class="line"><span class="lineno">  764</span>    )</div>
<div class="line"><span class="lineno">  765</span> </div>
<div class="line"><span class="lineno">  766</span>    <span class="comment"># build sum_sample_weight which contains the sum of the sample weights at</span></div>
<div class="line"><span class="lineno">  767</span>    <span class="comment"># each bin (for each feature). This must be equal to the sum_hessians</span></div>
<div class="line"><span class="lineno">  768</span>    <span class="comment"># field of the corresponding histogram</span></div>
<div class="line"><span class="lineno">  769</span>    sum_sw = np.zeros(shape=(n_features, bin_mapper.n_bins))</div>
<div class="line"><span class="lineno">  770</span>    <span class="keywordflow">for</span> feature_idx <span class="keywordflow">in</span> range(n_features):</div>
<div class="line"><span class="lineno">  771</span>        <span class="keywordflow">for</span> sample_idx <span class="keywordflow">in</span> range(n_samples):</div>
<div class="line"><span class="lineno">  772</span>            sum_sw[feature_idx, X_binned[sample_idx, feature_idx]] += sample_weight[</div>
<div class="line"><span class="lineno">  773</span>                sample_idx</div>
<div class="line"><span class="lineno">  774</span>            ]</div>
<div class="line"><span class="lineno">  775</span> </div>
<div class="line"><span class="lineno">  776</span>    <span class="comment"># Build histogram</span></div>
<div class="line"><span class="lineno">  777</span>    grower = TreeGrower(</div>
<div class="line"><span class="lineno">  778</span>        X_binned, gradients[:, 0], hessians[:, 0], n_bins=bin_mapper.n_bins</div>
<div class="line"><span class="lineno">  779</span>    )</div>
<div class="line"><span class="lineno">  780</span>    histograms = grower.histogram_builder.compute_histograms_brute(</div>
<div class="line"><span class="lineno">  781</span>        grower.root.sample_indices</div>
<div class="line"><span class="lineno">  782</span>    )</div>
<div class="line"><span class="lineno">  783</span> </div>
<div class="line"><span class="lineno">  784</span>    <span class="keywordflow">for</span> feature_idx <span class="keywordflow">in</span> range(n_features):</div>
<div class="line"><span class="lineno">  785</span>        <span class="keywordflow">for</span> bin_idx <span class="keywordflow">in</span> range(bin_mapper.n_bins):</div>
<div class="line"><span class="lineno">  786</span>            <span class="keyword">assert</span> histograms[feature_idx, bin_idx][<span class="stringliteral">&quot;sum_hessians&quot;</span>] == (</div>
<div class="line"><span class="lineno">  787</span>                pytest.approx(sum_sw[feature_idx, bin_idx], rel=1e-5)</div>
<div class="line"><span class="lineno">  788</span>            )</div>
<div class="line"><span class="lineno">  789</span> </div>
<div class="line"><span class="lineno">  790</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ade78a35bb512556c552f74e5771952e0" name="ade78a35bb512556c552f74e5771952e0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ade78a35bb512556c552f74e5771952e0">&#9670;&#160;</a></span>test_uint8_predict()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_uint8_predict </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Est</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno"> 1178</span><span class="keyword">def </span>test_uint8_predict(Est):</div>
<div class="line"><span class="lineno"> 1179</span>    <span class="comment"># Non regression test for</span></div>
<div class="line"><span class="lineno"> 1180</span>    <span class="comment"># https://github.com/scikit-learn/scikit-learn/issues/18408</span></div>
<div class="line"><span class="lineno"> 1181</span>    <span class="comment"># Make sure X can be of dtype uint8 (i.e. X_BINNED_DTYPE) in predict. It</span></div>
<div class="line"><span class="lineno"> 1182</span>    <span class="comment"># will be converted to X_DTYPE.</span></div>
<div class="line"><span class="lineno"> 1183</span> </div>
<div class="line"><span class="lineno"> 1184</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno"> 1185</span> </div>
<div class="line"><span class="lineno"> 1186</span>    X = rng.randint(0, 100, size=(10, 2)).astype(np.uint8)</div>
<div class="line"><span class="lineno"> 1187</span>    y = rng.randint(0, 2, size=10).astype(np.uint8)</div>
<div class="line"><span class="lineno"> 1188</span>    est = Est()</div>
<div class="line"><span class="lineno"> 1189</span>    est.fit(X, y)</div>
<div class="line"><span class="lineno"> 1190</span>    est.predict(X)</div>
<div class="line"><span class="lineno"> 1191</span> </div>
<div class="line"><span class="lineno"> 1192</span> </div>
<div class="line"><span class="lineno"> 1193</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1194</span>    <span class="stringliteral">&quot;interaction_cst, n_features, result&quot;</span>,</div>
<div class="line"><span class="lineno"> 1195</span>    [</div>
<div class="line"><span class="lineno"> 1196</span>        (<span class="keywordtype">None</span>, 931, <span class="keywordtype">None</span>),</div>
<div class="line"><span class="lineno"> 1197</span>        ([{0, 1}], 2, [{0, 1}]),</div>
<div class="line"><span class="lineno"> 1198</span>        (<span class="stringliteral">&quot;pairwise&quot;</span>, 2, [{0, 1}]),</div>
<div class="line"><span class="lineno"> 1199</span>        (<span class="stringliteral">&quot;pairwise&quot;</span>, 4, [{0, 1}, {0, 2}, {0, 3}, {1, 2}, {1, 3}, {2, 3}]),</div>
<div class="line"><span class="lineno"> 1200</span>        (<span class="stringliteral">&quot;no_interactions&quot;</span>, 2, [{0}, {1}]),</div>
<div class="line"><span class="lineno"> 1201</span>        (<span class="stringliteral">&quot;no_interactions&quot;</span>, 4, [{0}, {1}, {2}, {3}]),</div>
<div class="line"><span class="lineno"> 1202</span>        ([(1, 0), [5, 1]], 6, [{0, 1}, {1, 5}, {2, 3, 4}]),</div>
<div class="line"><span class="lineno"> 1203</span>    ],</div>
<div class="line"><span class="lineno"> 1204</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ad2ef12f655c29b8826305a455bdc2993" name="ad2ef12f655c29b8826305a455bdc2993"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad2ef12f655c29b8826305a455bdc2993">&#9670;&#160;</a></span>test_unknown_categories_nan()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_unknown_categories_nan </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>insert_missing</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>Est</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>bool_categorical_parameter</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  924</span><span class="keyword">def </span>test_unknown_categories_nan(insert_missing, Est, bool_categorical_parameter):</div>
<div class="line"><span class="lineno">  925</span>    <span class="comment"># Make sure no error is raised at predict if a category wasn&#39;t seen during</span></div>
<div class="line"><span class="lineno">  926</span>    <span class="comment"># fit. We also make sure they&#39;re treated as nans.</span></div>
<div class="line"><span class="lineno">  927</span> </div>
<div class="line"><span class="lineno">  928</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  929</span>    n_samples = 1000</div>
<div class="line"><span class="lineno">  930</span>    f1 = rng.rand(n_samples)</div>
<div class="line"><span class="lineno">  931</span>    f2 = rng.randint(4, size=n_samples)</div>
<div class="line"><span class="lineno">  932</span>    X = np.c_[f1, f2]</div>
<div class="line"><span class="lineno">  933</span>    y = np.zeros(shape=n_samples)</div>
<div class="line"><span class="lineno">  934</span>    y[X[:, 1] % 2 == 0] = 1</div>
<div class="line"><span class="lineno">  935</span> </div>
<div class="line"><span class="lineno">  936</span>    <span class="keywordflow">if</span> bool_categorical_parameter:</div>
<div class="line"><span class="lineno">  937</span>        categorical_features = [<span class="keyword">False</span>, <span class="keyword">True</span>]</div>
<div class="line"><span class="lineno">  938</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  939</span>        categorical_features = [1]</div>
<div class="line"><span class="lineno">  940</span> </div>
<div class="line"><span class="lineno">  941</span>    <span class="keywordflow">if</span> insert_missing:</div>
<div class="line"><span class="lineno">  942</span>        mask = rng.binomial(1, 0.01, size=X.shape).astype(bool)</div>
<div class="line"><span class="lineno">  943</span>        <span class="keyword">assert</span> mask.sum() &gt; 0</div>
<div class="line"><span class="lineno">  944</span>        X[mask] = np.nan</div>
<div class="line"><span class="lineno">  945</span> </div>
<div class="line"><span class="lineno">  946</span>    est = Est(max_iter=20, categorical_features=categorical_features).fit(X, y)</div>
<div class="line"><span class="lineno">  947</span>    assert_array_equal(est.is_categorical_, [<span class="keyword">False</span>, <span class="keyword">True</span>])</div>
<div class="line"><span class="lineno">  948</span> </div>
<div class="line"><span class="lineno">  949</span>    <span class="comment"># Make sure no error is raised on unknown categories and nans</span></div>
<div class="line"><span class="lineno">  950</span>    <span class="comment"># unknown categories will be treated as nans</span></div>
<div class="line"><span class="lineno">  951</span>    X_test = np.zeros((10, X.shape[1]), dtype=float)</div>
<div class="line"><span class="lineno">  952</span>    X_test[:5, 1] = 30</div>
<div class="line"><span class="lineno">  953</span>    X_test[5:, 1] = np.nan</div>
<div class="line"><span class="lineno">  954</span>    <span class="keyword">assert</span> len(np.unique(est.predict(X_test))) == 1</div>
<div class="line"><span class="lineno">  955</span> </div>
<div class="line"><span class="lineno">  956</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a021d57b8ad64db46dc18a12ca277d99b" name="a021d57b8ad64db46dc18a12ca277d99b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a021d57b8ad64db46dc18a12ca277d99b">&#9670;&#160;</a></span>test_unknown_category_that_are_negative()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_unknown_category_that_are_negative </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Check that unknown categories that are negative does not error.

Non-regression test for #24274.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1361</span><span class="keyword">def </span>test_unknown_category_that_are_negative():</div>
<div class="line"><span class="lineno"> 1362</span>    <span class="stringliteral">&quot;&quot;&quot;Check that unknown categories that are negative does not error.</span></div>
<div class="line"><span class="lineno"> 1363</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1364</span><span class="stringliteral">    Non-regression test for #24274.</span></div>
<div class="line"><span class="lineno"> 1365</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1366</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno"> 1367</span>    n_samples = 1000</div>
<div class="line"><span class="lineno"> 1368</span>    X = np.c_[rng.rand(n_samples), rng.randint(4, size=n_samples)]</div>
<div class="line"><span class="lineno"> 1369</span>    y = np.zeros(shape=n_samples)</div>
<div class="line"><span class="lineno"> 1370</span>    y[X[:, 1] % 2 == 0] = 1</div>
<div class="line"><span class="lineno"> 1371</span> </div>
<div class="line"><span class="lineno"> 1372</span>    hist = HistGradientBoostingRegressor(</div>
<div class="line"><span class="lineno"> 1373</span>        random_state=0,</div>
<div class="line"><span class="lineno"> 1374</span>        categorical_features=[<span class="keyword">False</span>, <span class="keyword">True</span>],</div>
<div class="line"><span class="lineno"> 1375</span>        max_iter=10,</div>
<div class="line"><span class="lineno"> 1376</span>    ).fit(X, y)</div>
<div class="line"><span class="lineno"> 1377</span> </div>
<div class="line"><span class="lineno"> 1378</span>    <span class="comment"># Check that negative values from the second column are treated like a</span></div>
<div class="line"><span class="lineno"> 1379</span>    <span class="comment"># missing category</span></div>
<div class="line"><span class="lineno"> 1380</span>    X_test_neg = np.asarray([[1, -2], [3, -4]])</div>
<div class="line"><span class="lineno"> 1381</span>    X_test_nan = np.asarray([[1, np.nan], [3, np.nan]])</div>
<div class="line"><span class="lineno"> 1382</span> </div>
<div class="line"><span class="lineno"> 1383</span>    assert_allclose(hist.predict(X_test_neg), hist.predict(X_test_nan))</div>
</div><!-- fragment -->
</div>
</div>
<a id="af98154dd121ef172cb649b7721329366" name="af98154dd121ef172cb649b7721329366"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af98154dd121ef172cb649b7721329366">&#9670;&#160;</a></span>test_zero_division_hessians()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_zero_division_hessians </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>data</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  430</span><span class="keyword">def </span>test_zero_division_hessians(data):</div>
<div class="line"><span class="lineno">  431</span>    <span class="comment"># non regression test for issue #14018</span></div>
<div class="line"><span class="lineno">  432</span>    <span class="comment"># make sure we avoid zero division errors when computing the leaves values.</span></div>
<div class="line"><span class="lineno">  433</span> </div>
<div class="line"><span class="lineno">  434</span>    <span class="comment"># If the learning rate is too high, the raw predictions are bad and will</span></div>
<div class="line"><span class="lineno">  435</span>    <span class="comment"># saturate the softmax (or sigmoid in binary classif). This leads to</span></div>
<div class="line"><span class="lineno">  436</span>    <span class="comment"># probabilities being exactly 0 or 1, gradients being constant, and</span></div>
<div class="line"><span class="lineno">  437</span>    <span class="comment"># hessians being zero.</span></div>
<div class="line"><span class="lineno">  438</span>    X, y = data</div>
<div class="line"><span class="lineno">  439</span>    gb = HistGradientBoostingClassifier(learning_rate=100, max_iter=10)</div>
<div class="line"><span class="lineno">  440</span>    gb.fit(X, y)</div>
<div class="line"><span class="lineno">  441</span> </div>
<div class="line"><span class="lineno">  442</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="af655eec50eaa5cfa28efd6c5b1e127be" name="af655eec50eaa5cfa28efd6c5b1e127be"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af655eec50eaa5cfa28efd6c5b1e127be">&#9670;&#160;</a></span>test_zero_sample_weights_classification()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_zero_sample_weights_classification </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  657</span><span class="keyword">def </span>test_zero_sample_weights_classification():</div>
<div class="line"><span class="lineno">  658</span>    <span class="comment"># Make sure setting a SW to zero amounts to ignoring the corresponding</span></div>
<div class="line"><span class="lineno">  659</span>    <span class="comment"># sample</span></div>
<div class="line"><span class="lineno">  660</span> </div>
<div class="line"><span class="lineno">  661</span>    X = [[1, 0], [1, 0], [1, 0], [0, 1]]</div>
<div class="line"><span class="lineno">  662</span>    y = [0, 0, 1, 0]</div>
<div class="line"><span class="lineno">  663</span>    <span class="comment"># ignore the first 2 training samples by setting their weight to 0</span></div>
<div class="line"><span class="lineno">  664</span>    sample_weight = [0, 0, 1, 1]</div>
<div class="line"><span class="lineno">  665</span>    gb = HistGradientBoostingClassifier(loss=<span class="stringliteral">&quot;log_loss&quot;</span>, min_samples_leaf=1)</div>
<div class="line"><span class="lineno">  666</span>    gb.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  667</span>    assert_array_equal(gb.predict([[1, 0]]), [1])</div>
<div class="line"><span class="lineno">  668</span> </div>
<div class="line"><span class="lineno">  669</span>    X = [[1, 0], [1, 0], [1, 0], [0, 1], [1, 1]]</div>
<div class="line"><span class="lineno">  670</span>    y = [0, 0, 1, 0, 2]</div>
<div class="line"><span class="lineno">  671</span>    <span class="comment"># ignore the first 2 training samples by setting their weight to 0</span></div>
<div class="line"><span class="lineno">  672</span>    sample_weight = [0, 0, 1, 1, 1]</div>
<div class="line"><span class="lineno">  673</span>    gb = HistGradientBoostingClassifier(loss=<span class="stringliteral">&quot;log_loss&quot;</span>, min_samples_leaf=1)</div>
<div class="line"><span class="lineno">  674</span>    gb.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  675</span>    assert_array_equal(gb.predict([[1, 0]]), [1])</div>
<div class="line"><span class="lineno">  676</span> </div>
<div class="line"><span class="lineno">  677</span> </div>
<div class="line"><span class="lineno">  678</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  679</span>    <span class="stringliteral">&quot;problem&quot;</span>, (<span class="stringliteral">&quot;regression&quot;</span>, <span class="stringliteral">&quot;binary_classification&quot;</span>, <span class="stringliteral">&quot;multiclass_classification&quot;</span>)</div>
<div class="line"><span class="lineno">  680</span>)</div>
<div class="line"><span class="lineno">  681</span><span class="preprocessor">@pytest.mark.parametrize(&quot;duplication&quot;, (&quot;half&quot;, &quot;all&quot;)</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a2e3011584396e3739af6aa29337c97fc" name="a2e3011584396e3739af6aa29337c97fc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2e3011584396e3739af6aa29337c97fc">&#9670;&#160;</a></span>test_zero_sample_weights_regression()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_zero_sample_weights_regression </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">  644</span><span class="keyword">def </span>test_zero_sample_weights_regression():</div>
<div class="line"><span class="lineno">  645</span>    <span class="comment"># Make sure setting a SW to zero amounts to ignoring the corresponding</span></div>
<div class="line"><span class="lineno">  646</span>    <span class="comment"># sample</span></div>
<div class="line"><span class="lineno">  647</span> </div>
<div class="line"><span class="lineno">  648</span>    X = [[1, 0], [1, 0], [1, 0], [0, 1]]</div>
<div class="line"><span class="lineno">  649</span>    y = [0, 0, 1, 0]</div>
<div class="line"><span class="lineno">  650</span>    <span class="comment"># ignore the first 2 training samples by setting their weight to 0</span></div>
<div class="line"><span class="lineno">  651</span>    sample_weight = [0, 0, 1, 1]</div>
<div class="line"><span class="lineno">  652</span>    gb = HistGradientBoostingRegressor(min_samples_leaf=1)</div>
<div class="line"><span class="lineno">  653</span>    gb.fit(X, y, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  654</span>    <span class="keyword">assert</span> gb.predict([[1, 0]])[0] &gt; 0.5</div>
<div class="line"><span class="lineno">  655</span> </div>
<div class="line"><span class="lineno">  656</span> </div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a08352f5c493029cd45a0ff1b73355fff" name="a08352f5c493029cd45a0ff1b73355fff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a08352f5c493029cd45a0ff1b73355fff">&#9670;&#160;</a></span>data_max_</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.data_max_</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a2d399df1ab632341043d8db61fb5a21c" name="a2d399df1ab632341043d8db61fb5a21c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2d399df1ab632341043d8db61fb5a21c">&#9670;&#160;</a></span>data_min_</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.data_min_</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a3231ca533f46d5f35be1669f1f81e67c" name="a3231ca533f46d5f35be1669f1f81e67c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3231ca533f46d5f35be1669f1f81e67c">&#9670;&#160;</a></span>n_classes</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.n_classes</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a672b351a8ca5f4e702a2bb14e756af87" name="a672b351a8ca5f4e702a2bb14e756af87"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a672b351a8ca5f4e702a2bb14e756af87">&#9670;&#160;</a></span>n_informative</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.n_informative</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a66ae9539a1d8c5532a3a3afafce7b0b5" name="a66ae9539a1d8c5532a3a3afafce7b0b5"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a66ae9539a1d8c5532a3a3afafce7b0b5">&#9670;&#160;</a></span>n_threads</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.n_threads = _openmp_effective_n_threads()</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a12c321c4cccfc2c4ab5486c01b7e824f" name="a12c321c4cccfc2c4ab5486c01b7e824f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a12c321c4cccfc2c4ab5486c01b7e824f">&#9670;&#160;</a></span>random_state</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.random_state</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a1fc4d72d69604589b032f9a2bfecfce8" name="a1fc4d72d69604589b032f9a2bfecfce8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1fc4d72d69604589b032f9a2bfecfce8">&#9670;&#160;</a></span>X_classification</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_classification</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a5a0de99b8fbcc43b09704857ae574a91" name="a5a0de99b8fbcc43b09704857ae574a91"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5a0de99b8fbcc43b09704857ae574a91">&#9670;&#160;</a></span>X_multi_classification</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_multi_classification</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ad7b4f1981157cf99aa73d929a21c7558" name="ad7b4f1981157cf99aa73d929a21c7558"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad7b4f1981157cf99aa73d929a21c7558">&#9670;&#160;</a></span>X_regression</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_regression</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a768b0cf666f4a3541ab4bb014871f89b" name="a768b0cf666f4a3541ab4bb014871f89b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a768b0cf666f4a3541ab4bb014871f89b">&#9670;&#160;</a></span>y_classification</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.y_classification</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a82d9e0f19d5e973342b805041caa122f" name="a82d9e0f19d5e973342b805041caa122f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a82d9e0f19d5e973342b805041caa122f">&#9670;&#160;</a></span>y_multi_classification</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.y_multi_classification</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a1d7dfef02e15041ac27c5865809e63d0" name="a1d7dfef02e15041ac27c5865809e63d0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1d7dfef02e15041ac27c5865809e63d0">&#9670;&#160;</a></span>y_regression</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.y_regression</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
