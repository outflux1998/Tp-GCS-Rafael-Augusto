<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn._loss.tests.test_loss Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1__loss.html">_loss</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1__loss_1_1tests.html">tests</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html">test_loss</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">sklearn._loss.tests.test_loss Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a394f99548ee5e0c18657763c5c1c018d" id="r_a394f99548ee5e0c18657763c5c1c018d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a394f99548ee5e0c18657763c5c1c018d">loss_instance_name</a> (param)</td></tr>
<tr class="separator:a394f99548ee5e0c18657763c5c1c018d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2b1f37b47fcfcbb7332c3669b2ba8a6" id="r_aa2b1f37b47fcfcbb7332c3669b2ba8a6"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#aa2b1f37b47fcfcbb7332c3669b2ba8a6">random_y_true_raw_prediction</a> (loss, n_samples, y_bound=(-100, 100), raw_bound=(-5, 5), seed=42)</td></tr>
<tr class="separator:aa2b1f37b47fcfcbb7332c3669b2ba8a6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a431181b5602a1a7b33b258a69ed3d4a7" id="r_a431181b5602a1a7b33b258a69ed3d4a7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a431181b5602a1a7b33b258a69ed3d4a7">numerical_derivative</a> (<a class="el" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>, x, <a class="el" href="__lapack__subroutines_8h.html#a57833d05f43fd1408080af6eec88fc43">eps</a>)</td></tr>
<tr class="separator:a431181b5602a1a7b33b258a69ed3d4a7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5d4a6ec724cf2afd511ada41995b3a37" id="r_a5d4a6ec724cf2afd511ada41995b3a37"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a5d4a6ec724cf2afd511ada41995b3a37">test_loss_boundary</a> (loss)</td></tr>
<tr class="separator:a5d4a6ec724cf2afd511ada41995b3a37"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae56255afa73f8bec4bf8a84f72f047a8" id="r_ae56255afa73f8bec4bf8a84f72f047a8"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#ae56255afa73f8bec4bf8a84f72f047a8">test_loss_boundary_y_true</a> (loss, y_true_success, y_true_fail)</td></tr>
<tr class="separator:ae56255afa73f8bec4bf8a84f72f047a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7a99b30b2769b07502d139824291cd45" id="r_a7a99b30b2769b07502d139824291cd45"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a7a99b30b2769b07502d139824291cd45">test_loss_boundary_y_pred</a> (loss, y_pred_success, y_pred_fail)</td></tr>
<tr class="separator:a7a99b30b2769b07502d139824291cd45"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acbb64142e6d4598db1b0d8f4e6606630" id="r_acbb64142e6d4598db1b0d8f4e6606630"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#acbb64142e6d4598db1b0d8f4e6606630">test_loss_on_specific_values</a> (loss, y_true, raw_prediction, loss_true)</td></tr>
<tr class="separator:acbb64142e6d4598db1b0d8f4e6606630"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a244ef4411a18107728e1ac4966639003" id="r_a244ef4411a18107728e1ac4966639003"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a244ef4411a18107728e1ac4966639003">test_loss_dtype</a> (loss, readonly_memmap, dtype_in, dtype_out, sample_weight, out1, out2, n_threads)</td></tr>
<tr class="separator:a244ef4411a18107728e1ac4966639003"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a31f0b42e1adf3846ad9d2dd01f347af9" id="r_a31f0b42e1adf3846ad9d2dd01f347af9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a31f0b42e1adf3846ad9d2dd01f347af9">test_loss_same_as_C_functions</a> (loss, sample_weight)</td></tr>
<tr class="separator:a31f0b42e1adf3846ad9d2dd01f347af9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a00f38156d3d6c5364582a272001ae958" id="r_a00f38156d3d6c5364582a272001ae958"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a00f38156d3d6c5364582a272001ae958">test_loss_gradients_are_the_same</a> (loss, sample_weight, global_random_seed)</td></tr>
<tr class="separator:a00f38156d3d6c5364582a272001ae958"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a936efc1d6f952c5be473378a41cbf761" id="r_a936efc1d6f952c5be473378a41cbf761"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a936efc1d6f952c5be473378a41cbf761">test_sample_weight_multiplies</a> (loss, sample_weight, global_random_seed)</td></tr>
<tr class="separator:a936efc1d6f952c5be473378a41cbf761"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac5868a699406c3aec9683359fd63ebb1" id="r_ac5868a699406c3aec9683359fd63ebb1"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#ac5868a699406c3aec9683359fd63ebb1">test_graceful_squeezing</a> (loss)</td></tr>
<tr class="separator:ac5868a699406c3aec9683359fd63ebb1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae10d3bf88c3a64607a239fce5530e35a" id="r_ae10d3bf88c3a64607a239fce5530e35a"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#ae10d3bf88c3a64607a239fce5530e35a">test_loss_of_perfect_prediction</a> (loss, sample_weight)</td></tr>
<tr class="separator:ae10d3bf88c3a64607a239fce5530e35a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af6443b1da2b78215f19dadf136a19716" id="r_af6443b1da2b78215f19dadf136a19716"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#af6443b1da2b78215f19dadf136a19716">test_gradients_hessians_numerically</a> (loss, sample_weight, global_random_seed)</td></tr>
<tr class="separator:af6443b1da2b78215f19dadf136a19716"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae2cf5e4c5116a170f359167fcfeb8a54" id="r_ae2cf5e4c5116a170f359167fcfeb8a54"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#ae2cf5e4c5116a170f359167fcfeb8a54">test_derivatives</a> (loss, x0, y_true)</td></tr>
<tr class="separator:ae2cf5e4c5116a170f359167fcfeb8a54"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad18e8017ef3e06ab7571a39b5a0a626e" id="r_ad18e8017ef3e06ab7571a39b5a0a626e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#ad18e8017ef3e06ab7571a39b5a0a626e">test_loss_intercept_only</a> (loss, sample_weight)</td></tr>
<tr class="separator:ad18e8017ef3e06ab7571a39b5a0a626e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a83fe0602eb695611eaee9e0cea74d7aa" id="r_a83fe0602eb695611eaee9e0cea74d7aa"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a83fe0602eb695611eaee9e0cea74d7aa">test_specific_fit_intercept_only</a> (loss, <a class="el" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>, random_dist, global_random_seed)</td></tr>
<tr class="separator:a83fe0602eb695611eaee9e0cea74d7aa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2eeea9ba800414d69a0aae766c08d9a2" id="r_a2eeea9ba800414d69a0aae766c08d9a2"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a2eeea9ba800414d69a0aae766c08d9a2">test_multinomial_loss_fit_intercept_only</a> ()</td></tr>
<tr class="separator:a2eeea9ba800414d69a0aae766c08d9a2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a399c5812263096939327a59669033bb1" id="r_a399c5812263096939327a59669033bb1"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a399c5812263096939327a59669033bb1">test_binomial_and_multinomial_loss</a> (global_random_seed)</td></tr>
<tr class="separator:a399c5812263096939327a59669033bb1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a64e4f9e9c3d30c779dc1df184f70c62d" id="r_a64e4f9e9c3d30c779dc1df184f70c62d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a64e4f9e9c3d30c779dc1df184f70c62d">test_predict_proba</a> (loss, global_random_seed)</td></tr>
<tr class="separator:a64e4f9e9c3d30c779dc1df184f70c62d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aed5e6b902447152bb1f9f5d398830d9d" id="r_aed5e6b902447152bb1f9f5d398830d9d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#aed5e6b902447152bb1f9f5d398830d9d">test_init_gradient_and_hessians</a> (loss, sample_weight, dtype, <a class="el" href="__lapack__subroutines_8h.html#a9993259f1ab17738593f079acd0507d9">order</a>)</td></tr>
<tr class="separator:aed5e6b902447152bb1f9f5d398830d9d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a225e895b285451d57c5eb2c0f92642de" id="r_a225e895b285451d57c5eb2c0f92642de"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a225e895b285451d57c5eb2c0f92642de">test_init_gradient_and_hessian_raises</a> (loss, params, err_msg)</td></tr>
<tr class="separator:a225e895b285451d57c5eb2c0f92642de"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa9d437e26e291830a6f7f3b2107f1842" id="r_aa9d437e26e291830a6f7f3b2107f1842"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#aa9d437e26e291830a6f7f3b2107f1842">test_loss_init_parameter_validation</a> (loss, params, err_type, err_msg)</td></tr>
<tr class="separator:aa9d437e26e291830a6f7f3b2107f1842"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afdc7d3349965971783f33103c7debb69" id="r_afdc7d3349965971783f33103c7debb69"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#afdc7d3349965971783f33103c7debb69">test_loss_pickle</a> (loss)</td></tr>
<tr class="separator:afdc7d3349965971783f33103c7debb69"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa9bb886015d6009257e9a601b5be95ae" id="r_aa9bb886015d6009257e9a601b5be95ae"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#aa9bb886015d6009257e9a601b5be95ae">test_tweedie_log_identity_consistency</a> (p)</td></tr>
<tr class="separator:aa9bb886015d6009257e9a601b5be95ae"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a2e547bf309ead91bde7600be03fc01b9" id="r_a2e547bf309ead91bde7600be03fc01b9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a2e547bf309ead91bde7600be03fc01b9">ALL_LOSSES</a> = list(_LOSSES.values())</td></tr>
<tr class="separator:a2e547bf309ead91bde7600be03fc01b9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f4094434d6c3280473404eba33f0bbb" id="r_a4f4094434d6c3280473404eba33f0bbb"><td class="memItemLeft" align="right" valign="top">list&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a4f4094434d6c3280473404eba33f0bbb">LOSS_INSTANCES</a> = [loss() for loss <a class="el" href="__lapack__subroutines_8h.html#aa83d4778c28341ab79b01b2371f666fe">in</a> <a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a2e547bf309ead91bde7600be03fc01b9">ALL_LOSSES</a>]</td></tr>
<tr class="separator:a4f4094434d6c3280473404eba33f0bbb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1e75af958ca19de34410a62fbf0ecb9d" id="r_a1e75af958ca19de34410a62fbf0ecb9d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a1e75af958ca19de34410a62fbf0ecb9d">quantile</a></td></tr>
<tr class="separator:a1e75af958ca19de34410a62fbf0ecb9d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38f62f770434f1daf34751961f519abd" id="r_a38f62f770434f1daf34751961f519abd"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a38f62f770434f1daf34751961f519abd">power</a></td></tr>
<tr class="separator:a38f62f770434f1daf34751961f519abd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab59b142e46405d390a88235f892d31d3" id="r_ab59b142e46405d390a88235f892d31d3"><td class="memItemLeft" align="right" valign="top">list&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#ab59b142e46405d390a88235f892d31d3">Y_COMMON_PARAMS</a></td></tr>
<tr class="separator:ab59b142e46405d390a88235f892d31d3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a68afee3ba6c626df0015e63d1e26ed8a" id="r_a68afee3ba6c626df0015e63d1e26ed8a"><td class="memItemLeft" align="right" valign="top">list&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a68afee3ba6c626df0015e63d1e26ed8a">Y_TRUE_PARAMS</a></td></tr>
<tr class="separator:a68afee3ba6c626df0015e63d1e26ed8a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9579babf070569c6cd1fd44ae71142dc" id="r_a9579babf070569c6cd1fd44ae71142dc"><td class="memItemLeft" align="right" valign="top">list&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a9579babf070569c6cd1fd44ae71142dc">Y_PRED_PARAMS</a></td></tr>
<tr class="separator:a9579babf070569c6cd1fd44ae71142dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Function Documentation</h2>
<a id="a394f99548ee5e0c18657763c5c1c018d" name="a394f99548ee5e0c18657763c5c1c018d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a394f99548ee5e0c18657763c5c1c018d">&#9670;&#160;</a></span>loss_instance_name()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.loss_instance_name </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>param</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<div class="fragment"><div class="line"><span class="lineno">   51</span><span class="keyword">def </span>loss_instance_name(param):</div>
<div class="line"><span class="lineno">   52</span>    <span class="keywordflow">if</span> isinstance(param, BaseLoss):</div>
<div class="line"><span class="lineno">   53</span>        loss = param</div>
<div class="line"><span class="lineno">   54</span>        name = loss.__class__.__name__</div>
<div class="line"><span class="lineno">   55</span>        <span class="keywordflow">if</span> hasattr(loss, <span class="stringliteral">&quot;quantile&quot;</span>):</div>
<div class="line"><span class="lineno">   56</span>            name += f<span class="stringliteral">&quot;(quantile={loss.closs.quantile})&quot;</span></div>
<div class="line"><span class="lineno">   57</span>        <span class="keywordflow">elif</span> hasattr(loss, <span class="stringliteral">&quot;power&quot;</span>):</div>
<div class="line"><span class="lineno">   58</span>            name += f<span class="stringliteral">&quot;(power={loss.closs.power})&quot;</span></div>
<div class="line"><span class="lineno">   59</span>        <span class="keywordflow">return</span> name</div>
<div class="line"><span class="lineno">   60</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   61</span>        <span class="keywordflow">return</span> str(param)</div>
<div class="line"><span class="lineno">   62</span> </div>
<div class="line"><span class="lineno">   63</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a431181b5602a1a7b33b258a69ed3d4a7" name="a431181b5602a1a7b33b258a69ed3d4a7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a431181b5602a1a7b33b258a69ed3d4a7">&#9670;&#160;</a></span>numerical_derivative()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.numerical_derivative </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>func</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>eps</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Helper function for numerical (first) derivatives.</pre> <div class="fragment"><div class="line"><span class="lineno">  101</span><span class="keyword">def </span>numerical_derivative(func, x, eps):</div>
<div class="line"><span class="lineno">  102</span>    <span class="stringliteral">&quot;&quot;&quot;Helper function for numerical (first) derivatives.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  103</span>    <span class="comment"># For numerical derivatives, see</span></div>
<div class="line"><span class="lineno">  104</span>    <span class="comment"># https://en.wikipedia.org/wiki/Numerical_differentiation</span></div>
<div class="line"><span class="lineno">  105</span>    <span class="comment"># https://en.wikipedia.org/wiki/Finite_difference_coefficient</span></div>
<div class="line"><span class="lineno">  106</span>    <span class="comment"># We use central finite differences of accuracy 4.</span></div>
<div class="line"><span class="lineno">  107</span>    h = np.full_like(x, fill_value=eps)</div>
<div class="line"><span class="lineno">  108</span>    f_minus_2h = <a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>(x - 2 * h)</div>
<div class="line"><span class="lineno">  109</span>    f_minus_1h = <a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>(x - h)</div>
<div class="line"><span class="lineno">  110</span>    f_plus_1h = <a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>(x + h)</div>
<div class="line"><span class="lineno">  111</span>    f_plus_2h = <a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>(x + 2 * h)</div>
<div class="line"><span class="lineno">  112</span>    <span class="keywordflow">return</span> (-f_plus_2h + 8 * f_plus_1h - 8 * f_minus_1h + f_minus_2h) / (12.0 * eps)</div>
<div class="line"><span class="lineno">  113</span> </div>
<div class="line"><span class="lineno">  114</span> </div>
<div class="line"><span class="lineno">  115</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
<div class="ttc" id="acallback_2foo_8f_html_a565fe2cc583df102f120752b0011c330"><div class="ttname"><a href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a></div><div class="ttdeci">subroutine func(a)</div><div class="ttdef"><b>Definition</b> foo.f:9</div></div>
</div><!-- fragment -->
</div>
</div>
<a id="aa2b1f37b47fcfcbb7332c3669b2ba8a6" name="aa2b1f37b47fcfcbb7332c3669b2ba8a6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2b1f37b47fcfcbb7332c3669b2ba8a6">&#9670;&#160;</a></span>random_y_true_raw_prediction()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.random_y_true_raw_prediction </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_bound</em> = <code>(-100,&#160;100)</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>raw_bound</em> = <code>(-5,&#160;5)</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>seed</em> = <code>42</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Random generate y_true and raw_prediction in valid range.</pre> <div class="fragment"><div class="line"><span class="lineno">   66</span>):</div>
<div class="line"><span class="lineno">   67</span>    <span class="stringliteral">&quot;&quot;&quot;Random generate y_true and raw_prediction in valid range.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   68</span>    rng = np.random.RandomState(seed)</div>
<div class="line"><span class="lineno">   69</span>    <span class="keywordflow">if</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">   70</span>        raw_prediction = np.empty((n_samples, loss.n_classes))</div>
<div class="line"><span class="lineno">   71</span>        raw_prediction.flat[:] = rng.uniform(</div>
<div class="line"><span class="lineno">   72</span>            low=raw_bound[0],</div>
<div class="line"><span class="lineno">   73</span>            high=raw_bound[1],</div>
<div class="line"><span class="lineno">   74</span>            size=n_samples * loss.n_classes,</div>
<div class="line"><span class="lineno">   75</span>        )</div>
<div class="line"><span class="lineno">   76</span>        y_true = np.arange(n_samples).astype(float) % loss.n_classes</div>
<div class="line"><span class="lineno">   77</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   78</span>        <span class="comment"># If link is identity, we must respect the interval of y_pred:</span></div>
<div class="line"><span class="lineno">   79</span>        <span class="keywordflow">if</span> isinstance(loss.link, IdentityLink):</div>
<div class="line"><span class="lineno">   80</span>            low, high = _inclusive_low_high(loss.interval_y_pred)</div>
<div class="line"><span class="lineno">   81</span>            low = np.amax([low, raw_bound[0]])</div>
<div class="line"><span class="lineno">   82</span>            high = np.amin([high, raw_bound[1]])</div>
<div class="line"><span class="lineno">   83</span>            raw_bound = (low, high)</div>
<div class="line"><span class="lineno">   84</span>        raw_prediction = rng.uniform(</div>
<div class="line"><span class="lineno">   85</span>            low=raw_bound[0], high=raw_bound[1], size=n_samples</div>
<div class="line"><span class="lineno">   86</span>        )</div>
<div class="line"><span class="lineno">   87</span>        <span class="comment"># generate a y_true in valid range</span></div>
<div class="line"><span class="lineno">   88</span>        low, high = _inclusive_low_high(loss.interval_y_true)</div>
<div class="line"><span class="lineno">   89</span>        low = max(low, y_bound[0])</div>
<div class="line"><span class="lineno">   90</span>        high = min(high, y_bound[1])</div>
<div class="line"><span class="lineno">   91</span>        y_true = rng.uniform(low, high, size=n_samples)</div>
<div class="line"><span class="lineno">   92</span>        <span class="comment"># set some values at special boundaries</span></div>
<div class="line"><span class="lineno">   93</span>        <span class="keywordflow">if</span> loss.interval_y_true.low == 0 <span class="keywordflow">and</span> loss.interval_y_true.low_inclusive:</div>
<div class="line"><span class="lineno">   94</span>            y_true[:: (n_samples // 3)] = 0</div>
<div class="line"><span class="lineno">   95</span>        <span class="keywordflow">if</span> loss.interval_y_true.high == 1 <span class="keywordflow">and</span> loss.interval_y_true.high_inclusive:</div>
<div class="line"><span class="lineno">   96</span>            y_true[1 :: (n_samples // 3)] = 1</div>
<div class="line"><span class="lineno">   97</span> </div>
<div class="line"><span class="lineno">   98</span>    <span class="keywordflow">return</span> y_true, raw_prediction</div>
<div class="line"><span class="lineno">   99</span> </div>
<div class="line"><span class="lineno">  100</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a399c5812263096939327a59669033bb1" name="a399c5812263096939327a59669033bb1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a399c5812263096939327a59669033bb1">&#9670;&#160;</a></span>test_binomial_and_multinomial_loss()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_binomial_and_multinomial_loss </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that multinomial loss with n_classes = 2 is the same as binomial loss.</pre> <div class="fragment"><div class="line"><span class="lineno">  959</span><span class="keyword">def </span>test_binomial_and_multinomial_loss(global_random_seed):</div>
<div class="line"><span class="lineno">  960</span>    <span class="stringliteral">&quot;&quot;&quot;Test that multinomial loss with n_classes = 2 is the same as binomial loss.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  961</span>    rng = np.random.RandomState(global_random_seed)</div>
<div class="line"><span class="lineno">  962</span>    n_samples = 20</div>
<div class="line"><span class="lineno">  963</span>    binom = HalfBinomialLoss()</div>
<div class="line"><span class="lineno">  964</span>    multinom = HalfMultinomialLoss(n_classes=2)</div>
<div class="line"><span class="lineno">  965</span>    y_train = rng.randint(0, 2, size=n_samples).astype(np.float64)</div>
<div class="line"><span class="lineno">  966</span>    raw_prediction = rng.normal(size=n_samples)</div>
<div class="line"><span class="lineno">  967</span>    raw_multinom = np.empty((n_samples, 2))</div>
<div class="line"><span class="lineno">  968</span>    raw_multinom[:, 0] = -0.5 * raw_prediction</div>
<div class="line"><span class="lineno">  969</span>    raw_multinom[:, 1] = 0.5 * raw_prediction</div>
<div class="line"><span class="lineno">  970</span>    assert_allclose(</div>
<div class="line"><span class="lineno">  971</span>        binom.loss(y_true=y_train, raw_prediction=raw_prediction),</div>
<div class="line"><span class="lineno">  972</span>        multinom.loss(y_true=y_train, raw_prediction=raw_multinom),</div>
<div class="line"><span class="lineno">  973</span>    )</div>
<div class="line"><span class="lineno">  974</span> </div>
<div class="line"><span class="lineno">  975</span> </div>
<div class="line"><span class="lineno">  976</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="ae2cf5e4c5116a170f359167fcfeb8a54" name="ae2cf5e4c5116a170f359167fcfeb8a54"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae2cf5e4c5116a170f359167fcfeb8a54">&#9670;&#160;</a></span>test_derivatives()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_derivatives </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>x0</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_true</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that gradients are zero at the minimum of the loss.

We check this on a single value/sample using Halley's method with the
first and second order derivatives computed by the Loss instance.
Note that methods of Loss instances operate on arrays while the newton
root finder expects a scalar or a one-element array for this purpose.
</pre> <div class="fragment"><div class="line"><span class="lineno">  780</span><span class="keyword">def </span>test_derivatives(loss, x0, y_true):</div>
<div class="line"><span class="lineno">  781</span>    <span class="stringliteral">&quot;&quot;&quot;Test that gradients are zero at the minimum of the loss.</span></div>
<div class="line"><span class="lineno">  782</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  783</span><span class="stringliteral">    We check this on a single value/sample using Halley&#39;s method with the</span></div>
<div class="line"><span class="lineno">  784</span><span class="stringliteral">    first and second order derivatives computed by the Loss instance.</span></div>
<div class="line"><span class="lineno">  785</span><span class="stringliteral">    Note that methods of Loss instances operate on arrays while the newton</span></div>
<div class="line"><span class="lineno">  786</span><span class="stringliteral">    root finder expects a scalar or a one-element array for this purpose.</span></div>
<div class="line"><span class="lineno">  787</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  788</span>    loss = _LOSSES[loss](sample_weight=<span class="keywordtype">None</span>)</div>
<div class="line"><span class="lineno">  789</span>    y_true = np.array([y_true], dtype=np.float64)</div>
<div class="line"><span class="lineno">  790</span>    x0 = np.array([x0], dtype=np.float64)</div>
<div class="line"><span class="lineno">  791</span> </div>
<div class="line"><span class="lineno">  792</span>    <span class="keyword">def </span><a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>(x: np.ndarray) -&gt; np.ndarray:</div>
<div class="line"><span class="lineno">  793</span>        <span class="stringliteral">&quot;&quot;&quot;Compute loss plus constant term.</span></div>
<div class="line"><span class="lineno">  794</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  795</span><span class="stringliteral">        The constant term is such that the minimum function value is zero,</span></div>
<div class="line"><span class="lineno">  796</span><span class="stringliteral">        which is required by the Newton method.</span></div>
<div class="line"><span class="lineno">  797</span><span class="stringliteral">        &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  798</span>        <span class="keywordflow">return</span> loss.loss(</div>
<div class="line"><span class="lineno">  799</span>            y_true=y_true, raw_prediction=x</div>
<div class="line"><span class="lineno">  800</span>        ) + loss.constant_to_optimal_zero(y_true=y_true)</div>
<div class="line"><span class="lineno">  801</span> </div>
<div class="line"><span class="lineno">  802</span>    <span class="keyword">def </span>fprime(x: np.ndarray) -&gt; np.ndarray:</div>
<div class="line"><span class="lineno">  803</span>        <span class="keywordflow">return</span> loss.gradient(y_true=y_true, raw_prediction=x)</div>
<div class="line"><span class="lineno">  804</span> </div>
<div class="line"><span class="lineno">  805</span>    <span class="keyword">def </span>fprime2(x: np.ndarray) -&gt; np.ndarray:</div>
<div class="line"><span class="lineno">  806</span>        <span class="keywordflow">return</span> loss.gradient_hessian(y_true=y_true, raw_prediction=x)[1]</div>
<div class="line"><span class="lineno">  807</span> </div>
<div class="line"><span class="lineno">  808</span>    optimum = newton(</div>
<div class="line"><span class="lineno">  809</span>        func,</div>
<div class="line"><span class="lineno">  810</span>        x0=x0,</div>
<div class="line"><span class="lineno">  811</span>        fprime=fprime,</div>
<div class="line"><span class="lineno">  812</span>        fprime2=fprime2,</div>
<div class="line"><span class="lineno">  813</span>        maxiter=100,</div>
<div class="line"><span class="lineno">  814</span>        tol=5e-8,</div>
<div class="line"><span class="lineno">  815</span>    )</div>
<div class="line"><span class="lineno">  816</span> </div>
<div class="line"><span class="lineno">  817</span>    <span class="comment"># Need to ravel arrays because assert_allclose requires matching</span></div>
<div class="line"><span class="lineno">  818</span>    <span class="comment"># dimensions.</span></div>
<div class="line"><span class="lineno">  819</span>    y_true = y_true.ravel()</div>
<div class="line"><span class="lineno">  820</span>    optimum = optimum.ravel()</div>
<div class="line"><span class="lineno">  821</span>    assert_allclose(loss.link.inverse(optimum), y_true)</div>
<div class="line"><span class="lineno">  822</span>    assert_allclose(<a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>(optimum), 0, atol=1e-14)</div>
<div class="line"><span class="lineno">  823</span>    assert_allclose(loss.gradient(y_true=y_true, raw_prediction=optimum), 0, atol=5e-7)</div>
<div class="line"><span class="lineno">  824</span> </div>
<div class="line"><span class="lineno">  825</span> </div>
<div class="line"><span class="lineno">  826</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
<div class="line"><span class="lineno">  827</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, &quot;range&quot;])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="ac5868a699406c3aec9683359fd63ebb1" name="ac5868a699406c3aec9683359fd63ebb1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac5868a699406c3aec9683359fd63ebb1">&#9670;&#160;</a></span>test_graceful_squeezing()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_graceful_squeezing </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that reshaped raw_prediction gives same results.</pre> <div class="fragment"><div class="line"><span class="lineno">  591</span><span class="keyword">def </span>test_graceful_squeezing(loss):</div>
<div class="line"><span class="lineno">  592</span>    <span class="stringliteral">&quot;&quot;&quot;Test that reshaped raw_prediction gives same results.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  593</span>    y_true, raw_prediction = random_y_true_raw_prediction(</div>
<div class="line"><span class="lineno">  594</span>        loss=loss,</div>
<div class="line"><span class="lineno">  595</span>        n_samples=20,</div>
<div class="line"><span class="lineno">  596</span>        y_bound=(-100, 100),</div>
<div class="line"><span class="lineno">  597</span>        raw_bound=(-10, 10),</div>
<div class="line"><span class="lineno">  598</span>        seed=42,</div>
<div class="line"><span class="lineno">  599</span>    )</div>
<div class="line"><span class="lineno">  600</span> </div>
<div class="line"><span class="lineno">  601</span>    <span class="keywordflow">if</span> raw_prediction.ndim == 1:</div>
<div class="line"><span class="lineno">  602</span>        raw_prediction_2d = raw_prediction[:, <span class="keywordtype">None</span>]</div>
<div class="line"><span class="lineno">  603</span>        assert_allclose(</div>
<div class="line"><span class="lineno">  604</span>            loss.loss(y_true=y_true, raw_prediction=raw_prediction_2d),</div>
<div class="line"><span class="lineno">  605</span>            loss.loss(y_true=y_true, raw_prediction=raw_prediction),</div>
<div class="line"><span class="lineno">  606</span>        )</div>
<div class="line"><span class="lineno">  607</span>        assert_allclose(</div>
<div class="line"><span class="lineno">  608</span>            loss.loss_gradient(y_true=y_true, raw_prediction=raw_prediction_2d),</div>
<div class="line"><span class="lineno">  609</span>            loss.loss_gradient(y_true=y_true, raw_prediction=raw_prediction),</div>
<div class="line"><span class="lineno">  610</span>        )</div>
<div class="line"><span class="lineno">  611</span>        assert_allclose(</div>
<div class="line"><span class="lineno">  612</span>            loss.gradient(y_true=y_true, raw_prediction=raw_prediction_2d),</div>
<div class="line"><span class="lineno">  613</span>            loss.gradient(y_true=y_true, raw_prediction=raw_prediction),</div>
<div class="line"><span class="lineno">  614</span>        )</div>
<div class="line"><span class="lineno">  615</span>        assert_allclose(</div>
<div class="line"><span class="lineno">  616</span>            loss.gradient_hessian(y_true=y_true, raw_prediction=raw_prediction_2d),</div>
<div class="line"><span class="lineno">  617</span>            loss.gradient_hessian(y_true=y_true, raw_prediction=raw_prediction),</div>
<div class="line"><span class="lineno">  618</span>        )</div>
<div class="line"><span class="lineno">  619</span> </div>
<div class="line"><span class="lineno">  620</span> </div>
<div class="line"><span class="lineno">  621</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
<div class="line"><span class="lineno">  622</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, &quot;range&quot;])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="af6443b1da2b78215f19dadf136a19716" name="af6443b1da2b78215f19dadf136a19716"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af6443b1da2b78215f19dadf136a19716">&#9670;&#160;</a></span>test_gradients_hessians_numerically()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_gradients_hessians_numerically </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test gradients and hessians with numerical derivatives.

Gradient should equal the numerical derivatives of the loss function.
Hessians should equal the numerical derivatives of gradients.
</pre> <div class="fragment"><div class="line"><span class="lineno">  673</span><span class="keyword">def </span>test_gradients_hessians_numerically(loss, sample_weight, global_random_seed):</div>
<div class="line"><span class="lineno">  674</span>    <span class="stringliteral">&quot;&quot;&quot;Test gradients and hessians with numerical derivatives.</span></div>
<div class="line"><span class="lineno">  675</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  676</span><span class="stringliteral">    Gradient should equal the numerical derivatives of the loss function.</span></div>
<div class="line"><span class="lineno">  677</span><span class="stringliteral">    Hessians should equal the numerical derivatives of gradients.</span></div>
<div class="line"><span class="lineno">  678</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  679</span>    n_samples = 20</div>
<div class="line"><span class="lineno">  680</span>    y_true, raw_prediction = random_y_true_raw_prediction(</div>
<div class="line"><span class="lineno">  681</span>        loss=loss,</div>
<div class="line"><span class="lineno">  682</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  683</span>        y_bound=(-100, 100),</div>
<div class="line"><span class="lineno">  684</span>        raw_bound=(-5, 5),</div>
<div class="line"><span class="lineno">  685</span>        seed=global_random_seed,</div>
<div class="line"><span class="lineno">  686</span>    )</div>
<div class="line"><span class="lineno">  687</span> </div>
<div class="line"><span class="lineno">  688</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;range&quot;</span>:</div>
<div class="line"><span class="lineno">  689</span>        sample_weight = np.linspace(1, y_true.shape[0], num=y_true.shape[0])</div>
<div class="line"><span class="lineno">  690</span> </div>
<div class="line"><span class="lineno">  691</span>    g, h = loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  692</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  693</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  694</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  695</span>    )</div>
<div class="line"><span class="lineno">  696</span> </div>
<div class="line"><span class="lineno">  697</span>    <span class="keyword">assert</span> g.shape == raw_prediction.shape</div>
<div class="line"><span class="lineno">  698</span>    <span class="keyword">assert</span> h.shape == raw_prediction.shape</div>
<div class="line"><span class="lineno">  699</span> </div>
<div class="line"><span class="lineno">  700</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  701</span> </div>
<div class="line"><span class="lineno">  702</span>        <span class="keyword">def </span>loss_func(x):</div>
<div class="line"><span class="lineno">  703</span>            <span class="keywordflow">return</span> loss.loss(</div>
<div class="line"><span class="lineno">  704</span>                y_true=y_true,</div>
<div class="line"><span class="lineno">  705</span>                raw_prediction=x,</div>
<div class="line"><span class="lineno">  706</span>                sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  707</span>            )</div>
<div class="line"><span class="lineno">  708</span> </div>
<div class="line"><span class="lineno">  709</span>        g_numeric = numerical_derivative(loss_func, raw_prediction, eps=1e-6)</div>
<div class="line"><span class="lineno">  710</span>        assert_allclose(g, g_numeric, rtol=5e-6, atol=1e-10)</div>
<div class="line"><span class="lineno">  711</span> </div>
<div class="line"><span class="lineno">  712</span>        <span class="keyword">def </span>grad_func(x):</div>
<div class="line"><span class="lineno">  713</span>            <span class="keywordflow">return</span> loss.gradient(</div>
<div class="line"><span class="lineno">  714</span>                y_true=y_true,</div>
<div class="line"><span class="lineno">  715</span>                raw_prediction=x,</div>
<div class="line"><span class="lineno">  716</span>                sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  717</span>            )</div>
<div class="line"><span class="lineno">  718</span> </div>
<div class="line"><span class="lineno">  719</span>        h_numeric = numerical_derivative(grad_func, raw_prediction, eps=1e-6)</div>
<div class="line"><span class="lineno">  720</span>        <span class="keywordflow">if</span> loss.approx_hessian:</div>
<div class="line"><span class="lineno">  721</span>            <span class="comment"># TODO: What could we test if loss.approx_hessian?</span></div>
<div class="line"><span class="lineno">  722</span>            <span class="keywordflow">pass</span></div>
<div class="line"><span class="lineno">  723</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  724</span>            assert_allclose(h, h_numeric, rtol=5e-6, atol=1e-10)</div>
<div class="line"><span class="lineno">  725</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  726</span>        <span class="comment"># For multiclass loss, we should only change the predictions of the</span></div>
<div class="line"><span class="lineno">  727</span>        <span class="comment"># class for which the derivative is taken for, e.g. offset[:, k] = eps</span></div>
<div class="line"><span class="lineno">  728</span>        <span class="comment"># for class k.</span></div>
<div class="line"><span class="lineno">  729</span>        <span class="comment"># As a softmax is computed, offsetting the whole array by a constant</span></div>
<div class="line"><span class="lineno">  730</span>        <span class="comment"># would have no effect on the probabilities, and thus on the loss.</span></div>
<div class="line"><span class="lineno">  731</span>        <span class="keywordflow">for</span> k <span class="keywordflow">in</span> range(loss.n_classes):</div>
<div class="line"><span class="lineno">  732</span> </div>
<div class="line"><span class="lineno">  733</span>            <span class="keyword">def </span>loss_func(x):</div>
<div class="line"><span class="lineno">  734</span>                raw = raw_prediction.copy()</div>
<div class="line"><span class="lineno">  735</span>                raw[:, k] = x</div>
<div class="line"><span class="lineno">  736</span>                <span class="keywordflow">return</span> loss.loss(</div>
<div class="line"><span class="lineno">  737</span>                    y_true=y_true,</div>
<div class="line"><span class="lineno">  738</span>                    raw_prediction=raw,</div>
<div class="line"><span class="lineno">  739</span>                    sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  740</span>                )</div>
<div class="line"><span class="lineno">  741</span> </div>
<div class="line"><span class="lineno">  742</span>            g_numeric = numerical_derivative(loss_func, raw_prediction[:, k], eps=1e-5)</div>
<div class="line"><span class="lineno">  743</span>            assert_allclose(g[:, k], g_numeric, rtol=5e-6, atol=1e-10)</div>
<div class="line"><span class="lineno">  744</span> </div>
<div class="line"><span class="lineno">  745</span>            <span class="keyword">def </span>grad_func(x):</div>
<div class="line"><span class="lineno">  746</span>                raw = raw_prediction.copy()</div>
<div class="line"><span class="lineno">  747</span>                raw[:, k] = x</div>
<div class="line"><span class="lineno">  748</span>                <span class="keywordflow">return</span> loss.gradient(</div>
<div class="line"><span class="lineno">  749</span>                    y_true=y_true,</div>
<div class="line"><span class="lineno">  750</span>                    raw_prediction=raw,</div>
<div class="line"><span class="lineno">  751</span>                    sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  752</span>                )[:, k]</div>
<div class="line"><span class="lineno">  753</span> </div>
<div class="line"><span class="lineno">  754</span>            h_numeric = numerical_derivative(grad_func, raw_prediction[:, k], eps=1e-6)</div>
<div class="line"><span class="lineno">  755</span>            <span class="keywordflow">if</span> loss.approx_hessian:</div>
<div class="line"><span class="lineno">  756</span>                <span class="comment"># TODO: What could we test if loss.approx_hessian?</span></div>
<div class="line"><span class="lineno">  757</span>                <span class="keywordflow">pass</span></div>
<div class="line"><span class="lineno">  758</span>            <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  759</span>                assert_allclose(h[:, k], h_numeric, rtol=5e-6, atol=1e-10)</div>
<div class="line"><span class="lineno">  760</span> </div>
<div class="line"><span class="lineno">  761</span> </div>
<div class="line"><span class="lineno">  762</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  763</span>    <span class="stringliteral">&quot;loss, x0, y_true&quot;</span>,</div>
<div class="line"><span class="lineno">  764</span>    [</div>
<div class="line"><span class="lineno">  765</span>        (<span class="stringliteral">&quot;squared_error&quot;</span>, -2.0, 42),</div>
<div class="line"><span class="lineno">  766</span>        (<span class="stringliteral">&quot;squared_error&quot;</span>, 117.0, 1.05),</div>
<div class="line"><span class="lineno">  767</span>        (<span class="stringliteral">&quot;squared_error&quot;</span>, 0.0, 0.0),</div>
<div class="line"><span class="lineno">  768</span>        <span class="comment"># The argmin of binomial_loss for y_true=0 and y_true=1 is resp.</span></div>
<div class="line"><span class="lineno">  769</span>        <span class="comment"># -inf and +inf due to logit, cf. &quot;complete separation&quot;. Therefore, we</span></div>
<div class="line"><span class="lineno">  770</span>        <span class="comment"># use 0 &lt; y_true &lt; 1.</span></div>
<div class="line"><span class="lineno">  771</span>        (<span class="stringliteral">&quot;binomial_loss&quot;</span>, 0.3, 0.1),</div>
<div class="line"><span class="lineno">  772</span>        (<span class="stringliteral">&quot;binomial_loss&quot;</span>, -12, 0.2),</div>
<div class="line"><span class="lineno">  773</span>        (<span class="stringliteral">&quot;binomial_loss&quot;</span>, 30, 0.9),</div>
<div class="line"><span class="lineno">  774</span>        (<span class="stringliteral">&quot;poisson_loss&quot;</span>, 12.0, 1.0),</div>
<div class="line"><span class="lineno">  775</span>        (<span class="stringliteral">&quot;poisson_loss&quot;</span>, 0.0, 2.0),</div>
<div class="line"><span class="lineno">  776</span>        (<span class="stringliteral">&quot;poisson_loss&quot;</span>, -22.0, 10.0),</div>
<div class="line"><span class="lineno">  777</span>    ],</div>
<div class="line"><span class="lineno">  778</span>)</div>
<div class="line"><span class="lineno">  779</span><span class="preprocessor">@skip_if_32bit</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a225e895b285451d57c5eb2c0f92642de" name="a225e895b285451d57c5eb2c0f92642de"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a225e895b285451d57c5eb2c0f92642de">&#9670;&#160;</a></span>test_init_gradient_and_hessian_raises()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_init_gradient_and_hessian_raises </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>err_msg</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that init_gradient_and_hessian raises errors for invalid input.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1070</span><span class="keyword">def </span>test_init_gradient_and_hessian_raises(loss, params, err_msg):</div>
<div class="line"><span class="lineno"> 1071</span>    <span class="stringliteral">&quot;&quot;&quot;Test that init_gradient_and_hessian raises errors for invalid input.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1072</span>    loss = loss()</div>
<div class="line"><span class="lineno"> 1073</span>    <span class="keyword">with</span> pytest.raises((ValueError, TypeError), match=err_msg):</div>
<div class="line"><span class="lineno"> 1074</span>        gradient, hessian = loss.init_gradient_and_hessian(n_samples=5, **params)</div>
<div class="line"><span class="lineno"> 1075</span> </div>
<div class="line"><span class="lineno"> 1076</span> </div>
<div class="line"><span class="lineno"> 1077</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1078</span>    <span class="stringliteral">&quot;loss, params, err_type, err_msg&quot;</span>,</div>
<div class="line"><span class="lineno"> 1079</span>    [</div>
<div class="line"><span class="lineno"> 1080</span>        (</div>
<div class="line"><span class="lineno"> 1081</span>            PinballLoss,</div>
<div class="line"><span class="lineno"> 1082</span>            {<span class="stringliteral">&quot;quantile&quot;</span>: <span class="keywordtype">None</span>},</div>
<div class="line"><span class="lineno"> 1083</span>            TypeError,</div>
<div class="line"><span class="lineno"> 1084</span>            <span class="stringliteral">&quot;quantile must be an instance of float, not NoneType.&quot;</span>,</div>
<div class="line"><span class="lineno"> 1085</span>        ),</div>
<div class="line"><span class="lineno"> 1086</span>        (</div>
<div class="line"><span class="lineno"> 1087</span>            PinballLoss,</div>
<div class="line"><span class="lineno"> 1088</span>            {<span class="stringliteral">&quot;quantile&quot;</span>: 0},</div>
<div class="line"><span class="lineno"> 1089</span>            ValueError,</div>
<div class="line"><span class="lineno"> 1090</span>            <span class="stringliteral">&quot;quantile == 0, must be &gt; 0.&quot;</span>,</div>
<div class="line"><span class="lineno"> 1091</span>        ),</div>
<div class="line"><span class="lineno"> 1092</span>        (PinballLoss, {<span class="stringliteral">&quot;quantile&quot;</span>: 1.1}, ValueError, <span class="stringliteral">&quot;quantile == 1.1, must be &lt; 1.&quot;</span>),</div>
<div class="line"><span class="lineno"> 1093</span>    ],</div>
<div class="line"><span class="lineno"> 1094</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="aed5e6b902447152bb1f9f5d398830d9d" name="aed5e6b902447152bb1f9f5d398830d9d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aed5e6b902447152bb1f9f5d398830d9d">&#9670;&#160;</a></span>test_init_gradient_and_hessians()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_init_gradient_and_hessians </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dtype</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>order</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that init_gradient_and_hessian works as expected.

passing sample_weight to a loss correctly influences the constant_hessian
attribute, and consequently the shape of the hessian array.
</pre> <div class="fragment"><div class="line"><span class="lineno"> 1024</span><span class="keyword">def </span>test_init_gradient_and_hessians(loss, sample_weight, dtype, order):</div>
<div class="line"><span class="lineno"> 1025</span>    <span class="stringliteral">&quot;&quot;&quot;Test that init_gradient_and_hessian works as expected.</span></div>
<div class="line"><span class="lineno"> 1026</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno"> 1027</span><span class="stringliteral">    passing sample_weight to a loss correctly influences the constant_hessian</span></div>
<div class="line"><span class="lineno"> 1028</span><span class="stringliteral">    attribute, and consequently the shape of the hessian array.</span></div>
<div class="line"><span class="lineno"> 1029</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1030</span>    n_samples = 5</div>
<div class="line"><span class="lineno"> 1031</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;range&quot;</span>:</div>
<div class="line"><span class="lineno"> 1032</span>        sample_weight = np.ones(n_samples)</div>
<div class="line"><span class="lineno"> 1033</span>    loss = loss(sample_weight=sample_weight)</div>
<div class="line"><span class="lineno"> 1034</span>    gradient, hessian = loss.init_gradient_and_hessian(</div>
<div class="line"><span class="lineno"> 1035</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno"> 1036</span>        dtype=dtype,</div>
<div class="line"><span class="lineno"> 1037</span>        order=order,</div>
<div class="line"><span class="lineno"> 1038</span>    )</div>
<div class="line"><span class="lineno"> 1039</span>    <span class="keywordflow">if</span> loss.constant_hessian:</div>
<div class="line"><span class="lineno"> 1040</span>        <span class="keyword">assert</span> gradient.shape == (n_samples,)</div>
<div class="line"><span class="lineno"> 1041</span>        <span class="keyword">assert</span> hessian.shape == (1,)</div>
<div class="line"><span class="lineno"> 1042</span>    <span class="keywordflow">elif</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno"> 1043</span>        <span class="keyword">assert</span> gradient.shape == (n_samples, loss.n_classes)</div>
<div class="line"><span class="lineno"> 1044</span>        <span class="keyword">assert</span> hessian.shape == (n_samples, loss.n_classes)</div>
<div class="line"><span class="lineno"> 1045</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1046</span>        <span class="keyword">assert</span> hessian.shape == (n_samples,)</div>
<div class="line"><span class="lineno"> 1047</span>        <span class="keyword">assert</span> hessian.shape == (n_samples,)</div>
<div class="line"><span class="lineno"> 1048</span> </div>
<div class="line"><span class="lineno"> 1049</span>    <span class="keyword">assert</span> gradient.dtype == dtype</div>
<div class="line"><span class="lineno"> 1050</span>    <span class="keyword">assert</span> hessian.dtype == dtype</div>
<div class="line"><span class="lineno"> 1051</span> </div>
<div class="line"><span class="lineno"> 1052</span>    <span class="keywordflow">if</span> order == <span class="stringliteral">&quot;C&quot;</span>:</div>
<div class="line"><span class="lineno"> 1053</span>        <span class="keyword">assert</span> gradient.flags.c_contiguous</div>
<div class="line"><span class="lineno"> 1054</span>        <span class="keyword">assert</span> hessian.flags.c_contiguous</div>
<div class="line"><span class="lineno"> 1055</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno"> 1056</span>        <span class="keyword">assert</span> gradient.flags.f_contiguous</div>
<div class="line"><span class="lineno"> 1057</span>        <span class="keyword">assert</span> hessian.flags.f_contiguous</div>
<div class="line"><span class="lineno"> 1058</span> </div>
<div class="line"><span class="lineno"> 1059</span> </div>
<div class="line"><span class="lineno"> 1060</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, ALL_LOSSES)</span></div>
<div class="line"><span class="lineno"> 1061</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno"> 1062</span>    <span class="stringliteral">&quot;params, err_msg&quot;</span>,</div>
<div class="line"><span class="lineno"> 1063</span>    [</div>
<div class="line"><span class="lineno"> 1064</span>        (</div>
<div class="line"><span class="lineno"> 1065</span>            {<span class="stringliteral">&quot;dtype&quot;</span>: np.int64},</div>
<div class="line"><span class="lineno"> 1066</span>            f<span class="stringliteral">&quot;Valid options for &#39;dtype&#39; are .* Got dtype={np.int64} instead.&quot;</span>,</div>
<div class="line"><span class="lineno"> 1067</span>        ),</div>
<div class="line"><span class="lineno"> 1068</span>    ],</div>
<div class="line"><span class="lineno"> 1069</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a5d4a6ec724cf2afd511ada41995b3a37" name="a5d4a6ec724cf2afd511ada41995b3a37"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5d4a6ec724cf2afd511ada41995b3a37">&#9670;&#160;</a></span>test_loss_boundary()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_boundary </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test interval ranges of y_true and y_pred in losses.</pre> <div class="fragment"><div class="line"><span class="lineno">  116</span><span class="keyword">def </span>test_loss_boundary(loss):</div>
<div class="line"><span class="lineno">  117</span>    <span class="stringliteral">&quot;&quot;&quot;Test interval ranges of y_true and y_pred in losses.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  118</span>    <span class="comment"># make sure low and high are always within the interval, used for linspace</span></div>
<div class="line"><span class="lineno">  119</span>    <span class="keywordflow">if</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  120</span>        y_true = np.linspace(0, 9, num=10)</div>
<div class="line"><span class="lineno">  121</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  122</span>        low, high = _inclusive_low_high(loss.interval_y_true)</div>
<div class="line"><span class="lineno">  123</span>        y_true = np.linspace(low, high, num=10)</div>
<div class="line"><span class="lineno">  124</span> </div>
<div class="line"><span class="lineno">  125</span>    <span class="comment"># add boundaries if they are included</span></div>
<div class="line"><span class="lineno">  126</span>    <span class="keywordflow">if</span> loss.interval_y_true.low_inclusive:</div>
<div class="line"><span class="lineno">  127</span>        y_true = np.r_[y_true, loss.interval_y_true.low]</div>
<div class="line"><span class="lineno">  128</span>    <span class="keywordflow">if</span> loss.interval_y_true.high_inclusive:</div>
<div class="line"><span class="lineno">  129</span>        y_true = np.r_[y_true, loss.interval_y_true.high]</div>
<div class="line"><span class="lineno">  130</span> </div>
<div class="line"><span class="lineno">  131</span>    <span class="keyword">assert</span> loss.in_y_true_range(y_true)</div>
<div class="line"><span class="lineno">  132</span> </div>
<div class="line"><span class="lineno">  133</span>    n = y_true.shape[0]</div>
<div class="line"><span class="lineno">  134</span>    low, high = _inclusive_low_high(loss.interval_y_pred)</div>
<div class="line"><span class="lineno">  135</span>    <span class="keywordflow">if</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  136</span>        y_pred = np.empty((n, 3))</div>
<div class="line"><span class="lineno">  137</span>        y_pred[:, 0] = np.linspace(low, high, num=n)</div>
<div class="line"><span class="lineno">  138</span>        y_pred[:, 1] = 0.5 * (1 - y_pred[:, 0])</div>
<div class="line"><span class="lineno">  139</span>        y_pred[:, 2] = 0.5 * (1 - y_pred[:, 0])</div>
<div class="line"><span class="lineno">  140</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  141</span>        y_pred = np.linspace(low, high, num=n)</div>
<div class="line"><span class="lineno">  142</span> </div>
<div class="line"><span class="lineno">  143</span>    <span class="keyword">assert</span> loss.in_y_pred_range(y_pred)</div>
<div class="line"><span class="lineno">  144</span> </div>
<div class="line"><span class="lineno">  145</span>    <span class="comment"># calculating losses should not fail</span></div>
<div class="line"><span class="lineno">  146</span>    raw_prediction = loss.link.link(y_pred)</div>
<div class="line"><span class="lineno">  147</span>    loss.loss(y_true=y_true, raw_prediction=raw_prediction)</div>
<div class="line"><span class="lineno">  148</span> </div>
<div class="line"><span class="lineno">  149</span> </div>
<div class="line"><span class="lineno">  150</span><span class="comment"># Fixture to test valid value ranges.</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a7a99b30b2769b07502d139824291cd45" name="a7a99b30b2769b07502d139824291cd45"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7a99b30b2769b07502d139824291cd45">&#9670;&#160;</a></span>test_loss_boundary_y_pred()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_boundary_y_pred </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_pred_success</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_pred_fail</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test boundaries of y_pred for loss functions.</pre> <div class="fragment"><div class="line"><span class="lineno">  213</span><span class="keyword">def </span>test_loss_boundary_y_pred(loss, y_pred_success, y_pred_fail):</div>
<div class="line"><span class="lineno">  214</span>    <span class="stringliteral">&quot;&quot;&quot;Test boundaries of y_pred for loss functions.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  215</span>    <span class="keywordflow">for</span> y <span class="keywordflow">in</span> y_pred_success:</div>
<div class="line"><span class="lineno">  216</span>        <span class="keyword">assert</span> loss.in_y_pred_range(np.array([y]))</div>
<div class="line"><span class="lineno">  217</span>    <span class="keywordflow">for</span> y <span class="keywordflow">in</span> y_pred_fail:</div>
<div class="line"><span class="lineno">  218</span>        <span class="keyword">assert</span> <span class="keywordflow">not</span> loss.in_y_pred_range(np.array([y]))</div>
<div class="line"><span class="lineno">  219</span> </div>
<div class="line"><span class="lineno">  220</span> </div>
<div class="line"><span class="lineno">  221</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  222</span>    <span class="stringliteral">&quot;loss, y_true, raw_prediction, loss_true&quot;</span>,</div>
<div class="line"><span class="lineno">  223</span>    [</div>
<div class="line"><span class="lineno">  224</span>        (HalfSquaredError(), 1.0, 5.0, 8),</div>
<div class="line"><span class="lineno">  225</span>        (AbsoluteError(), 1.0, 5.0, 4),</div>
<div class="line"><span class="lineno">  226</span>        (PinballLoss(quantile=0.5), 1.0, 5.0, 2),</div>
<div class="line"><span class="lineno">  227</span>        (PinballLoss(quantile=0.25), 1.0, 5.0, 4 * (1 - 0.25)),</div>
<div class="line"><span class="lineno">  228</span>        (PinballLoss(quantile=0.25), 5.0, 1.0, 4 * 0.25),</div>
<div class="line"><span class="lineno">  229</span>        (HalfPoissonLoss(), 2.0, np.log(4), 4 - 2 * np.log(4)),</div>
<div class="line"><span class="lineno">  230</span>        (HalfGammaLoss(), 2.0, np.log(4), np.log(4) + 2 / 4),</div>
<div class="line"><span class="lineno">  231</span>        (HalfTweedieLoss(power=3), 2.0, np.log(4), -1 / 4 + 1 / 4**2),</div>
<div class="line"><span class="lineno">  232</span>        (HalfTweedieLossIdentity(power=1), 2.0, 4.0, 2 - 2 * np.log(2)),</div>
<div class="line"><span class="lineno">  233</span>        (HalfTweedieLossIdentity(power=2), 2.0, 4.0, np.log(2) - 1 / 2),</div>
<div class="line"><span class="lineno">  234</span>        (HalfTweedieLossIdentity(power=3), 2.0, 4.0, -1 / 4 + 1 / 4**2 + 1 / 2 / 2),</div>
<div class="line"><span class="lineno">  235</span>        (HalfBinomialLoss(), 0.25, np.log(4), np.log(5) - 0.25 * np.log(4)),</div>
<div class="line"><span class="lineno">  236</span>        (</div>
<div class="line"><span class="lineno">  237</span>            HalfMultinomialLoss(n_classes=3),</div>
<div class="line"><span class="lineno">  238</span>            0.0,</div>
<div class="line"><span class="lineno">  239</span>            [0.2, 0.5, 0.3],</div>
<div class="line"><span class="lineno">  240</span>            logsumexp([0.2, 0.5, 0.3]) - 0.2,</div>
<div class="line"><span class="lineno">  241</span>        ),</div>
<div class="line"><span class="lineno">  242</span>        (</div>
<div class="line"><span class="lineno">  243</span>            HalfMultinomialLoss(n_classes=3),</div>
<div class="line"><span class="lineno">  244</span>            1.0,</div>
<div class="line"><span class="lineno">  245</span>            [0.2, 0.5, 0.3],</div>
<div class="line"><span class="lineno">  246</span>            logsumexp([0.2, 0.5, 0.3]) - 0.5,</div>
<div class="line"><span class="lineno">  247</span>        ),</div>
<div class="line"><span class="lineno">  248</span>        (</div>
<div class="line"><span class="lineno">  249</span>            HalfMultinomialLoss(n_classes=3),</div>
<div class="line"><span class="lineno">  250</span>            2.0,</div>
<div class="line"><span class="lineno">  251</span>            [0.2, 0.5, 0.3],</div>
<div class="line"><span class="lineno">  252</span>            logsumexp([0.2, 0.5, 0.3]) - 0.3,</div>
<div class="line"><span class="lineno">  253</span>        ),</div>
<div class="line"><span class="lineno">  254</span>    ],</div>
<div class="line"><span class="lineno">  255</span>    ids=loss_instance_name,</div>
<div class="line"><span class="lineno">  256</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae56255afa73f8bec4bf8a84f72f047a8" name="ae56255afa73f8bec4bf8a84f72f047a8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae56255afa73f8bec4bf8a84f72f047a8">&#9670;&#160;</a></span>test_loss_boundary_y_true()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_boundary_y_true </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_true_success</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_true_fail</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test boundaries of y_true for loss functions.</pre> <div class="fragment"><div class="line"><span class="lineno">  202</span><span class="keyword">def </span>test_loss_boundary_y_true(loss, y_true_success, y_true_fail):</div>
<div class="line"><span class="lineno">  203</span>    <span class="stringliteral">&quot;&quot;&quot;Test boundaries of y_true for loss functions.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  204</span>    <span class="keywordflow">for</span> y <span class="keywordflow">in</span> y_true_success:</div>
<div class="line"><span class="lineno">  205</span>        <span class="keyword">assert</span> loss.in_y_true_range(np.array([y]))</div>
<div class="line"><span class="lineno">  206</span>    <span class="keywordflow">for</span> y <span class="keywordflow">in</span> y_true_fail:</div>
<div class="line"><span class="lineno">  207</span>        <span class="keyword">assert</span> <span class="keywordflow">not</span> loss.in_y_true_range(np.array([y]))</div>
<div class="line"><span class="lineno">  208</span> </div>
<div class="line"><span class="lineno">  209</span> </div>
<div class="line"><span class="lineno">  210</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  211</span>    <span class="stringliteral">&quot;loss, y_pred_success, y_pred_fail&quot;</span>, Y_COMMON_PARAMS + Y_PRED_PARAMS  <span class="comment"># type: ignore</span></div>
<div class="line"><span class="lineno">  212</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a244ef4411a18107728e1ac4966639003" name="a244ef4411a18107728e1ac4966639003"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a244ef4411a18107728e1ac4966639003">&#9670;&#160;</a></span>test_loss_dtype()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_dtype </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>readonly_memmap</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dtype_in</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dtype_out</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>out1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>out2</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_threads</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test acceptance of dtypes, readonly and writeable arrays in loss functions.

Check that loss accepts if all input arrays are either all float32 or all
float64, and all output arrays are either all float32 or all float64.

Also check that input arrays can be readonly, e.g. memory mapped.
</pre> <div class="fragment"><div class="line"><span class="lineno">  274</span>):</div>
<div class="line"><span class="lineno">  275</span>    <span class="stringliteral">&quot;&quot;&quot;Test acceptance of dtypes, readonly and writeable arrays in loss functions.</span></div>
<div class="line"><span class="lineno">  276</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  277</span><span class="stringliteral">    Check that loss accepts if all input arrays are either all float32 or all</span></div>
<div class="line"><span class="lineno">  278</span><span class="stringliteral">    float64, and all output arrays are either all float32 or all float64.</span></div>
<div class="line"><span class="lineno">  279</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  280</span><span class="stringliteral">    Also check that input arrays can be readonly, e.g. memory mapped.</span></div>
<div class="line"><span class="lineno">  281</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  282</span>    loss = loss()</div>
<div class="line"><span class="lineno">  283</span>    <span class="comment"># generate a y_true and raw_prediction in valid range</span></div>
<div class="line"><span class="lineno">  284</span>    n_samples = 5</div>
<div class="line"><span class="lineno">  285</span>    y_true, raw_prediction = random_y_true_raw_prediction(</div>
<div class="line"><span class="lineno">  286</span>        loss=loss,</div>
<div class="line"><span class="lineno">  287</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  288</span>        y_bound=(-100, 100),</div>
<div class="line"><span class="lineno">  289</span>        raw_bound=(-10, 10),</div>
<div class="line"><span class="lineno">  290</span>        seed=42,</div>
<div class="line"><span class="lineno">  291</span>    )</div>
<div class="line"><span class="lineno">  292</span>    y_true = y_true.astype(dtype_in)</div>
<div class="line"><span class="lineno">  293</span>    raw_prediction = raw_prediction.astype(dtype_in)</div>
<div class="line"><span class="lineno">  294</span> </div>
<div class="line"><span class="lineno">  295</span>    <span class="keywordflow">if</span> sample_weight <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  296</span>        sample_weight = np.array([2.0] * n_samples, dtype=dtype_in)</div>
<div class="line"><span class="lineno">  297</span>    <span class="keywordflow">if</span> out1 <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  298</span>        out1 = np.empty_like(y_true, dtype=dtype_out)</div>
<div class="line"><span class="lineno">  299</span>    <span class="keywordflow">if</span> out2 <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  300</span>        out2 = np.empty_like(raw_prediction, dtype=dtype_out)</div>
<div class="line"><span class="lineno">  301</span> </div>
<div class="line"><span class="lineno">  302</span>    <span class="keywordflow">if</span> readonly_memmap:</div>
<div class="line"><span class="lineno">  303</span>        y_true = create_memmap_backed_data(y_true, aligned=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  304</span>        raw_prediction = create_memmap_backed_data(raw_prediction, aligned=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  305</span>        <span class="keywordflow">if</span> sample_weight <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">  306</span>            sample_weight = create_memmap_backed_data(sample_weight, aligned=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  307</span> </div>
<div class="line"><span class="lineno">  308</span>    loss.loss(</div>
<div class="line"><span class="lineno">  309</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  310</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  311</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  312</span>        loss_out=out1,</div>
<div class="line"><span class="lineno">  313</span>        n_threads=n_threads,</div>
<div class="line"><span class="lineno">  314</span>    )</div>
<div class="line"><span class="lineno">  315</span>    loss.gradient(</div>
<div class="line"><span class="lineno">  316</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  317</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  318</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  319</span>        gradient_out=out2,</div>
<div class="line"><span class="lineno">  320</span>        n_threads=n_threads,</div>
<div class="line"><span class="lineno">  321</span>    )</div>
<div class="line"><span class="lineno">  322</span>    loss.loss_gradient(</div>
<div class="line"><span class="lineno">  323</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  324</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  325</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  326</span>        loss_out=out1,</div>
<div class="line"><span class="lineno">  327</span>        gradient_out=out2,</div>
<div class="line"><span class="lineno">  328</span>        n_threads=n_threads,</div>
<div class="line"><span class="lineno">  329</span>    )</div>
<div class="line"><span class="lineno">  330</span>    <span class="keywordflow">if</span> out1 <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  331</span>        out1 = np.empty_like(raw_prediction, dtype=dtype_out)</div>
<div class="line"><span class="lineno">  332</span>    loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  333</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  334</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  335</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  336</span>        gradient_out=out1,</div>
<div class="line"><span class="lineno">  337</span>        hessian_out=out2,</div>
<div class="line"><span class="lineno">  338</span>        n_threads=n_threads,</div>
<div class="line"><span class="lineno">  339</span>    )</div>
<div class="line"><span class="lineno">  340</span>    loss(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  341</span>    loss.fit_intercept_only(y_true=y_true, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  342</span>    loss.constant_to_optimal_zero(y_true=y_true, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  343</span>    <span class="keywordflow">if</span> hasattr(loss, <span class="stringliteral">&quot;predict_proba&quot;</span>):</div>
<div class="line"><span class="lineno">  344</span>        loss.predict_proba(raw_prediction=raw_prediction)</div>
<div class="line"><span class="lineno">  345</span>    <span class="keywordflow">if</span> hasattr(loss, <span class="stringliteral">&quot;gradient_proba&quot;</span>):</div>
<div class="line"><span class="lineno">  346</span>        loss.gradient_proba(</div>
<div class="line"><span class="lineno">  347</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  348</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  349</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  350</span>            gradient_out=out1,</div>
<div class="line"><span class="lineno">  351</span>            proba_out=out2,</div>
<div class="line"><span class="lineno">  352</span>            n_threads=n_threads,</div>
<div class="line"><span class="lineno">  353</span>        )</div>
<div class="line"><span class="lineno">  354</span> </div>
<div class="line"><span class="lineno">  355</span> </div>
<div class="line"><span class="lineno">  356</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
<div class="line"><span class="lineno">  357</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, &quot;range&quot;])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a00f38156d3d6c5364582a272001ae958" name="a00f38156d3d6c5364582a272001ae958"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00f38156d3d6c5364582a272001ae958">&#9670;&#160;</a></span>test_loss_gradients_are_the_same()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_gradients_are_the_same </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that loss and gradient are the same across different functions.

Also test that output arguments contain correct results.
</pre> <div class="fragment"><div class="line"><span class="lineno">  440</span><span class="keyword">def </span>test_loss_gradients_are_the_same(loss, sample_weight, global_random_seed):</div>
<div class="line"><span class="lineno">  441</span>    <span class="stringliteral">&quot;&quot;&quot;Test that loss and gradient are the same across different functions.</span></div>
<div class="line"><span class="lineno">  442</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  443</span><span class="stringliteral">    Also test that output arguments contain correct results.</span></div>
<div class="line"><span class="lineno">  444</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  445</span>    y_true, raw_prediction = random_y_true_raw_prediction(</div>
<div class="line"><span class="lineno">  446</span>        loss=loss,</div>
<div class="line"><span class="lineno">  447</span>        n_samples=20,</div>
<div class="line"><span class="lineno">  448</span>        y_bound=(-100, 100),</div>
<div class="line"><span class="lineno">  449</span>        raw_bound=(-10, 10),</div>
<div class="line"><span class="lineno">  450</span>        seed=global_random_seed,</div>
<div class="line"><span class="lineno">  451</span>    )</div>
<div class="line"><span class="lineno">  452</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;range&quot;</span>:</div>
<div class="line"><span class="lineno">  453</span>        sample_weight = np.linspace(1, y_true.shape[0], num=y_true.shape[0])</div>
<div class="line"><span class="lineno">  454</span> </div>
<div class="line"><span class="lineno">  455</span>    out_l1 = np.empty_like(y_true)</div>
<div class="line"><span class="lineno">  456</span>    out_l2 = np.empty_like(y_true)</div>
<div class="line"><span class="lineno">  457</span>    out_g1 = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  458</span>    out_g2 = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  459</span>    out_g3 = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  460</span>    out_h3 = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  461</span> </div>
<div class="line"><span class="lineno">  462</span>    l1 = loss.loss(</div>
<div class="line"><span class="lineno">  463</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  464</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  465</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  466</span>        loss_out=out_l1,</div>
<div class="line"><span class="lineno">  467</span>    )</div>
<div class="line"><span class="lineno">  468</span>    g1 = loss.gradient(</div>
<div class="line"><span class="lineno">  469</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  470</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  471</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  472</span>        gradient_out=out_g1,</div>
<div class="line"><span class="lineno">  473</span>    )</div>
<div class="line"><span class="lineno">  474</span>    l2, g2 = loss.loss_gradient(</div>
<div class="line"><span class="lineno">  475</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  476</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  477</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  478</span>        loss_out=out_l2,</div>
<div class="line"><span class="lineno">  479</span>        gradient_out=out_g2,</div>
<div class="line"><span class="lineno">  480</span>    )</div>
<div class="line"><span class="lineno">  481</span>    g3, h3 = loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  482</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  483</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  484</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  485</span>        gradient_out=out_g3,</div>
<div class="line"><span class="lineno">  486</span>        hessian_out=out_h3,</div>
<div class="line"><span class="lineno">  487</span>    )</div>
<div class="line"><span class="lineno">  488</span>    assert_allclose(l1, l2)</div>
<div class="line"><span class="lineno">  489</span>    assert_array_equal(l1, out_l1)</div>
<div class="line"><span class="lineno">  490</span>    <span class="keyword">assert</span> np.shares_memory(l1, out_l1)</div>
<div class="line"><span class="lineno">  491</span>    assert_array_equal(l2, out_l2)</div>
<div class="line"><span class="lineno">  492</span>    <span class="keyword">assert</span> np.shares_memory(l2, out_l2)</div>
<div class="line"><span class="lineno">  493</span>    assert_allclose(g1, g2)</div>
<div class="line"><span class="lineno">  494</span>    assert_allclose(g1, g3)</div>
<div class="line"><span class="lineno">  495</span>    assert_array_equal(g1, out_g1)</div>
<div class="line"><span class="lineno">  496</span>    <span class="keyword">assert</span> np.shares_memory(g1, out_g1)</div>
<div class="line"><span class="lineno">  497</span>    assert_array_equal(g2, out_g2)</div>
<div class="line"><span class="lineno">  498</span>    <span class="keyword">assert</span> np.shares_memory(g2, out_g2)</div>
<div class="line"><span class="lineno">  499</span>    assert_array_equal(g3, out_g3)</div>
<div class="line"><span class="lineno">  500</span>    <span class="keyword">assert</span> np.shares_memory(g3, out_g3)</div>
<div class="line"><span class="lineno">  501</span> </div>
<div class="line"><span class="lineno">  502</span>    <span class="keywordflow">if</span> hasattr(loss, <span class="stringliteral">&quot;gradient_proba&quot;</span>):</div>
<div class="line"><span class="lineno">  503</span>        <span class="keyword">assert</span> loss.is_multiclass  <span class="comment"># only for HalfMultinomialLoss</span></div>
<div class="line"><span class="lineno">  504</span>        out_g4 = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  505</span>        out_proba = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  506</span>        g4, proba = loss.gradient_proba(</div>
<div class="line"><span class="lineno">  507</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  508</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  509</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  510</span>            gradient_out=out_g4,</div>
<div class="line"><span class="lineno">  511</span>            proba_out=out_proba,</div>
<div class="line"><span class="lineno">  512</span>        )</div>
<div class="line"><span class="lineno">  513</span>        assert_allclose(g1, out_g4)</div>
<div class="line"><span class="lineno">  514</span>        assert_allclose(g1, g4)</div>
<div class="line"><span class="lineno">  515</span>        assert_allclose(proba, out_proba)</div>
<div class="line"><span class="lineno">  516</span>        assert_allclose(np.sum(proba, axis=1), 1, rtol=1e-11)</div>
<div class="line"><span class="lineno">  517</span> </div>
<div class="line"><span class="lineno">  518</span> </div>
<div class="line"><span class="lineno">  519</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
<div class="line"><span class="lineno">  520</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [&quot;ones&quot;, &quot;random&quot;])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="aa9d437e26e291830a6f7f3b2107f1842" name="aa9d437e26e291830a6f7f3b2107f1842"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa9d437e26e291830a6f7f3b2107f1842">&#9670;&#160;</a></span>test_loss_init_parameter_validation()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_init_parameter_validation </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>err_type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>err_msg</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that loss raises errors for invalid input.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1095</span><span class="keyword">def </span>test_loss_init_parameter_validation(loss, params, err_type, err_msg):</div>
<div class="line"><span class="lineno"> 1096</span>    <span class="stringliteral">&quot;&quot;&quot;Test that loss raises errors for invalid input.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1097</span>    <span class="keyword">with</span> pytest.raises(err_type, match=err_msg):</div>
<div class="line"><span class="lineno"> 1098</span>        loss(**params)</div>
<div class="line"><span class="lineno"> 1099</span> </div>
<div class="line"><span class="lineno"> 1100</span> </div>
<div class="line"><span class="lineno"> 1101</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="ad18e8017ef3e06ab7571a39b5a0a626e" name="ad18e8017ef3e06ab7571a39b5a0a626e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad18e8017ef3e06ab7571a39b5a0a626e">&#9670;&#160;</a></span>test_loss_intercept_only()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_intercept_only </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that fit_intercept_only returns the argmin of the loss.

Also test that the gradient is zero at the minimum.
</pre> <div class="fragment"><div class="line"><span class="lineno">  828</span><span class="keyword">def </span>test_loss_intercept_only(loss, sample_weight):</div>
<div class="line"><span class="lineno">  829</span>    <span class="stringliteral">&quot;&quot;&quot;Test that fit_intercept_only returns the argmin of the loss.</span></div>
<div class="line"><span class="lineno">  830</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  831</span><span class="stringliteral">    Also test that the gradient is zero at the minimum.</span></div>
<div class="line"><span class="lineno">  832</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  833</span>    n_samples = 50</div>
<div class="line"><span class="lineno">  834</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  835</span>        y_true = loss.link.inverse(np.linspace(-4, 4, num=n_samples))</div>
<div class="line"><span class="lineno">  836</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  837</span>        y_true = np.arange(n_samples).astype(np.float64) % loss.n_classes</div>
<div class="line"><span class="lineno">  838</span>        y_true[::5] = 0  <span class="comment"># exceedance of class 0</span></div>
<div class="line"><span class="lineno">  839</span> </div>
<div class="line"><span class="lineno">  840</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;range&quot;</span>:</div>
<div class="line"><span class="lineno">  841</span>        sample_weight = np.linspace(0.1, 2, num=n_samples)</div>
<div class="line"><span class="lineno">  842</span> </div>
<div class="line"><span class="lineno">  843</span>    a = loss.fit_intercept_only(y_true=y_true, sample_weight=sample_weight)</div>
<div class="line"><span class="lineno">  844</span> </div>
<div class="line"><span class="lineno">  845</span>    <span class="comment"># find minimum by optimization</span></div>
<div class="line"><span class="lineno">  846</span>    <span class="keyword">def </span>fun(x):</div>
<div class="line"><span class="lineno">  847</span>        <span class="keywordflow">if</span> <span class="keywordflow">not</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  848</span>            raw_prediction = np.full(shape=(n_samples), fill_value=x)</div>
<div class="line"><span class="lineno">  849</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  850</span>            raw_prediction = np.ascontiguousarray(</div>
<div class="line"><span class="lineno">  851</span>                np.broadcast_to(x, shape=(n_samples, loss.n_classes))</div>
<div class="line"><span class="lineno">  852</span>            )</div>
<div class="line"><span class="lineno">  853</span>        <span class="keywordflow">return</span> loss(</div>
<div class="line"><span class="lineno">  854</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  855</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  856</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  857</span>        )</div>
<div class="line"><span class="lineno">  858</span> </div>
<div class="line"><span class="lineno">  859</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  860</span>        opt = minimize_scalar(fun, tol=1e-7, options={<span class="stringliteral">&quot;maxiter&quot;</span>: 100})</div>
<div class="line"><span class="lineno">  861</span>        grad = loss.gradient(</div>
<div class="line"><span class="lineno">  862</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  863</span>            raw_prediction=np.full_like(y_true, a),</div>
<div class="line"><span class="lineno">  864</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  865</span>        )</div>
<div class="line"><span class="lineno">  866</span>        <span class="keyword">assert</span> a.shape == tuple()  <span class="comment"># scalar</span></div>
<div class="line"><span class="lineno">  867</span>        <span class="keyword">assert</span> a.dtype == y_true.dtype</div>
<div class="line"><span class="lineno">  868</span>        assert_all_finite(a)</div>
<div class="line"><span class="lineno">  869</span>        a == approx(opt.x, rel=1e-7)</div>
<div class="line"><span class="lineno">  870</span>        grad.sum() == approx(0, abs=1e-12)</div>
<div class="line"><span class="lineno">  871</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  872</span>        <span class="comment"># The constraint corresponds to sum(raw_prediction) = 0. Without it, we would</span></div>
<div class="line"><span class="lineno">  873</span>        <span class="comment"># need to apply loss.symmetrize_raw_prediction to opt.x before comparing.</span></div>
<div class="line"><span class="lineno">  874</span>        opt = minimize(</div>
<div class="line"><span class="lineno">  875</span>            fun,</div>
<div class="line"><span class="lineno">  876</span>            np.zeros((loss.n_classes)),</div>
<div class="line"><span class="lineno">  877</span>            tol=1e-13,</div>
<div class="line"><span class="lineno">  878</span>            options={<span class="stringliteral">&quot;maxiter&quot;</span>: 100},</div>
<div class="line"><span class="lineno">  879</span>            method=<span class="stringliteral">&quot;SLSQP&quot;</span>,</div>
<div class="line"><span class="lineno">  880</span>            constraints=LinearConstraint(np.ones((1, loss.n_classes)), 0, 0),</div>
<div class="line"><span class="lineno">  881</span>        )</div>
<div class="line"><span class="lineno">  882</span>        grad = loss.gradient(</div>
<div class="line"><span class="lineno">  883</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  884</span>            raw_prediction=np.tile(a, (n_samples, 1)),</div>
<div class="line"><span class="lineno">  885</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  886</span>        )</div>
<div class="line"><span class="lineno">  887</span>        <span class="keyword">assert</span> a.dtype == y_true.dtype</div>
<div class="line"><span class="lineno">  888</span>        assert_all_finite(a)</div>
<div class="line"><span class="lineno">  889</span>        assert_allclose(a, opt.x, rtol=5e-6, atol=1e-12)</div>
<div class="line"><span class="lineno">  890</span>        assert_allclose(grad.sum(axis=0), 0, atol=1e-12)</div>
<div class="line"><span class="lineno">  891</span> </div>
<div class="line"><span class="lineno">  892</span> </div>
<div class="line"><span class="lineno">  893</span><span class="preprocessor">@pytest.mark.parametrize</span>(</div>
<div class="line"><span class="lineno">  894</span>    <span class="stringliteral">&quot;loss, func, random_dist&quot;</span>,</div>
<div class="line"><span class="lineno">  895</span>    [</div>
<div class="line"><span class="lineno">  896</span>        (HalfSquaredError(), np.mean, <span class="stringliteral">&quot;normal&quot;</span>),</div>
<div class="line"><span class="lineno">  897</span>        (AbsoluteError(), np.median, <span class="stringliteral">&quot;normal&quot;</span>),</div>
<div class="line"><span class="lineno">  898</span>        (PinballLoss(quantile=0.25), <span class="keyword">lambda</span> x: np.percentile(x, q=25), <span class="stringliteral">&quot;normal&quot;</span>),</div>
<div class="line"><span class="lineno">  899</span>        (HalfPoissonLoss(), np.mean, <span class="stringliteral">&quot;poisson&quot;</span>),</div>
<div class="line"><span class="lineno">  900</span>        (HalfGammaLoss(), np.mean, <span class="stringliteral">&quot;exponential&quot;</span>),</div>
<div class="line"><span class="lineno">  901</span>        (HalfTweedieLoss(), np.mean, <span class="stringliteral">&quot;exponential&quot;</span>),</div>
<div class="line"><span class="lineno">  902</span>        (HalfBinomialLoss(), np.mean, <span class="stringliteral">&quot;binomial&quot;</span>),</div>
<div class="line"><span class="lineno">  903</span>    ],</div>
<div class="line"><span class="lineno">  904</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="ae10d3bf88c3a64607a239fce5530e35a" name="ae10d3bf88c3a64607a239fce5530e35a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae10d3bf88c3a64607a239fce5530e35a">&#9670;&#160;</a></span>test_loss_of_perfect_prediction()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_of_perfect_prediction </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test value of perfect predictions.

Loss of y_pred = y_true plus constant_to_optimal_zero should sums up to
zero.
</pre> <div class="fragment"><div class="line"><span class="lineno">  623</span><span class="keyword">def </span>test_loss_of_perfect_prediction(loss, sample_weight):</div>
<div class="line"><span class="lineno">  624</span>    <span class="stringliteral">&quot;&quot;&quot;Test value of perfect predictions.</span></div>
<div class="line"><span class="lineno">  625</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  626</span><span class="stringliteral">    Loss of y_pred = y_true plus constant_to_optimal_zero should sums up to</span></div>
<div class="line"><span class="lineno">  627</span><span class="stringliteral">    zero.</span></div>
<div class="line"><span class="lineno">  628</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  629</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  630</span>        <span class="comment"># Use small values such that exp(value) is not nan.</span></div>
<div class="line"><span class="lineno">  631</span>        raw_prediction = np.array([-10, -0.1, 0, 0.1, 3, 10])</div>
<div class="line"><span class="lineno">  632</span>        <span class="comment"># If link is identity, we must respect the interval of y_pred:</span></div>
<div class="line"><span class="lineno">  633</span>        <span class="keywordflow">if</span> isinstance(loss.link, IdentityLink):</div>
<div class="line"><span class="lineno">  634</span>            eps = 1e-10</div>
<div class="line"><span class="lineno">  635</span>            low = loss.interval_y_pred.low</div>
<div class="line"><span class="lineno">  636</span>            <span class="keywordflow">if</span> <span class="keywordflow">not</span> loss.interval_y_pred.low_inclusive:</div>
<div class="line"><span class="lineno">  637</span>                low = low + eps</div>
<div class="line"><span class="lineno">  638</span>            high = loss.interval_y_pred.high</div>
<div class="line"><span class="lineno">  639</span>            <span class="keywordflow">if</span> <span class="keywordflow">not</span> loss.interval_y_pred.high_inclusive:</div>
<div class="line"><span class="lineno">  640</span>                high = high - eps</div>
<div class="line"><span class="lineno">  641</span>            raw_prediction = np.clip(raw_prediction, low, high)</div>
<div class="line"><span class="lineno">  642</span>        y_true = loss.link.inverse(raw_prediction)</div>
<div class="line"><span class="lineno">  643</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  644</span>        <span class="comment"># HalfMultinomialLoss</span></div>
<div class="line"><span class="lineno">  645</span>        y_true = np.arange(loss.n_classes).astype(float)</div>
<div class="line"><span class="lineno">  646</span>        <span class="comment"># raw_prediction with entries -exp(10), but +exp(10) on the diagonal</span></div>
<div class="line"><span class="lineno">  647</span>        <span class="comment"># this is close enough to np.inf which would produce nan</span></div>
<div class="line"><span class="lineno">  648</span>        raw_prediction = np.full(</div>
<div class="line"><span class="lineno">  649</span>            shape=(loss.n_classes, loss.n_classes),</div>
<div class="line"><span class="lineno">  650</span>            fill_value=-np.exp(10),</div>
<div class="line"><span class="lineno">  651</span>            dtype=float,</div>
<div class="line"><span class="lineno">  652</span>        )</div>
<div class="line"><span class="lineno">  653</span>        raw_prediction.flat[:: loss.n_classes + 1] = np.exp(10)</div>
<div class="line"><span class="lineno">  654</span> </div>
<div class="line"><span class="lineno">  655</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;range&quot;</span>:</div>
<div class="line"><span class="lineno">  656</span>        sample_weight = np.linspace(1, y_true.shape[0], num=y_true.shape[0])</div>
<div class="line"><span class="lineno">  657</span> </div>
<div class="line"><span class="lineno">  658</span>    loss_value = loss.loss(</div>
<div class="line"><span class="lineno">  659</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  660</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  661</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  662</span>    )</div>
<div class="line"><span class="lineno">  663</span>    constant_term = loss.constant_to_optimal_zero(</div>
<div class="line"><span class="lineno">  664</span>        y_true=y_true, sample_weight=sample_weight</div>
<div class="line"><span class="lineno">  665</span>    )</div>
<div class="line"><span class="lineno">  666</span>    <span class="comment"># Comparing loss_value + constant_term to zero would result in large</span></div>
<div class="line"><span class="lineno">  667</span>    <span class="comment"># round-off errors.</span></div>
<div class="line"><span class="lineno">  668</span>    assert_allclose(loss_value, -constant_term, atol=1e-14, rtol=1e-15)</div>
<div class="line"><span class="lineno">  669</span> </div>
<div class="line"><span class="lineno">  670</span> </div>
<div class="line"><span class="lineno">  671</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
<div class="line"><span class="lineno">  672</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, &quot;range&quot;])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="acbb64142e6d4598db1b0d8f4e6606630" name="acbb64142e6d4598db1b0d8f4e6606630"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acbb64142e6d4598db1b0d8f4e6606630">&#9670;&#160;</a></span>test_loss_on_specific_values()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_on_specific_values </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y_true</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>raw_prediction</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss_true</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test losses at specific values.</pre> <div class="fragment"><div class="line"><span class="lineno">  257</span><span class="keyword">def </span>test_loss_on_specific_values(loss, y_true, raw_prediction, loss_true):</div>
<div class="line"><span class="lineno">  258</span>    <span class="stringliteral">&quot;&quot;&quot;Test losses at specific values.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  259</span>    <span class="keyword">assert</span> loss(</div>
<div class="line"><span class="lineno">  260</span>        y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction])</div>
<div class="line"><span class="lineno">  261</span>    ) == approx(loss_true, rel=1e-11, abs=1e-12)</div>
<div class="line"><span class="lineno">  262</span> </div>
<div class="line"><span class="lineno">  263</span> </div>
<div class="line"><span class="lineno">  264</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, ALL_LOSSES)</span></div>
<div class="line"><span class="lineno">  265</span><span class="preprocessor">@pytest.mark.parametrize(&quot;readonly_memmap&quot;, [False, True])</span></div>
<div class="line"><span class="lineno">  266</span><span class="preprocessor">@pytest.mark.parametrize(&quot;dtype_in&quot;, [np.float32, np.float64])</span></div>
<div class="line"><span class="lineno">  267</span><span class="preprocessor">@pytest.mark.parametrize(&quot;dtype_out&quot;, [np.float32, np.float64])</span></div>
<div class="line"><span class="lineno">  268</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, 1])</span></div>
<div class="line"><span class="lineno">  269</span><span class="preprocessor">@pytest.mark.parametrize(&quot;out1&quot;, [None, 1])</span></div>
<div class="line"><span class="lineno">  270</span><span class="preprocessor">@pytest.mark.parametrize(&quot;out2&quot;, [None, 1])</span></div>
<div class="line"><span class="lineno">  271</span><span class="preprocessor">@pytest.mark.parametrize(&quot;n_threads&quot;, [1, 2])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="afdc7d3349965971783f33103c7debb69" name="afdc7d3349965971783f33103c7debb69"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afdc7d3349965971783f33103c7debb69">&#9670;&#160;</a></span>test_loss_pickle()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_pickle </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that losses can be pickled.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1102</span><span class="keyword">def </span>test_loss_pickle(loss):</div>
<div class="line"><span class="lineno"> 1103</span>    <span class="stringliteral">&quot;&quot;&quot;Test that losses can be pickled.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1104</span>    n_samples = 20</div>
<div class="line"><span class="lineno"> 1105</span>    y_true, raw_prediction = random_y_true_raw_prediction(</div>
<div class="line"><span class="lineno"> 1106</span>        loss=loss,</div>
<div class="line"><span class="lineno"> 1107</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno"> 1108</span>        y_bound=(-100, 100),</div>
<div class="line"><span class="lineno"> 1109</span>        raw_bound=(-5, 5),</div>
<div class="line"><span class="lineno"> 1110</span>        seed=42,</div>
<div class="line"><span class="lineno"> 1111</span>    )</div>
<div class="line"><span class="lineno"> 1112</span>    pickled_loss = pickle.dumps(loss)</div>
<div class="line"><span class="lineno"> 1113</span>    unpickled_loss = pickle.loads(pickled_loss)</div>
<div class="line"><span class="lineno"> 1114</span>    <span class="keyword">assert</span> loss(y_true=y_true, raw_prediction=raw_prediction) == approx(</div>
<div class="line"><span class="lineno"> 1115</span>        unpickled_loss(y_true=y_true, raw_prediction=raw_prediction)</div>
<div class="line"><span class="lineno"> 1116</span>    )</div>
<div class="line"><span class="lineno"> 1117</span> </div>
<div class="line"><span class="lineno"> 1118</span> </div>
<div class="line"><span class="lineno"> 1119</span><span class="preprocessor">@pytest.mark.parametrize(&quot;p&quot;, [-1.5, 0, 1, 1.5, 2, 3])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a31f0b42e1adf3846ad9d2dd01f347af9" name="a31f0b42e1adf3846ad9d2dd01f347af9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a31f0b42e1adf3846ad9d2dd01f347af9">&#9670;&#160;</a></span>test_loss_same_as_C_functions()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_loss_same_as_C_functions </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that Python and Cython functions return same results.</pre> <div class="fragment"><div class="line"><span class="lineno">  358</span><span class="keyword">def </span>test_loss_same_as_C_functions(loss, sample_weight):</div>
<div class="line"><span class="lineno">  359</span>    <span class="stringliteral">&quot;&quot;&quot;Test that Python and Cython functions return same results.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  360</span>    y_true, raw_prediction = random_y_true_raw_prediction(</div>
<div class="line"><span class="lineno">  361</span>        loss=loss,</div>
<div class="line"><span class="lineno">  362</span>        n_samples=20,</div>
<div class="line"><span class="lineno">  363</span>        y_bound=(-100, 100),</div>
<div class="line"><span class="lineno">  364</span>        raw_bound=(-10, 10),</div>
<div class="line"><span class="lineno">  365</span>        seed=42,</div>
<div class="line"><span class="lineno">  366</span>    )</div>
<div class="line"><span class="lineno">  367</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;range&quot;</span>:</div>
<div class="line"><span class="lineno">  368</span>        sample_weight = np.linspace(1, y_true.shape[0], num=y_true.shape[0])</div>
<div class="line"><span class="lineno">  369</span> </div>
<div class="line"><span class="lineno">  370</span>    out_l1 = np.empty_like(y_true)</div>
<div class="line"><span class="lineno">  371</span>    out_l2 = np.empty_like(y_true)</div>
<div class="line"><span class="lineno">  372</span>    out_g1 = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  373</span>    out_g2 = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  374</span>    out_h1 = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  375</span>    out_h2 = np.empty_like(raw_prediction)</div>
<div class="line"><span class="lineno">  376</span>    assert_allclose(</div>
<div class="line"><span class="lineno">  377</span>        loss.loss(</div>
<div class="line"><span class="lineno">  378</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  379</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  380</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  381</span>            loss_out=out_l1,</div>
<div class="line"><span class="lineno">  382</span>        ),</div>
<div class="line"><span class="lineno">  383</span>        loss.closs.loss(</div>
<div class="line"><span class="lineno">  384</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  385</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  386</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  387</span>            loss_out=out_l2,</div>
<div class="line"><span class="lineno">  388</span>        ),</div>
<div class="line"><span class="lineno">  389</span>    )</div>
<div class="line"><span class="lineno">  390</span>    assert_allclose(</div>
<div class="line"><span class="lineno">  391</span>        loss.gradient(</div>
<div class="line"><span class="lineno">  392</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  393</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  394</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  395</span>            gradient_out=out_g1,</div>
<div class="line"><span class="lineno">  396</span>        ),</div>
<div class="line"><span class="lineno">  397</span>        loss.closs.gradient(</div>
<div class="line"><span class="lineno">  398</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  399</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  400</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  401</span>            gradient_out=out_g2,</div>
<div class="line"><span class="lineno">  402</span>        ),</div>
<div class="line"><span class="lineno">  403</span>    )</div>
<div class="line"><span class="lineno">  404</span>    loss.closs.loss_gradient(</div>
<div class="line"><span class="lineno">  405</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  406</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  407</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  408</span>        loss_out=out_l1,</div>
<div class="line"><span class="lineno">  409</span>        gradient_out=out_g1,</div>
<div class="line"><span class="lineno">  410</span>    )</div>
<div class="line"><span class="lineno">  411</span>    loss.closs.loss_gradient(</div>
<div class="line"><span class="lineno">  412</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  413</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  414</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  415</span>        loss_out=out_l2,</div>
<div class="line"><span class="lineno">  416</span>        gradient_out=out_g2,</div>
<div class="line"><span class="lineno">  417</span>    )</div>
<div class="line"><span class="lineno">  418</span>    assert_allclose(out_l1, out_l2)</div>
<div class="line"><span class="lineno">  419</span>    assert_allclose(out_g1, out_g2)</div>
<div class="line"><span class="lineno">  420</span>    loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  421</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  422</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  423</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  424</span>        gradient_out=out_g1,</div>
<div class="line"><span class="lineno">  425</span>        hessian_out=out_h1,</div>
<div class="line"><span class="lineno">  426</span>    )</div>
<div class="line"><span class="lineno">  427</span>    loss.closs.gradient_hessian(</div>
<div class="line"><span class="lineno">  428</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  429</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  430</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  431</span>        gradient_out=out_g2,</div>
<div class="line"><span class="lineno">  432</span>        hessian_out=out_h2,</div>
<div class="line"><span class="lineno">  433</span>    )</div>
<div class="line"><span class="lineno">  434</span>    assert_allclose(out_g1, out_g2)</div>
<div class="line"><span class="lineno">  435</span>    assert_allclose(out_h1, out_h2)</div>
<div class="line"><span class="lineno">  436</span> </div>
<div class="line"><span class="lineno">  437</span> </div>
<div class="line"><span class="lineno">  438</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
<div class="line"><span class="lineno">  439</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, &quot;range&quot;])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a2eeea9ba800414d69a0aae766c08d9a2" name="a2eeea9ba800414d69a0aae766c08d9a2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2eeea9ba800414d69a0aae766c08d9a2">&#9670;&#160;</a></span>test_multinomial_loss_fit_intercept_only()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_multinomial_loss_fit_intercept_only </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that fit_intercept_only returns the mean functional for CCE.</pre> <div class="fragment"><div class="line"><span class="lineno">  936</span><span class="keyword">def </span>test_multinomial_loss_fit_intercept_only():</div>
<div class="line"><span class="lineno">  937</span>    <span class="stringliteral">&quot;&quot;&quot;Test that fit_intercept_only returns the mean functional for CCE.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  938</span>    rng = np.random.RandomState(0)</div>
<div class="line"><span class="lineno">  939</span>    n_classes = 4</div>
<div class="line"><span class="lineno">  940</span>    loss = HalfMultinomialLoss(n_classes=n_classes)</div>
<div class="line"><span class="lineno">  941</span>    <span class="comment"># Same logic as test_specific_fit_intercept_only. Here inverse link</span></div>
<div class="line"><span class="lineno">  942</span>    <span class="comment"># function = softmax and link function = log - symmetry term.</span></div>
<div class="line"><span class="lineno">  943</span>    y_train = rng.randint(0, n_classes + 1, size=100).astype(np.float64)</div>
<div class="line"><span class="lineno">  944</span>    baseline_prediction = loss.fit_intercept_only(y_true=y_train)</div>
<div class="line"><span class="lineno">  945</span>    <span class="keyword">assert</span> baseline_prediction.shape == (n_classes,)</div>
<div class="line"><span class="lineno">  946</span>    p = np.zeros(n_classes, dtype=y_train.dtype)</div>
<div class="line"><span class="lineno">  947</span>    <span class="keywordflow">for</span> k <span class="keywordflow">in</span> range(n_classes):</div>
<div class="line"><span class="lineno">  948</span>        p[k] = (y_train == k).mean()</div>
<div class="line"><span class="lineno">  949</span>    assert_allclose(baseline_prediction, np.log(p) - np.mean(np.log(p)))</div>
<div class="line"><span class="lineno">  950</span>    assert_allclose(baseline_prediction[<span class="keywordtype">None</span>, :], loss.link.link(p[<span class="keywordtype">None</span>, :]))</div>
<div class="line"><span class="lineno">  951</span> </div>
<div class="line"><span class="lineno">  952</span>    <span class="keywordflow">for</span> y_train <span class="keywordflow">in</span> (np.zeros(shape=10), np.ones(shape=10)):</div>
<div class="line"><span class="lineno">  953</span>        y_train = y_train.astype(np.float64)</div>
<div class="line"><span class="lineno">  954</span>        baseline_prediction = loss.fit_intercept_only(y_true=y_train)</div>
<div class="line"><span class="lineno">  955</span>        <span class="keyword">assert</span> baseline_prediction.dtype == y_train.dtype</div>
<div class="line"><span class="lineno">  956</span>        assert_all_finite(baseline_prediction)</div>
<div class="line"><span class="lineno">  957</span> </div>
<div class="line"><span class="lineno">  958</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a64e4f9e9c3d30c779dc1df184f70c62d" name="a64e4f9e9c3d30c779dc1df184f70c62d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a64e4f9e9c3d30c779dc1df184f70c62d">&#9670;&#160;</a></span>test_predict_proba()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_predict_proba </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that predict_proba and gradient_proba work as expected.</pre> <div class="fragment"><div class="line"><span class="lineno">  977</span><span class="keyword">def </span>test_predict_proba(loss, global_random_seed):</div>
<div class="line"><span class="lineno">  978</span>    <span class="stringliteral">&quot;&quot;&quot;Test that predict_proba and gradient_proba work as expected.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  979</span>    n_samples = 20</div>
<div class="line"><span class="lineno">  980</span>    y_true, raw_prediction = random_y_true_raw_prediction(</div>
<div class="line"><span class="lineno">  981</span>        loss=loss,</div>
<div class="line"><span class="lineno">  982</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  983</span>        y_bound=(-100, 100),</div>
<div class="line"><span class="lineno">  984</span>        raw_bound=(-5, 5),</div>
<div class="line"><span class="lineno">  985</span>        seed=global_random_seed,</div>
<div class="line"><span class="lineno">  986</span>    )</div>
<div class="line"><span class="lineno">  987</span> </div>
<div class="line"><span class="lineno">  988</span>    <span class="keywordflow">if</span> hasattr(loss, <span class="stringliteral">&quot;predict_proba&quot;</span>):</div>
<div class="line"><span class="lineno">  989</span>        proba = loss.predict_proba(raw_prediction)</div>
<div class="line"><span class="lineno">  990</span>        <span class="keyword">assert</span> proba.shape == (n_samples, loss.n_classes)</div>
<div class="line"><span class="lineno">  991</span>        <span class="keyword">assert</span> np.sum(proba, axis=1) == approx(1, rel=1e-11)</div>
<div class="line"><span class="lineno">  992</span> </div>
<div class="line"><span class="lineno">  993</span>    <span class="keywordflow">if</span> hasattr(loss, <span class="stringliteral">&quot;gradient_proba&quot;</span>):</div>
<div class="line"><span class="lineno">  994</span>        <span class="keywordflow">for</span> grad, proba <span class="keywordflow">in</span> (</div>
<div class="line"><span class="lineno">  995</span>            (<span class="keywordtype">None</span>, <span class="keywordtype">None</span>),</div>
<div class="line"><span class="lineno">  996</span>            (<span class="keywordtype">None</span>, np.empty_like(raw_prediction)),</div>
<div class="line"><span class="lineno">  997</span>            (np.empty_like(raw_prediction), <span class="keywordtype">None</span>),</div>
<div class="line"><span class="lineno">  998</span>            (np.empty_like(raw_prediction), np.empty_like(raw_prediction)),</div>
<div class="line"><span class="lineno">  999</span>        ):</div>
<div class="line"><span class="lineno"> 1000</span>            grad, proba = loss.gradient_proba(</div>
<div class="line"><span class="lineno"> 1001</span>                y_true=y_true,</div>
<div class="line"><span class="lineno"> 1002</span>                raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno"> 1003</span>                sample_weight=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno"> 1004</span>                gradient_out=grad,</div>
<div class="line"><span class="lineno"> 1005</span>                proba_out=proba,</div>
<div class="line"><span class="lineno"> 1006</span>            )</div>
<div class="line"><span class="lineno"> 1007</span>            <span class="keyword">assert</span> proba.shape == (n_samples, loss.n_classes)</div>
<div class="line"><span class="lineno"> 1008</span>            <span class="keyword">assert</span> np.sum(proba, axis=1) == approx(1, rel=1e-11)</div>
<div class="line"><span class="lineno"> 1009</span>            assert_allclose(</div>
<div class="line"><span class="lineno"> 1010</span>                grad,</div>
<div class="line"><span class="lineno"> 1011</span>                loss.gradient(</div>
<div class="line"><span class="lineno"> 1012</span>                    y_true=y_true,</div>
<div class="line"><span class="lineno"> 1013</span>                    raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno"> 1014</span>                    sample_weight=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno"> 1015</span>                    gradient_out=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno"> 1016</span>                ),</div>
<div class="line"><span class="lineno"> 1017</span>            )</div>
<div class="line"><span class="lineno"> 1018</span> </div>
<div class="line"><span class="lineno"> 1019</span> </div>
<div class="line"><span class="lineno"> 1020</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, ALL_LOSSES)</span></div>
<div class="line"><span class="lineno"> 1021</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, &quot;range&quot;])</span></div>
<div class="line"><span class="lineno"> 1022</span><span class="preprocessor">@pytest.mark.parametrize(&quot;dtype&quot;, (np.float32, np.float64)</span>)</div>
<div class="line"><span class="lineno"> 1023</span><span class="preprocessor">@pytest.mark.parametrize(&quot;order&quot;, (&quot;C&quot;, &quot;F&quot;)</span>)</div>
</div><!-- fragment -->
</div>
</div>
<a id="a936efc1d6f952c5be473378a41cbf761" name="a936efc1d6f952c5be473378a41cbf761"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a936efc1d6f952c5be473378a41cbf761">&#9670;&#160;</a></span>test_sample_weight_multiplies()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_sample_weight_multiplies </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test sample weights in loss, gradients and hessians.

Make sure that passing sample weights to loss, gradient and hessian
computation methods is equivalent to multiplying by the weights.
</pre> <div class="fragment"><div class="line"><span class="lineno">  521</span><span class="keyword">def </span>test_sample_weight_multiplies(loss, sample_weight, global_random_seed):</div>
<div class="line"><span class="lineno">  522</span>    <span class="stringliteral">&quot;&quot;&quot;Test sample weights in loss, gradients and hessians.</span></div>
<div class="line"><span class="lineno">  523</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  524</span><span class="stringliteral">    Make sure that passing sample weights to loss, gradient and hessian</span></div>
<div class="line"><span class="lineno">  525</span><span class="stringliteral">    computation methods is equivalent to multiplying by the weights.</span></div>
<div class="line"><span class="lineno">  526</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  527</span>    n_samples = 100</div>
<div class="line"><span class="lineno">  528</span>    y_true, raw_prediction = random_y_true_raw_prediction(</div>
<div class="line"><span class="lineno">  529</span>        loss=loss,</div>
<div class="line"><span class="lineno">  530</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">  531</span>        y_bound=(-100, 100),</div>
<div class="line"><span class="lineno">  532</span>        raw_bound=(-5, 5),</div>
<div class="line"><span class="lineno">  533</span>        seed=global_random_seed,</div>
<div class="line"><span class="lineno">  534</span>    )</div>
<div class="line"><span class="lineno">  535</span> </div>
<div class="line"><span class="lineno">  536</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;ones&quot;</span>:</div>
<div class="line"><span class="lineno">  537</span>        sample_weight = np.ones(shape=n_samples, dtype=np.float64)</div>
<div class="line"><span class="lineno">  538</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  539</span>        rng = np.random.RandomState(global_random_seed)</div>
<div class="line"><span class="lineno">  540</span>        sample_weight = rng.normal(size=n_samples).astype(np.float64)</div>
<div class="line"><span class="lineno">  541</span> </div>
<div class="line"><span class="lineno">  542</span>    assert_allclose(</div>
<div class="line"><span class="lineno">  543</span>        loss.loss(</div>
<div class="line"><span class="lineno">  544</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  545</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  546</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  547</span>        ),</div>
<div class="line"><span class="lineno">  548</span>        sample_weight</div>
<div class="line"><span class="lineno">  549</span>        * loss.loss(</div>
<div class="line"><span class="lineno">  550</span>            y_true=y_true,</div>
<div class="line"><span class="lineno">  551</span>            raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  552</span>            sample_weight=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno">  553</span>        ),</div>
<div class="line"><span class="lineno">  554</span>    )</div>
<div class="line"><span class="lineno">  555</span> </div>
<div class="line"><span class="lineno">  556</span>    losses, gradient = loss.loss_gradient(</div>
<div class="line"><span class="lineno">  557</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  558</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  559</span>        sample_weight=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno">  560</span>    )</div>
<div class="line"><span class="lineno">  561</span>    losses_sw, gradient_sw = loss.loss_gradient(</div>
<div class="line"><span class="lineno">  562</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  563</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  564</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  565</span>    )</div>
<div class="line"><span class="lineno">  566</span>    assert_allclose(losses * sample_weight, losses_sw)</div>
<div class="line"><span class="lineno">  567</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  568</span>        assert_allclose(gradient * sample_weight, gradient_sw)</div>
<div class="line"><span class="lineno">  569</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  570</span>        assert_allclose(gradient * sample_weight[:, <span class="keywordtype">None</span>], gradient_sw)</div>
<div class="line"><span class="lineno">  571</span> </div>
<div class="line"><span class="lineno">  572</span>    gradient, hessian = loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  573</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  574</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  575</span>        sample_weight=<span class="keywordtype">None</span>,</div>
<div class="line"><span class="lineno">  576</span>    )</div>
<div class="line"><span class="lineno">  577</span>    gradient_sw, hessian_sw = loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  578</span>        y_true=y_true,</div>
<div class="line"><span class="lineno">  579</span>        raw_prediction=raw_prediction,</div>
<div class="line"><span class="lineno">  580</span>        sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  581</span>    )</div>
<div class="line"><span class="lineno">  582</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> loss.is_multiclass:</div>
<div class="line"><span class="lineno">  583</span>        assert_allclose(gradient * sample_weight, gradient_sw)</div>
<div class="line"><span class="lineno">  584</span>        assert_allclose(hessian * sample_weight, hessian_sw)</div>
<div class="line"><span class="lineno">  585</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  586</span>        assert_allclose(gradient * sample_weight[:, <span class="keywordtype">None</span>], gradient_sw)</div>
<div class="line"><span class="lineno">  587</span>        assert_allclose(hessian * sample_weight[:, <span class="keywordtype">None</span>], hessian_sw)</div>
<div class="line"><span class="lineno">  588</span> </div>
<div class="line"><span class="lineno">  589</span> </div>
<div class="line"><span class="lineno">  590</span><span class="preprocessor">@pytest.mark.parametrize(&quot;loss&quot;, LOSS_INSTANCES, ids=loss_instance_name)</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a83fe0602eb695611eaee9e0cea74d7aa" name="a83fe0602eb695611eaee9e0cea74d7aa"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a83fe0602eb695611eaee9e0cea74d7aa">&#9670;&#160;</a></span>test_specific_fit_intercept_only()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_specific_fit_intercept_only </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>func</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>random_dist</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>global_random_seed</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that fit_intercept_only returns the correct functional.

We test the functional for specific, meaningful distributions, e.g.
squared error estimates the expectation of a probability distribution.
</pre> <div class="fragment"><div class="line"><span class="lineno">  905</span><span class="keyword">def </span>test_specific_fit_intercept_only(loss, func, random_dist, global_random_seed):</div>
<div class="line"><span class="lineno">  906</span>    <span class="stringliteral">&quot;&quot;&quot;Test that fit_intercept_only returns the correct functional.</span></div>
<div class="line"><span class="lineno">  907</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  908</span><span class="stringliteral">    We test the functional for specific, meaningful distributions, e.g.</span></div>
<div class="line"><span class="lineno">  909</span><span class="stringliteral">    squared error estimates the expectation of a probability distribution.</span></div>
<div class="line"><span class="lineno">  910</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  911</span>    rng = np.random.RandomState(global_random_seed)</div>
<div class="line"><span class="lineno">  912</span>    <span class="keywordflow">if</span> random_dist == <span class="stringliteral">&quot;binomial&quot;</span>:</div>
<div class="line"><span class="lineno">  913</span>        y_train = rng.binomial(1, 0.5, size=100)</div>
<div class="line"><span class="lineno">  914</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  915</span>        y_train = getattr(rng, random_dist)(size=100)</div>
<div class="line"><span class="lineno">  916</span>    baseline_prediction = loss.fit_intercept_only(y_true=y_train)</div>
<div class="line"><span class="lineno">  917</span>    <span class="comment"># Make sure baseline prediction is the expected functional=func, e.g. mean</span></div>
<div class="line"><span class="lineno">  918</span>    <span class="comment"># or median.</span></div>
<div class="line"><span class="lineno">  919</span>    assert_all_finite(baseline_prediction)</div>
<div class="line"><span class="lineno">  920</span>    <span class="keyword">assert</span> baseline_prediction == approx(loss.link.link(<a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>(y_train)))</div>
<div class="line"><span class="lineno">  921</span>    <span class="keyword">assert</span> loss.link.inverse(baseline_prediction) == approx(<a class="code hl_function" href="callback_2foo_8f.html#a565fe2cc583df102f120752b0011c330">func</a>(y_train))</div>
<div class="line"><span class="lineno">  922</span>    <span class="keywordflow">if</span> isinstance(loss, IdentityLink):</div>
<div class="line"><span class="lineno">  923</span>        assert_allclose(loss.link.inverse(baseline_prediction), baseline_prediction)</div>
<div class="line"><span class="lineno">  924</span> </div>
<div class="line"><span class="lineno">  925</span>    <span class="comment"># Test baseline at boundary</span></div>
<div class="line"><span class="lineno">  926</span>    <span class="keywordflow">if</span> loss.interval_y_true.low_inclusive:</div>
<div class="line"><span class="lineno">  927</span>        y_train.fill(loss.interval_y_true.low)</div>
<div class="line"><span class="lineno">  928</span>        baseline_prediction = loss.fit_intercept_only(y_true=y_train)</div>
<div class="line"><span class="lineno">  929</span>        assert_all_finite(baseline_prediction)</div>
<div class="line"><span class="lineno">  930</span>    <span class="keywordflow">if</span> loss.interval_y_true.high_inclusive:</div>
<div class="line"><span class="lineno">  931</span>        y_train.fill(loss.interval_y_true.high)</div>
<div class="line"><span class="lineno">  932</span>        baseline_prediction = loss.fit_intercept_only(y_true=y_train)</div>
<div class="line"><span class="lineno">  933</span>        assert_all_finite(baseline_prediction)</div>
<div class="line"><span class="lineno">  934</span> </div>
<div class="line"><span class="lineno">  935</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="aa9bb886015d6009257e9a601b5be95ae" name="aa9bb886015d6009257e9a601b5be95ae"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa9bb886015d6009257e9a601b5be95ae">&#9670;&#160;</a></span>test_tweedie_log_identity_consistency()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.test_tweedie_log_identity_consistency </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>p</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test for identical losses when only the link function is different.</pre> <div class="fragment"><div class="line"><span class="lineno"> 1120</span><span class="keyword">def </span>test_tweedie_log_identity_consistency(p):</div>
<div class="line"><span class="lineno"> 1121</span>    <span class="stringliteral">&quot;&quot;&quot;Test for identical losses when only the link function is different.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno"> 1122</span>    half_tweedie_log = HalfTweedieLoss(power=p)</div>
<div class="line"><span class="lineno"> 1123</span>    half_tweedie_identity = HalfTweedieLossIdentity(power=p)</div>
<div class="line"><span class="lineno"> 1124</span>    n_samples = 10</div>
<div class="line"><span class="lineno"> 1125</span>    y_true, raw_prediction = random_y_true_raw_prediction(</div>
<div class="line"><span class="lineno"> 1126</span>        loss=half_tweedie_log, n_samples=n_samples, seed=42</div>
<div class="line"><span class="lineno"> 1127</span>    )</div>
<div class="line"><span class="lineno"> 1128</span>    y_pred = half_tweedie_log.link.inverse(raw_prediction)  <span class="comment"># exp(raw_prediction)</span></div>
<div class="line"><span class="lineno"> 1129</span> </div>
<div class="line"><span class="lineno"> 1130</span>    <span class="comment"># Let&#39;s compare the loss values, up to some constant term that is dropped</span></div>
<div class="line"><span class="lineno"> 1131</span>    <span class="comment"># in HalfTweedieLoss but not in HalfTweedieLossIdentity.</span></div>
<div class="line"><span class="lineno"> 1132</span>    loss_log = half_tweedie_log.loss(</div>
<div class="line"><span class="lineno"> 1133</span>        y_true=y_true, raw_prediction=raw_prediction</div>
<div class="line"><span class="lineno"> 1134</span>    ) + half_tweedie_log.constant_to_optimal_zero(y_true)</div>
<div class="line"><span class="lineno"> 1135</span>    loss_identity = half_tweedie_identity.loss(</div>
<div class="line"><span class="lineno"> 1136</span>        y_true=y_true, raw_prediction=y_pred</div>
<div class="line"><span class="lineno"> 1137</span>    ) + half_tweedie_identity.constant_to_optimal_zero(y_true)</div>
<div class="line"><span class="lineno"> 1138</span>    <span class="comment"># Note that HalfTweedieLoss ignores different constant terms than</span></div>
<div class="line"><span class="lineno"> 1139</span>    <span class="comment"># HalfTweedieLossIdentity. Constant terms means terms not depending on</span></div>
<div class="line"><span class="lineno"> 1140</span>    <span class="comment"># raw_prediction. By adding these terms, `constant_to_optimal_zero`, both losses</span></div>
<div class="line"><span class="lineno"> 1141</span>    <span class="comment"># give the same values.</span></div>
<div class="line"><span class="lineno"> 1142</span>    assert_allclose(loss_log, loss_identity)</div>
<div class="line"><span class="lineno"> 1143</span> </div>
<div class="line"><span class="lineno"> 1144</span>    <span class="comment"># For gradients and hessians, the constant terms do not matter. We have, however,</span></div>
<div class="line"><span class="lineno"> 1145</span>    <span class="comment"># to account for the chain rule, i.e. with x=raw_prediction</span></div>
<div class="line"><span class="lineno"> 1146</span>    <span class="comment">#     gradient_log(x) = d/dx loss_log(x)</span></div>
<div class="line"><span class="lineno"> 1147</span>    <span class="comment">#                     = d/dx loss_identity(exp(x))</span></div>
<div class="line"><span class="lineno"> 1148</span>    <span class="comment">#                     = exp(x) * gradient_identity(exp(x))</span></div>
<div class="line"><span class="lineno"> 1149</span>    <span class="comment"># Similarly,</span></div>
<div class="line"><span class="lineno"> 1150</span>    <span class="comment">#     hessian_log(x) = exp(x) * gradient_identity(exp(x))</span></div>
<div class="line"><span class="lineno"> 1151</span>    <span class="comment">#                    + exp(x)**2 * hessian_identity(x)</span></div>
<div class="line"><span class="lineno"> 1152</span>    gradient_log, hessian_log = half_tweedie_log.gradient_hessian(</div>
<div class="line"><span class="lineno"> 1153</span>        y_true=y_true, raw_prediction=raw_prediction</div>
<div class="line"><span class="lineno"> 1154</span>    )</div>
<div class="line"><span class="lineno"> 1155</span>    gradient_identity, hessian_identity = half_tweedie_identity.gradient_hessian(</div>
<div class="line"><span class="lineno"> 1156</span>        y_true=y_true, raw_prediction=y_pred</div>
<div class="line"><span class="lineno"> 1157</span>    )</div>
<div class="line"><span class="lineno"> 1158</span>    assert_allclose(gradient_log, y_pred * gradient_identity)</div>
<div class="line"><span class="lineno"> 1159</span>    assert_allclose(</div>
<div class="line"><span class="lineno"> 1160</span>        hessian_log, y_pred * gradient_identity + y_pred**2 * hessian_identity</div>
<div class="line"><span class="lineno"> 1161</span>    )</div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a2e547bf309ead91bde7600be03fc01b9" name="a2e547bf309ead91bde7600be03fc01b9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2e547bf309ead91bde7600be03fc01b9">&#9670;&#160;</a></span>ALL_LOSSES</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.ALL_LOSSES = list(_LOSSES.values())</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a4f4094434d6c3280473404eba33f0bbb" name="a4f4094434d6c3280473404eba33f0bbb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4f4094434d6c3280473404eba33f0bbb">&#9670;&#160;</a></span>LOSS_INSTANCES</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">list sklearn._loss.tests.test_loss.LOSS_INSTANCES = [loss() for loss <a class="el" href="__lapack__subroutines_8h.html#aa83d4778c28341ab79b01b2371f666fe">in</a> <a class="el" href="namespacesklearn_1_1__loss_1_1tests_1_1test__loss.html#a2e547bf309ead91bde7600be03fc01b9">ALL_LOSSES</a>]</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a38f62f770434f1daf34751961f519abd" name="a38f62f770434f1daf34751961f519abd"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a38f62f770434f1daf34751961f519abd">&#9670;&#160;</a></span>power</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.power</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="a1e75af958ca19de34410a62fbf0ecb9d" name="a1e75af958ca19de34410a62fbf0ecb9d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a1e75af958ca19de34410a62fbf0ecb9d">&#9670;&#160;</a></span>quantile</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn._loss.tests.test_loss.quantile</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a id="ab59b142e46405d390a88235f892d31d3" name="ab59b142e46405d390a88235f892d31d3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab59b142e46405d390a88235f892d31d3">&#9670;&#160;</a></span>Y_COMMON_PARAMS</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">list sklearn._loss.tests.test_loss.Y_COMMON_PARAMS</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><span class="lineno">    1</span>=  [</div>
<div class="line"><span class="lineno">    2</span>    <span class="comment"># (loss, [y success], [y fail])</span></div>
<div class="line"><span class="lineno">    3</span>    (HalfSquaredError(), [-100, 0, 0.1, 100], [-np.inf, np.inf]),</div>
<div class="line"><span class="lineno">    4</span>    (AbsoluteError(), [-100, 0, 0.1, 100], [-np.inf, np.inf]),</div>
<div class="line"><span class="lineno">    5</span>    (PinballLoss(), [-100, 0, 0.1, 100], [-np.inf, np.inf]),</div>
<div class="line"><span class="lineno">    6</span>    (HalfPoissonLoss(), [0.1, 100], [-np.inf, -3, -0.1, np.inf]),</div>
<div class="line"><span class="lineno">    7</span>    (HalfGammaLoss(), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),</div>
<div class="line"><span class="lineno">    8</span>    (HalfTweedieLoss(power=-3), [0.1, 100], [-np.inf, np.inf]),</div>
<div class="line"><span class="lineno">    9</span>    (HalfTweedieLoss(power=0), [0.1, 100], [-np.inf, np.inf]),</div>
<div class="line"><span class="lineno">   10</span>    (HalfTweedieLoss(power=1.5), [0.1, 100], [-np.inf, -3, -0.1, np.inf]),</div>
<div class="line"><span class="lineno">   11</span>    (HalfTweedieLoss(power=2), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),</div>
<div class="line"><span class="lineno">   12</span>    (HalfTweedieLoss(power=3), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),</div>
<div class="line"><span class="lineno">   13</span>    (HalfTweedieLossIdentity(power=-3), [0.1, 100], [-np.inf, np.inf]),</div>
<div class="line"><span class="lineno">   14</span>    (HalfTweedieLossIdentity(power=0), [-3, -0.1, 0, 0.1, 100], [-np.inf, np.inf]),</div>
<div class="line"><span class="lineno">   15</span>    (HalfTweedieLossIdentity(power=1.5), [0.1, 100], [-np.inf, -3, -0.1, np.inf]),</div>
<div class="line"><span class="lineno">   16</span>    (HalfTweedieLossIdentity(power=2), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),</div>
<div class="line"><span class="lineno">   17</span>    (HalfTweedieLossIdentity(power=3), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),</div>
<div class="line"><span class="lineno">   18</span>    (HalfBinomialLoss(), [0.1, 0.5, 0.9], [-np.inf, -1, 2, np.inf]),</div>
<div class="line"><span class="lineno">   19</span>    (HalfMultinomialLoss(), [], [-np.inf, -1, 1.1, np.inf]),</div>
<div class="line"><span class="lineno">   20</span>]</div>
</div><!-- fragment -->
</div>
</div>
<a id="a9579babf070569c6cd1fd44ae71142dc" name="a9579babf070569c6cd1fd44ae71142dc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9579babf070569c6cd1fd44ae71142dc">&#9670;&#160;</a></span>Y_PRED_PARAMS</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">list sklearn._loss.tests.test_loss.Y_PRED_PARAMS</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><span class="lineno">    1</span>=  [</div>
<div class="line"><span class="lineno">    2</span>    <span class="comment"># (loss, [y success], [y fail])</span></div>
<div class="line"><span class="lineno">    3</span>    (HalfPoissonLoss(), [], [0]),</div>
<div class="line"><span class="lineno">    4</span>    (HalfTweedieLoss(power=-3), [], [-3, -0.1, 0]),</div>
<div class="line"><span class="lineno">    5</span>    (HalfTweedieLoss(power=0), [], [-3, -0.1, 0]),</div>
<div class="line"><span class="lineno">    6</span>    (HalfTweedieLoss(power=1.5), [], [0]),</div>
<div class="line"><span class="lineno">    7</span>    (HalfTweedieLossIdentity(power=-3), [], [-3, -0.1, 0]),</div>
<div class="line"><span class="lineno">    8</span>    (HalfTweedieLossIdentity(power=0), [-3, -0.1, 0], []),</div>
<div class="line"><span class="lineno">    9</span>    (HalfTweedieLossIdentity(power=1.5), [], [0]),</div>
<div class="line"><span class="lineno">   10</span>    (HalfBinomialLoss(), [], [0, 1]),</div>
<div class="line"><span class="lineno">   11</span>    (HalfMultinomialLoss(), [0.1, 0.5], [0, 1]),</div>
<div class="line"><span class="lineno">   12</span>]</div>
</div><!-- fragment -->
</div>
</div>
<a id="a68afee3ba6c626df0015e63d1e26ed8a" name="a68afee3ba6c626df0015e63d1e26ed8a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a68afee3ba6c626df0015e63d1e26ed8a">&#9670;&#160;</a></span>Y_TRUE_PARAMS</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">list sklearn._loss.tests.test_loss.Y_TRUE_PARAMS</td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Initial value:</b><div class="fragment"><div class="line"><span class="lineno">    1</span>=  [  <span class="comment"># type: ignore</span></div>
<div class="line"><span class="lineno">    2</span>    <span class="comment"># (loss, [y success], [y fail])</span></div>
<div class="line"><span class="lineno">    3</span>    (HalfPoissonLoss(), [0], []),</div>
<div class="line"><span class="lineno">    4</span>    (HalfTweedieLoss(power=-3), [-100, -0.1, 0], []),</div>
<div class="line"><span class="lineno">    5</span>    (HalfTweedieLoss(power=0), [-100, 0], []),</div>
<div class="line"><span class="lineno">    6</span>    (HalfTweedieLoss(power=1.5), [0], []),</div>
<div class="line"><span class="lineno">    7</span>    (HalfTweedieLossIdentity(power=-3), [-100, -0.1, 0], []),</div>
<div class="line"><span class="lineno">    8</span>    (HalfTweedieLossIdentity(power=0), [-100, 0], []),</div>
<div class="line"><span class="lineno">    9</span>    (HalfTweedieLossIdentity(power=1.5), [0], []),</div>
<div class="line"><span class="lineno">   10</span>    (HalfBinomialLoss(), [0, 1], []),</div>
<div class="line"><span class="lineno">   11</span>    (HalfMultinomialLoss(), [0.0, 1.0, 2], []),</div>
<div class="line"><span class="lineno">   12</span>]</div>
</div><!-- fragment -->
</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
