<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.linear_model.tests.test_linear_loss Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model.html">linear_model</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests.html">tests</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss.html">test_linear_loss</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">sklearn.linear_model.tests.test_linear_loss Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:aba6de1423bd923f8d4a792fb44100f4e" id="r_aba6de1423bd923f8d4a792fb44100f4e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss.html#aba6de1423bd923f8d4a792fb44100f4e">random_X_y_coef</a> (linear_model_loss, n_samples, n_features, coef_bound=(-2, 2), seed=42)</td></tr>
<tr class="separator:aba6de1423bd923f8d4a792fb44100f4e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad32a2edb2eab8ac7ec4ac1d53d50ce9b" id="r_ad32a2edb2eab8ac7ec4ac1d53d50ce9b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss.html#ad32a2edb2eab8ac7ec4ac1d53d50ce9b">test_init_zero_coef</a> (base_loss, fit_intercept, n_features, dtype)</td></tr>
<tr class="separator:ad32a2edb2eab8ac7ec4ac1d53d50ce9b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af73f41d1b64ad4cfcf2a830279b158d9" id="r_af73f41d1b64ad4cfcf2a830279b158d9"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss.html#af73f41d1b64ad4cfcf2a830279b158d9">test_loss_grad_hess_are_the_same</a> (base_loss, fit_intercept, sample_weight, l2_reg_strength)</td></tr>
<tr class="separator:af73f41d1b64ad4cfcf2a830279b158d9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0dd71d701c4003fd87529c5685f5645" id="r_aa0dd71d701c4003fd87529c5685f5645"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss.html#aa0dd71d701c4003fd87529c5685f5645">test_loss_gradients_hessp_intercept</a> (base_loss, sample_weight, l2_reg_strength, X_sparse)</td></tr>
<tr class="separator:aa0dd71d701c4003fd87529c5685f5645"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6a04c9e326214232b982e7442f159d04" id="r_a6a04c9e326214232b982e7442f159d04"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss.html#a6a04c9e326214232b982e7442f159d04">test_gradients_hessians_numerically</a> (base_loss, fit_intercept, sample_weight, l2_reg_strength)</td></tr>
<tr class="separator:a6a04c9e326214232b982e7442f159d04"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8d22bf3c6dd0de97e8eaa5137560b2b0" id="r_a8d22bf3c6dd0de97e8eaa5137560b2b0"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss.html#a8d22bf3c6dd0de97e8eaa5137560b2b0">test_multinomial_coef_shape</a> (fit_intercept)</td></tr>
<tr class="separator:a8d22bf3c6dd0de97e8eaa5137560b2b0"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:ae0121dcc583e66a2a93666c2beb23f9d" id="r_ae0121dcc583e66a2a93666c2beb23f9d"><td class="memItemLeft" align="right" valign="top">list&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1linear__model_1_1tests_1_1test__linear__loss.html#ae0121dcc583e66a2a93666c2beb23f9d">LOSSES</a> = [<a class="el" href="classsklearn_1_1__loss_1_1loss_1_1_half_binomial_loss.html">HalfBinomialLoss</a>, <a class="el" href="classsklearn_1_1__loss_1_1loss_1_1_half_multinomial_loss.html">HalfMultinomialLoss</a>, <a class="el" href="classsklearn_1_1__loss_1_1loss_1_1_half_poisson_loss.html">HalfPoissonLoss</a>]</td></tr>
<tr class="separator:ae0121dcc583e66a2a93666c2beb23f9d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Tests for LinearModelLoss

Note that correctness of losses (which compose LinearModelLoss) is already well
covered in the _loss module.
</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="aba6de1423bd923f8d4a792fb44100f4e" name="aba6de1423bd923f8d4a792fb44100f4e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aba6de1423bd923f8d4a792fb44100f4e">&#9670;&#160;</a></span>random_X_y_coef()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_linear_loss.random_X_y_coef </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>linear_model_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_samples</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>coef_bound</em> = <code>(-2,&#160;2)</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>seed</em> = <code>42</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Random generate y, X and coef in valid range.</pre> <div class="fragment"><div class="line"><span class="lineno">   29</span>):</div>
<div class="line"><span class="lineno">   30</span>    <span class="stringliteral">&quot;&quot;&quot;Random generate y, X and coef in valid range.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   31</span>    rng = np.random.RandomState(seed)</div>
<div class="line"><span class="lineno">   32</span>    n_dof = n_features + linear_model_loss.fit_intercept</div>
<div class="line"><span class="lineno">   33</span>    X = make_low_rank_matrix(</div>
<div class="line"><span class="lineno">   34</span>        n_samples=n_samples,</div>
<div class="line"><span class="lineno">   35</span>        n_features=n_features,</div>
<div class="line"><span class="lineno">   36</span>        random_state=rng,</div>
<div class="line"><span class="lineno">   37</span>    )</div>
<div class="line"><span class="lineno">   38</span>    coef = linear_model_loss.init_zero_coef(X)</div>
<div class="line"><span class="lineno">   39</span> </div>
<div class="line"><span class="lineno">   40</span>    <span class="keywordflow">if</span> linear_model_loss.base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">   41</span>        n_classes = linear_model_loss.base_loss.n_classes</div>
<div class="line"><span class="lineno">   42</span>        coef.flat[:] = rng.uniform(</div>
<div class="line"><span class="lineno">   43</span>            low=coef_bound[0],</div>
<div class="line"><span class="lineno">   44</span>            high=coef_bound[1],</div>
<div class="line"><span class="lineno">   45</span>            size=n_classes * n_dof,</div>
<div class="line"><span class="lineno">   46</span>        )</div>
<div class="line"><span class="lineno">   47</span>        <span class="keywordflow">if</span> linear_model_loss.fit_intercept:</div>
<div class="line"><span class="lineno">   48</span>            raw_prediction = X @ coef[:, :-1].T + coef[:, -1]</div>
<div class="line"><span class="lineno">   49</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   50</span>            raw_prediction = X @ coef.T</div>
<div class="line"><span class="lineno">   51</span>        proba = linear_model_loss.base_loss.link.inverse(raw_prediction)</div>
<div class="line"><span class="lineno">   52</span> </div>
<div class="line"><span class="lineno">   53</span>        <span class="comment"># y = rng.choice(np.arange(n_classes), p=proba) does not work.</span></div>
<div class="line"><span class="lineno">   54</span>        <span class="comment"># See https://stackoverflow.com/a/34190035/16761084</span></div>
<div class="line"><span class="lineno">   55</span>        <span class="keyword">def </span>choice_vectorized(items, p):</div>
<div class="line"><span class="lineno">   56</span>            s = p.cumsum(axis=1)</div>
<div class="line"><span class="lineno">   57</span>            r = rng.rand(p.shape[0])[:, <span class="keywordtype">None</span>]</div>
<div class="line"><span class="lineno">   58</span>            k = (s &lt; r).sum(axis=1)</div>
<div class="line"><span class="lineno">   59</span>            <span class="keywordflow">return</span> items[k]</div>
<div class="line"><span class="lineno">   60</span> </div>
<div class="line"><span class="lineno">   61</span>        y = choice_vectorized(np.arange(n_classes), p=proba).astype(np.float64)</div>
<div class="line"><span class="lineno">   62</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   63</span>        coef.flat[:] = rng.uniform(</div>
<div class="line"><span class="lineno">   64</span>            low=coef_bound[0],</div>
<div class="line"><span class="lineno">   65</span>            high=coef_bound[1],</div>
<div class="line"><span class="lineno">   66</span>            size=n_dof,</div>
<div class="line"><span class="lineno">   67</span>        )</div>
<div class="line"><span class="lineno">   68</span>        <span class="keywordflow">if</span> linear_model_loss.fit_intercept:</div>
<div class="line"><span class="lineno">   69</span>            raw_prediction = X @ coef[:-1] + coef[-1]</div>
<div class="line"><span class="lineno">   70</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   71</span>            raw_prediction = X @ coef</div>
<div class="line"><span class="lineno">   72</span>        y = linear_model_loss.base_loss.link.inverse(</div>
<div class="line"><span class="lineno">   73</span>            raw_prediction + rng.uniform(low=-1, high=1, size=n_samples)</div>
<div class="line"><span class="lineno">   74</span>        )</div>
<div class="line"><span class="lineno">   75</span> </div>
<div class="line"><span class="lineno">   76</span>    <span class="keywordflow">return</span> X, y, coef</div>
<div class="line"><span class="lineno">   77</span> </div>
<div class="line"><span class="lineno">   78</span> </div>
<div class="line"><span class="lineno">   79</span><span class="preprocessor">@pytest.mark.parametrize(&quot;base_loss&quot;, LOSSES)</span></div>
<div class="line"><span class="lineno">   80</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [False, True])</span></div>
<div class="line"><span class="lineno">   81</span><span class="preprocessor">@pytest.mark.parametrize(&quot;n_features&quot;, [0, 1, 10])</span></div>
<div class="line"><span class="lineno">   82</span><span class="preprocessor">@pytest.mark.parametrize(&quot;dtype&quot;, [None, np.float32, np.float64, np.int64])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a6a04c9e326214232b982e7442f159d04" name="a6a04c9e326214232b982e7442f159d04"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6a04c9e326214232b982e7442f159d04">&#9670;&#160;</a></span>test_gradients_hessians_numerically()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_linear_loss.test_gradients_hessians_numerically </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>base_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_strength</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test gradients and hessians with numerical derivatives.

Gradient should equal the numerical derivatives of the loss function.
Hessians should equal the numerical derivatives of gradients.
</pre> <div class="fragment"><div class="line"><span class="lineno">  245</span>):</div>
<div class="line"><span class="lineno">  246</span>    <span class="stringliteral">&quot;&quot;&quot;Test gradients and hessians with numerical derivatives.</span></div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral">    Gradient should equal the numerical derivatives of the loss function.</span></div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">    Hessians should equal the numerical derivatives of gradients.</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  251</span>    loss = LinearModelLoss(base_loss=base_loss(), fit_intercept=fit_intercept)</div>
<div class="line"><span class="lineno">  252</span>    n_samples, n_features = 10, 5</div>
<div class="line"><span class="lineno">  253</span>    X, y, coef = random_X_y_coef(</div>
<div class="line"><span class="lineno">  254</span>        linear_model_loss=loss, n_samples=n_samples, n_features=n_features, seed=42</div>
<div class="line"><span class="lineno">  255</span>    )</div>
<div class="line"><span class="lineno">  256</span>    coef = coef.ravel(order=<span class="stringliteral">&quot;F&quot;</span>)  <span class="comment"># this is important only for multinomial loss</span></div>
<div class="line"><span class="lineno">  257</span> </div>
<div class="line"><span class="lineno">  258</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;range&quot;</span>:</div>
<div class="line"><span class="lineno">  259</span>        sample_weight = np.linspace(1, y.shape[0], num=y.shape[0])</div>
<div class="line"><span class="lineno">  260</span> </div>
<div class="line"><span class="lineno">  261</span>    <span class="comment"># 1. Check gradients numerically</span></div>
<div class="line"><span class="lineno">  262</span>    eps = 1e-6</div>
<div class="line"><span class="lineno">  263</span>    g, hessp = loss.gradient_hessian_product(</div>
<div class="line"><span class="lineno">  264</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  265</span>    )</div>
<div class="line"><span class="lineno">  266</span>    <span class="comment"># Use a trick to get central finite difference of accuracy 4 (five-point stencil)</span></div>
<div class="line"><span class="lineno">  267</span>    <span class="comment"># https://en.wikipedia.org/wiki/Numerical_differentiation</span></div>
<div class="line"><span class="lineno">  268</span>    <span class="comment"># https://en.wikipedia.org/wiki/Finite_difference_coefficient</span></div>
<div class="line"><span class="lineno">  269</span>    <span class="comment"># approx_g1 = (f(x + eps) - f(x - eps)) / (2*eps)</span></div>
<div class="line"><span class="lineno">  270</span>    approx_g1 = optimize.approx_fprime(</div>
<div class="line"><span class="lineno">  271</span>        coef,</div>
<div class="line"><span class="lineno">  272</span>        <span class="keyword">lambda</span> coef: loss.loss(</div>
<div class="line"><span class="lineno">  273</span>            coef - eps,</div>
<div class="line"><span class="lineno">  274</span>            X,</div>
<div class="line"><span class="lineno">  275</span>            y,</div>
<div class="line"><span class="lineno">  276</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  277</span>            l2_reg_strength=l2_reg_strength,</div>
<div class="line"><span class="lineno">  278</span>        ),</div>
<div class="line"><span class="lineno">  279</span>        2 * eps,</div>
<div class="line"><span class="lineno">  280</span>    )</div>
<div class="line"><span class="lineno">  281</span>    <span class="comment"># approx_g2 = (f(x + 2*eps) - f(x - 2*eps)) / (4*eps)</span></div>
<div class="line"><span class="lineno">  282</span>    approx_g2 = optimize.approx_fprime(</div>
<div class="line"><span class="lineno">  283</span>        coef,</div>
<div class="line"><span class="lineno">  284</span>        <span class="keyword">lambda</span> coef: loss.loss(</div>
<div class="line"><span class="lineno">  285</span>            coef - 2 * eps,</div>
<div class="line"><span class="lineno">  286</span>            X,</div>
<div class="line"><span class="lineno">  287</span>            y,</div>
<div class="line"><span class="lineno">  288</span>            sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  289</span>            l2_reg_strength=l2_reg_strength,</div>
<div class="line"><span class="lineno">  290</span>        ),</div>
<div class="line"><span class="lineno">  291</span>        4 * eps,</div>
<div class="line"><span class="lineno">  292</span>    )</div>
<div class="line"><span class="lineno">  293</span>    <span class="comment"># Five-point stencil approximation</span></div>
<div class="line"><span class="lineno">  294</span>    <span class="comment"># See: https://en.wikipedia.org/wiki/Five-point_stencil#1D_first_derivative</span></div>
<div class="line"><span class="lineno">  295</span>    approx_g = (4 * approx_g1 - approx_g2) / 3</div>
<div class="line"><span class="lineno">  296</span>    assert_allclose(g, approx_g, rtol=1e-2, atol=1e-8)</div>
<div class="line"><span class="lineno">  297</span> </div>
<div class="line"><span class="lineno">  298</span>    <span class="comment"># 2. Check hessp numerically along the second direction of the gradient</span></div>
<div class="line"><span class="lineno">  299</span>    vector = np.zeros_like(g)</div>
<div class="line"><span class="lineno">  300</span>    vector[1] = 1</div>
<div class="line"><span class="lineno">  301</span>    hess_col = hessp(vector)</div>
<div class="line"><span class="lineno">  302</span>    <span class="comment"># Computation of the Hessian is particularly fragile to numerical errors when doing</span></div>
<div class="line"><span class="lineno">  303</span>    <span class="comment"># simple finite differences. Here we compute the grad along a path in the direction</span></div>
<div class="line"><span class="lineno">  304</span>    <span class="comment"># of the vector and then use a least-square regression to estimate the slope</span></div>
<div class="line"><span class="lineno">  305</span>    eps = 1e-3</div>
<div class="line"><span class="lineno">  306</span>    d_x = np.linspace(-eps, eps, 30)</div>
<div class="line"><span class="lineno">  307</span>    d_grad = np.array(</div>
<div class="line"><span class="lineno">  308</span>        [</div>
<div class="line"><span class="lineno">  309</span>            loss.gradient(</div>
<div class="line"><span class="lineno">  310</span>                coef + t * vector,</div>
<div class="line"><span class="lineno">  311</span>                X,</div>
<div class="line"><span class="lineno">  312</span>                y,</div>
<div class="line"><span class="lineno">  313</span>                sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  314</span>                l2_reg_strength=l2_reg_strength,</div>
<div class="line"><span class="lineno">  315</span>            )</div>
<div class="line"><span class="lineno">  316</span>            <span class="keywordflow">for</span> t <span class="keywordflow">in</span> d_x</div>
<div class="line"><span class="lineno">  317</span>        ]</div>
<div class="line"><span class="lineno">  318</span>    )</div>
<div class="line"><span class="lineno">  319</span>    d_grad -= d_grad.mean(axis=0)</div>
<div class="line"><span class="lineno">  320</span>    approx_hess_col = linalg.lstsq(d_x[:, np.newaxis], d_grad)[0].ravel()</div>
<div class="line"><span class="lineno">  321</span>    assert_allclose(approx_hess_col, hess_col, rtol=1e-3)</div>
<div class="line"><span class="lineno">  322</span> </div>
<div class="line"><span class="lineno">  323</span> </div>
<div class="line"><span class="lineno">  324</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [False, True])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="ad32a2edb2eab8ac7ec4ac1d53d50ce9b" name="ad32a2edb2eab8ac7ec4ac1d53d50ce9b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad32a2edb2eab8ac7ec4ac1d53d50ce9b">&#9670;&#160;</a></span>test_init_zero_coef()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_linear_loss.test_init_zero_coef </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>base_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>n_features</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>dtype</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that init_zero_coef initializes coef correctly.</pre> <div class="fragment"><div class="line"><span class="lineno">   83</span><span class="keyword">def </span>test_init_zero_coef(base_loss, fit_intercept, n_features, dtype):</div>
<div class="line"><span class="lineno">   84</span>    <span class="stringliteral">&quot;&quot;&quot;Test that init_zero_coef initializes coef correctly.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   85</span>    loss = LinearModelLoss(base_loss=base_loss(), fit_intercept=fit_intercept)</div>
<div class="line"><span class="lineno">   86</span>    rng = np.random.RandomState(42)</div>
<div class="line"><span class="lineno">   87</span>    X = rng.normal(size=(5, n_features))</div>
<div class="line"><span class="lineno">   88</span>    coef = loss.init_zero_coef(X, dtype=dtype)</div>
<div class="line"><span class="lineno">   89</span>    <span class="keywordflow">if</span> loss.base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">   90</span>        n_classes = loss.base_loss.n_classes</div>
<div class="line"><span class="lineno">   91</span>        <span class="keyword">assert</span> coef.shape == (n_classes, n_features + fit_intercept)</div>
<div class="line"><span class="lineno">   92</span>        <span class="keyword">assert</span> coef.flags[<span class="stringliteral">&quot;F_CONTIGUOUS&quot;</span>]</div>
<div class="line"><span class="lineno">   93</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   94</span>        <span class="keyword">assert</span> coef.shape == (n_features + fit_intercept,)</div>
<div class="line"><span class="lineno">   95</span> </div>
<div class="line"><span class="lineno">   96</span>    <span class="keywordflow">if</span> dtype <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div>
<div class="line"><span class="lineno">   97</span>        <span class="keyword">assert</span> coef.dtype == X.dtype</div>
<div class="line"><span class="lineno">   98</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">   99</span>        <span class="keyword">assert</span> coef.dtype == dtype</div>
<div class="line"><span class="lineno">  100</span> </div>
<div class="line"><span class="lineno">  101</span>    <span class="keyword">assert</span> np.count_nonzero(coef) == 0</div>
<div class="line"><span class="lineno">  102</span> </div>
<div class="line"><span class="lineno">  103</span> </div>
<div class="line"><span class="lineno">  104</span><span class="preprocessor">@pytest.mark.parametrize(&quot;base_loss&quot;, LOSSES)</span></div>
<div class="line"><span class="lineno">  105</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [False, True])</span></div>
<div class="line"><span class="lineno">  106</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, &quot;range&quot;])</span></div>
<div class="line"><span class="lineno">  107</span><span class="preprocessor">@pytest.mark.parametrize(&quot;l2_reg_strength&quot;, [0, 1])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="af73f41d1b64ad4cfcf2a830279b158d9" name="af73f41d1b64ad4cfcf2a830279b158d9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af73f41d1b64ad4cfcf2a830279b158d9">&#9670;&#160;</a></span>test_loss_grad_hess_are_the_same()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_linear_loss.test_loss_grad_hess_are_the_same </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>base_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_strength</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that loss and gradient are the same across different functions.</pre> <div class="fragment"><div class="line"><span class="lineno">  110</span>):</div>
<div class="line"><span class="lineno">  111</span>    <span class="stringliteral">&quot;&quot;&quot;Test that loss and gradient are the same across different functions.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  112</span>    loss = LinearModelLoss(base_loss=base_loss(), fit_intercept=fit_intercept)</div>
<div class="line"><span class="lineno">  113</span>    X, y, coef = random_X_y_coef(</div>
<div class="line"><span class="lineno">  114</span>        linear_model_loss=loss, n_samples=10, n_features=5, seed=42</div>
<div class="line"><span class="lineno">  115</span>    )</div>
<div class="line"><span class="lineno">  116</span> </div>
<div class="line"><span class="lineno">  117</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;range&quot;</span>:</div>
<div class="line"><span class="lineno">  118</span>        sample_weight = np.linspace(1, y.shape[0], num=y.shape[0])</div>
<div class="line"><span class="lineno">  119</span> </div>
<div class="line"><span class="lineno">  120</span>    l1 = loss.loss(</div>
<div class="line"><span class="lineno">  121</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  122</span>    )</div>
<div class="line"><span class="lineno">  123</span>    g1 = loss.gradient(</div>
<div class="line"><span class="lineno">  124</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  125</span>    )</div>
<div class="line"><span class="lineno">  126</span>    l2, g2 = loss.loss_gradient(</div>
<div class="line"><span class="lineno">  127</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  128</span>    )</div>
<div class="line"><span class="lineno">  129</span>    g3, h3 = loss.gradient_hessian_product(</div>
<div class="line"><span class="lineno">  130</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  131</span>    )</div>
<div class="line"><span class="lineno">  132</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  133</span>        g4, h4, _ = loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  134</span>            coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  135</span>        )</div>
<div class="line"><span class="lineno">  136</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  137</span>        <span class="keyword">with</span> pytest.raises(NotImplementedError):</div>
<div class="line"><span class="lineno">  138</span>            loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  139</span>                coef,</div>
<div class="line"><span class="lineno">  140</span>                X,</div>
<div class="line"><span class="lineno">  141</span>                y,</div>
<div class="line"><span class="lineno">  142</span>                sample_weight=sample_weight,</div>
<div class="line"><span class="lineno">  143</span>                l2_reg_strength=l2_reg_strength,</div>
<div class="line"><span class="lineno">  144</span>            )</div>
<div class="line"><span class="lineno">  145</span> </div>
<div class="line"><span class="lineno">  146</span>    assert_allclose(l1, l2)</div>
<div class="line"><span class="lineno">  147</span>    assert_allclose(g1, g2)</div>
<div class="line"><span class="lineno">  148</span>    assert_allclose(g1, g3)</div>
<div class="line"><span class="lineno">  149</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  150</span>        assert_allclose(g1, g4)</div>
<div class="line"><span class="lineno">  151</span>        assert_allclose(h4 @ g4, h3(g3))</div>
<div class="line"><span class="lineno">  152</span> </div>
<div class="line"><span class="lineno">  153</span>    <span class="comment"># same for sparse X</span></div>
<div class="line"><span class="lineno">  154</span>    X = sparse.csr_matrix(X)</div>
<div class="line"><span class="lineno">  155</span>    l1_sp = loss.loss(</div>
<div class="line"><span class="lineno">  156</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  157</span>    )</div>
<div class="line"><span class="lineno">  158</span>    g1_sp = loss.gradient(</div>
<div class="line"><span class="lineno">  159</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  160</span>    )</div>
<div class="line"><span class="lineno">  161</span>    l2_sp, g2_sp = loss.loss_gradient(</div>
<div class="line"><span class="lineno">  162</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  163</span>    )</div>
<div class="line"><span class="lineno">  164</span>    g3_sp, h3_sp = loss.gradient_hessian_product(</div>
<div class="line"><span class="lineno">  165</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  166</span>    )</div>
<div class="line"><span class="lineno">  167</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  168</span>        g4_sp, h4_sp, _ = loss.gradient_hessian(</div>
<div class="line"><span class="lineno">  169</span>            coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  170</span>        )</div>
<div class="line"><span class="lineno">  171</span> </div>
<div class="line"><span class="lineno">  172</span>    assert_allclose(l1, l1_sp)</div>
<div class="line"><span class="lineno">  173</span>    assert_allclose(l1, l2_sp)</div>
<div class="line"><span class="lineno">  174</span>    assert_allclose(g1, g1_sp)</div>
<div class="line"><span class="lineno">  175</span>    assert_allclose(g1, g2_sp)</div>
<div class="line"><span class="lineno">  176</span>    assert_allclose(g1, g3_sp)</div>
<div class="line"><span class="lineno">  177</span>    assert_allclose(h3(g1), h3_sp(g1_sp))</div>
<div class="line"><span class="lineno">  178</span>    <span class="keywordflow">if</span> <span class="keywordflow">not</span> base_loss.is_multiclass:</div>
<div class="line"><span class="lineno">  179</span>        assert_allclose(g1, g4_sp)</div>
<div class="line"><span class="lineno">  180</span>        assert_allclose(h4 @ g4, h4_sp @ g1_sp)</div>
<div class="line"><span class="lineno">  181</span> </div>
<div class="line"><span class="lineno">  182</span> </div>
<div class="line"><span class="lineno">  183</span><span class="preprocessor">@pytest.mark.parametrize(&quot;base_loss&quot;, LOSSES)</span></div>
<div class="line"><span class="lineno">  184</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, &quot;range&quot;])</span></div>
<div class="line"><span class="lineno">  185</span><span class="preprocessor">@pytest.mark.parametrize(&quot;l2_reg_strength&quot;, [0, 1])</span></div>
<div class="line"><span class="lineno">  186</span><span class="preprocessor">@pytest.mark.parametrize(&quot;X_sparse&quot;, [False, True])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="aa0dd71d701c4003fd87529c5685f5645" name="aa0dd71d701c4003fd87529c5685f5645"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa0dd71d701c4003fd87529c5685f5645">&#9670;&#160;</a></span>test_loss_gradients_hessp_intercept()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_linear_loss.test_loss_gradients_hessp_intercept </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>base_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>sample_weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>l2_reg_strength</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X_sparse</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that loss and gradient handle intercept correctly.</pre> <div class="fragment"><div class="line"><span class="lineno">  189</span>):</div>
<div class="line"><span class="lineno">  190</span>    <span class="stringliteral">&quot;&quot;&quot;Test that loss and gradient handle intercept correctly.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  191</span>    loss = LinearModelLoss(base_loss=base_loss(), fit_intercept=<span class="keyword">False</span>)</div>
<div class="line"><span class="lineno">  192</span>    loss_inter = LinearModelLoss(base_loss=base_loss(), fit_intercept=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">  193</span>    n_samples, n_features = 10, 5</div>
<div class="line"><span class="lineno">  194</span>    X, y, coef = random_X_y_coef(</div>
<div class="line"><span class="lineno">  195</span>        linear_model_loss=loss, n_samples=n_samples, n_features=n_features, seed=42</div>
<div class="line"><span class="lineno">  196</span>    )</div>
<div class="line"><span class="lineno">  197</span> </div>
<div class="line"><span class="lineno">  198</span>    X[:, -1] = 1  <span class="comment"># make last column of 1 to mimic intercept term</span></div>
<div class="line"><span class="lineno">  199</span>    X_inter = X[</div>
<div class="line"><span class="lineno">  200</span>        :, :-1</div>
<div class="line"><span class="lineno">  201</span>    ]  <span class="comment"># exclude intercept column as it is added automatically by loss_inter</span></div>
<div class="line"><span class="lineno">  202</span> </div>
<div class="line"><span class="lineno">  203</span>    <span class="keywordflow">if</span> X_sparse:</div>
<div class="line"><span class="lineno">  204</span>        X = sparse.csr_matrix(X)</div>
<div class="line"><span class="lineno">  205</span> </div>
<div class="line"><span class="lineno">  206</span>    <span class="keywordflow">if</span> sample_weight == <span class="stringliteral">&quot;range&quot;</span>:</div>
<div class="line"><span class="lineno">  207</span>        sample_weight = np.linspace(1, y.shape[0], num=y.shape[0])</div>
<div class="line"><span class="lineno">  208</span> </div>
<div class="line"><span class="lineno">  209</span>    l, g = loss.loss_gradient(</div>
<div class="line"><span class="lineno">  210</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  211</span>    )</div>
<div class="line"><span class="lineno">  212</span>    _, hessp = loss.gradient_hessian_product(</div>
<div class="line"><span class="lineno">  213</span>        coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  214</span>    )</div>
<div class="line"><span class="lineno">  215</span>    l_inter, g_inter = loss_inter.loss_gradient(</div>
<div class="line"><span class="lineno">  216</span>        coef, X_inter, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  217</span>    )</div>
<div class="line"><span class="lineno">  218</span>    _, hessp_inter = loss_inter.gradient_hessian_product(</div>
<div class="line"><span class="lineno">  219</span>        coef, X_inter, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength</div>
<div class="line"><span class="lineno">  220</span>    )</div>
<div class="line"><span class="lineno">  221</span> </div>
<div class="line"><span class="lineno">  222</span>    <span class="comment"># Note, that intercept gets no L2 penalty.</span></div>
<div class="line"><span class="lineno">  223</span>    <span class="keyword">assert</span> l == pytest.approx(</div>
<div class="line"><span class="lineno">  224</span>        l_inter + 0.5 * l2_reg_strength * squared_norm(coef.T[-1])</div>
<div class="line"><span class="lineno">  225</span>    )</div>
<div class="line"><span class="lineno">  226</span> </div>
<div class="line"><span class="lineno">  227</span>    g_inter_corrected = g_inter</div>
<div class="line"><span class="lineno">  228</span>    g_inter_corrected.T[-1] += l2_reg_strength * coef.T[-1]</div>
<div class="line"><span class="lineno">  229</span>    assert_allclose(g, g_inter_corrected)</div>
<div class="line"><span class="lineno">  230</span> </div>
<div class="line"><span class="lineno">  231</span>    s = np.random.RandomState(42).randn(*coef.shape)</div>
<div class="line"><span class="lineno">  232</span>    h = hessp(s)</div>
<div class="line"><span class="lineno">  233</span>    h_inter = hessp_inter(s)</div>
<div class="line"><span class="lineno">  234</span>    h_inter_corrected = h_inter</div>
<div class="line"><span class="lineno">  235</span>    h_inter_corrected.T[-1] += l2_reg_strength * s.T[-1]</div>
<div class="line"><span class="lineno">  236</span>    assert_allclose(h, h_inter_corrected)</div>
<div class="line"><span class="lineno">  237</span> </div>
<div class="line"><span class="lineno">  238</span> </div>
<div class="line"><span class="lineno">  239</span><span class="preprocessor">@pytest.mark.parametrize(&quot;base_loss&quot;, LOSSES)</span></div>
<div class="line"><span class="lineno">  240</span><span class="preprocessor">@pytest.mark.parametrize(&quot;fit_intercept&quot;, [False, True])</span></div>
<div class="line"><span class="lineno">  241</span><span class="preprocessor">@pytest.mark.parametrize(&quot;sample_weight&quot;, [None, &quot;range&quot;])</span></div>
<div class="line"><span class="lineno">  242</span><span class="preprocessor">@pytest.mark.parametrize(&quot;l2_reg_strength&quot;, [0, 1])</span></div>
</div><!-- fragment -->
</div>
</div>
<a id="a8d22bf3c6dd0de97e8eaa5137560b2b0" name="a8d22bf3c6dd0de97e8eaa5137560b2b0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8d22bf3c6dd0de97e8eaa5137560b2b0">&#9670;&#160;</a></span>test_multinomial_coef_shape()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.linear_model.tests.test_linear_loss.test_multinomial_coef_shape </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>fit_intercept</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Test that multinomial LinearModelLoss respects shape of coef.</pre> <div class="fragment"><div class="line"><span class="lineno">  325</span><span class="keyword">def </span>test_multinomial_coef_shape(fit_intercept):</div>
<div class="line"><span class="lineno">  326</span>    <span class="stringliteral">&quot;&quot;&quot;Test that multinomial LinearModelLoss respects shape of coef.&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  327</span>    loss = LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept)</div>
<div class="line"><span class="lineno">  328</span>    n_samples, n_features = 10, 5</div>
<div class="line"><span class="lineno">  329</span>    X, y, coef = random_X_y_coef(</div>
<div class="line"><span class="lineno">  330</span>        linear_model_loss=loss, n_samples=n_samples, n_features=n_features, seed=42</div>
<div class="line"><span class="lineno">  331</span>    )</div>
<div class="line"><span class="lineno">  332</span>    s = np.random.RandomState(42).randn(*coef.shape)</div>
<div class="line"><span class="lineno">  333</span> </div>
<div class="line"><span class="lineno">  334</span>    l, g = loss.loss_gradient(coef, X, y)</div>
<div class="line"><span class="lineno">  335</span>    g1 = loss.gradient(coef, X, y)</div>
<div class="line"><span class="lineno">  336</span>    g2, hessp = loss.gradient_hessian_product(coef, X, y)</div>
<div class="line"><span class="lineno">  337</span>    h = hessp(s)</div>
<div class="line"><span class="lineno">  338</span>    <span class="keyword">assert</span> g.shape == coef.shape</div>
<div class="line"><span class="lineno">  339</span>    <span class="keyword">assert</span> h.shape == coef.shape</div>
<div class="line"><span class="lineno">  340</span>    assert_allclose(g, g1)</div>
<div class="line"><span class="lineno">  341</span>    assert_allclose(g, g2)</div>
<div class="line"><span class="lineno">  342</span> </div>
<div class="line"><span class="lineno">  343</span>    coef_r = coef.ravel(order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  344</span>    s_r = s.ravel(order=<span class="stringliteral">&quot;F&quot;</span>)</div>
<div class="line"><span class="lineno">  345</span>    l_r, g_r = loss.loss_gradient(coef_r, X, y)</div>
<div class="line"><span class="lineno">  346</span>    g1_r = loss.gradient(coef_r, X, y)</div>
<div class="line"><span class="lineno">  347</span>    g2_r, hessp_r = loss.gradient_hessian_product(coef_r, X, y)</div>
<div class="line"><span class="lineno">  348</span>    h_r = hessp_r(s_r)</div>
<div class="line"><span class="lineno">  349</span>    <span class="keyword">assert</span> g_r.shape == coef_r.shape</div>
<div class="line"><span class="lineno">  350</span>    <span class="keyword">assert</span> h_r.shape == coef_r.shape</div>
<div class="line"><span class="lineno">  351</span>    assert_allclose(g_r, g1_r)</div>
<div class="line"><span class="lineno">  352</span>    assert_allclose(g_r, g2_r)</div>
<div class="line"><span class="lineno">  353</span> </div>
<div class="line"><span class="lineno">  354</span>    assert_allclose(g, g_r.reshape(loss.base_loss.n_classes, -1, order=<span class="stringliteral">&quot;F&quot;</span>))</div>
<div class="line"><span class="lineno">  355</span>    assert_allclose(h, h_r.reshape(loss.base_loss.n_classes, -1, order=<span class="stringliteral">&quot;F&quot;</span>))</div>
</div><!-- fragment -->
</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="ae0121dcc583e66a2a93666c2beb23f9d" name="ae0121dcc583e66a2a93666c2beb23f9d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae0121dcc583e66a2a93666c2beb23f9d">&#9670;&#160;</a></span>LOSSES</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">list sklearn.linear_model.tests.test_linear_loss.LOSSES = [<a class="el" href="classsklearn_1_1__loss_1_1loss_1_1_half_binomial_loss.html">HalfBinomialLoss</a>, <a class="el" href="classsklearn_1_1__loss_1_1loss_1_1_half_multinomial_loss.html">HalfMultinomialLoss</a>, <a class="el" href="classsklearn_1_1__loss_1_1loss_1_1_half_poisson_loss.html">HalfPoissonLoss</a>]</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
