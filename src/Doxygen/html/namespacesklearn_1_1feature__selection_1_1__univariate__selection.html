<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Tp_Gcs: sklearn.feature_selection._univariate_selection Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Tp_Gcs<span id="projectnumber">&#160;1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="namespacesklearn.html">sklearn</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1feature__selection.html">feature_selection</a></li><li class="navelem"><a class="el" href="namespacesklearn_1_1feature__selection_1_1__univariate__selection.html">_univariate_selection</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">sklearn.feature_selection._univariate_selection Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1feature__selection_1_1__univariate__selection_1_1___base_filter.html">_BaseFilter</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Base classes.  <a href="classsklearn_1_1feature__selection_1_1__univariate__selection_1_1___base_filter.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1feature__selection_1_1__univariate__selection_1_1_generic_univariate_select.html">GenericUnivariateSelect</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1feature__selection_1_1__univariate__selection_1_1_select_fdr.html">SelectFdr</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1feature__selection_1_1__univariate__selection_1_1_select_fpr.html">SelectFpr</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1feature__selection_1_1__univariate__selection_1_1_select_fwe.html">SelectFwe</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1feature__selection_1_1__univariate__selection_1_1_select_k_best.html">SelectKBest</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classsklearn_1_1feature__selection_1_1__univariate__selection_1_1_select_percentile.html">SelectPercentile</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a70745a2c61d07bb19180a2110979be3f" id="r_a70745a2c61d07bb19180a2110979be3f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1feature__selection_1_1__univariate__selection.html#a70745a2c61d07bb19180a2110979be3f">_clean_nans</a> (scores)</td></tr>
<tr class="separator:a70745a2c61d07bb19180a2110979be3f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace981915c526d804a95d186a927e1a04" id="r_ace981915c526d804a95d186a927e1a04"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1feature__selection_1_1__univariate__selection.html#ace981915c526d804a95d186a927e1a04">f_oneway</a> (*args)</td></tr>
<tr class="memdesc:ace981915c526d804a95d186a927e1a04"><td class="mdescLeft">&#160;</td><td class="mdescRight">Scoring functions.  <br /></td></tr>
<tr class="separator:ace981915c526d804a95d186a927e1a04"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a490bd5eefc2e1330c503dacd2ff0cce7" id="r_a490bd5eefc2e1330c503dacd2ff0cce7"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1feature__selection_1_1__univariate__selection.html#a490bd5eefc2e1330c503dacd2ff0cce7">f_classif</a> (X, y)</td></tr>
<tr class="separator:a490bd5eefc2e1330c503dacd2ff0cce7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a252449a3735ac3062f5036b514e26c33" id="r_a252449a3735ac3062f5036b514e26c33"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1feature__selection_1_1__univariate__selection.html#a252449a3735ac3062f5036b514e26c33">_chisquare</a> (f_obs, f_exp)</td></tr>
<tr class="separator:a252449a3735ac3062f5036b514e26c33"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a144d057f54c10af88f150c23a7f98333" id="r_a144d057f54c10af88f150c23a7f98333"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1feature__selection_1_1__univariate__selection.html#a144d057f54c10af88f150c23a7f98333">chi2</a> (X, y)</td></tr>
<tr class="separator:a144d057f54c10af88f150c23a7f98333"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afac3471e30dd6cfd854eb74be746b53d" id="r_afac3471e30dd6cfd854eb74be746b53d"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1feature__selection_1_1__univariate__selection.html#afac3471e30dd6cfd854eb74be746b53d">r_regression</a> (X, y, *center=True, force_finite=True)</td></tr>
<tr class="separator:afac3471e30dd6cfd854eb74be746b53d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adc55e84bd49169f1b07bf7ef6881c466" id="r_adc55e84bd49169f1b07bf7ef6881c466"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacesklearn_1_1feature__selection_1_1__univariate__selection.html#adc55e84bd49169f1b07bf7ef6881c466">f_regression</a> (X, y, *center=True, force_finite=True)</td></tr>
<tr class="separator:adc55e84bd49169f1b07bf7ef6881c466"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><pre class="fragment">Univariate features selection.</pre> </div><h2 class="groupheader">Function Documentation</h2>
<a id="a252449a3735ac3062f5036b514e26c33" name="a252449a3735ac3062f5036b514e26c33"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a252449a3735ac3062f5036b514e26c33">&#9670;&#160;</a></span>_chisquare()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.feature_selection._univariate_selection._chisquare </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>f_obs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>f_exp</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Fast replacement for scipy.stats.chisquare.

Version from https://github.com/scipy/scipy/pull/2525 with additional
optimizations.
</pre> <div class="fragment"><div class="line"><span class="lineno">  151</span><span class="keyword">def </span>_chisquare(f_obs, f_exp):</div>
<div class="line"><span class="lineno">  152</span>    <span class="stringliteral">&quot;&quot;&quot;Fast replacement for scipy.stats.chisquare.</span></div>
<div class="line"><span class="lineno">  153</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  154</span><span class="stringliteral">    Version from https://github.com/scipy/scipy/pull/2525 with additional</span></div>
<div class="line"><span class="lineno">  155</span><span class="stringliteral">    optimizations.</span></div>
<div class="line"><span class="lineno">  156</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  157</span>    f_obs = np.asarray(f_obs, dtype=np.float64)</div>
<div class="line"><span class="lineno">  158</span> </div>
<div class="line"><span class="lineno">  159</span>    k = len(f_obs)</div>
<div class="line"><span class="lineno">  160</span>    <span class="comment"># Reuse f_obs for chi-squared statistics</span></div>
<div class="line"><span class="lineno">  161</span>    chisq = f_obs</div>
<div class="line"><span class="lineno">  162</span>    chisq -= f_exp</div>
<div class="line"><span class="lineno">  163</span>    chisq **= 2</div>
<div class="line"><span class="lineno">  164</span>    <span class="keyword">with</span> np.errstate(invalid=<span class="stringliteral">&quot;ignore&quot;</span>):</div>
<div class="line"><span class="lineno">  165</span>        chisq /= f_exp</div>
<div class="line"><span class="lineno">  166</span>    chisq = chisq.sum(axis=0)</div>
<div class="line"><span class="lineno">  167</span>    <span class="keywordflow">return</span> chisq, special.chdtrc(k - 1, chisq)</div>
<div class="line"><span class="lineno">  168</span> </div>
<div class="line"><span class="lineno">  169</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a70745a2c61d07bb19180a2110979be3f" name="a70745a2c61d07bb19180a2110979be3f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a70745a2c61d07bb19180a2110979be3f">&#9670;&#160;</a></span>_clean_nans()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.feature_selection._univariate_selection._clean_nans </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>scores</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<pre class="fragment">Fixes Issue #1240: NaNs can't be properly compared, so change them to the
smallest value of scores's dtype. -inf seems to be unreliable.
</pre> <div class="fragment"><div class="line"><span class="lineno">   24</span><span class="keyword">def </span>_clean_nans(scores):</div>
<div class="line"><span class="lineno">   25</span>    <span class="stringliteral">&quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   26</span><span class="stringliteral">    Fixes Issue #1240: NaNs can&#39;t be properly compared, so change them to the</span></div>
<div class="line"><span class="lineno">   27</span><span class="stringliteral">    smallest value of scores&#39;s dtype. -inf seems to be unreliable.</span></div>
<div class="line"><span class="lineno">   28</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   29</span>    <span class="comment"># XXX where should this function be called? fit? scoring functions</span></div>
<div class="line"><span class="lineno">   30</span>    <span class="comment"># themselves?</span></div>
<div class="line"><span class="lineno">   31</span>    scores = as_float_array(scores, copy=<span class="keyword">True</span>)</div>
<div class="line"><span class="lineno">   32</span>    scores[np.isnan(scores)] = np.finfo(scores.dtype).min</div>
<div class="line"><span class="lineno">   33</span>    <span class="keywordflow">return</span> scores</div>
<div class="line"><span class="lineno">   34</span> </div>
<div class="line"><span class="lineno">   35</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a144d057f54c10af88f150c23a7f98333" name="a144d057f54c10af88f150c23a7f98333"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a144d057f54c10af88f150c23a7f98333">&#9670;&#160;</a></span>chi2()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.feature_selection._univariate_selection.chi2 </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute chi-squared stats between each non-negative feature and class.

This score can be used to select the n_features features with the
highest values for the test chi-squared statistic from X, which must
contain only non-negative features such as booleans or frequencies
(e.g., term counts in document classification), relative to the classes.

Recall that the chi-square test measures dependence between stochastic
variables, so using this function "weeds out" the features that are the
most likely to be independent of class and therefore irrelevant for
classification.

Read more in the :ref:`User Guide &lt;univariate_feature_selection&gt;`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    Sample vectors.

y : array-like of shape (n_samples,)
    Target vector (class labels).

Returns
-------
chi2 : ndarray of shape (n_features,)
    Chi2 statistics for each feature.

p_values : ndarray of shape (n_features,)
    P-values for each feature.

See Also
--------
f_classif : ANOVA F-value between label/feature for classification tasks.
f_regression : F-value between label/feature for regression tasks.

Notes
-----
Complexity of this algorithm is O(n_classes * n_features).
</pre> <div class="fragment"><div class="line"><span class="lineno">  170</span><span class="keyword">def </span>chi2(X, y):</div>
<div class="line"><span class="lineno">  171</span>    <span class="stringliteral">&quot;&quot;&quot;Compute chi-squared stats between each non-negative feature and class.</span></div>
<div class="line"><span class="lineno">  172</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  173</span><span class="stringliteral">    This score can be used to select the n_features features with the</span></div>
<div class="line"><span class="lineno">  174</span><span class="stringliteral">    highest values for the test chi-squared statistic from X, which must</span></div>
<div class="line"><span class="lineno">  175</span><span class="stringliteral">    contain only non-negative features such as booleans or frequencies</span></div>
<div class="line"><span class="lineno">  176</span><span class="stringliteral">    (e.g., term counts in document classification), relative to the classes.</span></div>
<div class="line"><span class="lineno">  177</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  178</span><span class="stringliteral">    Recall that the chi-square test measures dependence between stochastic</span></div>
<div class="line"><span class="lineno">  179</span><span class="stringliteral">    variables, so using this function &quot;weeds out&quot; the features that are the</span></div>
<div class="line"><span class="lineno">  180</span><span class="stringliteral">    most likely to be independent of class and therefore irrelevant for</span></div>
<div class="line"><span class="lineno">  181</span><span class="stringliteral">    classification.</span></div>
<div class="line"><span class="lineno">  182</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  183</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;univariate_feature_selection&gt;`.</span></div>
<div class="line"><span class="lineno">  184</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  185</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  186</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  187</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  188</span><span class="stringliteral">        Sample vectors.</span></div>
<div class="line"><span class="lineno">  189</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  190</span><span class="stringliteral">    y : array-like of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  191</span><span class="stringliteral">        Target vector (class labels).</span></div>
<div class="line"><span class="lineno">  192</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  193</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  194</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  195</span><span class="stringliteral">    chi2 : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  196</span><span class="stringliteral">        Chi2 statistics for each feature.</span></div>
<div class="line"><span class="lineno">  197</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  198</span><span class="stringliteral">    p_values : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  199</span><span class="stringliteral">        P-values for each feature.</span></div>
<div class="line"><span class="lineno">  200</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  201</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  202</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  203</span><span class="stringliteral">    f_classif : ANOVA F-value between label/feature for classification tasks.</span></div>
<div class="line"><span class="lineno">  204</span><span class="stringliteral">    f_regression : F-value between label/feature for regression tasks.</span></div>
<div class="line"><span class="lineno">  205</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  206</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">  207</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">  208</span><span class="stringliteral">    Complexity of this algorithm is O(n_classes * n_features).</span></div>
<div class="line"><span class="lineno">  209</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  210</span> </div>
<div class="line"><span class="lineno">  211</span>    <span class="comment"># XXX: we might want to do some of the following in logspace instead for</span></div>
<div class="line"><span class="lineno">  212</span>    <span class="comment"># numerical stability.</span></div>
<div class="line"><span class="lineno">  213</span>    <span class="comment"># Converting X to float allows getting better performance for the</span></div>
<div class="line"><span class="lineno">  214</span>    <span class="comment"># safe_sparse_dot call made below.</span></div>
<div class="line"><span class="lineno">  215</span>    X = check_array(X, accept_sparse=<span class="stringliteral">&quot;csr&quot;</span>, dtype=(np.float64, np.float32))</div>
<div class="line"><span class="lineno">  216</span>    <span class="keywordflow">if</span> np.any((X.data <span class="keywordflow">if</span> issparse(X) <span class="keywordflow">else</span> X) &lt; 0):</div>
<div class="line"><span class="lineno">  217</span>        <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Input X must be non-negative.&quot;</span>)</div>
<div class="line"><span class="lineno">  218</span> </div>
<div class="line"><span class="lineno">  219</span>    <span class="comment"># Use a sparse representation for Y by default to reduce memory usage when</span></div>
<div class="line"><span class="lineno">  220</span>    <span class="comment"># y has many unique classes.</span></div>
<div class="line"><span class="lineno">  221</span>    Y = LabelBinarizer(sparse_output=<span class="keyword">True</span>).fit_transform(y)</div>
<div class="line"><span class="lineno">  222</span>    <span class="keywordflow">if</span> Y.shape[1] == 1:</div>
<div class="line"><span class="lineno">  223</span>        Y = Y.toarray()</div>
<div class="line"><span class="lineno">  224</span>        Y = np.append(1 - Y, Y, axis=1)</div>
<div class="line"><span class="lineno">  225</span> </div>
<div class="line"><span class="lineno">  226</span>    observed = safe_sparse_dot(Y.T, X)  <span class="comment"># n_classes * n_features</span></div>
<div class="line"><span class="lineno">  227</span> </div>
<div class="line"><span class="lineno">  228</span>    <span class="keywordflow">if</span> issparse(observed):</div>
<div class="line"><span class="lineno">  229</span>        <span class="comment"># convert back to a dense array before calling _chisquare</span></div>
<div class="line"><span class="lineno">  230</span>        <span class="comment"># XXX: could _chisquare be reimplement to accept sparse matrices for</span></div>
<div class="line"><span class="lineno">  231</span>        <span class="comment"># cases where both n_classes and n_features are large (and X is</span></div>
<div class="line"><span class="lineno">  232</span>        <span class="comment"># sparse)?</span></div>
<div class="line"><span class="lineno">  233</span>        observed = observed.toarray()</div>
<div class="line"><span class="lineno">  234</span> </div>
<div class="line"><span class="lineno">  235</span>    feature_count = X.sum(axis=0).reshape(1, -1)</div>
<div class="line"><span class="lineno">  236</span>    class_prob = Y.mean(axis=0).reshape(1, -1)</div>
<div class="line"><span class="lineno">  237</span>    expected = np.dot(class_prob.T, feature_count)</div>
<div class="line"><span class="lineno">  238</span> </div>
<div class="line"><span class="lineno">  239</span>    <span class="keywordflow">return</span> _chisquare(observed, expected)</div>
<div class="line"><span class="lineno">  240</span> </div>
<div class="line"><span class="lineno">  241</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="a490bd5eefc2e1330c503dacd2ff0cce7" name="a490bd5eefc2e1330c503dacd2ff0cce7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a490bd5eefc2e1330c503dacd2ff0cce7">&#9670;&#160;</a></span>f_classif()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.feature_selection._univariate_selection.f_classif </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute the ANOVA F-value for the provided sample.

Read more in the :ref:`User Guide &lt;univariate_feature_selection&gt;`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The set of regressors that will be tested sequentially.

y : ndarray of shape (n_samples,)
    The target vector.

Returns
-------
f_statistic : ndarray of shape (n_features,)
    F-statistic for each feature.

p_values : ndarray of shape (n_features,)
    P-values associated with the F-statistic.

See Also
--------
chi2 : Chi-squared stats of non-negative features for classification tasks.
f_regression : F-value between label/feature for regression tasks.
</pre> <div class="fragment"><div class="line"><span class="lineno">  120</span><span class="keyword">def </span>f_classif(X, y):</div>
<div class="line"><span class="lineno">  121</span>    <span class="stringliteral">&quot;&quot;&quot;Compute the ANOVA F-value for the provided sample.</span></div>
<div class="line"><span class="lineno">  122</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  123</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;univariate_feature_selection&gt;`.</span></div>
<div class="line"><span class="lineno">  124</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  125</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  126</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  127</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  128</span><span class="stringliteral">        The set of regressors that will be tested sequentially.</span></div>
<div class="line"><span class="lineno">  129</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  130</span><span class="stringliteral">    y : ndarray of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  131</span><span class="stringliteral">        The target vector.</span></div>
<div class="line"><span class="lineno">  132</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  133</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  134</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  135</span><span class="stringliteral">    f_statistic : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  136</span><span class="stringliteral">        F-statistic for each feature.</span></div>
<div class="line"><span class="lineno">  137</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  138</span><span class="stringliteral">    p_values : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  139</span><span class="stringliteral">        P-values associated with the F-statistic.</span></div>
<div class="line"><span class="lineno">  140</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  141</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  142</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  143</span><span class="stringliteral">    chi2 : Chi-squared stats of non-negative features for classification tasks.</span></div>
<div class="line"><span class="lineno">  144</span><span class="stringliteral">    f_regression : F-value between label/feature for regression tasks.</span></div>
<div class="line"><span class="lineno">  145</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  146</span>    X, y = check_X_y(X, y, accept_sparse=[<span class="stringliteral">&quot;csr&quot;</span>, <span class="stringliteral">&quot;csc&quot;</span>, <span class="stringliteral">&quot;coo&quot;</span>])</div>
<div class="line"><span class="lineno">  147</span>    args = [X[safe_mask(X, y == k)] <span class="keywordflow">for</span> k <span class="keywordflow">in</span> np.unique(y)]</div>
<div class="line"><span class="lineno">  148</span>    <span class="keywordflow">return</span> f_oneway(*args)</div>
<div class="line"><span class="lineno">  149</span> </div>
<div class="line"><span class="lineno">  150</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="ace981915c526d804a95d186a927e1a04" name="ace981915c526d804a95d186a927e1a04"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ace981915c526d804a95d186a927e1a04">&#9670;&#160;</a></span>f_oneway()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.feature_selection._univariate_selection.f_oneway </td>
          <td>(</td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>args</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Scoring functions. </p>
<pre class="fragment">Perform a 1-way ANOVA.

The one-way ANOVA tests the null hypothesis that 2 or more groups have
the same population mean. The test is applied to samples from two or
more groups, possibly with differing sizes.

Read more in the :ref:`User Guide &lt;univariate_feature_selection&gt;`.

Parameters
----------
*args : {array-like, sparse matrix}
    Sample1, sample2... The sample measurements should be given as
    arguments.

Returns
-------
f_statistic : float
    The computed F-value of the test.
p_value : float
    The associated p-value from the F-distribution.

Notes
-----
The ANOVA test has important assumptions that must be satisfied in order
for the associated p-value to be valid.

1. The samples are independent
2. Each sample is from a normally distributed population
3. The population standard deviations of the groups are all equal. This
   property is known as homoscedasticity.

If these assumptions are not true for a given set of data, it may still be
possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`_) although
with some loss of power.

The algorithm is from Heiman[2], pp.394-7.

See ``scipy.stats.f_oneway`` that should give the same results while
being less efficient.

References
----------
.. [1] Lowry, Richard.  "Concepts and Applications of Inferential
       Statistics". Chapter 14.
       http://faculty.vassar.edu/lowry/ch14pt1.html

.. [2] Heiman, G.W.  Research Methods in Statistics. 2002.
</pre> <div class="fragment"><div class="line"><span class="lineno">   43</span><span class="keyword">def </span>f_oneway(*args):</div>
<div class="line"><span class="lineno">   44</span>    <span class="stringliteral">&quot;&quot;&quot;Perform a 1-way ANOVA.</span></div>
<div class="line"><span class="lineno">   45</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   46</span><span class="stringliteral">    The one-way ANOVA tests the null hypothesis that 2 or more groups have</span></div>
<div class="line"><span class="lineno">   47</span><span class="stringliteral">    the same population mean. The test is applied to samples from two or</span></div>
<div class="line"><span class="lineno">   48</span><span class="stringliteral">    more groups, possibly with differing sizes.</span></div>
<div class="line"><span class="lineno">   49</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   50</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;univariate_feature_selection&gt;`.</span></div>
<div class="line"><span class="lineno">   51</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   52</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">   53</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   54</span><span class="stringliteral">    *args : {array-like, sparse matrix}</span></div>
<div class="line"><span class="lineno">   55</span><span class="stringliteral">        Sample1, sample2... The sample measurements should be given as</span></div>
<div class="line"><span class="lineno">   56</span><span class="stringliteral">        arguments.</span></div>
<div class="line"><span class="lineno">   57</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   58</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">   59</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">   60</span><span class="stringliteral">    f_statistic : float</span></div>
<div class="line"><span class="lineno">   61</span><span class="stringliteral">        The computed F-value of the test.</span></div>
<div class="line"><span class="lineno">   62</span><span class="stringliteral">    p_value : float</span></div>
<div class="line"><span class="lineno">   63</span><span class="stringliteral">        The associated p-value from the F-distribution.</span></div>
<div class="line"><span class="lineno">   64</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   65</span><span class="stringliteral">    Notes</span></div>
<div class="line"><span class="lineno">   66</span><span class="stringliteral">    -----</span></div>
<div class="line"><span class="lineno">   67</span><span class="stringliteral">    The ANOVA test has important assumptions that must be satisfied in order</span></div>
<div class="line"><span class="lineno">   68</span><span class="stringliteral">    for the associated p-value to be valid.</span></div>
<div class="line"><span class="lineno">   69</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   70</span><span class="stringliteral">    1. The samples are independent</span></div>
<div class="line"><span class="lineno">   71</span><span class="stringliteral">    2. Each sample is from a normally distributed population</span></div>
<div class="line"><span class="lineno">   72</span><span class="stringliteral">    3. The population standard deviations of the groups are all equal. This</span></div>
<div class="line"><span class="lineno">   73</span><span class="stringliteral">       property is known as homoscedasticity.</span></div>
<div class="line"><span class="lineno">   74</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   75</span><span class="stringliteral">    If these assumptions are not true for a given set of data, it may still be</span></div>
<div class="line"><span class="lineno">   76</span><span class="stringliteral">    possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`_) although</span></div>
<div class="line"><span class="lineno">   77</span><span class="stringliteral">    with some loss of power.</span></div>
<div class="line"><span class="lineno">   78</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   79</span><span class="stringliteral">    The algorithm is from Heiman[2], pp.394-7.</span></div>
<div class="line"><span class="lineno">   80</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   81</span><span class="stringliteral">    See ``scipy.stats.f_oneway`` that should give the same results while</span></div>
<div class="line"><span class="lineno">   82</span><span class="stringliteral">    being less efficient.</span></div>
<div class="line"><span class="lineno">   83</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   84</span><span class="stringliteral">    References</span></div>
<div class="line"><span class="lineno">   85</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">   86</span><span class="stringliteral">    .. [1] Lowry, Richard.  &quot;Concepts and Applications of Inferential</span></div>
<div class="line"><span class="lineno">   87</span><span class="stringliteral">           Statistics&quot;. Chapter 14.</span></div>
<div class="line"><span class="lineno">   88</span><span class="stringliteral">           http://faculty.vassar.edu/lowry/ch14pt1.html</span></div>
<div class="line"><span class="lineno">   89</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">   90</span><span class="stringliteral">    .. [2] Heiman, G.W.  Research Methods in Statistics. 2002.</span></div>
<div class="line"><span class="lineno">   91</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">   92</span>    n_classes = len(args)</div>
<div class="line"><span class="lineno">   93</span>    args = [as_float_array(a) <span class="keywordflow">for</span> a <span class="keywordflow">in</span> args]</div>
<div class="line"><span class="lineno">   94</span>    n_samples_per_class = np.array([a.shape[0] <span class="keywordflow">for</span> a <span class="keywordflow">in</span> args])</div>
<div class="line"><span class="lineno">   95</span>    n_samples = np.sum(n_samples_per_class)</div>
<div class="line"><span class="lineno">   96</span>    ss_alldata = sum(safe_sqr(a).sum(axis=0) <span class="keywordflow">for</span> a <span class="keywordflow">in</span> args)</div>
<div class="line"><span class="lineno">   97</span>    sums_args = [np.asarray(a.sum(axis=0)) <span class="keywordflow">for</span> a <span class="keywordflow">in</span> args]</div>
<div class="line"><span class="lineno">   98</span>    square_of_sums_alldata = sum(sums_args) ** 2</div>
<div class="line"><span class="lineno">   99</span>    square_of_sums_args = [s**2 <span class="keywordflow">for</span> s <span class="keywordflow">in</span> sums_args]</div>
<div class="line"><span class="lineno">  100</span>    sstot = ss_alldata - square_of_sums_alldata / float(n_samples)</div>
<div class="line"><span class="lineno">  101</span>    ssbn = 0.0</div>
<div class="line"><span class="lineno">  102</span>    <span class="keywordflow">for</span> k, _ <span class="keywordflow">in</span> enumerate(args):</div>
<div class="line"><span class="lineno">  103</span>        ssbn += square_of_sums_args[k] / n_samples_per_class[k]</div>
<div class="line"><span class="lineno">  104</span>    ssbn -= square_of_sums_alldata / float(n_samples)</div>
<div class="line"><span class="lineno">  105</span>    sswn = sstot - ssbn</div>
<div class="line"><span class="lineno">  106</span>    dfbn = n_classes - 1</div>
<div class="line"><span class="lineno">  107</span>    dfwn = n_samples - n_classes</div>
<div class="line"><span class="lineno">  108</span>    msb = ssbn / float(dfbn)</div>
<div class="line"><span class="lineno">  109</span>    msw = sswn / float(dfwn)</div>
<div class="line"><span class="lineno">  110</span>    constant_features_idx = np.where(msw == 0.0)[0]</div>
<div class="line"><span class="lineno">  111</span>    <span class="keywordflow">if</span> np.nonzero(msb)[0].size != msb.size <span class="keywordflow">and</span> constant_features_idx.size:</div>
<div class="line"><span class="lineno">  112</span>        warnings.warn(<span class="stringliteral">&quot;Features %s are constant.&quot;</span> % constant_features_idx, UserWarning)</div>
<div class="line"><span class="lineno">  113</span>    f = msb / msw</div>
<div class="line"><span class="lineno">  114</span>    <span class="comment"># flatten matrix to vector in sparse case</span></div>
<div class="line"><span class="lineno">  115</span>    f = np.asarray(f).ravel()</div>
<div class="line"><span class="lineno">  116</span>    prob = special.fdtrc(dfbn, dfwn, f)</div>
<div class="line"><span class="lineno">  117</span>    <span class="keywordflow">return</span> f, prob</div>
<div class="line"><span class="lineno">  118</span> </div>
<div class="line"><span class="lineno">  119</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="adc55e84bd49169f1b07bf7ef6881c466" name="adc55e84bd49169f1b07bf7ef6881c466"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adc55e84bd49169f1b07bf7ef6881c466">&#9670;&#160;</a></span>f_regression()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.feature_selection._univariate_selection.f_regression </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>center</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>force_finite</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Univariate linear regression tests returning F-statistic and p-values.

Quick linear model for testing the effect of a single regressor,
sequentially for many regressors.

This is done in 2 steps:

1. The cross correlation between each regressor and the target is computed
   using :func:`r_regression` as::

       E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))

2. It is converted to an F score and then to a p-value.

:func:`f_regression` is derived from :func:`r_regression` and will rank
features in the same order if all the features are positively correlated
with the target.

Note however that contrary to :func:`f_regression`, :func:`r_regression`
values lie in [-1, 1] and can thus be negative. :func:`f_regression` is
therefore recommended as a feature selection criterion to identify
potentially predictive feature for a downstream classifier, irrespective of
the sign of the association with the target variable.

Furthermore :func:`f_regression` returns p-values while
:func:`r_regression` does not.

Read more in the :ref:`User Guide &lt;univariate_feature_selection&gt;`.

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data matrix.

y : array-like of shape (n_samples,)
    The target vector.

center : bool, default=True
    Whether or not to center the data matrix `X` and the target vector `y`.
    By default, `X` and `y` will be centered.

force_finite : bool, default=True
    Whether or not to force the F-statistics and associated p-values to
    be finite. There are two cases where the F-statistic is expected to not
    be finite:

    - when the target `y` or some features in `X` are constant. In this
      case, the Pearson's R correlation is not defined leading to obtain
      `np.nan` values in the F-statistic and p-value. When
      `force_finite=True`, the F-statistic is set to `0.0` and the
      associated p-value is set to `1.0`.
    - when a feature in `X` is perfectly correlated (or
      anti-correlated) with the target `y`. In this case, the F-statistic
      is expected to be `np.inf`. When `force_finite=True`, the F-statistic
      is set to `np.finfo(dtype).max` and the associated p-value is set to
      `0.0`.

    .. versionadded:: 1.1

Returns
-------
f_statistic : ndarray of shape (n_features,)
    F-statistic for each feature.

p_values : ndarray of shape (n_features,)
    P-values associated with the F-statistic.

See Also
--------
r_regression: Pearson's R between label/feature for regression tasks.
f_classif: ANOVA F-value between label/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
SelectKBest: Select features based on the k highest scores.
SelectFpr: Select features based on a false positive rate test.
SelectFdr: Select features based on an estimated false discovery rate.
SelectFwe: Select features based on family-wise error rate.
SelectPercentile: Select features based on percentile of the highest
    scores.
</pre> <div class="fragment"><div class="line"><span class="lineno">  325</span><span class="keyword">def </span>f_regression(X, y, *, center=True, force_finite=True):</div>
<div class="line"><span class="lineno">  326</span>    <span class="stringliteral">&quot;&quot;&quot;Univariate linear regression tests returning F-statistic and p-values.</span></div>
<div class="line"><span class="lineno">  327</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  328</span><span class="stringliteral">    Quick linear model for testing the effect of a single regressor,</span></div>
<div class="line"><span class="lineno">  329</span><span class="stringliteral">    sequentially for many regressors.</span></div>
<div class="line"><span class="lineno">  330</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  331</span><span class="stringliteral">    This is done in 2 steps:</span></div>
<div class="line"><span class="lineno">  332</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  333</span><span class="stringliteral">    1. The cross correlation between each regressor and the target is computed</span></div>
<div class="line"><span class="lineno">  334</span><span class="stringliteral">       using :func:`r_regression` as::</span></div>
<div class="line"><span class="lineno">  335</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  336</span><span class="stringliteral">           E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))</span></div>
<div class="line"><span class="lineno">  337</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  338</span><span class="stringliteral">    2. It is converted to an F score and then to a p-value.</span></div>
<div class="line"><span class="lineno">  339</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  340</span><span class="stringliteral">    :func:`f_regression` is derived from :func:`r_regression` and will rank</span></div>
<div class="line"><span class="lineno">  341</span><span class="stringliteral">    features in the same order if all the features are positively correlated</span></div>
<div class="line"><span class="lineno">  342</span><span class="stringliteral">    with the target.</span></div>
<div class="line"><span class="lineno">  343</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  344</span><span class="stringliteral">    Note however that contrary to :func:`f_regression`, :func:`r_regression`</span></div>
<div class="line"><span class="lineno">  345</span><span class="stringliteral">    values lie in [-1, 1] and can thus be negative. :func:`f_regression` is</span></div>
<div class="line"><span class="lineno">  346</span><span class="stringliteral">    therefore recommended as a feature selection criterion to identify</span></div>
<div class="line"><span class="lineno">  347</span><span class="stringliteral">    potentially predictive feature for a downstream classifier, irrespective of</span></div>
<div class="line"><span class="lineno">  348</span><span class="stringliteral">    the sign of the association with the target variable.</span></div>
<div class="line"><span class="lineno">  349</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  350</span><span class="stringliteral">    Furthermore :func:`f_regression` returns p-values while</span></div>
<div class="line"><span class="lineno">  351</span><span class="stringliteral">    :func:`r_regression` does not.</span></div>
<div class="line"><span class="lineno">  352</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  353</span><span class="stringliteral">    Read more in the :ref:`User Guide &lt;univariate_feature_selection&gt;`.</span></div>
<div class="line"><span class="lineno">  354</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  355</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  356</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  357</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  358</span><span class="stringliteral">        The data matrix.</span></div>
<div class="line"><span class="lineno">  359</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  360</span><span class="stringliteral">    y : array-like of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  361</span><span class="stringliteral">        The target vector.</span></div>
<div class="line"><span class="lineno">  362</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  363</span><span class="stringliteral">    center : bool, default=True</span></div>
<div class="line"><span class="lineno">  364</span><span class="stringliteral">        Whether or not to center the data matrix `X` and the target vector `y`.</span></div>
<div class="line"><span class="lineno">  365</span><span class="stringliteral">        By default, `X` and `y` will be centered.</span></div>
<div class="line"><span class="lineno">  366</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  367</span><span class="stringliteral">    force_finite : bool, default=True</span></div>
<div class="line"><span class="lineno">  368</span><span class="stringliteral">        Whether or not to force the F-statistics and associated p-values to</span></div>
<div class="line"><span class="lineno">  369</span><span class="stringliteral">        be finite. There are two cases where the F-statistic is expected to not</span></div>
<div class="line"><span class="lineno">  370</span><span class="stringliteral">        be finite:</span></div>
<div class="line"><span class="lineno">  371</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  372</span><span class="stringliteral">        - when the target `y` or some features in `X` are constant. In this</span></div>
<div class="line"><span class="lineno">  373</span><span class="stringliteral">          case, the Pearson&#39;s R correlation is not defined leading to obtain</span></div>
<div class="line"><span class="lineno">  374</span><span class="stringliteral">          `np.nan` values in the F-statistic and p-value. When</span></div>
<div class="line"><span class="lineno">  375</span><span class="stringliteral">          `force_finite=True`, the F-statistic is set to `0.0` and the</span></div>
<div class="line"><span class="lineno">  376</span><span class="stringliteral">          associated p-value is set to `1.0`.</span></div>
<div class="line"><span class="lineno">  377</span><span class="stringliteral">        - when a feature in `X` is perfectly correlated (or</span></div>
<div class="line"><span class="lineno">  378</span><span class="stringliteral">          anti-correlated) with the target `y`. In this case, the F-statistic</span></div>
<div class="line"><span class="lineno">  379</span><span class="stringliteral">          is expected to be `np.inf`. When `force_finite=True`, the F-statistic</span></div>
<div class="line"><span class="lineno">  380</span><span class="stringliteral">          is set to `np.finfo(dtype).max` and the associated p-value is set to</span></div>
<div class="line"><span class="lineno">  381</span><span class="stringliteral">          `0.0`.</span></div>
<div class="line"><span class="lineno">  382</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  383</span><span class="stringliteral">        .. versionadded:: 1.1</span></div>
<div class="line"><span class="lineno">  384</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  385</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  386</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  387</span><span class="stringliteral">    f_statistic : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  388</span><span class="stringliteral">        F-statistic for each feature.</span></div>
<div class="line"><span class="lineno">  389</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  390</span><span class="stringliteral">    p_values : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  391</span><span class="stringliteral">        P-values associated with the F-statistic.</span></div>
<div class="line"><span class="lineno">  392</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  393</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  394</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  395</span><span class="stringliteral">    r_regression: Pearson&#39;s R between label/feature for regression tasks.</span></div>
<div class="line"><span class="lineno">  396</span><span class="stringliteral">    f_classif: ANOVA F-value between label/feature for classification tasks.</span></div>
<div class="line"><span class="lineno">  397</span><span class="stringliteral">    chi2: Chi-squared stats of non-negative features for classification tasks.</span></div>
<div class="line"><span class="lineno">  398</span><span class="stringliteral">    SelectKBest: Select features based on the k highest scores.</span></div>
<div class="line"><span class="lineno">  399</span><span class="stringliteral">    SelectFpr: Select features based on a false positive rate test.</span></div>
<div class="line"><span class="lineno">  400</span><span class="stringliteral">    SelectFdr: Select features based on an estimated false discovery rate.</span></div>
<div class="line"><span class="lineno">  401</span><span class="stringliteral">    SelectFwe: Select features based on family-wise error rate.</span></div>
<div class="line"><span class="lineno">  402</span><span class="stringliteral">    SelectPercentile: Select features based on percentile of the highest</span></div>
<div class="line"><span class="lineno">  403</span><span class="stringliteral">        scores.</span></div>
<div class="line"><span class="lineno">  404</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  405</span>    correlation_coefficient = r_regression(</div>
<div class="line"><span class="lineno">  406</span>        X, y, center=center, force_finite=force_finite</div>
<div class="line"><span class="lineno">  407</span>    )</div>
<div class="line"><span class="lineno">  408</span>    deg_of_freedom = y.size - (2 <span class="keywordflow">if</span> center <span class="keywordflow">else</span> 1)</div>
<div class="line"><span class="lineno">  409</span> </div>
<div class="line"><span class="lineno">  410</span>    corr_coef_squared = correlation_coefficient**2</div>
<div class="line"><span class="lineno">  411</span> </div>
<div class="line"><span class="lineno">  412</span>    <span class="keyword">with</span> np.errstate(divide=<span class="stringliteral">&quot;ignore&quot;</span>, invalid=<span class="stringliteral">&quot;ignore&quot;</span>):</div>
<div class="line"><span class="lineno">  413</span>        f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom</div>
<div class="line"><span class="lineno">  414</span>        p_values = stats.f.sf(f_statistic, 1, deg_of_freedom)</div>
<div class="line"><span class="lineno">  415</span> </div>
<div class="line"><span class="lineno">  416</span>    <span class="keywordflow">if</span> force_finite <span class="keywordflow">and</span> <span class="keywordflow">not</span> np.isfinite(f_statistic).all():</div>
<div class="line"><span class="lineno">  417</span>        <span class="comment"># case where there is a perfect (anti-)correlation</span></div>
<div class="line"><span class="lineno">  418</span>        <span class="comment"># f-statistics can be set to the maximum and p-values to zero</span></div>
<div class="line"><span class="lineno">  419</span>        mask_inf = np.isinf(f_statistic)</div>
<div class="line"><span class="lineno">  420</span>        f_statistic[mask_inf] = np.finfo(f_statistic.dtype).max</div>
<div class="line"><span class="lineno">  421</span>        <span class="comment"># case where the target or some features are constant</span></div>
<div class="line"><span class="lineno">  422</span>        <span class="comment"># f-statistics would be minimum and thus p-values large</span></div>
<div class="line"><span class="lineno">  423</span>        mask_nan = np.isnan(f_statistic)</div>
<div class="line"><span class="lineno">  424</span>        f_statistic[mask_nan] = 0.0</div>
<div class="line"><span class="lineno">  425</span>        p_values[mask_nan] = 1.0</div>
<div class="line"><span class="lineno">  426</span>    <span class="keywordflow">return</span> f_statistic, p_values</div>
<div class="line"><span class="lineno">  427</span> </div>
<div class="line"><span class="lineno">  428</span> </div>
</div><!-- fragment -->
</div>
</div>
<a id="afac3471e30dd6cfd854eb74be746b53d" name="afac3471e30dd6cfd854eb74be746b53d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afac3471e30dd6cfd854eb74be746b53d">&#9670;&#160;</a></span>r_regression()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">sklearn.feature_selection._univariate_selection.r_regression </td>
          <td>(</td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">*&#160;</td>
          <td class="paramname"><em>center</em> = <code>True</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">&#160;</td>
          <td class="paramname"><em>force_finite</em> = <code>True</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">
<pre class="fragment">Compute Pearson's r for each features and the target.

Pearson's r is also known as the Pearson correlation coefficient.

Linear model for testing the individual effect of each of many regressors.
This is a scoring function to be used in a feature selection procedure, not
a free standing feature selection procedure.

The cross correlation between each regressor and the target is computed
as::

    E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))

For more on usage see the :ref:`User Guide &lt;univariate_feature_selection&gt;`.

.. versionadded:: 1.0

Parameters
----------
X : {array-like, sparse matrix} of shape (n_samples, n_features)
    The data matrix.

y : array-like of shape (n_samples,)
    The target vector.

center : bool, default=True
    Whether or not to center the data matrix `X` and the target vector `y`.
    By default, `X` and `y` will be centered.

force_finite : bool, default=True
    Whether or not to force the Pearson's R correlation to be finite.
    In the particular case where some features in `X` or the target `y`
    are constant, the Pearson's R correlation is not defined. When
    `force_finite=False`, a correlation of `np.nan` is returned to
    acknowledge this case. When `force_finite=True`, this value will be
    forced to a minimal correlation of `0.0`.

    .. versionadded:: 1.1

Returns
-------
correlation_coefficient : ndarray of shape (n_features,)
    Pearson's R correlation coefficients of features.

See Also
--------
f_regression: Univariate linear regression tests returning f-statistic
    and p-values.
mutual_info_regression: Mutual information for a continuous target.
f_classif: ANOVA F-value between label/feature for classification tasks.
chi2: Chi-squared stats of non-negative features for classification tasks.
</pre> <div class="fragment"><div class="line"><span class="lineno">  242</span><span class="keyword">def </span>r_regression(X, y, *, center=True, force_finite=True):</div>
<div class="line"><span class="lineno">  243</span>    <span class="stringliteral">&quot;&quot;&quot;Compute Pearson&#39;s r for each features and the target.</span></div>
<div class="line"><span class="lineno">  244</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  245</span><span class="stringliteral">    Pearson&#39;s r is also known as the Pearson correlation coefficient.</span></div>
<div class="line"><span class="lineno">  246</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  247</span><span class="stringliteral">    Linear model for testing the individual effect of each of many regressors.</span></div>
<div class="line"><span class="lineno">  248</span><span class="stringliteral">    This is a scoring function to be used in a feature selection procedure, not</span></div>
<div class="line"><span class="lineno">  249</span><span class="stringliteral">    a free standing feature selection procedure.</span></div>
<div class="line"><span class="lineno">  250</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  251</span><span class="stringliteral">    The cross correlation between each regressor and the target is computed</span></div>
<div class="line"><span class="lineno">  252</span><span class="stringliteral">    as::</span></div>
<div class="line"><span class="lineno">  253</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  254</span><span class="stringliteral">        E[(X[:, i] - mean(X[:, i])) * (y - mean(y))] / (std(X[:, i]) * std(y))</span></div>
<div class="line"><span class="lineno">  255</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  256</span><span class="stringliteral">    For more on usage see the :ref:`User Guide &lt;univariate_feature_selection&gt;`.</span></div>
<div class="line"><span class="lineno">  257</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  258</span><span class="stringliteral">    .. versionadded:: 1.0</span></div>
<div class="line"><span class="lineno">  259</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  260</span><span class="stringliteral">    Parameters</span></div>
<div class="line"><span class="lineno">  261</span><span class="stringliteral">    ----------</span></div>
<div class="line"><span class="lineno">  262</span><span class="stringliteral">    X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></div>
<div class="line"><span class="lineno">  263</span><span class="stringliteral">        The data matrix.</span></div>
<div class="line"><span class="lineno">  264</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  265</span><span class="stringliteral">    y : array-like of shape (n_samples,)</span></div>
<div class="line"><span class="lineno">  266</span><span class="stringliteral">        The target vector.</span></div>
<div class="line"><span class="lineno">  267</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  268</span><span class="stringliteral">    center : bool, default=True</span></div>
<div class="line"><span class="lineno">  269</span><span class="stringliteral">        Whether or not to center the data matrix `X` and the target vector `y`.</span></div>
<div class="line"><span class="lineno">  270</span><span class="stringliteral">        By default, `X` and `y` will be centered.</span></div>
<div class="line"><span class="lineno">  271</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  272</span><span class="stringliteral">    force_finite : bool, default=True</span></div>
<div class="line"><span class="lineno">  273</span><span class="stringliteral">        Whether or not to force the Pearson&#39;s R correlation to be finite.</span></div>
<div class="line"><span class="lineno">  274</span><span class="stringliteral">        In the particular case where some features in `X` or the target `y`</span></div>
<div class="line"><span class="lineno">  275</span><span class="stringliteral">        are constant, the Pearson&#39;s R correlation is not defined. When</span></div>
<div class="line"><span class="lineno">  276</span><span class="stringliteral">        `force_finite=False`, a correlation of `np.nan` is returned to</span></div>
<div class="line"><span class="lineno">  277</span><span class="stringliteral">        acknowledge this case. When `force_finite=True`, this value will be</span></div>
<div class="line"><span class="lineno">  278</span><span class="stringliteral">        forced to a minimal correlation of `0.0`.</span></div>
<div class="line"><span class="lineno">  279</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  280</span><span class="stringliteral">        .. versionadded:: 1.1</span></div>
<div class="line"><span class="lineno">  281</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  282</span><span class="stringliteral">    Returns</span></div>
<div class="line"><span class="lineno">  283</span><span class="stringliteral">    -------</span></div>
<div class="line"><span class="lineno">  284</span><span class="stringliteral">    correlation_coefficient : ndarray of shape (n_features,)</span></div>
<div class="line"><span class="lineno">  285</span><span class="stringliteral">        Pearson&#39;s R correlation coefficients of features.</span></div>
<div class="line"><span class="lineno">  286</span><span class="stringliteral"></span> </div>
<div class="line"><span class="lineno">  287</span><span class="stringliteral">    See Also</span></div>
<div class="line"><span class="lineno">  288</span><span class="stringliteral">    --------</span></div>
<div class="line"><span class="lineno">  289</span><span class="stringliteral">    f_regression: Univariate linear regression tests returning f-statistic</span></div>
<div class="line"><span class="lineno">  290</span><span class="stringliteral">        and p-values.</span></div>
<div class="line"><span class="lineno">  291</span><span class="stringliteral">    mutual_info_regression: Mutual information for a continuous target.</span></div>
<div class="line"><span class="lineno">  292</span><span class="stringliteral">    f_classif: ANOVA F-value between label/feature for classification tasks.</span></div>
<div class="line"><span class="lineno">  293</span><span class="stringliteral">    chi2: Chi-squared stats of non-negative features for classification tasks.</span></div>
<div class="line"><span class="lineno">  294</span><span class="stringliteral">    &quot;&quot;&quot;</span></div>
<div class="line"><span class="lineno">  295</span>    X, y = check_X_y(X, y, accept_sparse=[<span class="stringliteral">&quot;csr&quot;</span>, <span class="stringliteral">&quot;csc&quot;</span>, <span class="stringliteral">&quot;coo&quot;</span>], dtype=np.float64)</div>
<div class="line"><span class="lineno">  296</span>    n_samples = X.shape[0]</div>
<div class="line"><span class="lineno">  297</span> </div>
<div class="line"><span class="lineno">  298</span>    <span class="comment"># Compute centered values</span></div>
<div class="line"><span class="lineno">  299</span>    <span class="comment"># Note that E[(x - mean(x))*(y - mean(y))] = E[x*(y - mean(y))], so we</span></div>
<div class="line"><span class="lineno">  300</span>    <span class="comment"># need not center X</span></div>
<div class="line"><span class="lineno">  301</span>    <span class="keywordflow">if</span> center:</div>
<div class="line"><span class="lineno">  302</span>        y = y - np.mean(y)</div>
<div class="line"><span class="lineno">  303</span>        <span class="keywordflow">if</span> issparse(X):</div>
<div class="line"><span class="lineno">  304</span>            X_means = X.mean(axis=0).getA1()</div>
<div class="line"><span class="lineno">  305</span>        <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  306</span>            X_means = X.mean(axis=0)</div>
<div class="line"><span class="lineno">  307</span>        <span class="comment"># Compute the scaled standard deviations via moments</span></div>
<div class="line"><span class="lineno">  308</span>        X_norms = np.sqrt(row_norms(X.T, squared=<span class="keyword">True</span>) - n_samples * X_means**2)</div>
<div class="line"><span class="lineno">  309</span>    <span class="keywordflow">else</span>:</div>
<div class="line"><span class="lineno">  310</span>        X_norms = row_norms(X.T)</div>
<div class="line"><span class="lineno">  311</span> </div>
<div class="line"><span class="lineno">  312</span>    correlation_coefficient = safe_sparse_dot(y, X)</div>
<div class="line"><span class="lineno">  313</span>    <span class="keyword">with</span> np.errstate(divide=<span class="stringliteral">&quot;ignore&quot;</span>, invalid=<span class="stringliteral">&quot;ignore&quot;</span>):</div>
<div class="line"><span class="lineno">  314</span>        correlation_coefficient /= X_norms</div>
<div class="line"><span class="lineno">  315</span>        correlation_coefficient /= np.linalg.norm(y)</div>
<div class="line"><span class="lineno">  316</span> </div>
<div class="line"><span class="lineno">  317</span>    <span class="keywordflow">if</span> force_finite <span class="keywordflow">and</span> <span class="keywordflow">not</span> np.isfinite(correlation_coefficient).all():</div>
<div class="line"><span class="lineno">  318</span>        <span class="comment"># case where the target or some features are constant</span></div>
<div class="line"><span class="lineno">  319</span>        <span class="comment"># the correlation coefficient(s) is/are set to the minimum (i.e. 0.0)</span></div>
<div class="line"><span class="lineno">  320</span>        nan_mask = np.isnan(correlation_coefficient)</div>
<div class="line"><span class="lineno">  321</span>        correlation_coefficient[nan_mask] = 0.0</div>
<div class="line"><span class="lineno">  322</span>    <span class="keywordflow">return</span> correlation_coefficient</div>
<div class="line"><span class="lineno">  323</span> </div>
<div class="line"><span class="lineno">  324</span> </div>
</div><!-- fragment -->
</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
